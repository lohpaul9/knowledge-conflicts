{
  "metadata": {
    "generated_at": "2025-08-29T20:25:08.352909",
    "total_examples": 2955,
    "categories": [
      "factual_contradictions",
      "role_attribute_contradictions",
      "negation_contradictions",
      "temporal_contradictions"
    ],
    "description": "Intra-context knowledge conflict dataset for mechanistic interpretability research"
  },
  "examples": [
    {
      "clean_prompt": "The Eiffel Tower is a wrought-iron lattice tower located in Paris, France.",
      "conflict_prompt": "The Eiffel Tower is a wrought-iron lattice tower located in Berlin, Germany.",
      "question": "In which city is the Eiffel Tower located?",
      "options": [
        "A. Paris",
        "B. Berlin",
        "C. London",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Everest's elevation is 8,848 meters above sea level.",
      "conflict_prompt": "Mount Everest's elevation is 9,000 meters above sea level.",
      "question": "What is the commonly cited elevation of Mount Everest?",
      "options": [
        "A. 9,000 meters",
        "B. 8,000 meters",
        "C. 8,848 meters",
        "D. 10,000 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon River is the largest river by discharge volume in the world and flows primarily through Brazil.",
      "conflict_prompt": "The Amazon River is the largest river by discharge volume in the world and flows primarily through Egypt.",
      "question": "Through which country does the Amazon River primarily flow?",
      "options": [
        "A. Egypt",
        "B. Brazil",
        "C. India",
        "D. China"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Wall of China is a series of fortifications built mainly across northern China to protect against invasions.",
      "conflict_prompt": "The Great Wall of China is a series of fortifications built mainly across southern China to protect against invasions.",
      "question": "Across which part of China was the Great Wall mainly built?",
      "options": [
        "A. Southern China",
        "B. Northern China",
        "C. Eastern China",
        "D. Western China"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Neil Armstrong was the first person to walk on the Moon on July 20, 1969 during the Apollo 11 mission.",
      "conflict_prompt": "Neil Armstrong was the first person to walk on the Moon on July 20, 1979 during the Apollo 11 mission.",
      "question": "In what year did Neil Armstrong first walk on the Moon?",
      "options": [
        "A. 1979",
        "B. 1969",
        "C. 1959",
        "D. 1989"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human heart has four chambers: two atria and two ventricles.",
      "conflict_prompt": "The human heart has three chambers: two atria and one ventricle.",
      "question": "How many chambers does a typical human heart have?",
      "options": [
        "A. Three",
        "B. Five",
        "C. Four",
        "D. Two"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Water's chemical formula is H2O, consisting of two hydrogen atoms bonded to one oxygen atom.",
      "conflict_prompt": "Water's chemical formula is H3O, consisting of three hydrogen atoms bonded to one oxygen atom.",
      "question": "What is the chemical formula for water?",
      "options": [
        "A. H3O",
        "B. H2O",
        "C. HO2",
        "D. O2H2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Ocean is the largest ocean on Earth by surface area.",
      "conflict_prompt": "The Atlantic Ocean is the largest ocean on Earth by surface area.",
      "question": "Which ocean is the largest on Earth by surface area?",
      "options": [
        "A. Atlantic Ocean",
        "B. Indian Ocean",
        "C. Arctic Ocean",
        "D. Pacific Ocean"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light from the Sun takes about 8 minutes and 20 seconds to reach Earth.",
      "conflict_prompt": "Light from the Sun takes about 82 minutes to reach Earth.",
      "question": "Approximately how long does sunlight take to reach Earth?",
      "options": [
        "A. 82 minutes",
        "B. 8 minutes and 20 seconds",
        "C. 1 minute",
        "D. 24 hours"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Shakespeare wrote the play Romeo and Juliet in the late 16th century.",
      "conflict_prompt": "Shakespeare wrote the play Romeo and Juliet in the late 18th century.",
      "question": "In which century was Romeo and Juliet written by Shakespeare?",
      "options": [
        "A. 18th century",
        "B. 17th century",
        "C. 16th century",
        "D. 19th century"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Albert Einstein developed the theory of general relativity, published in 1915.",
      "conflict_prompt": "Albert Einstein developed the theory of general relativity, published in 1815.",
      "question": "In what year was Einstein's theory of general relativity published?",
      "options": [
        "A. 1915",
        "B. 1815",
        "C. 1955",
        "D. 1895"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Venus is the second planet from the Sun in our solar system.",
      "conflict_prompt": "Venus is the fourth planet from the Sun in our solar system.",
      "question": "What position from the Sun is Venus in our solar system?",
      "options": [
        "A. First",
        "B. Second",
        "C. Fourth",
        "D. Fifth"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty was a gift from France to the United States and is located in New York Harbor.",
      "conflict_prompt": "The Statue of Liberty was a gift from Germany to the United States and is located in New York Harbor.",
      "question": "Which country gifted the Statue of Liberty to the United States?",
      "options": [
        "A. Germany",
        "B. France",
        "C. England",
        "D. Spain"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 6 is carbon.",
      "conflict_prompt": "The chemical element with atomic number 6 is nitrogen.",
      "question": "Which element has atomic number 6?",
      "options": [
        "A. Nitrogen",
        "B. Oxygen",
        "C. Carbon",
        "D. Hydrogen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of Japan is Tokyo.",
      "conflict_prompt": "The capital city of Japan is Kyoto.",
      "question": "What is the capital city of Japan?",
      "options": [
        "A. Kyoto",
        "B. Osaka",
        "C. Tokyo",
        "D. Hiroshima"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language spoken in Brazil is Portuguese.",
      "conflict_prompt": "The primary language spoken in Brazil is Spanish.",
      "question": "What is the primary language spoken in Brazil?",
      "options": [
        "A. Spanish",
        "B. Portuguese",
        "C. French",
        "D. English"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A solar eclipse occurs when the Moon passes between the Sun and Earth, blocking the Sun's light.",
      "conflict_prompt": "A lunar eclipse occurs when the Moon passes between the Sun and Earth, blocking the Sun's light.",
      "question": "What type of eclipse occurs when the Moon passes between the Sun and Earth, blocking the Sun's light?",
      "options": [
        "A. Lunar eclipse",
        "B. Solar eclipse",
        "C. Annular eclipse",
        "D. Transient eclipse"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The process by which green plants use sunlight to synthesize nutrients from carbon dioxide and water is called photosynthesis.",
      "conflict_prompt": "The process by which green plants use sunlight to synthesize nutrients from oxygen and water is called photosynthesis.",
      "question": "Which gas do plants primarily use during photosynthesis to synthesize nutrients?",
      "options": [
        "A. Oxygen",
        "B. Carbon dioxide",
        "C. Nitrogen",
        "D. Hydrogen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Canada is Ottawa.",
      "conflict_prompt": "The capital of Canada is Toronto.",
      "question": "What is the capital city of Canada?",
      "options": [
        "A. Toronto",
        "B. Vancouver",
        "C. Montreal",
        "D. Ottawa"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet in our solar system is Jupiter.",
      "conflict_prompt": "The largest planet in our solar system is Saturn.",
      "question": "Which planet is the largest in our solar system?",
      "options": [
        "A. Saturn",
        "B. Neptune",
        "C. Jupiter",
        "D. Earth"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest river in Africa is the Nile River.",
      "conflict_prompt": "The longest river in Africa is the Congo River.",
      "question": "Which river is traditionally considered the longest in Africa?",
      "options": [
        "A. Congo River",
        "B. Niger River",
        "C. Zambezi River",
        "D. Nile River"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The theory of evolution by natural selection was co-developed by Charles Darwin and Alfred Russel Wallace.",
      "conflict_prompt": "The theory of evolution by natural selection was developed solely by Alfred Russel Wallace.",
      "question": "Who is commonly credited alongside Charles Darwin for co-developing the theory of natural selection?",
      "options": [
        "A. Gregor Mendel",
        "B. Alfred Russel Wallace",
        "C. Louis Pasteur",
        "D. Thomas Huxley"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Australia is Canberra.",
      "conflict_prompt": "The capital of Australia is Sydney.",
      "question": "What is the capital of Australia?",
      "options": [
        "A. Melbourne",
        "B. Sydney",
        "C. Canberra",
        "D. Brisbane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A triangle's interior angles sum to 180 degrees in Euclidean geometry.",
      "conflict_prompt": "A triangle's interior angles sum to 360 degrees in Euclidean geometry.",
      "question": "In Euclidean geometry, what is the sum of a triangle's interior angles?",
      "options": [
        "A. 360 degrees",
        "B. 90 degrees",
        "C. 180 degrees",
        "D. 270 degrees"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mercury is the closest planet to the Sun.",
      "conflict_prompt": "Venus is the closest planet to the Sun.",
      "question": "Which planet is closest to the Sun?",
      "options": [
        "A. Venus",
        "B. Earth",
        "C. Mercury",
        "D. Mars"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary author of the United States Declaration of Independence was Thomas Jefferson.",
      "conflict_prompt": "The primary author of the United States Declaration of Independence was Benjamin Franklin.",
      "question": "Who was the primary author of the United States Declaration of Independence?",
      "options": [
        "A. Benjamin Franklin",
        "B. Thomas Jefferson",
        "C. John Adams",
        "D. George Washington"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Honey bees communicate the location of food sources to hive mates using the waggle dance.",
      "conflict_prompt": "Honey bees communicate the location of food sources to hive mates using ultrasonic calls.",
      "question": "What method do honey bees use to communicate the location of food sources to other bees?",
      "options": [
        "A. Ultrasonic calls",
        "B. Waggle dance",
        "C. Visual billboards",
        "D. Chemical graffiti"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of the United Kingdom is the pound sterling.",
      "conflict_prompt": "The currency of the United Kingdom is the euro.",
      "question": "What is the official currency of the United Kingdom?",
      "options": [
        "A. Euro",
        "B. Pound sterling",
        "C. US Dollar",
        "D. Swiss Franc"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest mammal on Earth is the blue whale.",
      "conflict_prompt": "The largest mammal on Earth is the African elephant.",
      "question": "Which species is the largest mammal on Earth?",
      "options": [
        "A. African elephant",
        "B. Blue whale",
        "C. Giraffe",
        "D. Sperm whale"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The process of cellular respiration in eukaryotic cells primarily occurs in the mitochondria.",
      "conflict_prompt": "The process of cellular respiration in eukaryotic cells primarily occurs in the chloroplasts.",
      "question": "In which organelle does cellular respiration primarily occur in eukaryotic cells?",
      "options": [
        "A. Chloroplasts",
        "B. Endoplasmic reticulum",
        "C. Mitochondria",
        "D. Golgi apparatus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest building in the world is the Burj Khalifa in Dubai.",
      "conflict_prompt": "The tallest building in the world is the Shanghai Tower in Shanghai.",
      "question": "Which building is currently the tallest in the world?",
      "options": [
        "A. Shanghai Tower",
        "B. One World Trade Center",
        "C. Burj Khalifa",
        "D. Taipei 101"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for sodium is Na.",
      "conflict_prompt": "The chemical symbol for sodium is S.",
      "question": "What is the chemical symbol for sodium?",
      "options": [
        "A. S",
        "B. Na",
        "C. So",
        "D. Sd"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Battle of Waterloo occurred in 1815 and marked Napoleon Bonaparte's final defeat.",
      "conflict_prompt": "The Battle of Waterloo occurred in 1915 and marked Napoleon Bonaparte's final defeat.",
      "question": "In what year did the Battle of Waterloo occur, marking Napoleon's final defeat?",
      "options": [
        "A. 1915",
        "B. 1815",
        "C. 1799",
        "D. 1850"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main gas making up Earth's atmosphere is nitrogen, which constitutes about 78% by volume.",
      "conflict_prompt": "The main gas making up Earth's atmosphere is oxygen, which constitutes about 78% by volume.",
      "question": "Which gas makes up about 78% of Earth's atmosphere by volume?",
      "options": [
        "A. Oxygen",
        "B. Nitrogen",
        "C. Carbon dioxide",
        "D. Argon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "William Shakespeare is credited with writing Hamlet, a tragedy in five acts.",
      "conflict_prompt": "William Shakespeare is credited with writing Hamlet, a comedy in five acts.",
      "question": "How is Hamlet typically classified?",
      "options": [
        "A. Comedy",
        "B. Tragedy",
        "C. Satire",
        "D. Farce"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human skeleton is composed of 206 bones in a typical adult.",
      "conflict_prompt": "The human skeleton is composed of 306 bones in a typical adult.",
      "question": "How many bones does a typical adult human skeleton have?",
      "options": [
        "A. 306",
        "B. 206",
        "C. 156",
        "D. 256"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize in Physics in 1921 was awarded to Albert Einstein for his discovery of the photoelectric effect and contributions to theoretical physics.",
      "conflict_prompt": "The Nobel Prize in Physics in 1921 was awarded to Albert Einstein for his discovery of superconductivity and contributions to theoretical physics.",
      "question": "For which discovery was Einstein awarded the 1921 Nobel Prize in Physics?",
      "options": [
        "A. Superconductivity",
        "B. Photoelectric effect",
        "C. General relativity",
        "D. Quantum entanglement"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest desert in the world by area is the Sahara Desert in North Africa (excluding polar deserts).",
      "conflict_prompt": "The largest desert in the world by area is the Gobi Desert in East Asia.",
      "question": "Which desert is the largest by area (excluding polar deserts)?",
      "options": [
        "A. Gobi Desert",
        "B. Mojave Desert",
        "C. Sahara Desert",
        "D. Arabian Desert"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common cold is most often caused by rhinoviruses.",
      "conflict_prompt": "The common cold is most often caused by streptococcus bacteria.",
      "question": "Which pathogen is most commonly responsible for the common cold?",
      "options": [
        "A. Streptococcus bacteria",
        "B. Rhinoviruses",
        "C. Influenza viruses",
        "D. Candida fungi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Pablo Picasso was a Spanish painter and sculptor who co-founded the Cubist movement.",
      "conflict_prompt": "Pablo Picasso was a French painter and sculptor who co-founded the Cubist movement.",
      "question": "What was Pablo Picasso's nationality by birth?",
      "options": [
        "A. French",
        "B. Spanish",
        "C. Italian",
        "D. Portuguese"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The mitochondrion is often referred to as the powerhouse of the cell because it generates most of the cell's supply of ATP.",
      "conflict_prompt": "The chloroplast is often referred to as the powerhouse of the cell because it generates most of the cell's supply of ATP.",
      "question": "Which organelle is commonly called the powerhouse of the cell for producing ATP in most eukaryotic cells?",
      "options": [
        "A. Chloroplast",
        "B. Vacuole",
        "C. Mitochondrion",
        "D. Lysosome"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful powered airplane flight was made by the Wright brothers at Kitty Hawk, North Carolina in 1903.",
      "conflict_prompt": "The first successful powered airplane flight was made by the Wright brothers at Kitty Hawk, North Carolina in 1923.",
      "question": "In what year did the Wright brothers make the first successful powered airplane flight at Kitty Hawk?",
      "options": [
        "A. 1923",
        "B. 1913",
        "C. 1903",
        "D. 1893"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary energy source for the Sun is nuclear fusion of hydrogen into helium in its core.",
      "conflict_prompt": "The primary energy source for the Sun is nuclear fission of uranium in its core.",
      "question": "What is the primary process that powers the Sun?",
      "options": [
        "A. Nuclear fission of uranium",
        "B. Chemical combustion",
        "C. Nuclear fusion of hydrogen",
        "D. Gravitational contraction"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest continent by land area is Asia.",
      "conflict_prompt": "The largest continent by land area is Africa.",
      "question": "Which continent has the largest land area?",
      "options": [
        "A. Africa",
        "B. Antarctica",
        "C. Asia",
        "D. Europe"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Renaissance was a cultural movement that began in Italy in the 14th century.",
      "conflict_prompt": "The Renaissance was a cultural movement that began in Italy in the 18th century.",
      "question": "In which century did the Renaissance begin in Italy?",
      "options": [
        "A. 18th century",
        "B. 14th century",
        "C. 16th century",
        "D. 12th century"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The national sport of Japan is sumo wrestling, which has a long cultural tradition.",
      "conflict_prompt": "The national sport of Japan is baseball, which has a long cultural tradition.",
      "question": "Which sport is traditionally considered Japan's national sport with a long cultural tradition?",
      "options": [
        "A. Baseball",
        "B. Sumo wrestling",
        "C. Soccer",
        "D. Judo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fictional detective Sherlock Holmes was created by Sir Arthur Conan Doyle.",
      "conflict_prompt": "The fictional detective Sherlock Holmes was created by Agatha Christie.",
      "question": "Who created the fictional detective Sherlock Holmes?",
      "options": [
        "A. Agatha Christie",
        "B. Edgar Allan Poe",
        "C. Sir Arthur Conan Doyle",
        "D. Dashiell Hammett"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Richter scale was historically used to measure earthquake magnitude on a logarithmic scale.",
      "conflict_prompt": "The Richter scale was historically used to measure wind speed on a logarithmic scale.",
      "question": "What natural phenomenon was the Richter scale historically used to measure?",
      "options": [
        "A. Wind speed",
        "B. Temperature",
        "C. Earthquake magnitude",
        "D. Rainfall"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A light-year is a unit of distance equal to the distance light travels in one year.",
      "conflict_prompt": "A light-year is a unit of time equal to the time light takes to travel one astronomical unit.",
      "question": "What does a light-year measure?",
      "options": [
        "A. Temperature",
        "B. Time",
        "C. Distance",
        "D. Brightness"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The International Space Station orbits Earth at an altitude of approximately 400 kilometers.",
      "conflict_prompt": "The International Space Station orbits Earth at an altitude of approximately 4,000 kilometers.",
      "question": "Approximately at what altitude does the International Space Station orbit Earth?",
      "options": [
        "A. 4,000 kilometers",
        "B. 400 kilometers",
        "C. 40 kilometers",
        "D. 4 kilometers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for table salt (sodium chloride) is NaCl.",
      "conflict_prompt": "The chemical formula for table salt (sodium chloride) is KCl.",
      "question": "What is the chemical formula for sodium chloride (table salt)?",
      "options": [
        "A. KCl",
        "B. NaCl",
        "C. NaCO3",
        "D. Cl2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The protagonist of Herman Melville's Moby-Dick is Captain Ahab, who pursues the white whale.",
      "conflict_prompt": "The protagonist of Herman Melville's Moby-Dick is Captain Nemo, who pursues the white whale.",
      "question": "Who is the captain that pursues the white whale in Moby-Dick?",
      "options": [
        "A. Captain Nemo",
        "B. Captain Cook",
        "C. Captain Ahab",
        "D. Captain Hook"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional hummus is chickpeas (garbanzo beans).",
      "conflict_prompt": "The primary ingredient in traditional hummus is lentils.",
      "question": "What is the primary ingredient in traditional hummus?",
      "options": [
        "A. Lentils",
        "B. Chickpeas",
        "C. Black beans",
        "D. Peas"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The piano is a musical instrument classified as a stringed percussion instrument because strings are struck by hammers.",
      "conflict_prompt": "The piano is a musical instrument classified as a woodwind instrument because air passes through reeds.",
      "question": "How is the piano primarily classified as a musical instrument?",
      "options": [
        "A. Woodwind",
        "B. Brass",
        "C. Stringed percussion (strings struck by hammers)",
        "D. Electronic"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest organ in the human body by surface area is the skin.",
      "conflict_prompt": "The largest organ in the human body by surface area is the liver.",
      "question": "Which organ is the largest in the human body by surface area?",
      "options": [
        "A. Liver",
        "B. Skin",
        "C. Brain",
        "D. Heart"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Italy is Rome.",
      "conflict_prompt": "The capital of Italy is Milan.",
      "question": "What is the capital city of Italy?",
      "options": [
        "A. Milan",
        "B. Venice",
        "C. Florence",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Marie Curie was awarded Nobel Prizes in both Physics and Chemistry for her work on radioactivity and polonium/radium.",
      "conflict_prompt": "Marie Curie was awarded Nobel Prizes only in Physics for her work on radioactivity and polonium/radium.",
      "question": "In which scientific fields did Marie Curie receive Nobel Prizes?",
      "options": [
        "A. Only Physics",
        "B. Only Chemistry",
        "C. Both Physics and Chemistry",
        "D. Physics and Medicine"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon rainforest is known as the largest tropical rainforest in the world.",
      "conflict_prompt": "The Congo rainforest is known as the largest tropical rainforest in the world.",
      "question": "Which rainforest is considered the largest tropical rainforest in the world?",
      "options": [
        "A. Congo rainforest",
        "B. Daintree rainforest",
        "C. Amazon rainforest",
        "D. Southeast Asian rainforest"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial Earth satellite, Sputnik 1, was launched by the Soviet Union in 1957.",
      "conflict_prompt": "The first artificial Earth satellite, Sputnik 1, was launched by the United States in 1957.",
      "question": "Which country launched Sputnik 1, the first artificial Earth satellite, in 1957?",
      "options": [
        "A. United States",
        "B. Soviet Union",
        "C. United Kingdom",
        "D. France"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The instrument used to measure atmospheric pressure is called a barometer.",
      "conflict_prompt": "The instrument used to measure atmospheric pressure is called a thermometer.",
      "question": "What instrument measures atmospheric pressure?",
      "options": [
        "A. Thermometer",
        "B. Barometer",
        "C. Anemometer",
        "D. Hygrometer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pythagorean theorem states that in a right-angled triangle, the square of the hypotenuse equals the sum of the squares of the other two sides.",
      "conflict_prompt": "The Pythagorean theorem states that in a right-angled triangle, the square of one leg equals the sum of the squares of the other leg and the hypotenuse.",
      "question": "According to the Pythagorean theorem for a right-angled triangle, what is the relation involving the hypotenuse c and legs a and b?",
      "options": [
        "A. a^2 = b^2 + c^2",
        "B. b^2 = a^2 + c^2",
        "C. c^2 = a^2 + b^2",
        "D. c = a + b"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The composer Ludwig van Beethoven wrote nine symphonies, with the Ninth Symphony including the 'Ode to Joy'.",
      "conflict_prompt": "The composer Ludwig van Beethoven wrote ten symphonies, with the Ninth Symphony including the 'Ode to Joy'.",
      "question": "How many symphonies did Ludwig van Beethoven compose?",
      "options": [
        "A. Ten",
        "B. Nine",
        "C. Eight",
        "D. Seven"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The speed of sound in air at sea level under standard conditions is approximately 343 meters per second.",
      "conflict_prompt": "The speed of sound in air at sea level under standard conditions is approximately 34.3 meters per second.",
      "question": "Approximately what is the speed of sound in air at sea level under standard conditions?",
      "options": [
        "A. 34.3 m/s",
        "B. 3,430 m/s",
        "C. 343 m/s",
        "D. 34,300 m/s"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Wright brothers' first successful flight lasted 12 seconds and covered 120 feet.",
      "conflict_prompt": "The Wright brothers' first successful flight lasted 120 seconds and covered 12 feet.",
      "question": "Approximately how long did the Wright brothers' first successful flight last?",
      "options": [
        "A. 120 seconds",
        "B. 12 seconds",
        "C. 1 second",
        "D. 600 seconds"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pianist Frédéric Chopin was a Polish composer known primarily for his piano works during the Romantic era.",
      "conflict_prompt": "The Pianist Frédéric Chopin was a French composer known primarily for his piano works during the Romantic era.",
      "question": "What was Frédéric Chopin's nationality by birth?",
      "options": [
        "A. French",
        "B. Polish",
        "C. German",
        "D. Austrian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element oxygen has atomic number 8.",
      "conflict_prompt": "The chemical element oxygen has atomic number 6.",
      "question": "What is the atomic number of oxygen?",
      "options": [
        "A. 6",
        "B. 7",
        "C. 8",
        "D. 9"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Leaning Tower of Pisa is located in the city of Pisa, Italy.",
      "conflict_prompt": "The Leaning Tower of Pisa is located in the city of Florence, Italy.",
      "question": "In which Italian city is the Leaning Tower of Pisa located?",
      "options": [
        "A. Florence",
        "B. Pisa",
        "C. Venice",
        "D. Milan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most abundant element in the universe is hydrogen.",
      "conflict_prompt": "The most abundant element in the universe is helium.",
      "question": "Which element is most abundant in the universe?",
      "options": [
        "A. Helium",
        "B. Oxygen",
        "C. Hydrogen",
        "D. Carbon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Wright brothers were from the United States and are credited with the first controlled powered flight.",
      "conflict_prompt": "The Wright brothers were from Germany and are credited with the first controlled powered flight.",
      "question": "From which country did the Wright brothers originate?",
      "options": [
        "A. Germany",
        "B. United States",
        "C. Canada",
        "D. England"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Russia is Moscow.",
      "conflict_prompt": "The capital of Russia is Saint Petersburg.",
      "question": "What is the capital city of Russia?",
      "options": [
        "A. Saint Petersburg",
        "B. Novosibirsk",
        "C. Moscow",
        "D. Kazan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary cause of tides on Earth is the gravitational pull of the Moon and the Sun, with the Moon having the larger effect.",
      "conflict_prompt": "The primary cause of tides on Earth is the rotation of the Earth alone, without gravitational influence from the Moon or Sun.",
      "question": "What is the primary cause of Earth's tides?",
      "options": [
        "A. Earth's rotation alone",
        "B. Gravitational pull of the Moon and Sun",
        "C. Atmospheric pressure changes",
        "D. Ocean currents only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty's full name is 'Liberty Enlightening the World' and it stands on Liberty Island in New York Harbor.",
      "conflict_prompt": "The Statue of Liberty's full name is 'Liberty Enlightening the World' and it stands on Trafalgar Square in London.",
      "question": "On which island does the Statue of Liberty stand?",
      "options": [
        "A. Ellis Island",
        "B. Liberty Island",
        "C. Manhattan Island",
        "D. Governors Island"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful polio vaccine was developed by Jonas Salk in the 1950s and used an inactivated (killed) virus.",
      "conflict_prompt": "The first successful polio vaccine was developed by Jonas Salk in the 1950s and used a live attenuated virus.",
      "question": "What type of virus did Jonas Salk's polio vaccine use?",
      "options": [
        "A. Live attenuated virus",
        "B. Inactivated (killed) virus",
        "C. Recombinant protein",
        "D. DNA plasmid"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Japan is the yen, abbreviated JPY.",
      "conflict_prompt": "The currency of Japan is the won, abbreviated KRW.",
      "question": "What is the official currency of Japan?",
      "options": [
        "A. Won (KRW)",
        "B. Yen (JPY)",
        "C. Yuan (CNY)",
        "D. Dollar (USD)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island in the world is Greenland.",
      "conflict_prompt": "The largest island in the world is Australia.",
      "question": "Which is the largest island in the world by area (excluding continents)?",
      "options": [
        "A. Australia",
        "B. Greenland",
        "C. Borneo",
        "D. Madagascar"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "DNA stands for deoxyribonucleic acid, the molecule that carries genetic instructions in living organisms.",
      "conflict_prompt": "DNA stands for deoxyribonucleic amide, the molecule that carries genetic instructions in living organisms.",
      "question": "What does DNA stand for?",
      "options": [
        "A. Deoxyribonucleic amide",
        "B. Deoxyribonucleic acid",
        "C. Deoxyribose nucleic acid",
        "D. Deoxyribose nucleic amine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city by population in the United States is New York City.",
      "conflict_prompt": "The largest city by population in the United States is Los Angeles.",
      "question": "Which is the most populous city in the United States?",
      "options": [
        "A. Los Angeles",
        "B. Chicago",
        "C. Houston",
        "D. New York City"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Thomas Edison is credited with inventing the practical incandescent light bulb and improving many electrical technologies.",
      "conflict_prompt": "Thomas Edison is credited with inventing the radio.",
      "question": "Which inventor is commonly credited with developing the practical incandescent light bulb?",
      "options": [
        "A. Nikola Tesla",
        "B. Guglielmo Marconi",
        "C. Thomas Edison",
        "D. Alexander Graham Bell"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A half-life is the time required for half the quantity of a radioactive isotope to decay.",
      "conflict_prompt": "A half-life is the time required for a radioactive isotope to completely decay.",
      "question": "What does the term 'half-life' refer to in radioactivity?",
      "options": [
        "A. Time for complete decay",
        "B. Time for half the quantity to decay",
        "C. Time for one quarter to decay",
        "D. Time for doubling"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Galápagos Islands are located in the Pacific Ocean and are part of Ecuador.",
      "conflict_prompt": "The Galápagos Islands are located in the Atlantic Ocean and are part of Ecuador.",
      "question": "In which ocean are the Galápagos Islands located?",
      "options": [
        "A. Atlantic Ocean",
        "B. Pacific Ocean",
        "C. Indian Ocean",
        "D. Arctic Ocean"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The blood type that is considered the universal donor for red blood cells is O negative.",
      "conflict_prompt": "The blood type that is considered the universal donor for red blood cells is AB positive.",
      "question": "Which blood type is considered the universal donor for red blood cell transfusions?",
      "options": [
        "A. AB positive",
        "B. A negative",
        "C. O negative",
        "D. B positive"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest moon of Saturn is Titan.",
      "conflict_prompt": "The largest moon of Saturn is Europa.",
      "question": "Which is the largest moon of Saturn?",
      "options": [
        "A. Europa",
        "B. Ganymede",
        "C. Titan",
        "D. Io"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of France is Paris.",
      "conflict_prompt": "The capital of France is Marseille.",
      "question": "What is the capital city of France?",
      "options": [
        "A. Marseille",
        "B. Lyon",
        "C. Paris",
        "D. Nice"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Declaration of Independence of the United States was adopted on July 4, 1776.",
      "conflict_prompt": "The Declaration of Independence of the United States was adopted on July 4, 1786.",
      "question": "On what date was the United States Declaration of Independence adopted?",
      "options": [
        "A. July 4, 1786",
        "B. July 4, 1776",
        "C. June 4, 1776",
        "D. July 14, 1776"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest land animal is the cheetah, capable of short bursts up to around 100–120 km/h.",
      "conflict_prompt": "The fastest land animal is the pronghorn, capable of short bursts up to around 100–120 km/h.",
      "question": "Which is the fastest land animal over short bursts?",
      "options": [
        "A. Pronghorn",
        "B. Cheetah",
        "C. Lion",
        "D. Gazelle"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Louvre Museum in Paris houses the painting Mona Lisa by Leonardo da Vinci.",
      "conflict_prompt": "The Louvre Museum in Paris houses the painting Mona Lisa by Vincent van Gogh.",
      "question": "Which artist painted the Mona Lisa that is housed in the Louvre Museum?",
      "options": [
        "A. Vincent van Gogh",
        "B. Pablo Picasso",
        "C. Leonardo da Vinci",
        "D. Michelangelo"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary structural component of plant cell walls is cellulose.",
      "conflict_prompt": "The primary structural component of plant cell walls is keratin.",
      "question": "What is the primary structural component of plant cell walls?",
      "options": [
        "A. Keratin",
        "B. Cellulose",
        "C. Collagen",
        "D. Lignin"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The boiling point of water at sea level (1 atm) is 100 degrees Celsius.",
      "conflict_prompt": "The boiling point of water at sea level (1 atm) is 0 degrees Celsius.",
      "question": "At sea level under standard atmospheric pressure, what is the boiling point of water in degrees Celsius?",
      "options": [
        "A. 0°C",
        "B. 50°C",
        "C. 100°C",
        "D. 212°C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the telephone is commonly credited as Alexander Graham Bell.",
      "conflict_prompt": "The inventor of the telephone is commonly credited as Thomas Edison.",
      "question": "Who is commonly credited with inventing the telephone?",
      "options": [
        "A. Thomas Edison",
        "B. Alexander Graham Bell",
        "C. Nikola Tesla",
        "D. Guglielmo Marconi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest two countries by land area are Russia and Canada, with Russia being the largest.",
      "conflict_prompt": "The largest two countries by land area are China and Canada, with China being the largest.",
      "question": "Which country is the largest in the world by land area?",
      "options": [
        "A. Canada",
        "B. China",
        "C. United States",
        "D. Russia"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary taste receptor types include sweet, sour, salty, bitter, and umami.",
      "conflict_prompt": "The primary taste receptor types include sweet, sour, salty, bitter, and metallic.",
      "question": "Which of the following is considered one of the five primary taste receptor types?",
      "options": [
        "A. Metallic",
        "B. Umami",
        "C. Spicy",
        "D. Astringent"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The organ that filters blood to produce urine in humans is the kidney.",
      "conflict_prompt": "The organ that filters blood to produce urine in humans is the liver.",
      "question": "Which organ primarily filters blood to produce urine in humans?",
      "options": [
        "A. Liver",
        "B. Heart",
        "C. Kidney",
        "D. Pancreas"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The French Revolution began in 1789 with events like the Storming of the Bastille.",
      "conflict_prompt": "The French Revolution began in 1889 with events like the Storming of the Bastille.",
      "question": "In what year did the French Revolution begin?",
      "options": [
        "A. 1889",
        "B. 1789",
        "C. 1689",
        "D. 1989"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most common gas in Earth's atmosphere after nitrogen is oxygen, making up about 21%.",
      "conflict_prompt": "The most common gas in Earth's atmosphere after nitrogen is argon, making up about 21%.",
      "question": "Which gas makes up about 21% of Earth's atmosphere by volume?",
      "options": [
        "A. Argon",
        "B. Oxygen",
        "C. Carbon dioxide",
        "D. Nitrogen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The sport of basketball was invented by James Naismith in 1891.",
      "conflict_prompt": "The sport of basketball was invented by James Naismith in 1991.",
      "question": "In what year was basketball invented by James Naismith?",
      "options": [
        "A. 1991",
        "B. 1891",
        "C. 1791",
        "D. 1901"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The southernmost continent on Earth is Antarctica.",
      "conflict_prompt": "The southernmost continent on Earth is Australia.",
      "question": "Which continent is the southernmost on Earth?",
      "options": [
        "A. Australia",
        "B. South America",
        "C. Antarctica",
        "D. Africa"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element with atomic number 1 is hydrogen.",
      "conflict_prompt": "The element with atomic number 1 is helium.",
      "question": "Which element has atomic number 1?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Lithium",
        "D. Oxygen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "In baseball, a player who hits the ball and reaches first base without an error is credited with a single.",
      "conflict_prompt": "In baseball, a player who hits the ball and reaches home plate without an error is credited with a single.",
      "question": "What is a hit called when a batter reaches first base safely after hitting the ball (without an error)?",
      "options": [
        "A. Home run",
        "B. Double",
        "C. Single",
        "D. Triple"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for gold is Au.",
      "conflict_prompt": "The chemical symbol for gold is Ag.",
      "question": "What is the chemical symbol for gold?",
      "options": [
        "A. Ag",
        "B. Au",
        "C. Gd",
        "D. Go"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A covalent bond involves the sharing of electron pairs between atoms.",
      "conflict_prompt": "A covalent bond involves the transfer of electrons from one atom to another.",
      "question": "Which describes a covalent bond between atoms?",
      "options": [
        "A. Transfer of electrons from one atom to another",
        "B. Sharing of electron pairs between atoms",
        "C. Attraction between ions of opposite charge",
        "D. Magnetic alignment of atomic nuclei"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize in Literature in 1954 was awarded to Ernest Hemingway.",
      "conflict_prompt": "The Nobel Prize in Literature in 1954 was awarded to John Steinbeck.",
      "question": "Who received the Nobel Prize in Literature in 1954?",
      "options": [
        "A. John Steinbeck",
        "B. Ernest Hemingway",
        "C. William Faulkner",
        "D. T. S. Eliot"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest internal organ in the human body is the liver.",
      "conflict_prompt": "The largest internal organ in the human body is the pancreas.",
      "question": "Which organ is the largest internal organ in the human body?",
      "options": [
        "A. Pancreas",
        "B. Liver",
        "C. Kidney",
        "D. Lung"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The speed of light in vacuum is approximately 299,792 kilometers per second.",
      "conflict_prompt": "The speed of light in vacuum is approximately 29,979 kilometers per second.",
      "question": "Approximately what is the speed of light in a vacuum in kilometers per second?",
      "options": [
        "A. 29,979 km/s",
        "B. 299,792 km/s",
        "C. 3,000 km/s",
        "D. 30,000 km/s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary author of the novel '1984' is George Orwell.",
      "conflict_prompt": "The primary author of the novel '1984' is Aldous Huxley.",
      "question": "Who wrote the novel '1984'?",
      "options": [
        "A. Aldous Huxley",
        "B. George Orwell",
        "C. Ray Bradbury",
        "D. Franz Kafka"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most widely spoken native language in the world is Mandarin Chinese.",
      "conflict_prompt": "The most widely spoken native language in the world is English.",
      "question": "Which language has the largest number of native speakers worldwide?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. Mandarin Chinese",
        "D. Hindi"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon basin spans multiple South American countries including Brazil, Peru, and Colombia.",
      "conflict_prompt": "The Amazon basin spans multiple South American countries including Argentina, Chile, and Uruguay.",
      "question": "Which of the following countries is part of the Amazon basin?",
      "options": [
        "A. Argentina",
        "B. Chile",
        "C. Brazil",
        "D. Uruguay"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human brain's frontal lobe is primarily associated with decision making, planning, and voluntary movement.",
      "conflict_prompt": "The human brain's frontal lobe is primarily associated with visual processing and color recognition.",
      "question": "Which brain region is mainly associated with decision making and planning?",
      "options": [
        "A. Occipital lobe",
        "B. Temporal lobe",
        "C. Parietal lobe",
        "D. Frontal lobe"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first element in the periodic table is hydrogen, followed by helium.",
      "conflict_prompt": "The first element in the periodic table is helium, followed by hydrogen.",
      "question": "Which element is first in the periodic table?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Lithium",
        "D. Beryllium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The film 'The Godfather' was directed by Francis Ford Coppola and released in 1972.",
      "conflict_prompt": "The film 'The Godfather' was directed by Martin Scorsese and released in 1972.",
      "question": "Who directed the film 'The Godfather' released in 1972?",
      "options": [
        "A. Martin Scorsese",
        "B. Francis Ford Coppola",
        "C. Steven Spielberg",
        "D. Stanley Kubrick"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element with chemical symbol Fe is iron.",
      "conflict_prompt": "The element with chemical symbol Fe is fluorine.",
      "question": "Which element has the chemical symbol Fe?",
      "options": [
        "A. Fluorine",
        "B. Iron",
        "C. Francium",
        "D. Fermium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Gulf Stream is a warm Atlantic Ocean current that influences the climate of Western Europe.",
      "conflict_prompt": "The Gulf Stream is a cold Pacific Ocean current that influences the climate of Western Europe.",
      "question": "What type of ocean current is the Gulf Stream and which region's climate does it influence?",
      "options": [
        "A. Cold Pacific current influencing Western Europe",
        "B. Warm Atlantic current influencing Western Europe",
        "C. Warm Indian current influencing South Asia",
        "D. Cold Atlantic current influencing South America"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary component of natural gas used for heating and cooking is methane (CH4).",
      "conflict_prompt": "The primary component of natural gas used for heating and cooking is carbon dioxide (CO2).",
      "question": "What is the primary component of natural gas used for residential heating and cooking?",
      "options": [
        "A. Carbon dioxide (CO2)",
        "B. Methane (CH4)",
        "C. Propane (C3H8)",
        "D. Butane (C4H10)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest lake in Africa by surface area is Lake Victoria.",
      "conflict_prompt": "The largest lake in Africa by surface area is Lake Tanganyika.",
      "question": "Which lake is the largest in Africa by surface area?",
      "options": [
        "A. Lake Tanganyika",
        "B. Lake Malawi",
        "C. Lake Victoria",
        "D. Lake Turkana"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of India is New Delhi.",
      "conflict_prompt": "The capital city of India is Mumbai.",
      "question": "What is the capital city of India?",
      "options": [
        "A. Mumbai",
        "B. Kolkata",
        "C. New Delhi",
        "D. Chennai"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element with atomic number 26 is iron (Fe).",
      "conflict_prompt": "The element with atomic number 26 is nickel (Ni).",
      "question": "Which element has atomic number 26?",
      "options": [
        "A. Nickel (Ni)",
        "B. Iron (Fe)",
        "C. Cobalt (Co)",
        "D. Copper (Cu)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest volcano on Earth by volume is Mauna Loa in Hawaii.",
      "conflict_prompt": "The largest volcano on Earth by volume is Mount Fuji in Japan.",
      "question": "Which volcano is considered the largest on Earth by volume?",
      "options": [
        "A. Mount Fuji",
        "B. Mauna Loa",
        "C. Mount Kilimanjaro",
        "D. Mount St. Helens"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of red blood cells is to transport oxygen via hemoglobin.",
      "conflict_prompt": "The primary function of red blood cells is to fight infections by producing antibodies.",
      "question": "What is the main function of red blood cells?",
      "options": [
        "A. Produce antibodies to fight infections",
        "B. Transport oxygen using hemoglobin",
        "C. Regulate hormone production",
        "D. Store fat"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first Harry Potter book, 'Harry Potter and the Philosopher's Stone', was published in 1997 in the UK.",
      "conflict_prompt": "The first Harry Potter book, 'Harry Potter and the Philosopher's Stone', was published in 1987 in the UK.",
      "question": "In what year was the first Harry Potter book published in the UK?",
      "options": [
        "A. 1987",
        "B. 1997",
        "C. 2007",
        "D. 1977"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest bird by wingspan is the wandering albatross.",
      "conflict_prompt": "The largest bird by wingspan is the ostrich.",
      "question": "Which bird has the largest wingspan?",
      "options": [
        "A. Ostrich",
        "B. Wandering albatross",
        "C. Bald eagle",
        "D. Condor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common chemical name for NaHCO3 is sodium bicarbonate, also known as baking soda.",
      "conflict_prompt": "The common chemical name for NaHCO3 is sodium carbonate, also known as baking soda.",
      "question": "What is the common name for NaHCO3?",
      "options": [
        "A. Sodium carbonate",
        "B. Sodium bicarbonate (baking soda)",
        "C. Table salt",
        "D. Baking powder"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Barrier Reef is located off the northeast coast of Australia in the Coral Sea.",
      "conflict_prompt": "The Great Barrier Reef is located off the southeast coast of Australia in the Tasman Sea.",
      "question": "Off which coast of Australia is the Great Barrier Reef located?",
      "options": [
        "A. Southeast coast (Tasman Sea)",
        "B. Northeast coast (Coral Sea)",
        "C. Northwest coast (Indian Ocean)",
        "D. Southwest coast (Southern Ocean)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island in the Mediterranean Sea is Sicily.",
      "conflict_prompt": "The largest island in the Mediterranean Sea is Sardinia.",
      "question": "Which is the largest island in the Mediterranean Sea by area?",
      "options": [
        "A. Sardinia",
        "B. Corsica",
        "C. Crete",
        "D. Sicily"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary currency unit of China is the yuan (renminbi), abbreviated CNY.",
      "conflict_prompt": "The primary currency unit of China is the yen, abbreviated JPY.",
      "question": "What is the official currency of China?",
      "options": [
        "A. Yen (JPY)",
        "B. Yuan (CNY)",
        "C. Won (KRW)",
        "D. Dollar (USD)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary organ for hearing in humans is the cochlea within the inner ear.",
      "conflict_prompt": "The primary organ for hearing in humans is the semicircular canals within the inner ear.",
      "question": "Which inner ear structure is primarily responsible for hearing?",
      "options": [
        "A. Semicircular canals",
        "B. Cochlea",
        "C. Eustachian tube",
        "D. Tympanic membrane"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Germany is Berlin.",
      "conflict_prompt": "The capital of Germany is Munich.",
      "question": "What is the capital city of Germany?",
      "options": [
        "A. Munich",
        "B. Frankfurt",
        "C. Berlin",
        "D. Hamburg"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial pacemaker was implanted in a human in 1958.",
      "conflict_prompt": "The first artificial pacemaker was implanted in a human in 1858.",
      "question": "In which year was the first artificial pacemaker implanted in a human?",
      "options": [
        "A. 1858",
        "B. 1958",
        "C. 1908",
        "D. 1978"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The classical composer Johann Sebastian Bach lived during the Baroque period and died in 1750.",
      "conflict_prompt": "The classical composer Johann Sebastian Bach lived during the Classical period and died in 1750.",
      "question": "During which musical period did Johann Sebastian Bach compose?",
      "options": [
        "A. Classical period",
        "B. Romantic period",
        "C. Baroque period",
        "D. Modern period"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The study of heredity and variation in organisms is called genetics.",
      "conflict_prompt": "The study of heredity and variation in organisms is called geology.",
      "question": "What is the scientific study of heredity and variation called?",
      "options": [
        "A. Geology",
        "B. Genetics",
        "C. Ecology",
        "D. Ethology"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful human organ transplant (kidney) was performed in 1954 between identical twins.",
      "conflict_prompt": "The first successful human organ transplant (kidney) was performed in 1854 between identical twins.",
      "question": "In what year was the first successful human kidney transplant performed?",
      "options": [
        "A. 1854",
        "B. 1954",
        "C. 1924",
        "D. 1974"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Maldives is an island nation in the Indian Ocean southwest of India and Sri Lanka.",
      "conflict_prompt": "The Maldives is an island nation in the Atlantic Ocean southwest of India and Sri Lanka.",
      "question": "In which ocean are the Maldives located?",
      "options": [
        "A. Atlantic Ocean",
        "B. Indian Ocean",
        "C. Pacific Ocean",
        "D. Southern Ocean"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language of Argentina is Spanish.",
      "conflict_prompt": "The primary language of Argentina is Portuguese.",
      "question": "What is the primary language spoken in Argentina?",
      "options": [
        "A. Portuguese",
        "B. Spanish",
        "C. Italian",
        "D. English"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary role of hemoglobin in red blood cells is to bind and transport oxygen.",
      "conflict_prompt": "The primary role of hemoglobin in red blood cells is to bind and transport carbon dioxide exclusively.",
      "question": "What is the primary function of hemoglobin in red blood cells?",
      "options": [
        "A. Transport carbon dioxide exclusively",
        "B. Store iron for metabolism",
        "C. Bind and transport oxygen",
        "D. Produce antibodies"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest reptile by mass is the saltwater crocodile.",
      "conflict_prompt": "The largest reptile by mass is the Komodo dragon.",
      "question": "Which reptile is generally considered the largest by mass?",
      "options": [
        "A. Komodo dragon",
        "B. Green sea turtle",
        "C. Saltwater crocodile",
        "D. Anaconda"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first modern Olympic Games were held in Athens in 1896.",
      "conflict_prompt": "The first modern Olympic Games were held in Athens in 1996.",
      "question": "In what year were the first modern Olympic Games held in Athens?",
      "options": [
        "A. 1996",
        "B. 1896",
        "C. 1906",
        "D. 1856"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most common blood type worldwide is O positive.",
      "conflict_prompt": "The most common blood type worldwide is AB negative.",
      "question": "Which blood type is most common worldwide?",
      "options": [
        "A. AB negative",
        "B. O positive",
        "C. A negative",
        "D. B positive"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest coral reef system in the world is the Great Barrier Reef.",
      "conflict_prompt": "The largest coral reef system in the world is the Red Sea Coral Reef.",
      "question": "Which coral reef system is the largest in the world?",
      "options": [
        "A. Red Sea Coral Reef",
        "B. Mesoamerican Reef",
        "C. Great Barrier Reef",
        "D. New Caledonian Barrier Reef"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The dominant religion in Saudi Arabia is Islam.",
      "conflict_prompt": "The dominant religion in Saudi Arabia is Christianity.",
      "question": "What is the dominant religion in Saudi Arabia?",
      "options": [
        "A. Christianity",
        "B. Hinduism",
        "C. Islam",
        "D. Buddhism"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official language of Brazil is Portuguese.",
      "conflict_prompt": "The official language of Brazil is French.",
      "question": "What is the official language of Brazil?",
      "options": [
        "A. French",
        "B. Spanish",
        "C. Portuguese",
        "D. English"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The process by which bones lengthen during growth is called endochondral ossification.",
      "conflict_prompt": "The process by which bones lengthen during growth is called intramembranous ossification.",
      "question": "What is the process called by which long bones lengthen during growth?",
      "options": [
        "A. Intramembranous ossification",
        "B. Endochondral ossification",
        "C. Osteolysis",
        "D. Chondrogenesis"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The highest mountain in Africa is Mount Kilimanjaro.",
      "conflict_prompt": "The highest mountain in Africa is Mount Kenya.",
      "question": "Which mountain is the highest in Africa?",
      "options": [
        "A. Mount Kenya",
        "B. Mount Elgon",
        "C. Mount Kilimanjaro",
        "D. Ras Dashen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The international organization responsible for promoting global health is the World Health Organization (WHO).",
      "conflict_prompt": "The international organization responsible for promoting global health is the International Monetary Fund (IMF).",
      "question": "Which international organization is primarily responsible for global public health?",
      "options": [
        "A. International Monetary Fund (IMF)",
        "B. World Health Organization (WHO)",
        "C. World Trade Organization (WTO)",
        "D. United Nations Educational, Scientific and Cultural Organization (UNESCO)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most spoken language in South America by native speakers is Portuguese, primarily due to Brazil.",
      "conflict_prompt": "The most spoken language in South America by native speakers is English, primarily due to Brazil.",
      "question": "Which language has the most native speakers in South America?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. Portuguese",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element helium is inert and was first discovered in the solar spectrum before being found on Earth.",
      "conflict_prompt": "The element helium is highly reactive and was first discovered in the solar spectrum before being found on Earth.",
      "question": "Which characteristic correctly describes helium?",
      "options": [
        "A. Highly reactive",
        "B. Inert (noble gas)",
        "C. Radioactive",
        "D. Metal"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest-serving British monarch in history is Queen Elizabeth II.",
      "conflict_prompt": "The longest-serving British monarch in history is King George III.",
      "question": "Who is the longest-serving British monarch in history?",
      "options": [
        "A. King George III",
        "B. Queen Victoria",
        "C. Queen Elizabeth II",
        "D. King Henry VIII"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize was established by the will of Alfred Nobel, the inventor of dynamite.",
      "conflict_prompt": "The Nobel Prize was established by the will of Alfred Nobel, the inventor of the telephone.",
      "question": "Who established the Nobel Prize in his will?",
      "options": [
        "A. Alexander Graham Bell (inventor of the telephone)",
        "B. Alfred Nobel (inventor of dynamite)",
        "C. Thomas Edison (inventor of the light bulb)",
        "D. Guglielmo Marconi (pioneer of radio)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main ingredient in traditional Japanese miso soup is fermented soybean paste called miso.",
      "conflict_prompt": "The main ingredient in traditional Japanese miso soup is fermented rice paste called koji.",
      "question": "What is the main ingredient used to make traditional miso paste for miso soup?",
      "options": [
        "A. Fermented rice paste (koji)",
        "B. Fermented soybean paste (miso)",
        "C. Fermented fish paste",
        "D. Seaweed extract"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most populous country in Africa is Nigeria.",
      "conflict_prompt": "The most populous country in Africa is Egypt.",
      "question": "Which country is the most populous in Africa?",
      "options": [
        "A. Egypt",
        "B. Ethiopia",
        "C. Nigeria",
        "D. South Africa"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Peace Prize is awarded in Oslo, Norway, while most other Nobel Prizes are awarded in Stockholm, Sweden.",
      "conflict_prompt": "The Nobel Peace Prize is awarded in Stockholm, Sweden, while most other Nobel Prizes are awarded in Oslo, Norway.",
      "question": "In which city is the Nobel Peace Prize awarded?",
      "options": [
        "A. Stockholm, Sweden",
        "B. Oslo, Norway",
        "C. Copenhagen, Denmark",
        "D. Helsinki, Finland"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest metro area in South America by population is São Paulo, Brazil.",
      "conflict_prompt": "The largest metro area in South America by population is Buenos Aires, Argentina.",
      "question": "Which metro area is the largest in South America by population?",
      "options": [
        "A. Buenos Aires",
        "B. Rio de Janeiro",
        "C. Bogotá",
        "D. São Paulo"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The concept of supply and demand is a fundamental principle of economics that describes price determination in markets.",
      "conflict_prompt": "The concept of supply and demand is a fundamental principle of biology that describes price determination in markets.",
      "question": "Supply and demand is a fundamental principle in which field?",
      "options": [
        "A. Biology",
        "B. Chemistry",
        "C. Economics",
        "D. Physics"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most abundant mineral in Earth's crust is feldspar.",
      "conflict_prompt": "The most abundant mineral in Earth's crust is quartz.",
      "question": "Which mineral is the most abundant in Earth's crust?",
      "options": [
        "A. Quartz",
        "B. Feldspar",
        "C. Mica",
        "D. Olivine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest ocean trench on Earth is the Mariana Trench in the western Pacific Ocean.",
      "conflict_prompt": "The largest ocean trench on Earth is the Puerto Rico Trench in the Atlantic Ocean.",
      "question": "Which ocean trench is the deepest and largest on Earth?",
      "options": [
        "A. Puerto Rico Trench",
        "B. Mariana Trench",
        "C. Java Trench",
        "D. Tonga Trench"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful vaccine for smallpox was developed by Edward Jenner in 1796 using cowpox material.",
      "conflict_prompt": "The first successful vaccine for smallpox was developed by Edward Jenner in 1896 using cowpox material.",
      "question": "In what year did Edward Jenner develop the first successful smallpox vaccine?",
      "options": [
        "A. 1896",
        "B. 1796",
        "C. 1696",
        "D. 1996"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The national animal of Canada is the beaver.",
      "conflict_prompt": "The national animal of Canada is the moose.",
      "question": "What is the national animal of Canada?",
      "options": [
        "A. Moose",
        "B. Beaver",
        "C. Polar bear",
        "D. Canadian goose"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning emperor of Japan was Emperor Hirohito (Showa), who reigned from 1926 to 1989.",
      "conflict_prompt": "The longest reigning emperor of Japan was Emperor Meiji, who reigned from 1926 to 1989.",
      "question": "Who reigned as Emperor of Japan from 1926 to 1989?",
      "options": [
        "A. Emperor Meiji",
        "B. Emperor Taisho",
        "C. Emperor Hirohito (Showa)",
        "D. Emperor Akihito"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical compound commonly known as table sugar is sucrose, composed of glucose and fructose.",
      "conflict_prompt": "The chemical compound commonly known as table sugar is lactose, composed of glucose and fructose.",
      "question": "What is the common chemical name for table sugar?",
      "options": [
        "A. Lactose",
        "B. Sucrose",
        "C. Cellulose",
        "D. Maltose"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Antarctic Treaty, signed in 1959, sets aside Antarctica as a scientific preserve and bans military activity on the continent.",
      "conflict_prompt": "The Antarctic Treaty, signed in 1959, allows military activity and territorial claims in Antarctica.",
      "question": "What does the Antarctic Treaty primarily establish for Antarctica?",
      "options": [
        "A. Allows military presence and territorial claims",
        "B. Establishes Antarctica as a scientific preserve and bans military activity",
        "C. Declares Antarctica a private property of signatory nations",
        "D. Opens Antarctica for commercial mining"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The instrument used to measure wind speed is called an anemometer.",
      "conflict_prompt": "The instrument used to measure wind speed is called a barometer.",
      "question": "Which instrument is used to measure wind speed?",
      "options": [
        "A. Barometer",
        "B. Thermometer",
        "C. Hygrometer",
        "D. Anemometer"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Spain is Madrid.",
      "conflict_prompt": "The capital of Spain is Barcelona.",
      "question": "What is the capital city of Spain?",
      "options": [
        "A. Barcelona",
        "B. Valencia",
        "C. Madrid",
        "D. Seville"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet by mass in our solar system is Jupiter.",
      "conflict_prompt": "The largest planet by mass in our solar system is Saturn.",
      "question": "Which planet has the greatest mass in our solar system?",
      "options": [
        "A. Saturn",
        "B. Jupiter",
        "C. Uranus",
        "D. Neptune"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for carbon dioxide is CO2.",
      "conflict_prompt": "The chemical formula for carbon dioxide is C2O.",
      "question": "What is the correct chemical formula for carbon dioxide?",
      "options": [
        "A. C2O",
        "B. CO2",
        "C. O2C",
        "D. CCO"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Kangaroo is a marsupial native to Australia that carries its young in a pouch.",
      "conflict_prompt": "The Kangaroo is a monotreme native to Australia that carries its young in a pouch.",
      "question": "What type of mammal is the kangaroo?",
      "options": [
        "A. Monotreme",
        "B. Placental mammal",
        "C. Marsupial",
        "D. Ovoviviparous"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The classic novel 'Pride and Prejudice' was written by Jane Austen and published in 1813.",
      "conflict_prompt": "The classic novel 'Pride and Prejudice' was written by Charlotte Brontë and published in 1813.",
      "question": "Who wrote 'Pride and Prejudice'?",
      "options": [
        "A. Charlotte Brontë",
        "B. Mary Shelley",
        "C. Jane Austen",
        "D. Emily Brontë"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Eiffel Tower is located in Paris, France.",
      "conflict_prompt": "The Eiffel Tower is located in Berlin, Germany.",
      "question": "In which city is the Eiffel Tower located?",
      "options": [
        "A. Paris",
        "B. Berlin",
        "C. London",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Everest has an elevation of 8,848 meters above sea level.",
      "conflict_prompt": "Mount Everest has an elevation of 9,000 meters above sea level.",
      "question": "What is the commonly cited elevation of Mount Everest?",
      "options": [
        "A. 10,000 meters",
        "B. 8,000 meters",
        "C. 8,848 meters",
        "D. 9,500 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of Australia is Canberra.",
      "conflict_prompt": "The capital city of Australia is Sydney.",
      "question": "Which city is the capital of Australia?",
      "options": [
        "A. Melbourne",
        "B. Canberra",
        "C. Sydney",
        "D. Brisbane"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Water's chemical formula is H2O, composed of two hydrogen atoms and one oxygen atom.",
      "conflict_prompt": "Water's chemical formula is H3O, composed of three hydrogen atoms and one oxygen atom.",
      "question": "What is the chemical formula of water?",
      "options": [
        "A. CO2",
        "B. O2",
        "C. H2O",
        "D. H3O"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Wall of China is primarily located in northern China.",
      "conflict_prompt": "The Great Wall of China is primarily located in southern China.",
      "question": "In which part of China is the Great Wall primarily located?",
      "options": [
        "A. Eastern China",
        "B. Southern China",
        "C. Northern China",
        "D. Western China"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Jupiter is the largest planet in our solar system.",
      "conflict_prompt": "Earth is the largest planet in our solar system.",
      "question": "Which planet is the largest in our solar system?",
      "options": [
        "A. Mars",
        "B. Earth",
        "C. Jupiter",
        "D. Saturn"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The United States declared independence in 1776.",
      "conflict_prompt": "The United States declared independence in 1876.",
      "question": "In what year did the United States declare independence?",
      "options": [
        "A. 1800",
        "B. 1776",
        "C. 1876",
        "D. 1750"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human heart has four chambers: two atria and two ventricles.",
      "conflict_prompt": "The human heart has three chambers: two atria and one ventricle.",
      "question": "How many chambers does the human heart have?",
      "options": [
        "A. Three",
        "B. Four",
        "C. Two",
        "D. One"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon River is the largest river by discharge volume of water in the world.",
      "conflict_prompt": "The Nile River is the largest river by discharge volume of water in the world.",
      "question": "Which river has the greatest discharge volume of water?",
      "options": [
        "A. Nile",
        "B. Mississippi",
        "C. Amazon",
        "D. Yangtze"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light travels in a vacuum at approximately 299,792 kilometers per second.",
      "conflict_prompt": "Light travels in a vacuum at approximately 150,000 kilometers per second.",
      "question": "Approximately how fast does light travel in a vacuum?",
      "options": [
        "A. 150,000 km/s",
        "B. 299,792 km/s",
        "C. 1,080 km/s",
        "D. 500,000 km/s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language spoken in Brazil is Portuguese.",
      "conflict_prompt": "The primary language spoken in Brazil is Spanish.",
      "question": "What is the primary language spoken in Brazil?",
      "options": [
        "A. Spanish",
        "B. English",
        "C. Portuguese",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Shakespeare wrote the play Romeo and Juliet.",
      "conflict_prompt": "Shakespeare wrote the play The Odyssey.",
      "question": "Which playwright wrote Romeo and Juliet?",
      "options": [
        "A. Homer",
        "B. William Shakespeare",
        "C. Charles Dickens",
        "D. Jane Austen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The atomic number of carbon is 6.",
      "conflict_prompt": "The atomic number of carbon is 8.",
      "question": "What is the atomic number of carbon?",
      "options": [
        "A. 8",
        "B. 7",
        "C. 6",
        "D. 12"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Ocean is the largest ocean on Earth.",
      "conflict_prompt": "The Atlantic Ocean is the largest ocean on Earth.",
      "question": "Which ocean is the largest on Earth?",
      "options": [
        "A. Indian Ocean",
        "B. Atlantic Ocean",
        "C. Arctic Ocean",
        "D. Pacific Ocean"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty was a gift to the United States from France.",
      "conflict_prompt": "The Statue of Liberty was a gift to the United States from England.",
      "question": "From which country was the Statue of Liberty gifted to the United States?",
      "options": [
        "A. England",
        "B. Spain",
        "C. France",
        "D. Italy"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Gold has the chemical symbol Au on the periodic table.",
      "conflict_prompt": "Gold has the chemical symbol Ag on the periodic table.",
      "question": "What is the chemical symbol for gold?",
      "options": [
        "A. Ag",
        "B. Au",
        "C. Go",
        "D. Gd"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Canada is Ottawa.",
      "conflict_prompt": "The capital of Canada is Toronto.",
      "question": "Which city is the capital of Canada?",
      "options": [
        "A. Toronto",
        "B. Ottawa",
        "C. Vancouver",
        "D. Montreal"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Japan is the Japanese yen.",
      "conflict_prompt": "The currency of Japan is the Japanese won.",
      "question": "What is the official currency of Japan?",
      "options": [
        "A. Yuan",
        "B. Yen",
        "C. Won",
        "D. Dollar"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light amplification in lasers is achieved through stimulated emission.",
      "conflict_prompt": "Light amplification in lasers is achieved through spontaneous absorption.",
      "question": "Which process primarily produces light amplification in a laser?",
      "options": [
        "A. Spontaneous absorption",
        "B. Stimulated emission",
        "C. Thermal emission",
        "D. Radioactive decay"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human skeleton is made up of 206 bones in a typical adult.",
      "conflict_prompt": "The human skeleton is made up of 150 bones in a typical adult.",
      "question": "Approximately how many bones are in a typical adult human skeleton?",
      "options": [
        "A. 150",
        "B. 206",
        "C. 300",
        "D. 180"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mercury is the closest planet to the Sun in our solar system.",
      "conflict_prompt": "Venus is the closest planet to the Sun in our solar system.",
      "question": "Which planet is closest to the Sun?",
      "options": [
        "A. Earth",
        "B. Venus",
        "C. Mercury",
        "D. Mars"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon rainforest is primarily located in Brazil.",
      "conflict_prompt": "The Amazon rainforest is primarily located in Argentina.",
      "question": "In which country is most of the Amazon rainforest located?",
      "options": [
        "A. Argentina",
        "B. Peru",
        "C. Brazil",
        "D. Colombia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Thomas Edison is credited with inventing the practical incandescent light bulb.",
      "conflict_prompt": "Alexander Graham Bell is credited with inventing the practical incandescent light bulb.",
      "question": "Who is commonly credited with inventing the practical incandescent light bulb?",
      "options": [
        "A. Alexander Graham Bell",
        "B. Nikola Tesla",
        "C. Thomas Edison",
        "D. Guglielmo Marconi"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest mammal on Earth is the blue whale.",
      "conflict_prompt": "The largest mammal on Earth is the African elephant.",
      "question": "Which species is the largest mammal on Earth?",
      "options": [
        "A. African elephant",
        "B. Blue whale",
        "C. Giraffe",
        "D. Orca"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas in Earth's atmosphere is nitrogen, making up about 78 percent.",
      "conflict_prompt": "The primary gas in Earth's atmosphere is oxygen, making up about 78 percent.",
      "question": "What gas makes up the largest percentage of Earth's atmosphere?",
      "options": [
        "A. Oxygen",
        "B. Nitrogen",
        "C. Carbon dioxide",
        "D. Argon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of David was sculpted by Michelangelo.",
      "conflict_prompt": "The Statue of David was sculpted by Donatello.",
      "question": "Which artist sculpted the Statue of David that is famous at the Accademia in Florence?",
      "options": [
        "A. Donatello",
        "B. Bernini",
        "C. Michelangelo",
        "D. Rodin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first man to walk on the Moon was Neil Armstrong in 1969.",
      "conflict_prompt": "The first man to walk on the Moon was Buzz Aldrin in 1969.",
      "question": "Who was the first person to walk on the Moon?",
      "options": [
        "A. Michael Collins",
        "B. Buzz Aldrin",
        "C. Yuri Gagarin",
        "D. Neil Armstrong"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Italy is Rome.",
      "conflict_prompt": "The capital of Italy is Milan.",
      "question": "Which city is the capital of Italy?",
      "options": [
        "A. Venice",
        "B. Milan",
        "C. Florence",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "DNA stands for deoxyribonucleic acid.",
      "conflict_prompt": "DNA stands for deoxyribonucleic base.",
      "question": "What does the abbreviation DNA stand for?",
      "options": [
        "A. Deoxyribonucleic base",
        "B. Deoxyribonucleic acid",
        "C. Dioxyribonucleic acid",
        "D. Deoxyribose nucleic acid"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The speed limit on many U.S. interstate highways is commonly 65 miles per hour or higher, varying by state.",
      "conflict_prompt": "The speed limit on many U.S. interstate highways is commonly 35 miles per hour.",
      "question": "Which of the following is a common maximum speed limit on many U.S. interstate highways?",
      "options": [
        "A. 35 mph",
        "B. 45 mph",
        "C. 65 mph",
        "D. 25 mph"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Rubik's Cube was invented by Ernő Rubik.",
      "conflict_prompt": "The Rubik's Cube was invented by Erno Rubikson.",
      "question": "Who invented the Rubik's Cube?",
      "options": [
        "A. Erno Rubikson",
        "B. Ernő Rubik",
        "C. László Bíró",
        "D. Viktor Kaplan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Venice is a city in northeastern Italy built on a group of islands connected by canals and bridges.",
      "conflict_prompt": "Venice is a city in northeastern Italy built on a group of islands connected by highways and tunnels.",
      "question": "Which feature is Venice especially known for?",
      "options": [
        "A. Canals",
        "B. Highways",
        "C. Tunnels",
        "D. Railroads"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest continent by land area is Asia.",
      "conflict_prompt": "The largest continent by land area is Africa.",
      "question": "Which is the largest continent by land area?",
      "options": [
        "A. Africa",
        "B. North America",
        "C. Asia",
        "D. Europe"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Internet's World Wide Web was invented by Tim Berners-Lee.",
      "conflict_prompt": "The Internet's World Wide Web was invented by Vint Cerf.",
      "question": "Who is credited with inventing the World Wide Web?",
      "options": [
        "A. Vint Cerf",
        "B. Tim Berners-Lee",
        "C. Bill Gates",
        "D. Steve Jobs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 1 is hydrogen.",
      "conflict_prompt": "The chemical element with atomic number 1 is helium.",
      "question": "Which element has atomic number 1?",
      "options": [
        "A. Helium",
        "B. Lithium",
        "C. Hydrogen",
        "D. Oxygen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Battle of Waterloo occurred in 1815 and ended Napoleon Bonaparte's rule.",
      "conflict_prompt": "The Battle of Waterloo occurred in 1915 and ended Napoleon Bonaparte's rule.",
      "question": "In what year did the Battle of Waterloo take place?",
      "options": [
        "A. 1815",
        "B. 1915",
        "C. 1805",
        "D. 1799"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest desert in the world is the Sahara when considering hot deserts, but the Antarctic is the largest overall desert.",
      "conflict_prompt": "The largest desert in the world is the Sahara and the Antarctic is much smaller.",
      "question": "Which region is the largest desert on Earth overall?",
      "options": [
        "A. Sahara",
        "B. Gobi",
        "C. Antarctic",
        "D. Arabian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredients in traditional bread are flour, water, yeast, and salt.",
      "conflict_prompt": "The primary ingredients in traditional bread are flour, water, and sugar only.",
      "question": "Which of the following is commonly a primary ingredient in traditional bread recipes?",
      "options": [
        "A. Sugar",
        "B. Yeast",
        "C. Chocolate",
        "D. Butter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency used in the United Kingdom is the pound sterling.",
      "conflict_prompt": "The currency used in the United Kingdom is the euro.",
      "question": "What is the official currency of the United Kingdom?",
      "options": [
        "A. Euro",
        "B. Pound sterling",
        "C. Dollar",
        "D. Franc"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest bone in the human body is the femur (thigh bone).",
      "conflict_prompt": "The longest bone in the human body is the tibia (shin bone).",
      "question": "Which bone is the longest in the adult human body?",
      "options": [
        "A. Tibia",
        "B. Fibula",
        "C. Femur",
        "D. Humerus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize in Physics in 1921 was awarded to Albert Einstein.",
      "conflict_prompt": "The Nobel Prize in Physics in 1921 was awarded to Niels Bohr.",
      "question": "Who received the Nobel Prize in Physics in 1921?",
      "options": [
        "A. Niels Bohr",
        "B. Max Planck",
        "C. Albert Einstein",
        "D. Marie Curie"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest country by land area is Russia.",
      "conflict_prompt": "The largest country by land area is Canada.",
      "question": "Which country has the largest land area in the world?",
      "options": [
        "A. Canada",
        "B. United States",
        "C. China",
        "D. Russia"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary organ responsible for filtering blood and producing urine in humans is the kidney.",
      "conflict_prompt": "The primary organ responsible for filtering blood and producing urine in humans is the liver.",
      "question": "Which organ primarily filters blood to produce urine?",
      "options": [
        "A. Liver",
        "B. Kidney",
        "C. Pancreas",
        "D. Heart"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The anthem 'The Star-Spangled Banner' is the national anthem of the United States.",
      "conflict_prompt": "'God Save the Queen' is the national anthem of the United States.",
      "question": "What is the national anthem of the United States?",
      "options": [
        "A. God Save the Queen",
        "B. The Star-Spangled Banner",
        "C. La Marseillaise",
        "D. O Canada"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for sodium is Na.",
      "conflict_prompt": "The chemical symbol for sodium is So.",
      "question": "What is the chemical symbol for sodium?",
      "options": [
        "A. So",
        "B. Sa",
        "C. Na",
        "D. Sd"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest volcano on Earth by volume is Mauna Loa in Hawaii.",
      "conflict_prompt": "The largest volcano on Earth by volume is Mount Fuji in Japan.",
      "question": "Which volcano is considered the largest on Earth by volume?",
      "options": [
        "A. Mount Fuji",
        "B. Mauna Loa",
        "C. Mount St. Helens",
        "D. Mount Kilimanjaro"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official language of Egypt is Arabic.",
      "conflict_prompt": "The official language of Egypt is Turkish.",
      "question": "What is the official language of Egypt?",
      "options": [
        "A. Turkish",
        "B. Arabic",
        "C. Persian",
        "D. English"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The smallest prime number is 2.",
      "conflict_prompt": "The smallest prime number is 1.",
      "question": "Which of the following is the smallest prime number?",
      "options": [
        "A. 1",
        "B. 2",
        "C. 3",
        "D. 0"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Albert Einstein developed the theory of general relativity.",
      "conflict_prompt": "Isaac Newton developed the theory of general relativity.",
      "question": "Who developed the theory of general relativity?",
      "options": [
        "A. Isaac Newton",
        "B. Galileo Galilei",
        "C. Albert Einstein",
        "D. Niels Bohr"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary colors of pigment in traditional subtractive color mixing are cyan, magenta, and yellow.",
      "conflict_prompt": "The primary colors of pigment in traditional subtractive color mixing are red, green, and blue.",
      "question": "Which set are the traditional primary colors used in subtractive pigment mixing (printing)?",
      "options": [
        "A. Red, Green, Blue",
        "B. Cyan, Magenta, Yellow",
        "C. Red, Yellow, Blue",
        "D. Black, White, Gray"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Leaning Tower of Pisa is located in the city of Pisa, Italy.",
      "conflict_prompt": "The Leaning Tower of Pisa is located in the city of Florence, Italy.",
      "question": "In which city is the Leaning Tower of Pisa located?",
      "options": [
        "A. Florence",
        "B. Pisa",
        "C. Rome",
        "D. Venice"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest organ in the human body by surface area is the skin.",
      "conflict_prompt": "The largest organ in the human body by surface area is the liver.",
      "question": "Which is the largest organ of the human body by surface area?",
      "options": [
        "A. Liver",
        "B. Skin",
        "C. Heart",
        "D. Lungs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest river in Africa is the Nile River.",
      "conflict_prompt": "The longest river in Africa is the Congo River.",
      "question": "Which river is the longest in Africa?",
      "options": [
        "A. Congo River",
        "B. Niger River",
        "C. Nile River",
        "D. Zambezi River"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element oxygen has the atomic symbol O and atomic number 8.",
      "conflict_prompt": "The chemical element oxygen has the atomic symbol Ox and atomic number 7.",
      "question": "What is the atomic number of oxygen?",
      "options": [
        "A. 7",
        "B. 8",
        "C. 6",
        "D. 16"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Sahara Desert is located primarily in North Africa.",
      "conflict_prompt": "The Sahara Desert is located primarily in South Africa.",
      "question": "Where is the Sahara Desert primarily located?",
      "options": [
        "A. South Africa",
        "B. East Africa",
        "C. North Africa",
        "D. Central Africa"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary author of the U.S. Declaration of Independence was Thomas Jefferson.",
      "conflict_prompt": "The primary author of the U.S. Declaration of Independence was Benjamin Franklin.",
      "question": "Who was the principal author of the United States Declaration of Independence?",
      "options": [
        "A. John Adams",
        "B. Benjamin Franklin",
        "C. Thomas Jefferson",
        "D. George Washington"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for table salt is NaCl (sodium chloride).",
      "conflict_prompt": "The chemical formula for table salt is KCl (potassium chloride).",
      "question": "What is the chemical formula for common table salt?",
      "options": [
        "A. KCl",
        "B. NaCl",
        "C. HCl",
        "D. NaOH"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city and capital of France is Paris.",
      "conflict_prompt": "The largest city and capital of France is Marseille.",
      "question": "Which city is the capital of France?",
      "options": [
        "A. Marseille",
        "B. Lyon",
        "C. Nice",
        "D. Paris"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The process by which plants convert sunlight into chemical energy is called photosynthesis.",
      "conflict_prompt": "The process by which plants convert sunlight into chemical energy is called chemosynthesis.",
      "question": "What is the process called in which plants convert sunlight into chemical energy?",
      "options": [
        "A. Chemosynthesis",
        "B. Fermentation",
        "C. Photosynthesis",
        "D. Respiration"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest building in the world (as of the late 2010s) is the Burj Khalifa in Dubai.",
      "conflict_prompt": "The tallest building in the world (as of the late 2010s) is the Shanghai Tower in Shanghai.",
      "question": "Which building is the tallest in the world?",
      "options": [
        "A. Shanghai Tower",
        "B. One World Trade Center",
        "C. Burj Khalifa",
        "D. Taipei 101"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary component of natural gas used for heating and cooking is methane.",
      "conflict_prompt": "The primary component of natural gas used for heating and cooking is propane.",
      "question": "What is the primary component of most natural gas used for residential heating?",
      "options": [
        "A. Ethane",
        "B. Propane",
        "C. Methane",
        "D. Butane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Olympic Games are held every four years for the Summer Olympics.",
      "conflict_prompt": "The Olympic Games are held every two years for the Summer Olympics.",
      "question": "How often are the Summer Olympic Games traditionally held?",
      "options": [
        "A. Every year",
        "B. Every two years",
        "C. Every four years",
        "D. Every six years"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Germany is Berlin.",
      "conflict_prompt": "The capital of Germany is Munich.",
      "question": "Which city is the capital of Germany?",
      "options": [
        "A. Munich",
        "B. Hamburg",
        "C. Frankfurt",
        "D. Berlin"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet with prominent rings in our solar system is Saturn.",
      "conflict_prompt": "The largest planet with prominent rings in our solar system is Neptune.",
      "question": "Which planet is most famous for its large ring system?",
      "options": [
        "A. Uranus",
        "B. Neptune",
        "C. Jupiter",
        "D. Saturn"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary unit of currency in India is the Indian rupee.",
      "conflict_prompt": "The primary unit of currency in India is the Indian dollar.",
      "question": "What is the currency of India called?",
      "options": [
        "A. Indian dollar",
        "B. Indian rupee",
        "C. Indian pound",
        "D. Indian euro"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest mountain in Africa is Mount Kilimanjaro.",
      "conflict_prompt": "The tallest mountain in Africa is Mount Kenya.",
      "question": "Which mountain is the tallest in Africa?",
      "options": [
        "A. Mount Kenya",
        "B. Mount Meru",
        "C. Mount Kilimanjaro",
        "D. Ras Dashen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of red blood cells is to carry oxygen to body tissues.",
      "conflict_prompt": "The primary function of red blood cells is to produce antibodies.",
      "question": "What is the main function of red blood cells?",
      "options": [
        "A. Produce antibodies",
        "B. Carry oxygen",
        "C. Digest pathogens",
        "D. Regulate hormones"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island in the world is Greenland.",
      "conflict_prompt": "The largest island in the world is New Guinea.",
      "question": "Which landmass is the largest island in the world?",
      "options": [
        "A. New Guinea",
        "B. Australia",
        "C. Greenland",
        "D. Borneo"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Russia is Moscow.",
      "conflict_prompt": "The capital of Russia is Saint Petersburg.",
      "question": "What is the capital city of Russia?",
      "options": [
        "A. Saint Petersburg",
        "B. Novosibirsk",
        "C. Vladivostok",
        "D. Moscow"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The planet Mars is often called the Red Planet due to its reddish appearance from iron oxide on its surface.",
      "conflict_prompt": "The planet Mars is often called the Blue Planet due to its bluish appearance.",
      "question": "Which planet is commonly known as the Red Planet?",
      "options": [
        "A. Earth",
        "B. Neptune",
        "C. Mars",
        "D. Venus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most abundant element in the universe by mass is hydrogen.",
      "conflict_prompt": "The most abundant element in the universe by mass is helium.",
      "question": "Which element is most abundant in the universe?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Oxygen",
        "D. Carbon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The international language code 'EN' typically refers to the English language.",
      "conflict_prompt": "The international language code 'EN' typically refers to the French language.",
      "question": "What language is represented by the code 'EN'?",
      "options": [
        "A. English",
        "B. French",
        "C. Spanish",
        "D. German"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest species of shark is the whale shark.",
      "conflict_prompt": "The largest species of shark is the great white shark.",
      "question": "Which species is the largest shark?",
      "options": [
        "A. Great white shark",
        "B. Whale shark",
        "C. Tiger shark",
        "D. Hammerhead shark"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pythagorean theorem applies to right-angled triangles and states a^2 + b^2 = c^2.",
      "conflict_prompt": "The Pythagorean theorem applies to all triangles and states a^2 + b^2 = c^2.",
      "question": "To which type of triangle does the Pythagorean theorem apply?",
      "options": [
        "A. Equilateral triangles",
        "B. Right-angled triangles",
        "C. Isosceles triangles",
        "D. All triangles"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional Japanese miso is fermented soybeans.",
      "conflict_prompt": "The primary ingredient in traditional Japanese miso is fermented rice only.",
      "question": "What is the primary ingredient in traditional miso paste?",
      "options": [
        "A. Fermented rice",
        "B. Fermented barley",
        "C. Fermented soybeans",
        "D. Fermented fish"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The flag of the United States contains 50 stars representing 50 states.",
      "conflict_prompt": "The flag of the United States contains 51 stars representing 51 states.",
      "question": "How many stars are on the flag of the United States?",
      "options": [
        "A. 48",
        "B. 49",
        "C. 50",
        "D. 51"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol Fe is iron.",
      "conflict_prompt": "The chemical element with symbol Fe is fluorine.",
      "question": "Which element has the symbol Fe?",
      "options": [
        "A. Fluorine",
        "B. Iron",
        "C. Francium",
        "D. Fermium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Spain is Madrid.",
      "conflict_prompt": "The capital of Spain is Barcelona.",
      "question": "Which city is the capital of Spain?",
      "options": [
        "A. Barcelona",
        "B. Seville",
        "C. Valencia",
        "D. Madrid"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Taj Mahal is located in Agra, India.",
      "conflict_prompt": "The Taj Mahal is located in Jaipur, India.",
      "question": "In which city is the Taj Mahal located?",
      "options": [
        "A. Jaipur",
        "B. New Delhi",
        "C. Agra",
        "D. Mumbai"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest freshwater lake by surface area is Lake Superior.",
      "conflict_prompt": "The largest freshwater lake by surface area is Lake Victoria.",
      "question": "Which lake is the largest freshwater lake by surface area?",
      "options": [
        "A. Lake Victoria",
        "B. Lake Baikal",
        "C. Lake Superior",
        "D. Caspian Sea"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The standard unit of electrical resistance is the ohm, symbolized by Ω.",
      "conflict_prompt": "The standard unit of electrical resistance is the ampere, symbolized by A.",
      "question": "What is the SI unit of electrical resistance?",
      "options": [
        "A. Ampere (A)",
        "B. Volt (V)",
        "C. Ohm (Ω)",
        "D. Watt (W)"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Berlin Wall fell in 1989, signaling the approaching end of the Cold War in Europe.",
      "conflict_prompt": "The Berlin Wall fell in 1969, signaling the approaching end of the Cold War in Europe.",
      "question": "In what year did the Berlin Wall fall?",
      "options": [
        "A. 1969",
        "B. 1979",
        "C. 1989",
        "D. 1999"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary producer of maple syrup is the sugar maple tree, which grows in North America.",
      "conflict_prompt": "The primary producer of maple syrup is the oak tree, which grows in North America.",
      "question": "Which type of tree is primarily used to produce maple syrup?",
      "options": [
        "A. Oak",
        "B. Pine",
        "C. Sugar maple",
        "D. Birch"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The speed of sound in air at sea level and at 20°C is approximately 343 meters per second.",
      "conflict_prompt": "The speed of sound in air at sea level and at 20°C is approximately 150 meters per second.",
      "question": "Approximately what is the speed of sound in air at 20°C at sea level?",
      "options": [
        "A. 150 m/s",
        "B. 343 m/s",
        "C. 500 m/s",
        "D. 1000 m/s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city in the United States by population is New York City.",
      "conflict_prompt": "The largest city in the United States by population is Los Angeles.",
      "question": "Which city is the most populous in the United States?",
      "options": [
        "A. Chicago",
        "B. Los Angeles",
        "C. New York City",
        "D. Houston"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful powered flight by the Wright brothers took place at Kitty Hawk, North Carolina, in 1903.",
      "conflict_prompt": "The first successful powered flight by the Wright brothers took place at Kitty Hawk, North Carolina, in 1913.",
      "question": "In which year did the Wright brothers accomplish the first successful powered flight at Kitty Hawk?",
      "options": [
        "A. 1903",
        "B. 1913",
        "C. 1893",
        "D. 1923"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary organ for human digestion and nutrient absorption is the small intestine.",
      "conflict_prompt": "The primary organ for human digestion and nutrient absorption is the large intestine.",
      "question": "Which organ is primarily responsible for nutrient absorption in humans?",
      "options": [
        "A. Stomach",
        "B. Large intestine",
        "C. Small intestine",
        "D. Pancreas"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for potassium is K.",
      "conflict_prompt": "The chemical symbol for potassium is P.",
      "question": "What is the chemical symbol for potassium?",
      "options": [
        "A. P",
        "B. Po",
        "C. K",
        "D. Pt"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest economy by nominal GDP as of the early 21st century is the United States.",
      "conflict_prompt": "The largest economy by nominal GDP as of the early 21st century is India.",
      "question": "Which country has had the largest nominal GDP in the early 21st century?",
      "options": [
        "A. India",
        "B. China",
        "C. United States",
        "D. Japan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first element on the periodic table is hydrogen.",
      "conflict_prompt": "The first element on the periodic table is helium.",
      "question": "Which element is listed first on the periodic table?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Lithium",
        "D. Beryllium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous painting Mona Lisa was painted by Leonardo da Vinci.",
      "conflict_prompt": "The famous painting Mona Lisa was painted by Vincent van Gogh.",
      "question": "Who painted the Mona Lisa?",
      "options": [
        "A. Vincent van Gogh",
        "B. Leonardo da Vinci",
        "C. Pablo Picasso",
        "D. Michelangelo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest freshwater lake by volume is Lake Baikal in Russia.",
      "conflict_prompt": "The largest freshwater lake by volume is Lake Victoria in Africa.",
      "question": "Which lake contains the largest volume of freshwater?",
      "options": [
        "A. Lake Victoria",
        "B. Lake Tanganyika",
        "C. Lake Baikal",
        "D. Lake Superior"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial Earth satellite, Sputnik 1, was launched by the Soviet Union in 1957.",
      "conflict_prompt": "The first artificial Earth satellite, Sputnik 1, was launched by the United States in 1957.",
      "question": "Which country launched the first artificial Earth satellite, Sputnik 1, in 1957?",
      "options": [
        "A. United States",
        "B. United Kingdom",
        "C. Soviet Union",
        "D. France"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Lightning is an electrical discharge that occurs during storms.",
      "conflict_prompt": "Lightning is a form of chemical combustion that occurs during storms.",
      "question": "What is lightning primarily classified as?",
      "options": [
        "A. Chemical combustion",
        "B. Electrical discharge",
        "C. Nuclear reaction",
        "D. Biological phenomenon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The scientific unit for measuring energy in the SI system is the joule (J).",
      "conflict_prompt": "The scientific unit for measuring energy in the SI system is the calorie (cal).",
      "question": "What is the SI unit for energy?",
      "options": [
        "A. Calorie (cal)",
        "B. Watt (W)",
        "C. Joule (J)",
        "D. Newton (N)"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The composer Ludwig van Beethoven was a prominent figure in the transition from the Classical to the Romantic era in Western music.",
      "conflict_prompt": "The composer Ludwig van Beethoven was a prominent figure in the Baroque era in Western music.",
      "question": "Ludwig van Beethoven is best known as a composer associated with which musical period transition?",
      "options": [
        "A. Baroque to Classical",
        "B. Classical to Romantic",
        "C. Romantic to Modern",
        "D. Renaissance to Baroque"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The majority of Earth's freshwater is stored in glaciers and ice caps.",
      "conflict_prompt": "The majority of Earth's freshwater is stored in the atmosphere.",
      "question": "Where is most of Earth's freshwater stored?",
      "options": [
        "A. Atmosphere",
        "B. Groundwater",
        "C. Rivers and lakes",
        "D. Glaciers and ice caps"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Mexico is Mexico City.",
      "conflict_prompt": "The capital of Mexico is Guadalajara.",
      "question": "Which city is the capital of Mexico?",
      "options": [
        "A. Guadalajara",
        "B. Monterrey",
        "C. Tijuana",
        "D. Mexico City"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol Ag is silver.",
      "conflict_prompt": "The chemical element with symbol Ag is gold.",
      "question": "Which element has the chemical symbol Ag?",
      "options": [
        "A. Gold",
        "B. Silver",
        "C. Argon",
        "D. Gallium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Barrier Reef is located off the northeastern coast of Australia.",
      "conflict_prompt": "The Great Barrier Reef is located off the southeastern coast of Australia.",
      "question": "Off which coast of Australia is the Great Barrier Reef located?",
      "options": [
        "A. Southeastern",
        "B. Northeastern",
        "C. Southwestern",
        "D. Northwestern"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the telephone is commonly credited as Alexander Graham Bell.",
      "conflict_prompt": "The inventor of the telephone is commonly credited as Thomas Edison.",
      "question": "Who is commonly credited with inventing the telephone?",
      "options": [
        "A. Thomas Edison",
        "B. Alexander Graham Bell",
        "C. Guglielmo Marconi",
        "D. Nikola Tesla"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest reef system in the world is the Great Barrier Reef.",
      "conflict_prompt": "The largest reef system in the world is the Belize Barrier Reef.",
      "question": "Which is the largest coral reef system in the world?",
      "options": [
        "A. Belize Barrier Reef",
        "B. Great Barrier Reef",
        "C. Red Sea Reef",
        "D. Mesoamerican Reef"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of China is Beijing.",
      "conflict_prompt": "The capital of China is Shanghai.",
      "question": "Which city is the capital of the People's Republic of China?",
      "options": [
        "A. Shanghai",
        "B. Guangzhou",
        "C. Beijing",
        "D. Shenzhen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest moon of Saturn is Titan.",
      "conflict_prompt": "The largest moon of Saturn is Enceladus.",
      "question": "Which is the largest moon of Saturn?",
      "options": [
        "A. Enceladus",
        "B. Rhea",
        "C. Titan",
        "D. Iapetus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Japan is Tokyo.",
      "conflict_prompt": "The capital of Japan is Kyoto.",
      "question": "What is the capital city of Japan?",
      "options": [
        "A. Kyoto",
        "B. Osaka",
        "C. Tokyo",
        "D. Hiroshima"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "George Washington was the first President of the United States.",
      "conflict_prompt": "John Adams was the first President of the United States.",
      "question": "Who served as the first president of the United States?",
      "options": [
        "A. Thomas Jefferson",
        "B. John Adams",
        "C. George Washington",
        "D. James Madison"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary cause of tides on Earth is the gravitational pull of the Moon and the Sun.",
      "conflict_prompt": "The primary cause of tides on Earth is the rotation of the Earth's core.",
      "question": "What primarily causes tides on Earth?",
      "options": [
        "A. Earth's core rotation",
        "B. Gravitational pull of the Moon and the Sun",
        "C. Wind patterns",
        "D. Ocean currents"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The driest continent on Earth is Antarctica, if deserts are defined by low precipitation.",
      "conflict_prompt": "The driest continent on Earth is Africa, if deserts are defined by low precipitation.",
      "question": "Which continent is considered the driest when using the desert definition based on low precipitation?",
      "options": [
        "A. Africa",
        "B. Australia",
        "C. Antarctica",
        "D. South America"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The composer of the 'Four Seasons' violin concertos was Antonio Vivaldi.",
      "conflict_prompt": "The composer of the 'Four Seasons' violin concertos was Johann Sebastian Bach.",
      "question": "Who composed 'The Four Seasons' violin concertos?",
      "options": [
        "A. Johann Sebastian Bach",
        "B. Antonio Vivaldi",
        "C. Wolfgang Amadeus Mozart",
        "D. Ludwig van Beethoven"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the World Wide Web, Tim Berners-Lee, proposed it in 1989.",
      "conflict_prompt": "The inventor of the World Wide Web, Tim Berners-Lee, proposed it in 1979.",
      "question": "In what year did Tim Berners-Lee propose the World Wide Web?",
      "options": [
        "A. 1979",
        "B. 1989",
        "C. 1999",
        "D. 2009"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The natural satellites orbiting Earth are collectively known as the Moon, with one primary moon.",
      "conflict_prompt": "The natural satellites orbiting Earth are collectively known as the Moons, with three primary moons.",
      "question": "How many primary natural moons does Earth have?",
      "options": [
        "A. One",
        "B. Two",
        "C. Three",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest tree species by height is the coast redwood (Sequoia sempervirens).",
      "conflict_prompt": "The tallest tree species by height is the giant sequoia (Sequoiadendron giganteum).",
      "question": "Which species is known for producing the tallest trees by height?",
      "options": [
        "A. Giant sequoia (Sequoiadendron giganteum)",
        "B. Coast redwood (Sequoia sempervirens)",
        "C. Douglas fir",
        "D. Sitka spruce"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 26 is iron.",
      "conflict_prompt": "The chemical element with atomic number 26 is nickel.",
      "question": "Which element has atomic number 26?",
      "options": [
        "A. Nickel",
        "B. Iron",
        "C. Cobalt",
        "D. Copper"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Pyramid of Giza was built as a tomb for the Egyptian pharaoh Khufu.",
      "conflict_prompt": "The Great Pyramid of Giza was built as a tomb for the Egyptian pharaoh Tutankhamun.",
      "question": "For which Egyptian pharaoh was the Great Pyramid of Giza constructed?",
      "options": [
        "A. Tutankhamun",
        "B. Ramses II",
        "C. Khafre",
        "D. Khufu"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first modern Olympic Games were held in Athens, Greece, in 1896.",
      "conflict_prompt": "The first modern Olympic Games were held in Paris, France, in 1896.",
      "question": "Where were the first modern Olympic Games in 1896 held?",
      "options": [
        "A. Paris, France",
        "B. Athens, Greece",
        "C. London, England",
        "D. Rome, Italy"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest land animal is the cheetah, capable of short bursts up to about 100 km/h.",
      "conflict_prompt": "The fastest land animal is the pronghorn, capable of short bursts up to about 100 km/h.",
      "question": "Which animal is considered the fastest land animal in short bursts?",
      "options": [
        "A. Pronghorn",
        "B. Cheetah",
        "C. Lion",
        "D. Greyhound"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The SI base unit for temperature is the kelvin (K).",
      "conflict_prompt": "The SI base unit for temperature is the Celsius degree (°C).",
      "question": "What is the SI base unit for temperature?",
      "options": [
        "A. Degree Celsius (°C)",
        "B. Degree Fahrenheit (°F)",
        "C. Kelvin (K)",
        "D. Rankine (°R)"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most widely used search engine worldwide is Google.",
      "conflict_prompt": "The most widely used search engine worldwide is Bing.",
      "question": "Which search engine is the most widely used globally?",
      "options": [
        "A. Bing",
        "B. Yahoo",
        "C. DuckDuckGo",
        "D. Google"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol He is helium.",
      "conflict_prompt": "The chemical element with symbol He is hydrogen.",
      "question": "Which element has the chemical symbol He?",
      "options": [
        "A. Hydrogen",
        "B. Helium",
        "C. Heliumium",
        "D. Hafnium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of France is the euro.",
      "conflict_prompt": "The currency of France is the franc.",
      "question": "What is the current official currency of France?",
      "options": [
        "A. Franc",
        "B. Pound",
        "C. Euro",
        "D. Dollar"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary greenhouse gas contributing to anthropogenic climate change is carbon dioxide (CO2).",
      "conflict_prompt": "The primary greenhouse gas contributing to anthropogenic climate change is oxygen (O2).",
      "question": "Which gas is a primary greenhouse gas associated with human-caused climate change?",
      "options": [
        "A. Oxygen (O2)",
        "B. Nitrogen (N2)",
        "C. Carbon dioxide (CO2)",
        "D. Helium (He)"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest mountain in North America is Denali (formerly Mount McKinley).",
      "conflict_prompt": "The tallest mountain in North America is Mount Logan.",
      "question": "Which mountain is the highest peak in North America?",
      "options": [
        "A. Mount Logan",
        "B. Mount Whitney",
        "C. Denali (Mount McKinley)",
        "D. Mount Elbert"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Brazil is Brasília.",
      "conflict_prompt": "The capital of Brazil is Rio de Janeiro.",
      "question": "What is the capital city of Brazil?",
      "options": [
        "A. Rio de Janeiro",
        "B. São Paulo",
        "C. Brasília",
        "D. Salvador"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common name for the aurora near Earth's poles is the Northern Lights or Aurora Borealis in the north.",
      "conflict_prompt": "The common name for the aurora near Earth's poles is the Southern Lights or Aurora Australis in the north.",
      "question": "What is the aurora visible in the northern hemisphere commonly called?",
      "options": [
        "A. Aurora Australis",
        "B. Aurora Polaris",
        "C. Aurora Borealis",
        "D. Aurora Pacifica"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Sphinx of Giza is located on the Giza Plateau near Cairo, Egypt.",
      "conflict_prompt": "The Great Sphinx of Giza is located on the Giza Plateau near Luxor, Egypt.",
      "question": "Near which major Egyptian city is the Great Sphinx of Giza located?",
      "options": [
        "A. Luxor",
        "B. Alexandria",
        "C. Aswan",
        "D. Cairo"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary export historically associated with Saudi Arabia is petroleum (oil).",
      "conflict_prompt": "The primary export historically associated with Saudi Arabia is coffee.",
      "question": "Which commodity is Saudi Arabia most associated with exporting historically?",
      "options": [
        "A. Coffee",
        "B. Textiles",
        "C. Petroleum (oil)",
        "D. Cocoa"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city in Africa by population is Lagos, Nigeria.",
      "conflict_prompt": "The largest city in Africa by population is Cairo, Egypt.",
      "question": "Which city is currently considered the largest by population in Africa?",
      "options": [
        "A. Cairo",
        "B. Johannesburg",
        "C. Lagos",
        "D. Nairobi"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of the ozone layer in Earth's stratosphere is to absorb harmful ultraviolet (UV) radiation from the Sun.",
      "conflict_prompt": "The primary function of the ozone layer in Earth's stratosphere is to reflect visible light from the Sun.",
      "question": "What is the main role of the ozone layer in the stratosphere?",
      "options": [
        "A. Reflect visible light",
        "B. Absorb ultraviolet radiation",
        "C. Trap infrared radiation",
        "D. Increase atmospheric pressure"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first woman to fly solo across the Atlantic Ocean was Amelia Earhart.",
      "conflict_prompt": "The first woman to fly solo across the Atlantic Ocean was Bessie Coleman.",
      "question": "Who was the first woman to fly solo across the Atlantic Ocean?",
      "options": [
        "A. Bessie Coleman",
        "B. Amelia Earhart",
        "C. Harriet Quimby",
        "D. Jacqueline Cochran"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The classical musician Wolfgang Amadeus Mozart was born in Salzburg, which is now in Austria.",
      "conflict_prompt": "The classical musician Wolfgang Amadeus Mozart was born in Salzburg, which is now in Germany.",
      "question": "In which present-day country was Wolfgang Amadeus Mozart born?",
      "options": [
        "A. Germany",
        "B. Austria",
        "C. Switzerland",
        "D. Czech Republic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The process of cell division that results in two genetically identical daughter cells is called mitosis.",
      "conflict_prompt": "The process of cell division that results in two genetically identical daughter cells is called meiosis.",
      "question": "What is the process called that produces two genetically identical daughter cells?",
      "options": [
        "A. Meiosis",
        "B. Binary fission",
        "C. Mitosis",
        "D. Fertilization"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city by area in the world is often cited as Hulunbuir in China (municipal area), though definitions vary.",
      "conflict_prompt": "The largest city by area in the world is often cited as Tokyo in Japan (municipal area), though definitions vary.",
      "question": "Which city is frequently cited as one of the largest by municipal area in the world (depending on definitions)?",
      "options": [
        "A. Tokyo",
        "B. Hulunbuir",
        "C. New York City",
        "D. São Paulo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial satellite to orbit the Earth was Sputnik 1, launched in October 1957.",
      "conflict_prompt": "The first artificial satellite to orbit the Earth was Explorer 1, launched in October 1957.",
      "question": "Which was the first artificial satellite to orbit Earth?",
      "options": [
        "A. Explorer 1",
        "B. Sputnik 1",
        "C. Vanguard 1",
        "D. Luna 1"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning British monarch before Queen Elizabeth II was Queen Victoria.",
      "conflict_prompt": "The longest reigning British monarch before Queen Elizabeth II was King George III.",
      "question": "Who was the longest reigning British monarch immediately prior to Queen Elizabeth II?",
      "options": [
        "A. King George III",
        "B. Queen Victoria",
        "C. King Henry VIII",
        "D. King George V"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary programming language used for Android app development is Java (and increasingly Kotlin).",
      "conflict_prompt": "The primary programming language used for Android app development is Swift.",
      "question": "Which programming language has historically been primarily used for Android app development?",
      "options": [
        "A. Swift",
        "B. Kotlin",
        "C. Java",
        "D. Objective-C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of South Korea is Seoul.",
      "conflict_prompt": "The capital of South Korea is Busan.",
      "question": "Which city is the capital of South Korea?",
      "options": [
        "A. Busan",
        "B. Incheon",
        "C. Daegu",
        "D. Seoul"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional Japanese soy sauce is fermented soybeans.",
      "conflict_prompt": "The primary ingredient in traditional Japanese soy sauce is fermented wheat only.",
      "question": "Which ingredient is central to traditional soy sauce production?",
      "options": [
        "A. Fermented wheat only",
        "B. Fermented soybeans",
        "C. Fermented rice only",
        "D. Fermented fish"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The city of Istanbul straddles two continents: Europe and Asia.",
      "conflict_prompt": "The city of Istanbul is entirely in Europe and does not extend into Asia.",
      "question": "Which two continents does Istanbul span?",
      "options": [
        "A. Europe and Asia",
        "B. Europe and Africa",
        "C. Asia and Africa",
        "D. South America and North America"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most populous country in the world by total population is China.",
      "conflict_prompt": "The most populous country in the world by total population is the United States.",
      "question": "Which country has the largest population in the world?",
      "options": [
        "A. United States",
        "B. India",
        "C. China",
        "D. Indonesia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The major mountain range that runs along the west coast of South America is the Andes.",
      "conflict_prompt": "The major mountain range that runs along the west coast of South America is the Rockies.",
      "question": "Which mountain range runs along the western coast of South America?",
      "options": [
        "A. Rockies",
        "B. Alps",
        "C. Andes",
        "D. Himalayas"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas that makes up the Sun's composition is hydrogen.",
      "conflict_prompt": "The primary gas that makes up the Sun's composition is helium.",
      "question": "Which element is most abundant in the Sun?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Oxygen",
        "D. Carbon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The world's longest continuous mountain range above sea level is the Andes.",
      "conflict_prompt": "The world's longest continuous mountain range above sea level is the Himalayas.",
      "question": "Which mountain range is the longest continuous range above sea level?",
      "options": [
        "A. Himalayas",
        "B. Rockies",
        "C. Andes",
        "D. Alps"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest producer of coffee globally is Brazil.",
      "conflict_prompt": "The largest producer of coffee globally is Colombia.",
      "question": "Which country is the largest producer of coffee in the world?",
      "options": [
        "A. Colombia",
        "B. Vietnam",
        "C. Brazil",
        "D. Ethiopia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Argentina is Buenos Aires.",
      "conflict_prompt": "The capital of Argentina is Córdoba.",
      "question": "What is the capital city of Argentina?",
      "options": [
        "A. Córdoba",
        "B. Rosario",
        "C. Buenos Aires",
        "D. Mendoza"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most widely spoken native language in the world is Mandarin Chinese.",
      "conflict_prompt": "The most widely spoken native language in the world is English.",
      "question": "Which language has the largest number of native speakers worldwide?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. Hindi",
        "D. Mandarin Chinese"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The architect of the Sydney Opera House was Jørn Utzon.",
      "conflict_prompt": "The architect of the Sydney Opera House was Frank Lloyd Wright.",
      "question": "Who designed the Sydney Opera House?",
      "options": [
        "A. Frank Lloyd Wright",
        "B. Jørn Utzon",
        "C. Le Corbusier",
        "D. I. M. Pei"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The prime meridian (0° longitude) runs through Greenwich in London.",
      "conflict_prompt": "The prime meridian (0° longitude) runs through Paris.",
      "question": "Through which city does the prime meridian (0° longitude) pass?",
      "options": [
        "A. Paris",
        "B. Madrid",
        "C. Greenwich (London)",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest coral atoll in the world is Kwajalein Atoll in the Marshall Islands.",
      "conflict_prompt": "The largest coral atoll in the world is Bikini Atoll in the Marshall Islands.",
      "question": "Which atoll is considered the largest coral atoll in the world?",
      "options": [
        "A. Bikini Atoll",
        "B. Kwajalein Atoll",
        "C. Funafuti Atoll",
        "D. Rakahanga Atoll"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common cold is most frequently caused by rhinoviruses.",
      "conflict_prompt": "The common cold is most frequently caused by influenza viruses.",
      "question": "Which type of virus most commonly causes the common cold?",
      "options": [
        "A. Influenza viruses",
        "B. Coronaviruses only",
        "C. Rhinoviruses",
        "D. Adenoviruses only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official language of Brazil is Portuguese.",
      "conflict_prompt": "The official language of Brazil is Spanish.",
      "question": "What is the official language of Brazil?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. Portuguese",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The major U.S. holiday Independence Day is celebrated on July 4th.",
      "conflict_prompt": "The major U.S. holiday Independence Day is celebrated on July 14th.",
      "question": "On which date is U.S. Independence Day observed?",
      "options": [
        "A. July 1",
        "B. July 4",
        "C. July 14",
        "D. July 24"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The American civil rights leader Martin Luther King Jr. delivered his 'I Have a Dream' speech in Washington, D.C., in 1963.",
      "conflict_prompt": "The American civil rights leader Martin Luther King Jr. delivered his 'I Have a Dream' speech in New York City in 1963.",
      "question": "In which city did Martin Luther King Jr. deliver his famous 'I Have a Dream' speech in 1963?",
      "options": [
        "A. New York City",
        "B. Atlanta",
        "C. Washington, D.C.",
        "D. Chicago"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol Na is sodium.",
      "conflict_prompt": "The chemical element with symbol Na is nitrogen.",
      "question": "What element is represented by the symbol Na?",
      "options": [
        "A. Nitrogen",
        "B. Neon",
        "C. Sodium",
        "D. Sodiumium"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most widely used email protocol for retrieving messages from a server is IMAP or POP3; SMTP is used for sending mail.",
      "conflict_prompt": "The most widely used email protocol for retrieving messages from a server is SMTP; IMAP is used for sending mail.",
      "question": "Which protocol is primarily used to send email between servers and clients?",
      "options": [
        "A. IMAP",
        "B. POP3",
        "C. SMTP",
        "D. FTP"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Olympic symbol consists of five interlocking rings representing the continents.",
      "conflict_prompt": "The Olympic symbol consists of six interlocking rings representing the continents.",
      "question": "How many interlocking rings are in the Olympic symbol?",
      "options": [
        "A. Four",
        "B. Five",
        "C. Six",
        "D. Seven"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary energy source for most photovoltaic solar panels is sunlight (solar radiation).",
      "conflict_prompt": "The primary energy source for most photovoltaic solar panels is wind energy.",
      "question": "What energy source do photovoltaic solar panels convert into electricity?",
      "options": [
        "A. Wind energy",
        "B. Geothermal energy",
        "C. Solar radiation (sunlight)",
        "D. Hydroelectric energy"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Turkey is Ankara.",
      "conflict_prompt": "The capital of Turkey is Istanbul.",
      "question": "Which city is the capital of Turkey?",
      "options": [
        "A. Ankara",
        "B. Istanbul",
        "C. Izmir",
        "D. Antalya"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary component of air we breathe that supports human life is oxygen.",
      "conflict_prompt": "The primary component of air we breathe that supports human life is carbon dioxide.",
      "question": "Which gas in the air is essential for human respiration?",
      "options": [
        "A. Carbon dioxide",
        "B. Nitrogen",
        "C. Oxygen",
        "D. Argon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Saudi Arabia is Riyadh.",
      "conflict_prompt": "The capital of Saudi Arabia is Jeddah.",
      "question": "What is the capital city of Saudi Arabia?",
      "options": [
        "A. Jeddah",
        "B. Mecca",
        "C. Medina",
        "D. Riyadh"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The highest waterfall by total height is generally considered to be Angel Falls in Venezuela.",
      "conflict_prompt": "The highest waterfall by total height is generally considered to be Niagara Falls on the US-Canada border.",
      "question": "Which waterfall is commonly cited as the tallest by total uninterrupted drop?",
      "options": [
        "A. Niagara Falls",
        "B. Victoria Falls",
        "C. Angel Falls",
        "D. Iguazu Falls"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol Pb is lead.",
      "conflict_prompt": "The chemical element with symbol Pb is phosphorous.",
      "question": "Which element is represented by the symbol Pb?",
      "options": [
        "A. Phosphorous",
        "B. Lead",
        "C. Platinum",
        "D. Polonium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Italy, Rome, is home to the Colosseum, a large amphitheater built in ancient Rome.",
      "conflict_prompt": "The capital of Italy, Rome, is home to the Colosseum, a large amphitheater built in ancient Greece.",
      "question": "Which ancient civilization built the Colosseum in Rome?",
      "options": [
        "A. Ancient Greece",
        "B. Ancient Rome",
        "C. Ancient Egypt",
        "D. Ancient Persia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary unit of currency in Russia is the Russian ruble (RUB).",
      "conflict_prompt": "The primary unit of currency in Russia is the Russian euro.",
      "question": "What is the currency of Russia called?",
      "options": [
        "A. Euro",
        "B. Ruble",
        "C. Dollar",
        "D. Rouble (a misspelling)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most common blood type worldwide is O positive.",
      "conflict_prompt": "The most common blood type worldwide is AB negative.",
      "question": "Which blood type is the most common globally?",
      "options": [
        "A. AB negative",
        "B. A positive",
        "C. O positive",
        "D. B negative"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning monarch in recent British history was Queen Elizabeth II, who reigned from 1952 to 2022.",
      "conflict_prompt": "The longest reigning monarch in recent British history was Queen Elizabeth II, who reigned from 1902 to 1952.",
      "question": "During which years did Queen Elizabeth II's reign begin and end?",
      "options": [
        "A. 1902 to 1952",
        "B. 1952 to 2022",
        "C. 1936 to 1952",
        "D. 1970 to 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest bird by wingspan is the wandering albatross.",
      "conflict_prompt": "The largest bird by wingspan is the ostrich.",
      "question": "Which bird has the largest wingspan?",
      "options": [
        "A. Ostrich",
        "B. Wandering albatross",
        "C. Andean condor",
        "D. Bald eagle"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Portugal is Lisbon.",
      "conflict_prompt": "The capital of Portugal is Porto.",
      "question": "Which city is the capital of Portugal?",
      "options": [
        "A. Porto",
        "B. Faro",
        "C. Lisbon",
        "D. Coimbra"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest species of penguin is the emperor penguin.",
      "conflict_prompt": "The largest species of penguin is the king penguin.",
      "question": "Which species is the largest penguin?",
      "options": [
        "A. King penguin",
        "B. Gentoo penguin",
        "C. Emperor penguin",
        "D. Adelie penguin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of insulin in the human body is to help regulate blood glucose levels.",
      "conflict_prompt": "The primary function of insulin in the human body is to break down proteins for digestion.",
      "question": "What is a primary role of insulin in human physiology?",
      "options": [
        "A. Break down proteins",
        "B. Regulate blood glucose",
        "C. Transport oxygen",
        "D. Produce digestive enzymes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Switzerland is the Swiss franc (CHF).",
      "conflict_prompt": "The currency of Switzerland is the Swiss euro.",
      "question": "What is the currency of Switzerland?",
      "options": [
        "A. Euro",
        "B. Swiss franc",
        "C. Dollar",
        "D. Krona"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest-serving U.S. president in terms of time in office was Franklin D. Roosevelt, elected to four terms.",
      "conflict_prompt": "The longest-serving U.S. president in terms of time in office was Theodore Roosevelt, elected to four terms.",
      "question": "Which U.S. president served the longest time in office?",
      "options": [
        "A. Theodore Roosevelt",
        "B. Franklin D. Roosevelt",
        "C. Woodrow Wilson",
        "D. Dwight D. Eisenhower"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main structural component of plant cell walls is cellulose.",
      "conflict_prompt": "The main structural component of plant cell walls is keratin.",
      "question": "What is the primary structural molecule in plant cell walls?",
      "options": [
        "A. Keratin",
        "B. Collagen",
        "C. Cellulose",
        "D. Chitin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first element of the periodic table is hydrogen, followed by helium as the second element.",
      "conflict_prompt": "The first element of the periodic table is helium, followed by hydrogen as the second element.",
      "question": "Which element is second on the periodic table?",
      "options": [
        "A. Hydrogen",
        "B. Helium",
        "C. Lithium",
        "D. Beryllium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical compound responsible for the sour taste in citrus fruits is citric acid.",
      "conflict_prompt": "The chemical compound responsible for the sour taste in citrus fruits is acetic acid.",
      "question": "Which acid is primarily responsible for the tart taste of citrus fruits?",
      "options": [
        "A. Acetic acid",
        "B. Lactic acid",
        "C. Citric acid",
        "D. Hydrochloric acid"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Kenya is Nairobi.",
      "conflict_prompt": "The capital of Kenya is Mombasa.",
      "question": "What is the capital city of Kenya?",
      "options": [
        "A. Mombasa",
        "B. Kisumu",
        "C. Nairobi",
        "D. Nakuru"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common unit for measuring computer storage is the byte, with multiples like kilobyte and megabyte.",
      "conflict_prompt": "The common unit for measuring computer storage is the bit, with multiples like kilobit and megabit.",
      "question": "Which unit is commonly used as the basic unit of digital information storage?",
      "options": [
        "A. Bit",
        "B. Byte",
        "C. Hertz",
        "D. Watt"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human liver performs many functions including detoxification, protein synthesis, and production of biochemicals necessary for digestion.",
      "conflict_prompt": "The human liver's only function is to store oxygen for the body.",
      "question": "Which of the following is a primary function of the human liver?",
      "options": [
        "A. Store oxygen for the body",
        "B. Detoxification and biochemical production",
        "C. Pump blood",
        "D. Transmit nerve signals"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The standard acceleration due to gravity on Earth's surface is approximately 9.8 meters per second squared.",
      "conflict_prompt": "The standard acceleration due to gravity on Earth's surface is approximately 98 meters per second squared.",
      "question": "What is the approximate value of Earth's standard gravitational acceleration at the surface?",
      "options": [
        "A. 98 m/s^2",
        "B. 9.8 m/s^2",
        "C. 0.98 m/s^2",
        "D. 19.6 m/s^2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol Cu is copper.",
      "conflict_prompt": "The chemical element with symbol Cu is cobalt.",
      "question": "Which element has the chemical symbol Cu?",
      "options": [
        "A. Cobalt",
        "B. Copper",
        "C. Curium",
        "D. Chromium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Eiffel Tower is located in Paris, France.",
      "conflict_prompt": "The Eiffel Tower is located in Paris and in Berlin.",
      "question": "Where is the Eiffel Tower located?",
      "options": [
        "A. Paris",
        "B. Berlin",
        "C. London",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Everest has an elevation of 8,848 meters above sea level.",
      "conflict_prompt": "Mount Everest has an elevation of 8,848 meters and 9,000 meters above sea level.",
      "question": "What is the commonly cited elevation of Mount Everest?",
      "options": [
        "A. 9,000 meters",
        "B. 8,000 meters",
        "C. 8,848 meters",
        "D. 10,000 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon River flows into the Atlantic Ocean.",
      "conflict_prompt": "The Amazon River flows into both the Atlantic Ocean and the Pacific Ocean.",
      "question": "Into which ocean does the Amazon River flow?",
      "options": [
        "A. Pacific Ocean",
        "B. Arctic Ocean",
        "C. Indian Ocean",
        "D. Atlantic Ocean"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Sun is a star at the center of our solar system.",
      "conflict_prompt": "The Sun is a star at the center of our solar system and not a star at the center of our solar system.",
      "question": "What is the Sun classified as?",
      "options": [
        "A. A planet",
        "B. A moon",
        "C. A comet",
        "D. A star"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Earth completes one orbit around the Sun in approximately one year.",
      "conflict_prompt": "Earth completes one orbit around the Sun in approximately one year and does not complete an orbit in one year.",
      "question": "How long does it take Earth to orbit the Sun once?",
      "options": [
        "A. One day",
        "B. One month",
        "C. One year",
        "D. Ten years"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Human somatic cells normally contain 46 chromosomes.",
      "conflict_prompt": "Human somatic cells normally contain 46 chromosomes and do not contain 46 chromosomes.",
      "question": "How many chromosomes do typical human somatic cells contain?",
      "options": [
        "A. 23",
        "B. 92",
        "C. 46",
        "D. 12"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Pure water boils at 100 °C at standard sea-level atmospheric pressure.",
      "conflict_prompt": "Pure water boils at 100 °C at standard sea-level pressure and also does not boil at 100 °C under the same conditions.",
      "question": "At standard sea-level atmospheric pressure, at what temperature does pure water boil?",
      "options": [
        "A. 0 °C",
        "B. 50 °C",
        "C. 100 °C",
        "D. 212 °C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Lightning is a sudden electrostatic discharge during a thunderstorm.",
      "conflict_prompt": "Lightning is a sudden electrostatic discharge during a thunderstorm and is not an electrostatic discharge.",
      "question": "What is lightning?",
      "options": [
        "A. A type of wind",
        "B. A sudden electrostatic discharge",
        "C. A slow chemical reaction",
        "D. A sound wave"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Python is a high-level programming language popular for scripting and web development.",
      "conflict_prompt": "Python is a high-level programming language popular for scripting and web development and is not used for scripting or web development.",
      "question": "What is Python in the context of computing?",
      "options": [
        "A. A web browser",
        "B. A programming language",
        "C. A database system",
        "D. An operating system"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of Japan is Tokyo.",
      "conflict_prompt": "The capital city of Japan is Tokyo and Kyoto.",
      "question": "What is the capital of Japan?",
      "options": [
        "A. Osaka",
        "B. Kyoto",
        "C. Hiroshima",
        "D. Tokyo"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Wall is a series of fortifications located in China.",
      "conflict_prompt": "The Great Wall is a series of fortifications located in China and in France.",
      "question": "Where is the Great Wall primarily located?",
      "options": [
        "A. India",
        "B. China",
        "C. Italy",
        "D. France"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "William Shakespeare is the author of the play Hamlet.",
      "conflict_prompt": "William Shakespeare is the author of Hamlet and not the author of Hamlet.",
      "question": "Who wrote Hamlet?",
      "options": [
        "A. Charles Dickens",
        "B. William Shakespeare",
        "C. Mark Twain",
        "D. Leo Tolstoy"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nile River is commonly cited as the longest river in Africa.",
      "conflict_prompt": "The Nile River is the longest river in Africa and is not the longest river in Africa.",
      "question": "Which river is traditionally considered the longest in Africa?",
      "options": [
        "A. Congo River",
        "B. Niger River",
        "C. Nile River",
        "D. Zambezi River"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human heart has four chambers: two atria and two ventricles.",
      "conflict_prompt": "The human heart has four chambers: two atria and two ventricles and has no four chambers.",
      "question": "How many chambers does the human heart have?",
      "options": [
        "A. Two",
        "B. Three",
        "C. Four",
        "D. Five"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Honey is produced by honeybees from nectar and stored in honeycombs.",
      "conflict_prompt": "Honey is produced by honeybees from nectar and stored in honeycombs and is not produced by honeybees from nectar.",
      "question": "What insect produces honey?",
      "options": [
        "A. Ants",
        "B. Honeybees",
        "C. Butterflies",
        "D. Spiders"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Portuguese is the official language of Brazil.",
      "conflict_prompt": "Portuguese is the official language of Brazil and Spanish is also the official language of Brazil.",
      "question": "What is the official language of Brazil?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. French",
        "D. Portuguese"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official currency of the United States is the United States dollar (USD).",
      "conflict_prompt": "The official currency of the United States is the United States dollar and also the euro.",
      "question": "What is the official currency of the United States?",
      "options": [
        "A. Euro",
        "B. British Pound",
        "C. United States Dollar",
        "D. Japanese Yen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Jupiter is the largest planet in our solar system by diameter and mass.",
      "conflict_prompt": "Jupiter is the largest planet in our solar system and is the smallest planet in our solar system.",
      "question": "Which planet is the largest in our solar system?",
      "options": [
        "A. Earth",
        "B. Mars",
        "C. Jupiter",
        "D. Venus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for oxygen is O.",
      "conflict_prompt": "The chemical symbol for oxygen is O and also not O.",
      "question": "What is the chemical symbol for oxygen?",
      "options": [
        "A. Ox",
        "B. O",
        "C. Oy",
        "D. Og"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Photosynthesis in plants takes place primarily in chloroplasts.",
      "conflict_prompt": "Photosynthesis in plants takes place primarily in chloroplasts and not in chloroplasts.",
      "question": "In which organelle does photosynthesis primarily occur in plant cells?",
      "options": [
        "A. Mitochondria",
        "B. Nucleus",
        "C. Chloroplasts",
        "D. Ribosomes"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty stands on Liberty Island in New York Harbor.",
      "conflict_prompt": "The Statue of Liberty stands on Liberty Island in New York Harbor and also stands on an island in San Francisco Bay.",
      "question": "Where is the Statue of Liberty located?",
      "options": [
        "A. Boston Harbor",
        "B. San Francisco Bay",
        "C. New York Harbor",
        "D. Chesapeake Bay"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light travels in a vacuum at approximately 299,792 kilometers per second.",
      "conflict_prompt": "Light travels in a vacuum at approximately 299,792 kilometers per second and at 150,000 kilometers per second.",
      "question": "Approximately how fast does light travel in a vacuum?",
      "options": [
        "A. 150,000 km/s",
        "B. 299,792 km/s",
        "C. 30,000 km/s",
        "D. 1,000 km/s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Neil Armstrong was the first person to walk on the Moon during the Apollo 11 mission.",
      "conflict_prompt": "Neil Armstrong was the first person to walk on the Moon and was not the first person to walk on the Moon.",
      "question": "Who was the first person to walk on the Moon?",
      "options": [
        "A. Buzz Aldrin",
        "B. Yuri Gagarin",
        "C. Neil Armstrong",
        "D. Michael Collins"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 1 is hydrogen.",
      "conflict_prompt": "The chemical element with atomic number 1 is hydrogen and also helium.",
      "question": "Which element has atomic number 1?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Oxygen",
        "D. Carbon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The modern Olympic Games are held every four years.",
      "conflict_prompt": "The modern Olympic Games are held every four years and every year.",
      "question": "How often are the modern Olympic Games held under normal circumstances?",
      "options": [
        "A. Every year",
        "B. Every two years",
        "C. Every four years",
        "D. Every ten years"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Canada is Ottawa.",
      "conflict_prompt": "The capital of Canada is Ottawa and Toronto.",
      "question": "What is the capital of Canada?",
      "options": [
        "A. Vancouver",
        "B. Toronto",
        "C. Montreal",
        "D. Ottawa"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "George Washington was the first President of the United States.",
      "conflict_prompt": "George Washington was the first President of the United States and the second President of the United States.",
      "question": "Who was the first President of the United States?",
      "options": [
        "A. John Adams",
        "B. Thomas Jefferson",
        "C. George Washington",
        "D. James Madison"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Sahara Desert is the largest hot desert on Earth.",
      "conflict_prompt": "The Sahara Desert is the largest hot desert on Earth and is the smallest desert on Earth.",
      "question": "Which desert is the largest hot desert on Earth?",
      "options": [
        "A. Gobi Desert",
        "B. Sahara Desert",
        "C. Mojave Desert",
        "D. Kalahari Desert"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Penguins are birds that are flightless and live primarily in the Southern Hemisphere.",
      "conflict_prompt": "Penguins are birds that are flightless and live primarily in the Southern Hemisphere and also fly frequently.",
      "question": "Are penguins capable of flight?",
      "options": [
        "A. Yes, they can fly",
        "B. No, they are flightless",
        "C. Only some species can fly",
        "D. They glide but do not fly"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency used in the United Kingdom is the Pound Sterling (GBP).",
      "conflict_prompt": "The currency used in the United Kingdom is the Pound Sterling and the Euro.",
      "question": "What is the official currency of the United Kingdom?",
      "options": [
        "A. Euro",
        "B. US Dollar",
        "C. Pound Sterling",
        "D. Swiss Franc"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The binary number system is base 2.",
      "conflict_prompt": "The binary number system is base 2 and base 10.",
      "question": "What base does the binary numeral system use?",
      "options": [
        "A. Base 10",
        "B. Base 8",
        "C. Base 2",
        "D. Base 16"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "DNA stands for deoxyribonucleic acid.",
      "conflict_prompt": "DNA stands for deoxyribonucleic acid and also stands for a different acronym.",
      "question": "What does DNA stand for?",
      "options": [
        "A. Deoxyribonucleic acid",
        "B. Dioxyribonucleic acid",
        "C. Deoxyribose nucleic acid",
        "D. Dicarboxylic nucleic acid"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Ocean is the largest ocean on Earth by area.",
      "conflict_prompt": "The Pacific Ocean is the largest ocean on Earth and also the smallest ocean on Earth.",
      "question": "Which ocean is the largest by surface area?",
      "options": [
        "A. Atlantic Ocean",
        "B. Indian Ocean",
        "C. Pacific Ocean",
        "D. Arctic Ocean"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The typical adult human skeleton consists of 206 bones.",
      "conflict_prompt": "The typical adult human skeleton consists of 206 bones and also consists of fewer than 100 bones.",
      "question": "Approximately how many bones are in an adult human skeleton?",
      "options": [
        "A. 106",
        "B. 206",
        "C. 306",
        "D. 406"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Taj Mahal is a mausoleum located in Agra, India.",
      "conflict_prompt": "The Taj Mahal is a mausoleum located in Agra, India and in Cairo, Egypt.",
      "question": "In which Indian city is the Taj Mahal located?",
      "options": [
        "A. Mumbai",
        "B. New Delhi",
        "C. Jaipur",
        "D. Agra"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Ludwig van Beethoven composed the Ninth Symphony (the 'Choral' Symphony).",
      "conflict_prompt": "Ludwig van Beethoven composed the Ninth Symphony and did not compose the Ninth Symphony.",
      "question": "Who composed the Ninth Symphony (the 'Choral' Symphony)?",
      "options": [
        "A. Wolfgang Amadeus Mozart",
        "B. Johann Sebastian Bach",
        "C. Ludwig van Beethoven",
        "D. Frédéric Chopin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The blue whale is the largest known animal by mass.",
      "conflict_prompt": "The blue whale is the largest known animal by mass and also the smallest known animal by mass.",
      "question": "Which animal is the largest by mass?",
      "options": [
        "A. African elephant",
        "B. Blue whale",
        "C. Giraffe",
        "D. Great white shark"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Canberra is the capital city of Australia.",
      "conflict_prompt": "Canberra is the capital city of Australia and Sydney is also the capital city of Australia.",
      "question": "What is the capital of Australia?",
      "options": [
        "A. Sydney",
        "B. Melbourne",
        "C. Canberra",
        "D. Brisbane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The cheetah is the fastest land animal, capable of short bursts of speed.",
      "conflict_prompt": "The cheetah is the fastest land animal and is the slowest land animal.",
      "question": "Which animal is known as the fastest land animal?",
      "options": [
        "A. Horse",
        "B. Cheetah",
        "C. Ostrich",
        "D. Gazelle"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Alexander Graham Bell is commonly credited with inventing the telephone.",
      "conflict_prompt": "Alexander Graham Bell is commonly credited with inventing the telephone and did not invent the telephone.",
      "question": "Who is commonly credited with inventing the telephone?",
      "options": [
        "A. Thomas Edison",
        "B. Alexander Graham Bell",
        "C. Nikola Tesla",
        "D. Guglielmo Marconi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Sound travels slower than light in air; light travels faster than sound.",
      "conflict_prompt": "Light travels faster than sound and also travels slower than sound.",
      "question": "Comparing light and sound in air, which travels faster?",
      "options": [
        "A. Sound",
        "B. Light",
        "C. They travel at the same speed",
        "D. It depends on the frequency"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Rome is the capital of Italy.",
      "conflict_prompt": "Rome is the capital of Italy and Milan is the capital of Italy.",
      "question": "What is the capital of Italy?",
      "options": [
        "A. Milan",
        "B. Venice",
        "C. Rome",
        "D. Naples"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mercury is the planet closest to the Sun in our solar system.",
      "conflict_prompt": "Mercury is the planet closest to the Sun and is the farthest planet from the Sun.",
      "question": "Which planet is closest to the Sun?",
      "options": [
        "A. Venus",
        "B. Earth",
        "C. Mercury",
        "D. Mars"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Asia is the largest continent by both area and population.",
      "conflict_prompt": "Asia is the largest continent and the smallest continent.",
      "question": "Which continent is the largest by area?",
      "options": [
        "A. Africa",
        "B. Europe",
        "C. Asia",
        "D. Antarctica"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A deficiency of vitamin C in the diet causes scurvy.",
      "conflict_prompt": "A deficiency of vitamin C causes scurvy and also prevents scurvy.",
      "question": "Which vitamin deficiency causes scurvy?",
      "options": [
        "A. Vitamin A",
        "B. Vitamin C",
        "C. Vitamin D",
        "D. Vitamin B12"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Carbon has an atomic number of 6.",
      "conflict_prompt": "Carbon has an atomic number of 6 and 8.",
      "question": "What is the atomic number of carbon?",
      "options": [
        "A. 4",
        "B. 6",
        "C. 8",
        "D. 12"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Most species of penguins are native to the Southern Hemisphere.",
      "conflict_prompt": "Most species of penguins are native to the Southern Hemisphere and to the Northern Hemisphere equally.",
      "question": "In which hemisphere do most penguin species live?",
      "options": [
        "A. Northern Hemisphere",
        "B. Southern Hemisphere",
        "C. Equally in both hemispheres",
        "D. They live only at the equator"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Angel Falls in Venezuela is often cited as the world's highest uninterrupted waterfall.",
      "conflict_prompt": "Angel Falls is the world's highest waterfall and is not the world's highest waterfall.",
      "question": "Which waterfall is cited as the world's highest uninterrupted waterfall?",
      "options": [
        "A. Niagara Falls",
        "B. Victoria Falls",
        "C. Angel Falls",
        "D. Iguazu Falls"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon Rainforest is the largest tropical rainforest on Earth.",
      "conflict_prompt": "The Amazon Rainforest is the largest tropical rainforest on Earth and the smallest tropical rainforest on Earth.",
      "question": "Which rainforest is the largest tropical rainforest on Earth?",
      "options": [
        "A. Congo Rainforest",
        "B. Daintree Rainforest",
        "C. Amazon Rainforest",
        "D. Southeast Asian Rainforest"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Sputnik 1 was the first artificial satellite launched into orbit by the Soviet Union in 1957.",
      "conflict_prompt": "Sputnik 1 was the first artificial satellite launched into orbit in 1957 and was not launched in 1957.",
      "question": "What was the name of the first artificial satellite launched into orbit?",
      "options": [
        "A. Explorer 1",
        "B. Sputnik 1",
        "C. Apollo 11",
        "D. Vanguard 1"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Japan is the yen.",
      "conflict_prompt": "The currency of Japan is the yen and the dollar.",
      "question": "What is the official currency of Japan?",
      "options": [
        "A. Yen",
        "B. Euro",
        "C. Dollar",
        "D. Pound"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Nitrogen makes up approximately 78% of Earth's atmosphere by volume.",
      "conflict_prompt": "Nitrogen makes up approximately 78% of Earth's atmosphere and less than 1%.",
      "question": "Which gas makes up the largest percentage of Earth's atmosphere?",
      "options": [
        "A. Oxygen",
        "B. Argon",
        "C. Nitrogen",
        "D. Carbon dioxide"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The smallest prime number is 2.",
      "conflict_prompt": "The smallest prime number is 2 and 1.",
      "question": "What is the smallest prime number?",
      "options": [
        "A. 0",
        "B. 1",
        "C. 2",
        "D. 3"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest organ of the human body is the skin.",
      "conflict_prompt": "The largest organ of the human body is the skin and also the liver.",
      "question": "Which is the largest organ of the human body?",
      "options": [
        "A. Brain",
        "B. Liver",
        "C. Skin",
        "D. Heart"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Berlin is the capital city of Germany.",
      "conflict_prompt": "Berlin is the capital city of Germany and Munich is the capital city of Germany.",
      "question": "What is the capital of Germany?",
      "options": [
        "A. Munich",
        "B. Hamburg",
        "C. Berlin",
        "D. Frankfurt"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mona Lisa was painted by Leonardo da Vinci.",
      "conflict_prompt": "The Mona Lisa was painted by Leonardo da Vinci and by Vincent van Gogh.",
      "question": "Who painted the Mona Lisa?",
      "options": [
        "A. Pablo Picasso",
        "B. Leonardo da Vinci",
        "C. Vincent van Gogh",
        "D. Claude Monet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A standard professional soccer match lasts 90 minutes, excluding stoppage time and extra time.",
      "conflict_prompt": "A standard professional soccer match lasts 90 minutes and lasts 45 minutes.",
      "question": "How long is a standard professional soccer match (regular time)?",
      "options": [
        "A. 45 minutes",
        "B. 60 minutes",
        "C. 90 minutes",
        "D. 120 minutes"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Table salt is composed of sodium and chlorine with the chemical formula NaCl.",
      "conflict_prompt": "Table salt is composed of sodium and chlorine with the chemical formula NaCl and not NaCl.",
      "question": "What is the chemical formula for common table salt?",
      "options": [
        "A. KCl",
        "B. NaCl",
        "C. Na2O",
        "D. Cl2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The film Titanic was released in 1997.",
      "conflict_prompt": "The film Titanic was released in 1997 and in 2005.",
      "question": "In what year was the film Titanic released?",
      "options": [
        "A. 1990",
        "B. 1997",
        "C. 2000",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The novel Nineteen Eighty-Four (1984) was written by George Orwell.",
      "conflict_prompt": "The novel 1984 was written by George Orwell and by J. K. Rowling.",
      "question": "Who wrote the novel 1984?",
      "options": [
        "A. Aldous Huxley",
        "B. George Orwell",
        "C. J. K. Rowling",
        "D. F. Scott Fitzgerald"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "China is the most populous country in the world as of the early 2020s.",
      "conflict_prompt": "China is the most populous country in the world and is not the most populous country in the world.",
      "question": "Which country is the most populous in the world (as of early 2020s)?",
      "options": [
        "A. India",
        "B. United States",
        "C. China",
        "D. Indonesia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Olympic symbol consists of five interlaced rings.",
      "conflict_prompt": "The Olympic symbol consists of five interlaced rings and consists of three rings.",
      "question": "How many rings are in the Olympic symbol?",
      "options": [
        "A. Three",
        "B. Four",
        "C. Five",
        "D. Six"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Burj Khalifa in Dubai is currently the tallest building in the world by architectural height.",
      "conflict_prompt": "The Burj Khalifa is the tallest building in the world and also not the tallest building in the world.",
      "question": "Which building is currently recognized as the tallest by architectural height?",
      "options": [
        "A. Shanghai Tower",
        "B. One World Trade Center",
        "C. Burj Khalifa",
        "D. Taipei 101"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Thomas Edison is commonly credited with inventing the practical incandescent light bulb.",
      "conflict_prompt": "Thomas Edison is commonly credited with inventing the practical incandescent light bulb and did not invent it.",
      "question": "Who is commonly credited with inventing the practical incandescent light bulb?",
      "options": [
        "A. Nikola Tesla",
        "B. Thomas Edison",
        "C. Alexander Bell",
        "D. James Watt"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human brain uses roughly 20% of the body's resting energy consumption.",
      "conflict_prompt": "The human brain uses roughly 20% of the body's resting energy consumption and uses none of the body's energy.",
      "question": "Approximately what percentage of the body's resting energy does the human brain consume?",
      "options": [
        "A. 5%",
        "B. 10%",
        "C. 20%",
        "D. 50%"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for gold is Au.",
      "conflict_prompt": "The chemical symbol for gold is Au and Ag.",
      "question": "What is the chemical symbol for gold?",
      "options": [
        "A. Ag",
        "B. Go",
        "C. Au",
        "D. Gd"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "There are seven continents on Earth by the most common modern classification.",
      "conflict_prompt": "There are seven continents on Earth and only one continent.",
      "question": "How many continents are there according to the most common classification?",
      "options": [
        "A. Five",
        "B. Six",
        "C. Seven",
        "D. Eight"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mariana Trench contains the deepest known point in Earth's oceans, the Challenger Deep.",
      "conflict_prompt": "The Mariana Trench contains the deepest known point in Earth's oceans and the shallowest point in Earth's oceans.",
      "question": "What is the name of the deepest known point in Earth's oceans located in the Mariana Trench?",
      "options": [
        "A. Mid-Atlantic Ridge",
        "B. Challenger Deep",
        "C. Mariana Ridge",
        "D. Puerto Rico Trench"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The United States Declaration of Independence was adopted in 1776.",
      "conflict_prompt": "The United States Declaration of Independence was adopted in 1776 and in 1800.",
      "question": "In what year was the United States Declaration of Independence adopted?",
      "options": [
        "A. 1776",
        "B. 1789",
        "C. 1800",
        "D. 1812"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional guacamole is avocado.",
      "conflict_prompt": "The primary ingredient in traditional guacamole is avocado and potatoes.",
      "question": "What is the main ingredient in traditional guacamole?",
      "options": [
        "A. Tomato",
        "B. Avocado",
        "C. Potato",
        "D. Carrot"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Sphinx is located on the Giza Plateau near Cairo, Egypt.",
      "conflict_prompt": "The Great Sphinx is located on the Giza Plateau near Cairo and near Tokyo.",
      "question": "Near which city is the Great Sphinx located?",
      "options": [
        "A. Athens",
        "B. Cairo",
        "C. Rome",
        "D. Tokyo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Sharks are a group of fish known for having cartilaginous skeletons.",
      "conflict_prompt": "Sharks are a group of fish with cartilaginous skeletons and with bony skeletons.",
      "question": "What type of skeleton do sharks have?",
      "options": [
        "A. Bony skeleton",
        "B. Cartilaginous skeleton",
        "C. No skeleton",
        "D. Exoskeleton"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Madrid is the capital city of Spain.",
      "conflict_prompt": "Madrid is the capital city of Spain and Barcelona is the capital city of Spain.",
      "question": "What is the capital of Spain?",
      "options": [
        "A. Barcelona",
        "B. Seville",
        "C. Madrid",
        "D. Valencia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Greenland is the world's largest island that is not a continent.",
      "conflict_prompt": "Greenland is the world's largest island and the smallest island.",
      "question": "Which is the largest island in the world (excluding continents)?",
      "options": [
        "A. New Guinea",
        "B. Greenland",
        "C. Borneo",
        "D. Madagascar"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Ludwig van Beethoven lost much of his hearing and was famously deaf later in life.",
      "conflict_prompt": "Ludwig van Beethoven lost much of his hearing and was completely deaf later in life and also had perfect hearing his entire life.",
      "question": "Which composer is known for becoming deaf later in life?",
      "options": [
        "A. Ludwig van Beethoven",
        "B. Johann Sebastian Bach",
        "C. Frédéric Chopin",
        "D. Antonio Vivaldi"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human skeleton includes the skull as part of the axial skeleton.",
      "conflict_prompt": "The human skeleton includes the skull as part of the axial skeleton and does not include the skull.",
      "question": "Is the skull part of the human skeleton?",
      "options": [
        "A. Yes, it is part of the skeleton",
        "B. No, it is not part of the skeleton",
        "C. Only in children",
        "D. Only in adults"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary colors of light are red, green, and blue (RGB).",
      "conflict_prompt": "The primary colors of light are red, green, and blue and also cyan, magenta, and yellow.",
      "question": "Which set lists the primary colors of light?",
      "options": [
        "A. Cyan, Magenta, Yellow (CMY)",
        "B. Red, Green, Blue (RGB)",
        "C. Red, Yellow, Blue (RYB)",
        "D. Black, White, Gray"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Type O negative red blood cells are often referred to as universal donor for transfusions.",
      "conflict_prompt": "Type O negative red blood cells are universal donors and are not universal donors.",
      "question": "Which blood type is commonly called the universal donor for red blood cell transfusions?",
      "options": [
        "A. AB positive",
        "B. O negative",
        "C. A positive",
        "D. B negative"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "H2O is the chemical formula for water.",
      "conflict_prompt": "H2O is the chemical formula for water and is not the chemical formula for water.",
      "question": "What is the chemical formula for water?",
      "options": [
        "A. CO2",
        "B. H2O",
        "C. O2",
        "D. H2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The average distance from Earth to the Moon is about 384,400 kilometers.",
      "conflict_prompt": "The average distance from Earth to the Moon is about 384,400 kilometers and about 10,000 kilometers.",
      "question": "Approximately how far is the Moon from Earth on average?",
      "options": [
        "A. 3,840 km",
        "B. 38,440 km",
        "C. 384,400 km",
        "D. 3,844,000 km"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Tim Berners-Lee is credited with inventing the World Wide Web.",
      "conflict_prompt": "Tim Berners-Lee is credited with inventing the World Wide Web and did not invent it.",
      "question": "Who is credited with inventing the World Wide Web?",
      "options": [
        "A. Vint Cerf",
        "B. Tim Berners-Lee",
        "C. Alan Turing",
        "D. Bill Gates"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Milky Way is the galaxy that contains our Solar System.",
      "conflict_prompt": "The Milky Way is the galaxy that contains our Solar System and another galaxy contains our Solar System.",
      "question": "Which galaxy contains our Solar System?",
      "options": [
        "A. Andromeda Galaxy",
        "B. Triangulum Galaxy",
        "C. Milky Way",
        "D. Whirlpool Galaxy"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Fish primarily use gills to exchange gases and breathe underwater.",
      "conflict_prompt": "Fish primarily use gills to exchange gases and breathe underwater and primarily use lungs to breathe underwater.",
      "question": "What organ do most fish use to extract oxygen from water?",
      "options": [
        "A. Lungs",
        "B. Gills",
        "C. Skin",
        "D. Fins"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Stradivarius violins were made by Antonio Stradivari and are renowned for their sound.",
      "conflict_prompt": "Stradivarius violins were made by Antonio Stradivari and were made by a mass-production factory.",
      "question": "Who made the famous Stradivarius violins?",
      "options": [
        "A. Antonio Stradivari",
        "B. Amati Brothers",
        "C. Giuseppe Guarneri",
        "D. A mass-production factory"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Moscow is the capital of Russia.",
      "conflict_prompt": "Moscow is the capital of Russia and Saint Petersburg is the capital of Russia.",
      "question": "What is the capital of Russia?",
      "options": [
        "A. Saint Petersburg",
        "B. Novosibirsk",
        "C. Moscow",
        "D. Kazan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for iron is Fe.",
      "conflict_prompt": "The chemical symbol for iron is Fe and Ir.",
      "question": "What is the chemical symbol for iron?",
      "options": [
        "A. Ir",
        "B. Fe",
        "C. I",
        "D. Fr"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Golf is a sport played on a course where players hit a ball into a series of holes using clubs.",
      "conflict_prompt": "Golf is a sport played on a course where players hit a ball into a series of holes using clubs and is played with hands only.",
      "question": "In golf, how do players move the ball toward the hole?",
      "options": [
        "A. By kicking it",
        "B. By throwing it",
        "C. By hitting it with clubs",
        "D. By carrying it by hand"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Rusting of iron is a chemical process called oxidation.",
      "conflict_prompt": "Rusting of iron is oxidation and is not oxidation.",
      "question": "What chemical process causes iron to rust?",
      "options": [
        "A. Reduction",
        "B. Oxidation",
        "C. Sublimation",
        "D. Polymerization"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Fuji is an iconic stratovolcano located on Honshu Island in Japan.",
      "conflict_prompt": "Mount Fuji is an iconic stratovolcano located on Honshu Island in Japan and in Hawaii.",
      "question": "On which country is Mount Fuji located?",
      "options": [
        "A. South Korea",
        "B. Japan",
        "C. China",
        "D. Indonesia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The kilogram is the SI base unit of mass.",
      "conflict_prompt": "The kilogram is the SI base unit of mass and is not the SI base unit of mass.",
      "question": "What is the SI base unit for mass?",
      "options": [
        "A. Gram",
        "B. Pound",
        "C. Kilogram",
        "D. Newton"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Jane Austen is the author of the novel Pride and Prejudice.",
      "conflict_prompt": "Jane Austen is the author of Pride and Prejudice and Charles Dickens is also the author of Pride and Prejudice.",
      "question": "Who wrote Pride and Prejudice?",
      "options": [
        "A. Charlotte Brontë",
        "B. Jane Austen",
        "C. Charles Dickens",
        "D. Mary Shelley"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Weather in Earth's atmosphere occurs mainly in the troposphere.",
      "conflict_prompt": "Weather occurs mainly in the troposphere and also occurs mainly in the stratosphere.",
      "question": "In which layer of Earth's atmosphere does most weather occur?",
      "options": [
        "A. Stratosphere",
        "B. Mesosphere",
        "C. Troposphere",
        "D. Thermosphere"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The mathematical constant pi is approximately 3.14159.",
      "conflict_prompt": "The mathematical constant pi is approximately 3.14159 and approximately 2.71828.",
      "question": "What is the approximate value of pi?",
      "options": [
        "A. 2.71828",
        "B. 3.14159",
        "C. 1.61803",
        "D. 0.57721"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "There are five players from each team on the court during an official basketball game.",
      "conflict_prompt": "There are five players from each team on the court during a basketball game and ten players from one team.",
      "question": "How many players per team are on the court in standard basketball play?",
      "options": [
        "A. Three",
        "B. Five",
        "C. Seven",
        "D. Nine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for sodium is Na.",
      "conflict_prompt": "The chemical symbol for sodium is Na and S.",
      "question": "What is the chemical symbol for sodium?",
      "options": [
        "A. So",
        "B. Na",
        "C. S",
        "D. Sd"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Cairo is the capital of Egypt.",
      "conflict_prompt": "Cairo is the capital of Egypt and Alexandria is the capital of Egypt.",
      "question": "What is the capital of Egypt?",
      "options": [
        "A. Alexandria",
        "B. Luxor",
        "C. Cairo",
        "D. Aswan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Wright brothers, Orville and Wilbur, are credited with inventing and flying the first successful powered airplane.",
      "conflict_prompt": "The Wright brothers invented and flew the first successful powered airplane and did not.",
      "question": "Who are credited with the first successful powered airplane flight?",
      "options": [
        "A. The Wright brothers",
        "B. The Montgolfier brothers",
        "C. Thomas Edison and Nikola Tesla",
        "D. The Lumière brothers"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Helium is a noble gas and is chemically inert under most conditions.",
      "conflict_prompt": "Helium is a noble gas and chemically inert and is chemically reactive under normal conditions.",
      "question": "Which gas is a noble gas and chemically inert?",
      "options": [
        "A. Oxygen",
        "B. Nitrogen",
        "C. Helium",
        "D. Chlorine"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The wandering albatross has one of the largest wingspans of any living bird.",
      "conflict_prompt": "The wandering albatross has one of the largest wingspans and one of the smallest wingspans of any living bird.",
      "question": "Which bird is known for having one of the largest wingspans?",
      "options": [
        "A. Sparrow",
        "B. Hummingbird",
        "C. Wandering albatross",
        "D. Pigeon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Hydrogen is the first element in the periodic table.",
      "conflict_prompt": "Hydrogen is the first element in the periodic table and the last element in the periodic table.",
      "question": "Which element is first in the periodic table?",
      "options": [
        "A. Helium",
        "B. Lithium",
        "C. Hydrogen",
        "D. Carbon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official currency of India is the Indian rupee (INR).",
      "conflict_prompt": "The official currency of India is the Indian rupee and the US dollar.",
      "question": "What is the official currency of India?",
      "options": [
        "A. Indian Rupee",
        "B. US Dollar",
        "C. Euro",
        "D. Pound Sterling"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Chlorophyll is the pigment that gives most plants their green color.",
      "conflict_prompt": "Chlorophyll gives plants their green color and gives them red color.",
      "question": "Which pigment gives most plants their green color?",
      "options": [
        "A. Carotene",
        "B. Anthocyanin",
        "C. Chlorophyll",
        "D. Melanin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mexico City is the capital of Mexico.",
      "conflict_prompt": "Mexico City is the capital of Mexico and Guadalajara is the capital of Mexico.",
      "question": "What is the capital of Mexico?",
      "options": [
        "A. Guadalajara",
        "B. Monterrey",
        "C. Mexico City",
        "D. Cancún"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Spanish is the primary language spoken in Argentina.",
      "conflict_prompt": "Spanish is the primary language spoken in Argentina and Portuguese is the primary language spoken in Argentina.",
      "question": "What is the primary language of Argentina?",
      "options": [
        "A. Portuguese",
        "B. Spanish",
        "C. English",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Venus has the hottest average surface temperature of the planets in our solar system.",
      "conflict_prompt": "Venus has the hottest average surface temperature and the coldest average surface temperature among the planets.",
      "question": "Which planet has the hottest average surface temperature in the solar system?",
      "options": [
        "A. Mercury",
        "B. Venus",
        "C. Mars",
        "D. Jupiter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The SI base unit of temperature is the kelvin (K).",
      "conflict_prompt": "The SI base unit of temperature is the kelvin and the Celsius degree.",
      "question": "What is the SI base unit for temperature?",
      "options": [
        "A. Celsius",
        "B. Fahrenheit",
        "C. Kelvin",
        "D. Rankine"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for carbon dioxide is CO2.",
      "conflict_prompt": "The chemical formula for carbon dioxide is CO2 and CO.",
      "question": "What is the chemical formula for carbon dioxide?",
      "options": [
        "A. CO",
        "B. CO2",
        "C. C2O",
        "D. O2C"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "J. K. Rowling is the author of the Harry Potter series.",
      "conflict_prompt": "J. K. Rowling is the author of the Harry Potter series and is not the author of the Harry Potter series.",
      "question": "Who wrote the Harry Potter series?",
      "options": [
        "A. J. R. R. Tolkien",
        "B. J. K. Rowling",
        "C. C. S. Lewis",
        "D. Suzanne Collins"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Suez Canal connects the Mediterranean Sea with the Red Sea.",
      "conflict_prompt": "The Suez Canal connects the Mediterranean Sea with the Red Sea and connects the Mediterranean Sea with the Pacific Ocean.",
      "question": "Which two seas does the Suez Canal connect?",
      "options": [
        "A. Mediterranean Sea and Atlantic Ocean",
        "B. Mediterranean Sea and Red Sea",
        "C. Black Sea and Caspian Sea",
        "D. Arctic Ocean and Pacific Ocean"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Ankara is the capital of Turkey.",
      "conflict_prompt": "Ankara is the capital of Turkey and Istanbul is the capital of Turkey.",
      "question": "What is the capital of Turkey?",
      "options": [
        "A. Istanbul",
        "B. Antalya",
        "C. Ankara",
        "D. Izmir"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The French flag has three vertical stripes: blue, white, and red.",
      "conflict_prompt": "The French flag has three vertical stripes blue, white, and red and horizontal stripes blue, white, and red.",
      "question": "Which arrangement describes the French national flag?",
      "options": [
        "A. Three horizontal stripes",
        "B. A solid blue flag",
        "C. Three vertical stripes blue, white, red",
        "D. A diagonal cross"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "New York City is the most populous city in the United States.",
      "conflict_prompt": "New York City is the most populous city in the United States and Los Angeles is the most populous city in the United States.",
      "question": "Which city is the most populous in the United States?",
      "options": [
        "A. Los Angeles",
        "B. Chicago",
        "C. Houston",
        "D. New York City"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Amelia Earhart was the first woman to fly solo across the Atlantic Ocean.",
      "conflict_prompt": "Amelia Earhart was the first woman to fly solo across the Atlantic Ocean and she never flew across the Atlantic Ocean.",
      "question": "Who was the first woman to fly solo across the Atlantic Ocean?",
      "options": [
        "A. Amelia Earhart",
        "B. Bessie Coleman",
        "C. Jacqueline Cochran",
        "D. Harriet Quimby"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Death Valley in California has recorded some of the highest air temperatures on Earth.",
      "conflict_prompt": "Death Valley has recorded some of the highest air temperatures on Earth and the coldest air temperatures on Earth.",
      "question": "Which location has recorded some of the highest air temperatures on Earth?",
      "options": [
        "A. Siberia",
        "B. Death Valley",
        "C. Greenland",
        "D. Antarctica"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Methane is the primary constituent of natural gas.",
      "conflict_prompt": "Methane is the primary constituent of natural gas and methane is not present in natural gas.",
      "question": "What is the primary component of natural gas?",
      "options": [
        "A. Carbon dioxide",
        "B. Propane",
        "C. Methane",
        "D. Nitrogen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The retina is the light-sensitive layer at the back of the human eye.",
      "conflict_prompt": "The retina is the light-sensitive layer at the back of the human eye and is not light-sensitive.",
      "question": "What is the light-sensitive layer at the back of the eye called?",
      "options": [
        "A. Cornea",
        "B. Lens",
        "C. Retina",
        "D. Iris"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Titan is the largest moon of Saturn and has a substantial atmosphere.",
      "conflict_prompt": "Titan is the largest moon of Saturn and has a substantial atmosphere and has no atmosphere.",
      "question": "Which moon is the largest of Saturn's moons?",
      "options": [
        "A. Rhea",
        "B. Enceladus",
        "C. Titan",
        "D. Iapetus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Orient Express was a long-distance passenger train service connecting Paris and Istanbul.",
      "conflict_prompt": "The Orient Express was a train connecting Paris and Istanbul and connecting Paris and New York.",
      "question": "Which two major cities were historically linked by the Orient Express?",
      "options": [
        "A. Paris and Rome",
        "B. Paris and Istanbul",
        "C. London and New York",
        "D. Berlin and Tokyo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for potassium is K.",
      "conflict_prompt": "The chemical symbol for potassium is K and P.",
      "question": "What is the chemical symbol for potassium?",
      "options": [
        "A. P",
        "B. Po",
        "C. K",
        "D. Pt"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mark Zuckerberg is one of the founders of Facebook.",
      "conflict_prompt": "Mark Zuckerberg is one of the founders of Facebook and is not one of the founders of Facebook.",
      "question": "Who is one of the founders of Facebook?",
      "options": [
        "A. Steve Jobs",
        "B. Mark Zuckerberg",
        "C. Jeff Bezos",
        "D. Elon Musk"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Barrier Reef is located off the northeastern coast of Australia.",
      "conflict_prompt": "The Great Barrier Reef is located off the northeastern coast of Australia and off the coast of South Africa.",
      "question": "Off the coast of which country is the Great Barrier Reef located?",
      "options": [
        "A. Australia",
        "B. South Africa",
        "C. Brazil",
        "D. India"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Beijing is the capital of the People's Republic of China.",
      "conflict_prompt": "Beijing is the capital of China and Shanghai is the capital of China.",
      "question": "What is the capital of China?",
      "options": [
        "A. Shanghai",
        "B. Guangzhou",
        "C. Beijing",
        "D. Shenzhen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mona Lisa is displayed in the Louvre Museum in Paris.",
      "conflict_prompt": "The Mona Lisa is displayed in the Louvre Museum in Paris and in the British Museum in London.",
      "question": "In which museum is the Mona Lisa displayed?",
      "options": [
        "A. British Museum",
        "B. The Met",
        "C. Louvre Museum",
        "D. Uffizi Gallery"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first modern Olympic Games were held in Athens in 1896.",
      "conflict_prompt": "The first modern Olympic Games were held in Athens in 1896 and in Paris in 1896.",
      "question": "Where were the first modern Olympic Games held in 1896?",
      "options": [
        "A. Paris",
        "B. Athens",
        "C. London",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Silver has the chemical symbol Ag.",
      "conflict_prompt": "Silver has the chemical symbol Ag and the chemical symbol Au.",
      "question": "What is the chemical symbol for silver?",
      "options": [
        "A. Au",
        "B. Ag",
        "C. Si",
        "D. Sn"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Tuberculosis is an infectious disease caused by the bacterium Mycobacterium tuberculosis.",
      "conflict_prompt": "Tuberculosis is caused by Mycobacterium tuberculosis and is not caused by Mycobacterium tuberculosis.",
      "question": "What bacterium causes tuberculosis?",
      "options": [
        "A. Streptococcus pneumoniae",
        "B. Mycobacterium tuberculosis",
        "C. Escherichia coli",
        "D. Staphylococcus aureus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The giraffe is the tallest living terrestrial animal.",
      "conflict_prompt": "The giraffe is the tallest living terrestrial animal and is the shortest living terrestrial animal.",
      "question": "Which animal is the tallest living terrestrial animal?",
      "options": [
        "A. Elephant",
        "B. Giraffe",
        "C. Moose",
        "D. Kangaroo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Japanese writing uses kanji as one of its primary scripts.",
      "conflict_prompt": "Japanese writing uses kanji as one of its primary scripts and uses only Latin letters.",
      "question": "Which script is used as part of Japanese writing?",
      "options": [
        "A. Cyrillic",
        "B. Arabic",
        "C. Kanji",
        "D. Devanagari"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Ford Model T was introduced by Henry Ford and his company as an affordable automobile.",
      "conflict_prompt": "The Ford Model T was introduced by Henry Ford as an affordable automobile and was introduced as an ultra-luxury car for the wealthy.",
      "question": "How was the Ford Model T primarily marketed when introduced?",
      "options": [
        "A. As an ultra-luxury car",
        "B. As a racing vehicle",
        "C. As an affordable mass-market car",
        "D. As a military vehicle"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Socrates was a classical Greek philosopher who taught Plato.",
      "conflict_prompt": "Socrates was a classical Greek philosopher who taught Plato and was taught by Plato.",
      "question": "Which philosopher was a teacher of Plato?",
      "options": [
        "A. Aristotle",
        "B. Socrates",
        "C. Epicurus",
        "D. Zeno"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The equator divides the Earth into the Northern and Southern Hemispheres.",
      "conflict_prompt": "The equator divides the Earth into the Northern and Southern Hemispheres and into the Eastern and Western Hemispheres.",
      "question": "What two hemispheres does the equator separate?",
      "options": [
        "A. Eastern and Western",
        "B. Northern and Southern",
        "C. Arctic and Antarctic",
        "D. Tropical and Temperate"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Pretoria is one of the three capital cities of South Africa, serving as the executive (administrative) capital.",
      "conflict_prompt": "Pretoria is one of South Africa's capitals and is the only capital of South Africa.",
      "question": "Which city serves as South Africa's executive (administrative) capital?",
      "options": [
        "A. Cape Town",
        "B. Bloemfontein",
        "C. Pretoria",
        "D. Johannesburg"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human circulatory system is driven by the pumping action of the heart.",
      "conflict_prompt": "The human circulatory system is driven by the heart and is not driven by the heart.",
      "question": "What organ primarily pumps blood through the human circulatory system?",
      "options": [
        "A. Liver",
        "B. Kidney",
        "C. Heart",
        "D. Lungs"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Albert Einstein developed the special theory of relativity.",
      "conflict_prompt": "Albert Einstein developed the special theory of relativity and did not develop it.",
      "question": "Who developed the special theory of relativity?",
      "options": [
        "A. Isaac Newton",
        "B. Albert Einstein",
        "C. Galileo Galilei",
        "D. Johannes Kepler"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The painting The Starry Night was created by Vincent van Gogh.",
      "conflict_prompt": "The Starry Night was created by Vincent van Gogh and by Pablo Picasso.",
      "question": "Who painted The Starry Night?",
      "options": [
        "A. Pablo Picasso",
        "B. Vincent van Gogh",
        "C. Claude Monet",
        "D. Salvador Dalí"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Chickpeas are the main ingredient in traditional hummus.",
      "conflict_prompt": "Chickpeas are the main ingredient in traditional hummus and potatoes are the main ingredient in traditional hummus.",
      "question": "What is the primary ingredient in traditional hummus?",
      "options": [
        "A. Potatoes",
        "B. Chickpeas",
        "C. Lentils",
        "D. Rice"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Wellington is the capital of New Zealand.",
      "conflict_prompt": "Wellington is the capital of New Zealand and Auckland is the capital of New Zealand.",
      "question": "What is the capital of New Zealand?",
      "options": [
        "A. Auckland",
        "B. Christchurch",
        "C. Wellington",
        "D. Dunedin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official currency of Russia is the Russian ruble (RUB).",
      "conflict_prompt": "The official currency of Russia is the Russian ruble and the euro.",
      "question": "What is the official currency of Russia?",
      "options": [
        "A. Euro",
        "B. Russian Ruble",
        "C. US Dollar",
        "D. Chinese Yuan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for lead is Pb.",
      "conflict_prompt": "The chemical symbol for lead is Pb and Le.",
      "question": "What is the chemical symbol for lead?",
      "options": [
        "A. Pb",
        "B. Le",
        "C. Ld",
        "D. Pd"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The World Series determines the champion of Major League Baseball (MLB) in the United States.",
      "conflict_prompt": "The World Series determines the champion of Major League Baseball and determines the champion of the National Football League.",
      "question": "Which sport's championship is decided by the World Series?",
      "options": [
        "A. American football",
        "B. Basketball",
        "C. Major League Baseball",
        "D. Ice hockey"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Grand Canyon is one of the largest and most famous canyons in the world, located in Arizona, USA.",
      "conflict_prompt": "The Grand Canyon is one of the largest canyons and is located in Arizona and Ohio.",
      "question": "In which US state is the Grand Canyon primarily located?",
      "options": [
        "A. Colorado",
        "B. Utah",
        "C. Arizona",
        "D. Nevada"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Bill Gates is a co-founder of Microsoft.",
      "conflict_prompt": "Bill Gates is a co-founder of Microsoft and is not a co-founder of Microsoft.",
      "question": "Who co-founded Microsoft?",
      "options": [
        "A. Steve Jobs",
        "B. Bill Gates",
        "C. Mark Zuckerberg",
        "D. Larry Page"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for nitrogen is N.",
      "conflict_prompt": "The chemical symbol for nitrogen is N and Ni.",
      "question": "What is the chemical symbol for nitrogen?",
      "options": [
        "A. Ni",
        "B. N",
        "C. Nt",
        "D. Ng"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Istanbul is a transcontinental city that spans parts of Europe and Asia.",
      "conflict_prompt": "Istanbul spans both Europe and Asia and is located only in Europe.",
      "question": "Which two continents does Istanbul span?",
      "options": [
        "A. Africa and Asia",
        "B. Europe and Asia",
        "C. North America and Europe",
        "D. South America and Africa"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The SI unit of electrical resistance is the ohm, symbol Ω.",
      "conflict_prompt": "The SI unit of electrical resistance is the ohm and the ampere.",
      "question": "What is the SI unit of electrical resistance?",
      "options": [
        "A. Volt",
        "B. Ampere",
        "C. Ohm",
        "D. Coulomb"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "In subtractive color mixing (printing), the primary colors are cyan, magenta, and yellow (CMY).",
      "conflict_prompt": "In subtractive color mixing the primary colors are cyan, magenta, and yellow and red, green, blue.",
      "question": "Which set lists the primary colors used in subtractive color mixing for printing?",
      "options": [
        "A. Red, Green, Blue (RGB)",
        "B. Cyan, Magenta, Yellow (CMY)",
        "C. Red, Yellow, Blue (RYB)",
        "D. Black, White, Gray"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The African elephant is the largest land animal by mass.",
      "conflict_prompt": "The African elephant is the largest land animal by mass and the smallest land animal by mass.",
      "question": "Which animal is the largest land animal by mass?",
      "options": [
        "A. Blue whale",
        "B. African elephant",
        "C. Rhinoceros",
        "D. Hippopotamus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mars is commonly referred to as the 'Red Planet' because of its iron oxide surface.",
      "conflict_prompt": "Mars is called the Red Planet because of its iron oxide surface and is also called the Blue Planet for the same reason.",
      "question": "Which planet is commonly known as the Red Planet?",
      "options": [
        "A. Venus",
        "B. Mars",
        "C. Jupiter",
        "D. Saturn"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Paris is the capital city of France.",
      "conflict_prompt": "Paris is not the capital city of France.",
      "question": "What is the capital city of France?",
      "options": [
        "A. Berlin",
        "B. Madrid",
        "C. Paris",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Everest is the tallest mountain on Earth above sea level.",
      "conflict_prompt": "Mount Everest is not the tallest mountain on Earth above sea level.",
      "question": "Which mountain is the tallest on Earth above sea level?",
      "options": [
        "A. K2",
        "B. Kangchenjunga",
        "C. Mount Everest",
        "D. Lhotse"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nile River is the longest river in Africa.",
      "conflict_prompt": "The Nile River is not the longest river in Africa.",
      "question": "Which river is the longest in Africa?",
      "options": [
        "A. Congo River",
        "B. Niger River",
        "C. Nile River",
        "D. Zambezi River"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Ocean is the largest ocean on Earth by surface area.",
      "conflict_prompt": "The Pacific Ocean is not the largest ocean on Earth by surface area.",
      "question": "Which ocean is the largest on Earth by surface area?",
      "options": [
        "A. Atlantic Ocean",
        "B. Indian Ocean",
        "C. Arctic Ocean",
        "D. Pacific Ocean"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human heart has four chambers: two atria and two ventricles.",
      "conflict_prompt": "The human heart does not have four chambers.",
      "question": "How many chambers does the human heart have?",
      "options": [
        "A. Two",
        "B. Three",
        "C. Four",
        "D. Five"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Water freezes at 0 degrees Celsius at standard atmospheric pressure.",
      "conflict_prompt": "Water does not freeze at 0 degrees Celsius at standard atmospheric pressure.",
      "question": "At what temperature does water freeze at standard atmospheric pressure?",
      "options": [
        "A. 100°C",
        "B. 0°C",
        "C. -273°C",
        "D. 32°C"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Japan is Tokyo.",
      "conflict_prompt": "The capital of Japan is not Tokyo.",
      "question": "What is the capital of Japan?",
      "options": [
        "A. Osaka",
        "B. Kyoto",
        "C. Tokyo",
        "D. Hiroshima"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Shakespeare wrote the play Macbeth.",
      "conflict_prompt": "Shakespeare did not write the play Macbeth.",
      "question": "Who wrote the play Macbeth?",
      "options": [
        "A. Christopher Marlowe",
        "B. William Shakespeare",
        "C. Ben Jonson",
        "D. John Webster"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for gold is Au.",
      "conflict_prompt": "The chemical symbol for gold is not Au.",
      "question": "What is the chemical symbol for gold?",
      "options": [
        "A. Ag",
        "B. Au",
        "C. Pb",
        "D. Fe"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The speed of light in a vacuum is approximately 299,792 kilometers per second.",
      "conflict_prompt": "The speed of light in a vacuum is not approximately 299,792 kilometers per second.",
      "question": "Approximately how fast is the speed of light in a vacuum?",
      "options": [
        "A. 3 km/s",
        "B. 299,792 km/s",
        "C. 1,000 km/s",
        "D. 9,800 km/s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet in our solar system is Jupiter.",
      "conflict_prompt": "The largest planet in our solar system is not Jupiter.",
      "question": "Which planet is the largest in our solar system?",
      "options": [
        "A. Earth",
        "B. Saturn",
        "C. Jupiter",
        "D. Neptune"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Humans have 46 chromosomes in most body cells (23 pairs).",
      "conflict_prompt": "Humans do not have 46 chromosomes in most body cells.",
      "question": "How many chromosomes do most human body cells contain?",
      "options": [
        "A. 23",
        "B. 46",
        "C. 92",
        "D. 34"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Barrier Reef is located off the coast of Queensland, Australia.",
      "conflict_prompt": "The Great Barrier Reef is not located off the coast of Queensland, Australia.",
      "question": "Off the coast of which Australian state is the Great Barrier Reef located?",
      "options": [
        "A. Western Australia",
        "B. Victoria",
        "C. Queensland",
        "D. New South Wales"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light-year is a unit of distance used in astronomy, representing the distance light travels in one year.",
      "conflict_prompt": "A light-year is not a unit of distance in astronomy.",
      "question": "What does a light-year measure?",
      "options": [
        "A. Time",
        "B. Mass",
        "C. Distance",
        "D. Brightness"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty was a gift to the United States from France.",
      "conflict_prompt": "The Statue of Liberty was not a gift to the United States from France.",
      "question": "Which country gifted the Statue of Liberty to the United States?",
      "options": [
        "A. Spain",
        "B. France",
        "C. United Kingdom",
        "D. Italy"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas in Earth's atmosphere is nitrogen, making up about 78%.",
      "conflict_prompt": "The primary gas in Earth's atmosphere is not nitrogen.",
      "question": "Which gas makes up about 78% of Earth's atmosphere?",
      "options": [
        "A. Oxygen",
        "B. Carbon dioxide",
        "C. Nitrogen",
        "D. Argon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Declaration of Independence of the United States was adopted in 1776.",
      "conflict_prompt": "The Declaration of Independence of the United States was not adopted in 1776.",
      "question": "In what year was the United States Declaration of Independence adopted?",
      "options": [
        "A. 1776",
        "B. 1783",
        "C. 1801",
        "D. 1754"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human brain is made up of two hemispheres: left and right.",
      "conflict_prompt": "The human brain is not made up of two hemispheres.",
      "question": "How many hemispheres is the human brain commonly described as having?",
      "options": [
        "A. One",
        "B. Two",
        "C. Three",
        "D. Four"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Photosynthesis in plants primarily occurs in the chloroplasts of leaf cells.",
      "conflict_prompt": "Photosynthesis in plants does not occur in the chloroplasts of leaf cells.",
      "question": "In which organelle does photosynthesis primarily occur in plant cells?",
      "options": [
        "A. Mitochondria",
        "B. Nucleus",
        "C. Chloroplast",
        "D. Ribosome"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of the United Kingdom is the pound sterling.",
      "conflict_prompt": "The currency of the United Kingdom is not the pound sterling.",
      "question": "What is the currency of the United Kingdom?",
      "options": [
        "A. Euro",
        "B. Pound sterling",
        "C. US Dollar",
        "D. Swiss Franc"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Albert Einstein developed the theory of general relativity.",
      "conflict_prompt": "Albert Einstein did not develop the theory of general relativity.",
      "question": "Who developed the theory of general relativity?",
      "options": [
        "A. Isaac Newton",
        "B. Albert Einstein",
        "C. Niels Bohr",
        "D. Galileo Galilei"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest desert in the world is the Antarctic Desert by area.",
      "conflict_prompt": "The largest desert in the world is not the Antarctic Desert by area.",
      "question": "Which desert is the largest in the world by area?",
      "options": [
        "A. Sahara Desert",
        "B. Gobi Desert",
        "C. Antarctic Desert",
        "D. Arabian Desert"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The piano is a keyboard instrument invented in Italy in the early 18th century by Bartolomeo Cristofori.",
      "conflict_prompt": "The piano was not invented in Italy by Bartolomeo Cristofori.",
      "question": "Who is credited with inventing the piano in the early 18th century?",
      "options": [
        "A. Antonio Stradivari",
        "B. Bartolomeo Cristofori",
        "C. Johann Sebastian Bach",
        "D. Giuseppe Verdi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest building in the world is the Burj Khalifa in Dubai.",
      "conflict_prompt": "The tallest building in the world is not the Burj Khalifa in Dubai.",
      "question": "Which building is currently the tallest in the world?",
      "options": [
        "A. Shanghai Tower",
        "B. One World Trade Center",
        "C. Burj Khalifa",
        "D. Taipei 101"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "DNA stands for deoxyribonucleic acid.",
      "conflict_prompt": "DNA does not stand for deoxyribonucleic acid.",
      "question": "What does DNA stand for?",
      "options": [
        "A. Deoxyribonucleic acid",
        "B. Deoxyribonuclear acid",
        "C. Dioxyribonucleic acid",
        "D. Dextroribonucleic acid"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon Rainforest is primarily located in Brazil.",
      "conflict_prompt": "The Amazon Rainforest is not primarily located in Brazil.",
      "question": "In which country is the majority of the Amazon Rainforest located?",
      "options": [
        "A. Peru",
        "B. Colombia",
        "C. Brazil",
        "D. Venezuela"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The file format PDF stands for Portable Document Format.",
      "conflict_prompt": "The file format PDF does not stand for Portable Document Format.",
      "question": "What does PDF stand for?",
      "options": [
        "A. Portable Data File",
        "B. Personal Document File",
        "C. Portable Document Format",
        "D. Public Document Format"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Treaty of Versailles was signed in 1919 after World War I.",
      "conflict_prompt": "The Treaty of Versailles was signed in 1918 after World War I.",
      "question": "In what year was the Treaty of Versailles signed?",
      "options": [
        "A. 1914",
        "B. 1918",
        "C. 1919",
        "D. 1923"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mercury is the closest planet to the Sun in our solar system.",
      "conflict_prompt": "Mercury is not the closest planet to the Sun in our solar system.",
      "question": "Which planet is closest to the Sun?",
      "options": [
        "A. Venus",
        "B. Mercury",
        "C. Earth",
        "D. Mars"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element oxygen has atomic number 8.",
      "conflict_prompt": "The element oxygen does not have atomic number 8.",
      "question": "What is the atomic number of oxygen?",
      "options": [
        "A. 6",
        "B. 7",
        "C. 8",
        "D. 10"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Canada is Ottawa.",
      "conflict_prompt": "The capital of Canada is Toronto.",
      "question": "What is the capital city of Canada?",
      "options": [
        "A. Toronto",
        "B. Vancouver",
        "C. Ottawa",
        "D. Montreal"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the telephone is commonly credited as Alexander Graham Bell.",
      "conflict_prompt": "Alexander Graham Bell did not invent the telephone.",
      "question": "Who is commonly credited with inventing the telephone?",
      "options": [
        "A. Nikola Tesla",
        "B. Guglielmo Marconi",
        "C. Alexander Graham Bell",
        "D. Thomas Edison"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest mammal in the world is the blue whale.",
      "conflict_prompt": "The largest mammal in the world is not the blue whale.",
      "question": "Which species is the largest mammal in the world?",
      "options": [
        "A. African elephant",
        "B. Blue whale",
        "C. Giraffe",
        "D. Sperm whale"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language spoken in Brazil is Portuguese.",
      "conflict_prompt": "The primary language spoken in Brazil is Spanish.",
      "question": "What is the primary language spoken in Brazil?",
      "options": [
        "A. Spanish",
        "B. Portuguese",
        "C. English",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for table salt (sodium chloride) is NaCl.",
      "conflict_prompt": "The chemical formula for table salt is NaBr.",
      "question": "What is the chemical formula for table salt?",
      "options": [
        "A. NaBr",
        "B. KCl",
        "C. NaCl",
        "D. CaCl2"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Olympic Games are held every four years for the Summer Olympics.",
      "conflict_prompt": "The Olympic Games are held every two years for the Summer Olympics.",
      "question": "How often are the Summer Olympic Games held?",
      "options": [
        "A. Every year",
        "B. Every two years",
        "C. Every four years",
        "D. Every six years"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon River has the largest discharge volume of any river in the world.",
      "conflict_prompt": "The Amazon River does not have the largest discharge volume of any river in the world.",
      "question": "Which river has the largest discharge volume in the world?",
      "options": [
        "A. Nile River",
        "B. Mississippi River",
        "C. Amazon River",
        "D. Yangtze River"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first man to walk on the Moon was Neil Armstrong in 1969.",
      "conflict_prompt": "Neil Armstrong was not the first man to walk on the Moon in 1969.",
      "question": "Who was the first person to walk on the Moon?",
      "options": [
        "A. Buzz Aldrin",
        "B. Michael Collins",
        "C. Yuri Gagarin",
        "D. Neil Armstrong"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest continent by land area is Asia.",
      "conflict_prompt": "The largest continent by land area is not Asia.",
      "question": "Which continent is largest by land area?",
      "options": [
        "A. Africa",
        "B. North America",
        "C. Asia",
        "D. Antarctica"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Honey never spoils if stored properly due to its low water content and acidity.",
      "conflict_prompt": "Honey spoils quickly even if stored properly.",
      "question": "Why does honey resist spoiling when stored properly?",
      "options": [
        "A. High water content",
        "B. Low water content and acidity",
        "C. Presence of preservatives",
        "D. Refrigeration is required"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The author of 'Pride and Prejudice' is Jane Austen.",
      "conflict_prompt": "Jane Austen did not write 'Pride and Prejudice'.",
      "question": "Who wrote 'Pride and Prejudice'?",
      "options": [
        "A. Charlotte Brontë",
        "B. Mary Shelley",
        "C. Jane Austen",
        "D. George Eliot"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the World Wide Web is Tim Berners-Lee.",
      "conflict_prompt": "Tim Berners-Lee did not invent the World Wide Web.",
      "question": "Who is credited with inventing the World Wide Web?",
      "options": [
        "A. Vint Cerf",
        "B. Tim Berners-Lee",
        "C. Bill Gates",
        "D. Linus Torvalds"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The kangaroo is native to Australia.",
      "conflict_prompt": "Kangaroos are not native to Australia.",
      "question": "Where are kangaroos native to?",
      "options": [
        "A. Africa",
        "B. South America",
        "C. Australia",
        "D. Asia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 1 is hydrogen.",
      "conflict_prompt": "The chemical element with atomic number 1 is helium.",
      "question": "Which element has atomic number 1?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Oxygen",
        "D. Carbon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The smallest prime number is 2.",
      "conflict_prompt": "The smallest prime number is 1.",
      "question": "What is the smallest prime number?",
      "options": [
        "A. 1",
        "B. 2",
        "C. 3",
        "D. 0"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Italy is Rome.",
      "conflict_prompt": "The capital of Italy is Milan.",
      "question": "What is the capital of Italy?",
      "options": [
        "A. Milan",
        "B. Venice",
        "C. Rome",
        "D. Florence"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mona Lisa was painted by Leonardo da Vinci.",
      "conflict_prompt": "The Mona Lisa was not painted by Leonardo da Vinci.",
      "question": "Who painted the Mona Lisa?",
      "options": [
        "A. Michelangelo",
        "B. Leonardo da Vinci",
        "C. Raphael",
        "D. Rembrandt"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of red blood cells is to transport oxygen.",
      "conflict_prompt": "The primary function of red blood cells is not to transport oxygen.",
      "question": "What is the primary function of red blood cells?",
      "options": [
        "A. Fight infection",
        "B. Transport oxygen",
        "C. Produce hormones",
        "D. Digest nutrients"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most widely spoken language in the world by number of native speakers is Mandarin Chinese.",
      "conflict_prompt": "The most widely spoken language in the world by number of native speakers is English.",
      "question": "Which language has the most native speakers in the world?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. Hindi",
        "D. Mandarin Chinese"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The deepest point in Earth's oceans is the Mariana Trench.",
      "conflict_prompt": "The deepest point in Earth's oceans is not the Mariana Trench.",
      "question": "What is the name of the deepest point in Earth's oceans?",
      "options": [
        "A. Mariana Trench",
        "B. Puerto Rico Trench",
        "C. Java Trench",
        "D. Tonga Trench"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary energy source for the Sun is nuclear fusion.",
      "conflict_prompt": "The primary energy source for the Sun is nuclear fission.",
      "question": "What is the primary process that powers the Sun?",
      "options": [
        "A. Chemical combustion",
        "B. Nuclear fission",
        "C. Nuclear fusion",
        "D. Gravitational contraction"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city in the United States by population is New York City.",
      "conflict_prompt": "The largest city in the United States by population is Los Angeles.",
      "question": "Which city is the largest in the United States by population?",
      "options": [
        "A. Los Angeles",
        "B. Chicago",
        "C. New York City",
        "D. Houston"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A mile is longer than a kilometer.",
      "conflict_prompt": "A mile is shorter than a kilometer.",
      "question": "Which is longer: a mile or a kilometer?",
      "options": [
        "A. Kilometer",
        "B. Mile",
        "C. They are equal",
        "D. Depends on context"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary organ of the respiratory system is the lungs.",
      "conflict_prompt": "The primary organ of the respiratory system is the liver.",
      "question": "What is the primary organ of the respiratory system?",
      "options": [
        "A. Heart",
        "B. Liver",
        "C. Lungs",
        "D. Kidneys"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Japan is the yen.",
      "conflict_prompt": "The currency of Japan is the won.",
      "question": "What is the currency of Japan?",
      "options": [
        "A. Yuan",
        "B. Won",
        "C. Yen",
        "D. Baht"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful powered flight was by the Wright brothers in 1903.",
      "conflict_prompt": "The first successful powered flight was not by the Wright brothers in 1903.",
      "question": "Who achieved the first successful powered flight in 1903?",
      "options": [
        "A. Santos-Dumont",
        "B. The Wright brothers",
        "C. Charles Lindbergh",
        "D. Amelia Earhart"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The standard SI unit of mass is the kilogram.",
      "conflict_prompt": "The standard SI unit of mass is the gram.",
      "question": "What is the standard SI unit of mass?",
      "options": [
        "A. Gram",
        "B. Kilogram",
        "C. Pound",
        "D. Ounce"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element iron has the symbol Fe.",
      "conflict_prompt": "The chemical element iron does not have the symbol Fe.",
      "question": "What is the chemical symbol for iron?",
      "options": [
        "A. Ir",
        "B. If",
        "C. Fe",
        "D. In"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest internal organ of the human body is the liver.",
      "conflict_prompt": "The largest internal organ of the human body is the spleen.",
      "question": "Which is the largest internal organ in the human body?",
      "options": [
        "A. Spleen",
        "B. Liver",
        "C. Heart",
        "D. Lungs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous physicist Marie Curie won Nobel Prizes in both Physics and Chemistry.",
      "conflict_prompt": "Marie Curie won Nobel Prizes only in Physics and not in Chemistry.",
      "question": "In which fields did Marie Curie win Nobel Prizes?",
      "options": [
        "A. Physics and Chemistry",
        "B. Physics only",
        "C. Chemistry only",
        "D. Medicine and Physics"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest organ in the human body is the skin.",
      "conflict_prompt": "The largest organ in the human body is the liver.",
      "question": "What is the largest organ of the human body?",
      "options": [
        "A. Liver",
        "B. Heart",
        "C. Skin",
        "D. Brain"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary author of the U.S. Constitution is often considered to be James Madison.",
      "conflict_prompt": "James Madison was not a primary author of the U.S. Constitution.",
      "question": "Who is often considered the primary author of the U.S. Constitution?",
      "options": [
        "A. Thomas Jefferson",
        "B. George Washington",
        "C. James Madison",
        "D. Benjamin Franklin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island in the world is Greenland.",
      "conflict_prompt": "The largest island in the world is Australia.",
      "question": "Which is the largest island in the world by area?",
      "options": [
        "A. Australia",
        "B. Greenland",
        "C. New Guinea",
        "D. Borneo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest land animal is the cheetah.",
      "conflict_prompt": "The fastest land animal is the pronghorn antelope.",
      "question": "Which animal is the fastest on land?",
      "options": [
        "A. Pronghorn antelope",
        "B. Cheetah",
        "C. Lion",
        "D. Gazelle"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary greenhouse gas produced by human activities is carbon dioxide.",
      "conflict_prompt": "The primary greenhouse gas produced by human activities is methane.",
      "question": "Which greenhouse gas is most abundantly produced by human activities?",
      "options": [
        "A. Methane",
        "B. Nitrous oxide",
        "C. Carbon dioxide",
        "D. Ozone"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Russia is Moscow.",
      "conflict_prompt": "The capital of Russia is Saint Petersburg.",
      "question": "What is the capital of Russia?",
      "options": [
        "A. Saint Petersburg",
        "B. Moscow",
        "C. Novosibirsk",
        "D. Kazan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common cold is most often caused by rhinoviruses.",
      "conflict_prompt": "The common cold is most often caused by bacteria.",
      "question": "What type of pathogen most commonly causes the common cold?",
      "options": [
        "A. Bacteria",
        "B. Fungi",
        "C. Viruses (rhinoviruses)",
        "D. Parasites"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The conventional SI unit for electric current is the ampere (amp).",
      "conflict_prompt": "The conventional SI unit for electric current is the volt.",
      "question": "What is the SI unit for electric current?",
      "options": [
        "A. Volt",
        "B. Watt",
        "C. Ohm",
        "D. Ampere"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The gene-editing technology CRISPR is based on a natural bacterial immune system.",
      "conflict_prompt": "CRISPR is not based on any natural bacterial immune system.",
      "question": "What is CRISPR technology based on?",
      "options": [
        "A. A synthetic chemical process",
        "B. A natural bacterial immune system",
        "C. Plant immune systems",
        "D. Viral replication machinery"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Australia is Canberra.",
      "conflict_prompt": "The capital of Australia is Sydney.",
      "question": "What is the capital city of Australia?",
      "options": [
        "A. Sydney",
        "B. Melbourne",
        "C. Canberra",
        "D. Brisbane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The composer Ludwig van Beethoven was born in Bonn, Germany.",
      "conflict_prompt": "Ludwig van Beethoven was born in Vienna, Austria.",
      "question": "Where was Ludwig van Beethoven born?",
      "options": [
        "A. Vienna, Austria",
        "B. Salzburg, Austria",
        "C. Bonn, Germany",
        "D. Berlin, Germany"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common chemical symbol for potassium is K.",
      "conflict_prompt": "The chemical symbol for potassium is P.",
      "question": "What is the chemical symbol for potassium?",
      "options": [
        "A. P",
        "B. Pt",
        "C. K",
        "D. Po"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning British monarch in recorded history was Queen Elizabeth II.",
      "conflict_prompt": "The longest reigning British monarch in recorded history was Queen Victoria.",
      "question": "Who is the longest reigning British monarch in recorded history?",
      "options": [
        "A. Queen Victoria",
        "B. King George III",
        "C. Queen Elizabeth II",
        "D. King Henry VIII"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The highest waterfall in the world is Angel Falls in Venezuela.",
      "conflict_prompt": "The highest waterfall in the world is Niagra Falls in Canada/USA.",
      "question": "Which waterfall is the highest in the world?",
      "options": [
        "A. Niagara Falls",
        "B. Victoria Falls",
        "C. Angel Falls",
        "D. Iguazu Falls"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The sport with five players per side on the court is basketball.",
      "conflict_prompt": "The sport with five players per side on the court is soccer.",
      "question": "Which sport is played with five players per team on the court?",
      "options": [
        "A. Soccer",
        "B. Basketball",
        "C. Baseball",
        "D. Hockey"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary building block of proteins are amino acids.",
      "conflict_prompt": "The primary building blocks of proteins are nucleotides.",
      "question": "What are the primary building blocks of proteins?",
      "options": [
        "A. Sugars",
        "B. Nucleotides",
        "C. Amino acids",
        "D. Fatty acids"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The act of an organism converting genetic information from DNA into RNA is called transcription.",
      "conflict_prompt": "Transcription is the conversion of RNA into DNA.",
      "question": "What is the process called that converts DNA into RNA?",
      "options": [
        "A. Translation",
        "B. Transcription",
        "C. Replication",
        "D. Translocation"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional Japanese sushi rice seasoning is rice vinegar.",
      "conflict_prompt": "The primary ingredient in traditional Japanese sushi rice seasoning is lemon juice.",
      "question": "What is the traditional acidic seasoning used for sushi rice?",
      "options": [
        "A. Lemon juice",
        "B. Rice vinegar",
        "C. Apple cider vinegar",
        "D. Soy sauce"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest volcano on Earth (by volume) is Mauna Loa in Hawaii.",
      "conflict_prompt": "The largest volcano on Earth by volume is Mount Kilimanjaro.",
      "question": "Which volcano is the largest on Earth by volume?",
      "options": [
        "A. Mount Kilimanjaro",
        "B. Mauna Loa",
        "C. Mount Fuji",
        "D. Mount St. Helens"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The North Star, Polaris, is located nearly at the north celestial pole.",
      "conflict_prompt": "The North Star, Polaris, is located near the south celestial pole.",
      "question": "Near which celestial pole is Polaris located?",
      "options": [
        "A. South celestial pole",
        "B. East celestial pole",
        "C. West celestial pole",
        "D. North celestial pole"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest producer of coffee worldwide is Brazil.",
      "conflict_prompt": "The largest producer of coffee worldwide is Vietnam.",
      "question": "Which country is the largest producer of coffee in the world?",
      "options": [
        "A. Vietnam",
        "B. Colombia",
        "C. Ethiopia",
        "D. Brazil"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The instrument commonly used to measure atmospheric pressure is a barometer.",
      "conflict_prompt": "The instrument commonly used to measure atmospheric pressure is a thermometer.",
      "question": "What instrument measures atmospheric pressure?",
      "options": [
        "A. Thermometer",
        "B. Hygrometer",
        "C. Barometer",
        "D. Anemometer"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Egypt is Cairo.",
      "conflict_prompt": "The capital of Egypt is Alexandria.",
      "question": "What is the capital of Egypt?",
      "options": [
        "A. Alexandria",
        "B. Cairo",
        "C. Giza",
        "D. Luxor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency used in India is the Indian rupee (INR).",
      "conflict_prompt": "The currency used in India is the Indian dollar.",
      "question": "What is the official currency of India?",
      "options": [
        "A. Indian dollar",
        "B. Pakistani rupee",
        "C. Indian rupee",
        "D. Nepalese rupee"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The classical music period known for composers like Mozart and Haydn is the Classical era.",
      "conflict_prompt": "Mozart and Haydn are composers from the Baroque era.",
      "question": "To which musical era do Mozart and Haydn belong?",
      "options": [
        "A. Baroque",
        "B. Romantic",
        "C. Classical",
        "D. Modern"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet by mass and volume in the Solar System is Jupiter.",
      "conflict_prompt": "The largest planet by mass and volume in the Solar System is Saturn.",
      "question": "Which planet is largest by mass and volume in the Solar System?",
      "options": [
        "A. Saturn",
        "B. Uranus",
        "C. Jupiter",
        "D. Neptune"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The device used to amplify electrical signals in early radios and electronics was the vacuum tube.",
      "conflict_prompt": "The device used to amplify electrical signals in early radios and electronics was the transistor.",
      "question": "What device was commonly used to amplify electrical signals in early radios before transistors?",
      "options": [
        "A. Transistor",
        "B. Vacuum tube",
        "C. Integrated circuit",
        "D. Diode"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize in Literature in 1954 was awarded to Ernest Hemingway.",
      "conflict_prompt": "The Nobel Prize in Literature in 1954 was not awarded to Ernest Hemingway.",
      "question": "Who won the Nobel Prize in Literature in 1954?",
      "options": [
        "A. William Faulkner",
        "B. Albert Camus",
        "C. Ernest Hemingway",
        "D. Gabriel García Márquez"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary component of natural gas used for heating and cooking is methane.",
      "conflict_prompt": "The primary component of natural gas used for heating and cooking is propane.",
      "question": "What is the main component of natural gas used in residential heating?",
      "options": [
        "A. Butane",
        "B. Propane",
        "C. Methane",
        "D. Ethane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest active volcano in Europe is Mount Etna on Sicily.",
      "conflict_prompt": "The largest active volcano in Europe is Mount Vesuvius on the Italian mainland.",
      "question": "Which is the largest active volcano in Europe?",
      "options": [
        "A. Mount Vesuvius",
        "B. Mount Etna",
        "C. Mount Stromboli",
        "D. Mount Olympus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary document that established the United Nations was signed in 1945 in San Francisco.",
      "conflict_prompt": "The United Nations was established by a treaty signed in 1919.",
      "question": "In what year was the United Nations established with its founding conference in San Francisco?",
      "options": [
        "A. 1919",
        "B. 1939",
        "C. 1945",
        "D. 1950"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Spain is Madrid.",
      "conflict_prompt": "The capital of Spain is Barcelona.",
      "question": "What is the capital of Spain?",
      "options": [
        "A. Barcelona",
        "B. Seville",
        "C. Madrid",
        "D. Valencia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest organelle in a plant cell is the central vacuole.",
      "conflict_prompt": "The largest organelle in a plant cell is the chloroplast.",
      "question": "Which organelle is typically the largest in a mature plant cell?",
      "options": [
        "A. Nucleus",
        "B. Chloroplast",
        "C. Central vacuole",
        "D. Mitochondrion"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The American Civil War ended in 1865.",
      "conflict_prompt": "The American Civil War ended in 1870.",
      "question": "In what year did the American Civil War end?",
      "options": [
        "A. 1861",
        "B. 1865",
        "C. 1870",
        "D. 1880"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous scientist Sir Isaac Newton formulated the laws of motion and universal gravitation.",
      "conflict_prompt": "Sir Isaac Newton did not formulate the laws of motion and universal gravitation.",
      "question": "Who formulated the classical laws of motion and universal gravitation?",
      "options": [
        "A. Galileo Galilei",
        "B. Johannes Kepler",
        "C. Sir Isaac Newton",
        "D. Albert Einstein"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary building blocks of DNA are nucleotides composed of a sugar, phosphate, and base.",
      "conflict_prompt": "The primary building blocks of DNA are amino acids.",
      "question": "What are the basic building blocks of DNA?",
      "options": [
        "A. Amino acids",
        "B. Nucleotides",
        "C. Monosaccharides",
        "D. Fatty acids"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The study of past human activity through excavation and analysis of artifacts is called archaeology.",
      "conflict_prompt": "The study of past human activity through excavation is called paleontology.",
      "question": "What is the field that studies past human activity through excavation of artifacts?",
      "options": [
        "A. Paleontology",
        "B. Archaeology",
        "C. Anthropology",
        "D. Geology"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The basic unit of life is the cell.",
      "conflict_prompt": "The basic unit of life is the atom.",
      "question": "What is considered the basic unit of life?",
      "options": [
        "A. Atom",
        "B. Cell",
        "C. Molecule",
        "D. Organ"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Germany is Berlin.",
      "conflict_prompt": "The capital of Germany is Munich.",
      "question": "What is the capital of Germany?",
      "options": [
        "A. Munich",
        "B. Frankfurt",
        "C. Berlin",
        "D. Hamburg"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial satellite launched into orbit was Sputnik 1 by the Soviet Union in 1957.",
      "conflict_prompt": "The first artificial satellite launched into orbit was Explorer 1 by the United States in 1957.",
      "question": "Which was the first artificial satellite launched into orbit in 1957?",
      "options": [
        "A. Explorer 1",
        "B. Sputnik 1",
        "C. Vanguard 1",
        "D. Luna 1"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary cause of tides on Earth is the gravitational pull of the Moon and the Sun.",
      "conflict_prompt": "Tides on Earth are caused primarily by Earth's rotation alone.",
      "question": "What primarily causes tides on Earth?",
      "options": [
        "A. Earth's rotation alone",
        "B. Wind patterns",
        "C. Gravitational pull of the Moon and Sun",
        "D. Ocean currents"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous painting 'The Starry Night' was painted by Vincent van Gogh.",
      "conflict_prompt": "The famous painting 'The Starry Night' was painted by Claude Monet.",
      "question": "Who painted 'The Starry Night'?",
      "options": [
        "A. Claude Monet",
        "B. Vincent van Gogh",
        "C. Pablo Picasso",
        "D. Edvard Munch"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The classical gas law PV = nRT relates pressure, volume, amount of gas, and temperature for ideal gases.",
      "conflict_prompt": "The gas law PV = nRT is unrelated to pressure and volume of gases.",
      "question": "Which law relates pressure and volume to temperature and amount for ideal gases?",
      "options": [
        "A. Newton's second law",
        "B. Hooke's law",
        "C. PV = nRT (ideal gas law)",
        "D. Ohm's law"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Turkey is Ankara.",
      "conflict_prompt": "The capital of Turkey is Istanbul.",
      "question": "What is the capital of Turkey?",
      "options": [
        "A. Istanbul",
        "B. Ankara",
        "C. Izmir",
        "D. Bursa"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first woman to fly solo across the Atlantic Ocean was Amelia Earhart.",
      "conflict_prompt": "The first woman to fly solo across the Atlantic Ocean was Bessie Coleman.",
      "question": "Who was the first woman to fly solo across the Atlantic Ocean?",
      "options": [
        "A. Bessie Coleman",
        "B. Amelia Earhart",
        "C. Harriet Quimby",
        "D. Jacqueline Cochran"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest freshwater lake by surface area is Lake Superior.",
      "conflict_prompt": "The largest freshwater lake by surface area is Lake Baikal.",
      "question": "Which is the largest freshwater lake by surface area?",
      "options": [
        "A. Lake Baikal",
        "B. Lake Victoria",
        "C. Lake Superior",
        "D. Caspian Sea"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The period of rapid economic growth in postwar Japan is often called the Japanese economic miracle.",
      "conflict_prompt": "The period of rapid economic growth in postwar Japan is commonly called the Great Depression.",
      "question": "What term is commonly used for Japan's rapid postwar economic growth?",
      "options": [
        "A. Great Depression",
        "B. Meiji Restoration",
        "C. Japanese economic miracle",
        "D. Cultural Revolution"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The two main types of nucleic acids in biology are DNA and RNA.",
      "conflict_prompt": "The two main types of nucleic acids in biology are DNA and proteins.",
      "question": "What are the two primary types of nucleic acids?",
      "options": [
        "A. DNA and proteins",
        "B. RNA and proteins",
        "C. DNA and RNA",
        "D. Carbohydrates and lipids"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary colors of light are red, green, and blue.",
      "conflict_prompt": "The primary colors of light are red, yellow, and blue.",
      "question": "Which set lists the primary colors of light?",
      "options": [
        "A. Red, yellow, blue",
        "B. Red, green, blue",
        "C. Cyan, magenta, yellow",
        "D. Black, white, gray"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most populous city in India is Mumbai by city proper population.",
      "conflict_prompt": "The most populous city in India is New Delhi by city proper population.",
      "question": "Which city is the most populous in India by city proper population?",
      "options": [
        "A. New Delhi",
        "B. Kolkata",
        "C. Mumbai",
        "D. Chennai"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous scientist Charles Darwin is known for proposing the theory of evolution by natural selection.",
      "conflict_prompt": "Charles Darwin is not associated with the theory of evolution by natural selection.",
      "question": "Who proposed the theory of evolution by natural selection?",
      "options": [
        "A. Gregor Mendel",
        "B. Charles Darwin",
        "C. Louis Pasteur",
        "D. James Watson"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest bay in the world by area is the Bay of Bengal.",
      "conflict_prompt": "The largest bay in the world by area is Hudson Bay.",
      "question": "Which is the largest bay in the world by area?",
      "options": [
        "A. Bay of Bengal",
        "B. Hudson Bay",
        "C. Gulf of Mexico",
        "D. Gulf of Guinea"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The composer of the music for the ballet 'Swan Lake' was Pyotr Ilyich Tchaikovsky.",
      "conflict_prompt": "The composer of the music for 'Swan Lake' was Igor Stravinsky.",
      "question": "Who composed the music for 'Swan Lake'?",
      "options": [
        "A. Igor Stravinsky",
        "B. Pyotr Ilyich Tchaikovsky",
        "C. Sergei Prokofiev",
        "D. Dmitri Shostakovich"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Taj Mahal is located in Agra, India.",
      "conflict_prompt": "The Taj Mahal is located in Delhi, India.",
      "question": "In which city is the Taj Mahal located?",
      "options": [
        "A. Delhi",
        "B. Agra",
        "C. Jaipur",
        "D. Lucknow"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The standard boiling point of water at sea level is 100 degrees Celsius.",
      "conflict_prompt": "The standard boiling point of water at sea level is 90 degrees Celsius.",
      "question": "At standard sea-level pressure, what is the boiling point of water?",
      "options": [
        "A. 90°C",
        "B. 100°C",
        "C. 212°C",
        "D. 0°C"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest exporter of crude oil historically has been Saudi Arabia.",
      "conflict_prompt": "The largest exporter of crude oil historically has been Russia.",
      "question": "Which country has historically been the largest exporter of crude oil?",
      "options": [
        "A. Russia",
        "B. United States",
        "C. Saudi Arabia",
        "D. Canada"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Argentina is Buenos Aires.",
      "conflict_prompt": "The capital of Argentina is Córdoba.",
      "question": "What is the capital of Argentina?",
      "options": [
        "A. Córdoba",
        "B. Mendoza",
        "C. Buenos Aires",
        "D. Rosario"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The scientific name for modern humans is Homo sapiens.",
      "conflict_prompt": "The scientific name for modern humans is Homo erectus.",
      "question": "What is the scientific name for modern humans?",
      "options": [
        "A. Homo erectus",
        "B. Homo habilis",
        "C. Homo sapiens",
        "D. Australopithecus afarensis"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first President of the United States was George Washington.",
      "conflict_prompt": "The first President of the United States was John Adams.",
      "question": "Who was the first President of the United States?",
      "options": [
        "A. John Adams",
        "B. Thomas Jefferson",
        "C. George Washington",
        "D. James Madison"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The programming language Python was created by Guido van Rossum.",
      "conflict_prompt": "Python was created by Brendan Eich.",
      "question": "Who created the Python programming language?",
      "options": [
        "A. Brendan Eich",
        "B. Guido van Rossum",
        "C. James Gosling",
        "D. Bjarne Stroustrup"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest landlocked country by area is Kazakhstan.",
      "conflict_prompt": "The largest landlocked country by area is Mongolia.",
      "question": "Which is the largest landlocked country by area?",
      "options": [
        "A. Mongolia",
        "B. Kazakhstan",
        "C. Chad",
        "D. Niger"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The lightest noble gas is helium.",
      "conflict_prompt": "The lightest noble gas is neon.",
      "question": "Which noble gas is the lightest?",
      "options": [
        "A. Neon",
        "B. Argon",
        "C. Helium",
        "D. Krypton"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest sea mammal predator is the orca (killer whale).",
      "conflict_prompt": "The largest sea mammal predator is the great white shark.",
      "question": "Which is the largest predatory marine mammal?",
      "options": [
        "A. Great white shark",
        "B. Orca (killer whale)",
        "C. Sperm whale",
        "D. Tiger shark"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most abundant element in the Earth's crust by weight is oxygen.",
      "conflict_prompt": "The most abundant element in the Earth's crust by weight is silicon.",
      "question": "Which element is most abundant in Earth's crust by weight?",
      "options": [
        "A. Silicon",
        "B. Aluminum",
        "C. Oxygen",
        "D. Iron"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The founder of the Mongol Empire was Genghis Khan.",
      "conflict_prompt": "The founder of the Mongol Empire was Kublai Khan.",
      "question": "Who founded the Mongol Empire?",
      "options": [
        "A. Kublai Khan",
        "B. Ogedei Khan",
        "C. Genghis Khan",
        "D. Timur"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest species of shark is the whale shark.",
      "conflict_prompt": "The largest species of shark is the great white shark.",
      "question": "Which species is the largest shark?",
      "options": [
        "A. Great white shark",
        "B. Tiger shark",
        "C. Whale shark",
        "D. Basking shark"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Disney movie 'The Lion King' was originally released in 1994.",
      "conflict_prompt": "The Disney movie 'The Lion King' was originally released in 2004.",
      "question": "In what year was the original release of Disney's 'The Lion King'?",
      "options": [
        "A. 1994",
        "B. 1999",
        "C. 2004",
        "D. 1989"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The instrument used to measure electric resistance is the ohmmeter.",
      "conflict_prompt": "The instrument used to measure electric resistance is the voltmeter.",
      "question": "What instrument is used to measure electrical resistance?",
      "options": [
        "A. Voltmeter",
        "B. Ammeter",
        "C. Ohmmeter",
        "D. Wattmeter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The founder of Microsoft is Bill Gates and Paul Allen.",
      "conflict_prompt": "Microsoft was founded solely by Bill Gates without Paul Allen.",
      "question": "Who co-founded Microsoft along with Bill Gates?",
      "options": [
        "A. Steve Ballmer",
        "B. Paul Allen",
        "C. Steve Jobs",
        "D. Larry Page"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest breed of dog by height is the Irish Wolfhound.",
      "conflict_prompt": "The largest breed of dog by height is the Chihuahua.",
      "question": "Which breed is typically considered the tallest dog by height?",
      "options": [
        "A. Chihuahua",
        "B. Irish Wolfhound",
        "C. Dachshund",
        "D. Beagle"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main ingredient in traditional hummus is chickpeas (garbanzo beans).",
      "conflict_prompt": "The main ingredient in traditional hummus is lentils.",
      "question": "What is the primary ingredient in traditional hummus?",
      "options": [
        "A. Lentils",
        "B. Chickpeas",
        "C. Black beans",
        "D. Peas"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary musical instrument associated with jazz is the saxophone (among others).",
      "conflict_prompt": "The primary musical instrument associated with jazz is the sitar.",
      "question": "Which instrument is commonly associated with jazz music?",
      "options": [
        "A. Sitar",
        "B. Bagpipes",
        "C. Saxophone",
        "D. Balalaika"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The average adult human body has 206 bones.",
      "conflict_prompt": "The average adult human body has 306 bones.",
      "question": "How many bones does an average adult human have?",
      "options": [
        "A. 306",
        "B. 206",
        "C. 106",
        "D. 406"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Eiffel Tower is located in Paris, France.",
      "conflict_prompt": "The Eiffel Tower is located in Lyon, France.",
      "question": "In which city is the Eiffel Tower located?",
      "options": [
        "A. Lyon",
        "B. Marseille",
        "C. Nice",
        "D. Paris"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest moon of Saturn is Titan.",
      "conflict_prompt": "The largest moon of Saturn is Enceladus.",
      "question": "Which is the largest moon of Saturn?",
      "options": [
        "A. Enceladus",
        "B. Titan",
        "C. Rhea",
        "D. Iapetus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The founder of modern nursing is Florence Nightingale.",
      "conflict_prompt": "Florence Nightingale is not associated with founding modern nursing.",
      "question": "Who is commonly considered the founder of modern nursing?",
      "options": [
        "A. Clara Barton",
        "B. Florence Nightingale",
        "C. Mary Seacole",
        "D. Margaret Sanger"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Pyramid of Giza was built as a tomb for the Pharaoh Khufu.",
      "conflict_prompt": "The Great Pyramid of Giza was built as a tomb for the Pharaoh Tutankhamun.",
      "question": "For which Pharaoh was the Great Pyramid of Giza built?",
      "options": [
        "A. Tutankhamun",
        "B. Ramses II",
        "C. Khufu",
        "D. Akhenaten"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol 'Ag' is silver.",
      "conflict_prompt": "The chemical element with symbol 'Ag' is gold.",
      "question": "Which element has the chemical symbol 'Ag'?",
      "options": [
        "A. Gold",
        "B. Silver",
        "C. Argon",
        "D. Aluminium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas humans exhale is carbon dioxide.",
      "conflict_prompt": "Humans primarily exhale oxygen.",
      "question": "What gas is primarily exhaled by humans?",
      "options": [
        "A. Oxygen",
        "B. Helium",
        "C. Nitrogen",
        "D. Carbon dioxide"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Greece is Athens.",
      "conflict_prompt": "The capital of Greece is Thessaloniki.",
      "question": "What is the capital of Greece?",
      "options": [
        "A. Thessaloniki",
        "B. Athens",
        "C. Patras",
        "D. Heraklion"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most common blood type globally is O positive.",
      "conflict_prompt": "The most common blood type globally is AB negative.",
      "question": "Which blood type is most common worldwide?",
      "options": [
        "A. AB negative",
        "B. A positive",
        "C. O positive",
        "D. B negative"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary material used to make traditional glass is silica (silicon dioxide).",
      "conflict_prompt": "Traditional glass is primarily made from calcium carbonate.",
      "question": "What is the primary raw material used to make traditional glass?",
      "options": [
        "A. Calcium carbonate",
        "B. Silica (silicon dioxide)",
        "C. Aluminum oxide",
        "D. Iron oxide"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest archipelago country by number of islands is Indonesia.",
      "conflict_prompt": "The largest archipelago country by number of islands is the Philippines.",
      "question": "Which country is the largest archipelago by number of islands?",
      "options": [
        "A. Philippines",
        "B. Japan",
        "C. Indonesia",
        "D. Greece"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The term 'Renaissance' refers to a cultural movement that began in Italy in the 14th century.",
      "conflict_prompt": "The Renaissance began in France in the 14th century.",
      "question": "In which country did the Renaissance cultural movement originate?",
      "options": [
        "A. France",
        "B. Spain",
        "C. Italy",
        "D. England"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical compound H2O is the formula for water.",
      "conflict_prompt": "The chemical compound H2O is the formula for hydrogen peroxide.",
      "question": "What compound has the chemical formula H2O?",
      "options": [
        "A. Hydrogen peroxide",
        "B. Water",
        "C. Carbon dioxide",
        "D. Methane"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of South Korea is Seoul.",
      "conflict_prompt": "The capital city of South Korea is Busan.",
      "question": "What is the capital of South Korea?",
      "options": [
        "A. Busan",
        "B. Incheon",
        "C. Seoul",
        "D. Daegu"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The metric prefix 'kilo-' denotes a factor of one thousand (10^3).",
      "conflict_prompt": "The metric prefix 'kilo-' denotes a factor of one million (10^6).",
      "question": "What factor does the metric prefix 'kilo-' represent?",
      "options": [
        "A. 10^6 (one million)",
        "B. 10^9 (one billion)",
        "C. 10^3 (one thousand)",
        "D. 10^2 (one hundred)"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous novelist George Orwell wrote '1984'.",
      "conflict_prompt": "George Orwell did not write '1984'.",
      "question": "Who wrote the novel '1984'?",
      "options": [
        "A. Aldous Huxley",
        "B. George Orwell",
        "C. Ray Bradbury",
        "D. J.R.R. Tolkien"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning pope in modern history was Pope John Paul II.",
      "conflict_prompt": "The longest reigning pope in modern history was Pope Benedict XVI.",
      "question": "Who was the long-reigning pope of the late 20th century known for a lengthy papacy?",
      "options": [
        "A. Pope Benedict XVI",
        "B. Pope John Paul II",
        "C. Pope Francis",
        "D. Pope Pius XII"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest continent entirely in the Western Hemisphere is North America.",
      "conflict_prompt": "The largest continent entirely in the Western Hemisphere is South America.",
      "question": "Which continent is the largest entirely within the Western Hemisphere?",
      "options": [
        "A. South America",
        "B. Europe",
        "C. North America",
        "D. Africa"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous physicist who developed the special theory of relativity is Albert Einstein.",
      "conflict_prompt": "Albert Einstein did not develop the special theory of relativity.",
      "question": "Who developed the special theory of relativity?",
      "options": [
        "A. Niels Bohr",
        "B. Albert Einstein",
        "C. Werner Heisenberg",
        "D. Max Planck"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest river in South America is the Amazon River.",
      "conflict_prompt": "The longest river in South America is the Paraná River.",
      "question": "Which is the longest river in South America?",
      "options": [
        "A. Paraná River",
        "B. Amazon River",
        "C. Orinoco River",
        "D. Magdalena River"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 6 is carbon.",
      "conflict_prompt": "The chemical element with atomic number 6 is nitrogen.",
      "question": "Which element has atomic number 6?",
      "options": [
        "A. Nitrogen",
        "B. Oxygen",
        "C. Carbon",
        "D. Boron"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary sector of the economy involves extraction of raw materials, such as mining and agriculture.",
      "conflict_prompt": "The primary sector of the economy involves manufacturing and services.",
      "question": "Which activities belong to the primary sector of the economy?",
      "options": [
        "A. Manufacturing and retail",
        "B. Banking and insurance",
        "C. Extraction of raw materials (mining, agriculture)",
        "D. Software development"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Mexico is Mexico City.",
      "conflict_prompt": "The capital of Mexico is Guadalajara.",
      "question": "What is the capital of Mexico?",
      "options": [
        "A. Guadalajara",
        "B. Monterrey",
        "C. Mexico City",
        "D. Tijuana"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The star at the center of our solar system is the Sun.",
      "conflict_prompt": "The center of our solar system is a black hole, not the Sun.",
      "question": "What object is at the center of our solar system?",
      "options": [
        "A. A black hole",
        "B. The Sun",
        "C. Planet Jupiter",
        "D. A neutron star"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The device used to convert alternating current (AC) to direct current (DC) is called a rectifier.",
      "conflict_prompt": "The device used to convert AC to DC is called an inverter.",
      "question": "What device converts alternating current (AC) to direct current (DC)?",
      "options": [
        "A. Inverter",
        "B. Transformer",
        "C. Rectifier",
        "D. Oscillator"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The traditional birthplace of democracy is ancient Athens in Greece.",
      "conflict_prompt": "The traditional birthplace of democracy is ancient Rome in Italy.",
      "question": "Which city-state is traditionally considered the birthplace of democracy?",
      "options": [
        "A. Rome",
        "B. Sparta",
        "C. Athens",
        "D. Carthage"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The number of continents commonly recognized is seven.",
      "conflict_prompt": "The number of continents commonly recognized is six.",
      "question": "How many continents are commonly recognized in the usual model?",
      "options": [
        "A. Five",
        "B. Six",
        "C. Seven",
        "D. Eight"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary components of air at sea level are nitrogen (~78%) and oxygen (~21%).",
      "conflict_prompt": "The primary components of air at sea level are oxygen (~78%) and nitrogen (~21%).",
      "question": "Which two gases make up the majority of Earth's atmosphere at sea level?",
      "options": [
        "A. Oxygen and carbon dioxide",
        "B. Nitrogen and oxygen",
        "C. Argon and nitrogen",
        "D. Helium and hydrogen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous speech 'I Have a Dream' was delivered by Martin Luther King Jr. in 1963.",
      "conflict_prompt": "The 'I Have a Dream' speech was delivered by Malcolm X in 1963.",
      "question": "Who delivered the 'I Have a Dream' speech in 1963?",
      "options": [
        "A. Malcolm X",
        "B. Martin Luther King Jr.",
        "C. Booker T. Washington",
        "D. W.E.B. Du Bois"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The SI base unit for temperature is the kelvin.",
      "conflict_prompt": "The SI base unit for temperature is the degree Celsius.",
      "question": "What is the SI base unit for temperature?",
      "options": [
        "A. Degree Celsius",
        "B. Degree Fahrenheit",
        "C. Kelvin",
        "D. Rankine"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The world's tallest tree species by average maximum height is the coast redwood (Sequoia sempervirens).",
      "conflict_prompt": "The world's tallest tree species by average maximum height is the baobab.",
      "question": "Which tree species is known for being the tallest on average?",
      "options": [
        "A. Baobab",
        "B. Coast redwood (Sequoia sempervirens)",
        "C. Douglas fir",
        "D. Eucalyptus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest economy in the world by nominal GDP is the United States.",
      "conflict_prompt": "The largest economy in the world by nominal GDP is China.",
      "question": "Which country has the largest nominal GDP as of recent years?",
      "options": [
        "A. China",
        "B. Japan",
        "C. Germany",
        "D. United States"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary mineral in limestone is calcium carbonate (CaCO3).",
      "conflict_prompt": "The primary mineral in limestone is silicon dioxide (SiO2).",
      "question": "What is the primary mineral component of limestone?",
      "options": [
        "A. Silicon dioxide (SiO2)",
        "B. Calcium carbonate (CaCO3)",
        "C. Sodium chloride (NaCl)",
        "D. Iron oxide (Fe2O3)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous physicist who developed the three laws of planetary motion was Johannes Kepler.",
      "conflict_prompt": "Johannes Kepler did not formulate the laws of planetary motion.",
      "question": "Who formulated the three laws of planetary motion?",
      "options": [
        "A. Galileo Galilei",
        "B. Johannes Kepler",
        "C. Nicolaus Copernicus",
        "D. Tycho Brahe"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Eiffel Tower is located in Paris, France.",
      "conflict_prompt": "The Eiffel Tower is located in Paris, France but it is situated in Berlin.",
      "question": "In which city is the Eiffel Tower located?",
      "options": [
        "A. London",
        "B. Rome",
        "C. Paris",
        "D. Berlin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Everest is the highest mountain on Earth.",
      "conflict_prompt": "Mount Everest is the highest mountain on Earth and it is the lowest mountain on Earth.",
      "question": "What is Mount Everest's distinction among Earth's mountains?",
      "options": [
        "A. It is the lowest mountain",
        "B. It is the oldest mountain",
        "C. It is the highest mountain",
        "D. It is the flattest mountain"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nile is commonly considered the longest river in the world.",
      "conflict_prompt": "The Nile is commonly considered the longest river in the world and also the shortest river in the world.",
      "question": "Which of the following best describes the Nile River's length ranking?",
      "options": [
        "A. Shortest river in the world",
        "B. Most polluted river in the world",
        "C. Longest river in the world",
        "D. Deepest river in the world"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Ocean is the largest ocean on Earth by area.",
      "conflict_prompt": "The Pacific Ocean is the largest ocean on Earth by area and it is the smallest ocean on Earth by area.",
      "question": "Which ocean is the largest by area?",
      "options": [
        "A. Atlantic Ocean",
        "B. Indian Ocean",
        "C. Arctic Ocean",
        "D. Pacific Ocean"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human heart has four chambers: two atria and two ventricles.",
      "conflict_prompt": "The human heart has four chambers: two atria and two ventricles, and it has zero chambers.",
      "question": "How many chambers does the human heart have?",
      "options": [
        "A. Two",
        "B. Four",
        "C. Six",
        "D. Zero"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light from the Sun takes about 8 minutes to reach Earth.",
      "conflict_prompt": "Light from the Sun takes about 8 minutes to reach Earth and it takes several days to reach Earth.",
      "question": "Approximately how long does it take sunlight to travel from the Sun to Earth?",
      "options": [
        "A. 8 minutes",
        "B. 24 hours",
        "C. 1 week",
        "D. 1 year"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Water freezes at 0 degrees Celsius at standard atmospheric pressure.",
      "conflict_prompt": "Water freezes at 0 degrees Celsius at standard atmospheric pressure and it freezes at 100 degrees Celsius at standard atmospheric pressure.",
      "question": "At what temperature does pure water freeze at standard atmospheric pressure?",
      "options": [
        "A. 100 °C",
        "B. 0 °C",
        "C. 50 °C",
        "D. -100 °C"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The United States declared independence in 1776.",
      "conflict_prompt": "The United States declared independence in 1776 and it declared independence in 1800.",
      "question": "In what year did the United States declare independence?",
      "options": [
        "A. 1800",
        "B. 1776",
        "C. 1865",
        "D. 1492"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Albert Einstein developed the theory of general relativity.",
      "conflict_prompt": "Albert Einstein developed the theory of general relativity and he also rejected the theory entirely.",
      "question": "Which physicist developed the theory of general relativity?",
      "options": [
        "A. Isaac Newton",
        "B. Nikola Tesla",
        "C. Albert Einstein",
        "D. Galileo Galilei"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Wall of China was primarily built to protect against invasions.",
      "conflict_prompt": "The Great Wall of China was primarily built to protect against invasions and it was built to allow free passage of invaders.",
      "question": "What was the primary purpose of the Great Wall of China historically?",
      "options": [
        "A. A trade route",
        "B. A religious site",
        "C. A defensive structure against invasions",
        "D. A canal system"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Moon orbits the Earth approximately once every 27.3 days (sidereal month).",
      "conflict_prompt": "The Moon orbits the Earth approximately once every 27.3 days and it does not orbit the Earth at all.",
      "question": "Approximately how long is the Moon's sidereal orbital period around Earth?",
      "options": [
        "A. 365 days",
        "B. 27.3 days",
        "C. 12 hours",
        "D. 7 days"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Photosynthesis in plants converts carbon dioxide and water into glucose using sunlight.",
      "conflict_prompt": "Photosynthesis in plants converts carbon dioxide and water into glucose using sunlight and it converts glucose into sunlight.",
      "question": "What do plants produce during photosynthesis from carbon dioxide and water using sunlight?",
      "options": [
        "A. Iron",
        "B. Glucose",
        "C. Oxygen only",
        "D. Nitrogen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of Japan is Tokyo.",
      "conflict_prompt": "The capital city of Japan is Tokyo and the capital city of Japan is also Madrid.",
      "question": "What is the capital city of Japan?",
      "options": [
        "A. Seoul",
        "B. Beijing",
        "C. Tokyo",
        "D. Madrid"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Shakespeare wrote the play 'Romeo and Juliet'.",
      "conflict_prompt": "Shakespeare wrote the play 'Romeo and Juliet' and he did not write it at all.",
      "question": "Who wrote 'Romeo and Juliet'?",
      "options": [
        "A. Charles Dickens",
        "B. William Shakespeare",
        "C. J.K. Rowling",
        "D. Mark Twain"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Honeybees communicate the location of food sources to each other through a waggle dance.",
      "conflict_prompt": "Honeybees communicate the location of food sources through a waggle dance and they never communicate location information.",
      "question": "How do honeybees communicate the location of food to nestmates?",
      "options": [
        "A. By written messages",
        "B. By waggle dance",
        "C. By scentless silence",
        "D. By telephone"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty was a gift from France to the United States.",
      "conflict_prompt": "The Statue of Liberty was a gift from France to the United States and it was built and gifted by Japan.",
      "question": "Which country gifted the Statue of Liberty to the United States?",
      "options": [
        "A. Spain",
        "B. France",
        "C. Italy",
        "D. Japan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element oxygen has the atomic number 8.",
      "conflict_prompt": "The element oxygen has the atomic number 8 and it has atomic number 92.",
      "question": "What is the atomic number of oxygen?",
      "options": [
        "A. 92",
        "B. 8",
        "C. 16",
        "D. 1"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Jupiter is the largest planet in our solar system by mass and diameter.",
      "conflict_prompt": "Jupiter is the largest planet in our solar system by mass and diameter and it is also the smallest planet in the solar system.",
      "question": "Which planet is the largest in our solar system?",
      "options": [
        "A. Mercury",
        "B. Mars",
        "C. Earth",
        "D. Jupiter"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A leap year occurs every four years with the insertion of February 29.",
      "conflict_prompt": "A leap year occurs every four years with the insertion of February 29 and there are never leap years.",
      "question": "How often does a leap year occur in the Gregorian calendar (generally)?",
      "options": [
        "A. Every year",
        "B. Every two years",
        "C. Every four years",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon rainforest is often called the 'lungs of the Earth' because of its vast capacity for photosynthesis.",
      "conflict_prompt": "The Amazon rainforest is often called the 'lungs of the Earth' because of its vast capacity for photosynthesis and it produces no oxygen at all.",
      "question": "Why is the Amazon rainforest sometimes called the 'lungs of the Earth'?",
      "options": [
        "A. Because it is made of metal",
        "B. Because of its role in oxygen production through photosynthesis",
        "C. Because it breathes air like an animal",
        "D. Because it is underground"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Honey is produced by bees from nectar and can last for many years without spoiling.",
      "conflict_prompt": "Honey is produced by bees from nectar and can last for many years without spoiling and it spoils within a day.",
      "question": "Which of the following is true about honey produced by bees?",
      "options": [
        "A. It spoils within a day",
        "B. It is produced from nectar and can remain edible for years",
        "C. It is made of metal",
        "D. It is poisonous to humans always"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official language of Brazil is Portuguese.",
      "conflict_prompt": "The official language of Brazil is Portuguese and the official language of Brazil is Swedish.",
      "question": "What is the official language of Brazil?",
      "options": [
        "A. Spanish",
        "B. English",
        "C. Portuguese",
        "D. Swedish"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Continental Congress met in Philadelphia during the American Revolution.",
      "conflict_prompt": "The Continental Congress met in Philadelphia during the American Revolution and it met nowhere at all.",
      "question": "In which city did the Continental Congress meet during the American Revolution?",
      "options": [
        "A. Boston",
        "B. Philadelphia",
        "C. New York",
        "D. Atlanta"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest mammal is the blue whale.",
      "conflict_prompt": "The largest mammal is the blue whale and the largest mammal is a mouse.",
      "question": "Which animal is the largest mammal on Earth?",
      "options": [
        "A. African elephant",
        "B. Blue whale",
        "C. Giraffe",
        "D. Mouse"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Venus rotates on its axis more slowly than Earth and has a retrograde rotation.",
      "conflict_prompt": "Venus rotates on its axis more slowly than Earth and has a retrograde rotation, and it rotates rapidly in the same direction as Earth.",
      "question": "How does Venus rotate compared to Earth?",
      "options": [
        "A. Faster with the same direction as Earth",
        "B. More slowly with retrograde rotation",
        "C. It does not rotate",
        "D. It teleports instead of rotating"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of the United Kingdom is the pound sterling (GBP).",
      "conflict_prompt": "The currency of the United Kingdom is the pound sterling (GBP) and the currency is the euro as its only official currency.",
      "question": "What is the official currency of the United Kingdom?",
      "options": [
        "A. Euro",
        "B. US Dollar",
        "C. Pound sterling",
        "D. Yen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Spanish Armada was defeated by England in 1588.",
      "conflict_prompt": "The Spanish Armada was defeated by England in 1588 and it was victorious and conquered England the same year.",
      "question": "What happened to the Spanish Armada in 1588?",
      "options": [
        "A. It conquered England",
        "B. It was defeated by England",
        "C. It discovered America",
        "D. It sank due to a volcanic eruption"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Isaac Newton formulated the laws of motion and universal gravitation.",
      "conflict_prompt": "Isaac Newton formulated the laws of motion and universal gravitation and he denied gravity existed.",
      "question": "Which scientist formulated the laws of motion and universal gravitation?",
      "options": [
        "A. Marie Curie",
        "B. Isaac Newton",
        "C. Thomas Edison",
        "D. Louis Pasteur"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The ozone layer absorbs much of the Sun's ultraviolet radiation.",
      "conflict_prompt": "The ozone layer absorbs much of the Sun's ultraviolet radiation and it amplifies all ultraviolet radiation passing through it.",
      "question": "What is one primary function of the Earth's ozone layer?",
      "options": [
        "A. To amplify ultraviolet radiation",
        "B. To absorb much of the Sun's ultraviolet radiation",
        "C. To produce visible light",
        "D. To increase global temperatures by direct heating"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Leonardo da Vinci painted the Mona Lisa.",
      "conflict_prompt": "Leonardo da Vinci painted the Mona Lisa and he did not create it at all.",
      "question": "Who painted the Mona Lisa?",
      "options": [
        "A. Vincent van Gogh",
        "B. Pablo Picasso",
        "C. Leonardo da Vinci",
        "D. Andy Warhol"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Canada is Ottawa.",
      "conflict_prompt": "The capital of Canada is Ottawa and the capital is also Toronto.",
      "question": "What is the capital city of Canada?",
      "options": [
        "A. Vancouver",
        "B. Ottawa",
        "C. Toronto",
        "D. Montreal"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Sound travels faster in water than in air.",
      "conflict_prompt": "Sound travels faster in water than in air and sound travels slower in water than in air.",
      "question": "In which medium does sound generally travel faster?",
      "options": [
        "A. Air",
        "B. Vacuum",
        "C. Water",
        "D. Outer space"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first man to walk on the Moon was Neil Armstrong in 1969.",
      "conflict_prompt": "The first man to walk on the Moon was Neil Armstrong in 1969 and the first man to walk on the Moon was not Neil Armstrong.",
      "question": "Who was the first man to walk on the Moon?",
      "options": [
        "A. Yuri Gagarin",
        "B. Neil Armstrong",
        "C. Buzz Aldrin",
        "D. Michael Collins"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest land animal is the cheetah.",
      "conflict_prompt": "The fastest land animal is the cheetah and it is also the slowest land animal.",
      "question": "Which animal is known as the fastest land animal?",
      "options": [
        "A. Sloth",
        "B. Elephant",
        "C. Cheetah",
        "D. Tortoise"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Barrier Reef is located off the coast of Queensland, Australia.",
      "conflict_prompt": "The Great Barrier Reef is located off the coast of Queensland, Australia and it is located in the center of the Sahara Desert.",
      "question": "Off the coast of which Australian state is the Great Barrier Reef located?",
      "options": [
        "A. New South Wales",
        "B. Victoria",
        "C. Queensland",
        "D. Tasmania"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Antarctica is the coldest continent on Earth.",
      "conflict_prompt": "Antarctica is the coldest continent on Earth and it is the hottest continent on Earth.",
      "question": "Which continent is the coldest on Earth?",
      "options": [
        "A. Africa",
        "B. Antarctica",
        "C. Australia",
        "D. South America"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas in Earth's atmosphere is nitrogen, making up about 78%.",
      "conflict_prompt": "The primary gas in Earth's atmosphere is nitrogen, making up about 78%, and oxygen makes up 100%.",
      "question": "What gas makes up about 78% of Earth's atmosphere?",
      "options": [
        "A. Oxygen",
        "B. Nitrogen",
        "C. Carbon dioxide",
        "D. Helium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Wright brothers made the first controlled, powered airplane flight in 1903.",
      "conflict_prompt": "The Wright brothers made the first controlled, powered airplane flight in 1903 and they never built any airplane.",
      "question": "Who is credited with the first controlled, powered airplane flight in 1903?",
      "options": [
        "A. The Wright brothers",
        "B. The Montgolfier brothers",
        "C. The Lumière brothers",
        "D. The Marx brothers"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A solar eclipse occurs when the Moon passes between the Sun and Earth, casting a shadow on Earth.",
      "conflict_prompt": "A solar eclipse occurs when the Moon passes between the Sun and Earth, casting a shadow on Earth and when no celestial bodies line up at all.",
      "question": "What astronomical event involves the Moon passing between the Sun and Earth and casting a shadow on Earth?",
      "options": [
        "A. Lunar eclipse",
        "B. Solar eclipse",
        "C. Meteor shower",
        "D. Aurora borealis"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for gold is Au.",
      "conflict_prompt": "The chemical symbol for gold is Au and the chemical symbol for gold is Pb.",
      "question": "What is the chemical symbol for gold?",
      "options": [
        "A. Pb",
        "B. Au",
        "C. Ag",
        "D. Fe"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional sushi is vinegared rice.",
      "conflict_prompt": "The primary ingredient in traditional sushi is vinegared rice and it is primarily made of chocolate.",
      "question": "What is the primary ingredient in traditional sushi?",
      "options": [
        "A. Vinegared rice",
        "B. Chocolate",
        "C. Beef",
        "D. Pasta"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Berlin Wall fell in 1989, leading to the reunification of Germany.",
      "conflict_prompt": "The Berlin Wall fell in 1989, leading to the reunification of Germany and it remained fully intact and in use after 1989.",
      "question": "In what year did the Berlin Wall fall?",
      "options": [
        "A. 1975",
        "B. 1989",
        "C. 1999",
        "D. 1961"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mercury is the closest planet to the Sun in our solar system.",
      "conflict_prompt": "Mercury is the closest planet to the Sun in our solar system and it is the farthest planet from the Sun.",
      "question": "Which planet is closest to the Sun?",
      "options": [
        "A. Neptune",
        "B. Earth",
        "C. Mercury",
        "D. Mars"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Ring of Fire is an area with frequent earthquakes and volcanic activity.",
      "conflict_prompt": "The Pacific Ring of Fire is an area with frequent earthquakes and volcanic activity and it is an area with no seismic or volcanic activity.",
      "question": "What characterizes the Pacific Ring of Fire?",
      "options": [
        "A. Very low seismic activity",
        "B. Frequent earthquakes and volcanic activity",
        "C. No oceans",
        "D. Permanent daylight"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The hottest planet in the solar system is Venus due to its thick greenhouse atmosphere.",
      "conflict_prompt": "The hottest planet in the solar system is Venus due to its thick greenhouse atmosphere and it is the coldest planet because it has no atmosphere.",
      "question": "Which planet is the hottest in the solar system?",
      "options": [
        "A. Mercury",
        "B. Venus",
        "C. Mars",
        "D. Pluto"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The French Revolution began in 1789.",
      "conflict_prompt": "The French Revolution began in 1789 and it began in 1900.",
      "question": "In what year did the French Revolution begin?",
      "options": [
        "A. 1900",
        "B. 1789",
        "C. 1815",
        "D. 1600"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Italy is Rome.",
      "conflict_prompt": "The capital of Italy is Rome and the capital of Italy is also Beijing.",
      "question": "What is the capital of Italy?",
      "options": [
        "A. Rome",
        "B. Milan",
        "C. Florence",
        "D. Beijing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human adult skeleton typically has 206 bones.",
      "conflict_prompt": "The human adult skeleton typically has 206 bones and it has zero bones.",
      "question": "About how many bones are in a typical adult human skeleton?",
      "options": [
        "A. 206",
        "B. 1000",
        "C. 10",
        "D. 0"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Northwest is known for its temperate rainforests and high annual rainfall in parts.",
      "conflict_prompt": "The Pacific Northwest is known for its temperate rainforests and high annual rainfall in parts and it is a desert with no rainfall.",
      "question": "What type of climate or feature is the Pacific Northwest known for in parts?",
      "options": [
        "A. Temperate rainforests with high rainfall",
        "B. Tropical coral reefs",
        "C. Sahara-like deserts",
        "D. Perpetual blizzards year-round"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary source of energy for the Sun is nuclear fusion in its core.",
      "conflict_prompt": "The primary source of energy for the Sun is nuclear fusion in its core and it gains energy by consuming trees on Earth.",
      "question": "What is the primary source of the Sun's energy?",
      "options": [
        "A. Burning coal",
        "B. Nuclear fusion",
        "C. Chemical batteries",
        "D. Wind turbines"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Bermuda Triangle is a region in the North Atlantic associated with reports of lost ships and aircraft.",
      "conflict_prompt": "The Bermuda Triangle is a region in the North Atlantic associated with reports of lost ships and aircraft and it is a completely safe region with no reported incidents ever.",
      "question": "Where is the Bermuda Triangle located?",
      "options": [
        "A. North Atlantic Ocean",
        "B. Indian Ocean",
        "C. Mediterranean Sea",
        "D. Arctic Ocean"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for table salt (sodium chloride) is NaCl.",
      "conflict_prompt": "The chemical formula for table salt (sodium chloride) is NaCl and its formula is H2O.",
      "question": "What is the chemical formula for sodium chloride (table salt)?",
      "options": [
        "A. H2O",
        "B. CO2",
        "C. NaCl",
        "D. O2"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest bird in level flight is the common swift (or some consider the peregrine falcon in stoop), but the peregrine falcon is the fastest diving bird.",
      "conflict_prompt": "The fastest bird in level flight is the common swift and the peregrine falcon is the fastest diving bird and they cannot fly at any speed.",
      "question": "Which bird is known as the fastest in diving speed?",
      "options": [
        "A. Ostrich",
        "B. Penguin",
        "C. Peregrine falcon",
        "D. Hummingbird"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human brain is composed of the cerebrum, cerebellum, and brainstem as major parts.",
      "conflict_prompt": "The human brain is composed of the cerebrum, cerebellum, and brainstem as major parts and it has no distinct parts at all.",
      "question": "Which of the following is a major part of the human brain?",
      "options": [
        "A. Cerebrum",
        "B. Pancreas",
        "C. Femur",
        "D. Appendix"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A panda's diet is primarily bamboo, though it is classified as a carnivore taxonomically.",
      "conflict_prompt": "A panda's diet is primarily bamboo, though it is classified as a carnivore taxonomically, and it eats only rocks.",
      "question": "What is the primary component of a giant panda's diet?",
      "options": [
        "A. Bamboo",
        "B. Fish",
        "C. Rocks",
        "D. Grass only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The international dialing code for the United States is +1.",
      "conflict_prompt": "The international dialing code for the United States is +1 and it is +99 for all calls.",
      "question": "What is the international dialing code for the United States?",
      "options": [
        "A. +44",
        "B. +1",
        "C. +86",
        "D. +99"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon River basin contains the largest tropical rainforest in the world.",
      "conflict_prompt": "The Amazon River basin contains the largest tropical rainforest in the world and it contains no forests at all.",
      "question": "What major natural feature is associated with the Amazon River basin?",
      "options": [
        "A. Largest tropical rainforest",
        "B. Largest desert",
        "C. Largest ice cap",
        "D. Largest urban area"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The shortest day of the year in the Northern Hemisphere occurs at the December solstice.",
      "conflict_prompt": "The shortest day of the year in the Northern Hemisphere occurs at the December solstice and it occurs at the June solstice at the same time.",
      "question": "When does the shortest day of the year typically occur in the Northern Hemisphere?",
      "options": [
        "A. March equinox",
        "B. June solstice",
        "C. December solstice",
        "D. September equinox"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest recorded land speed for a human running is over 44 km/h (Usain Bolt's top speed).",
      "conflict_prompt": "The fastest recorded land speed for a human running is over 44 km/h and humans cannot run faster than 1 km/h.",
      "question": "Approximately what top speed has a human been recorded running at (e.g., Usain Bolt)?",
      "options": [
        "A. Over 44 km/h",
        "B. About 2 km/h",
        "C. 100 km/h",
        "D. 0.1 km/h"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main ingredient in traditional Italian pesto is basil.",
      "conflict_prompt": "The main ingredient in traditional Italian pesto is basil and it is made entirely of ice.",
      "question": "What is the primary herb used in traditional Italian pesto?",
      "options": [
        "A. Cilantro",
        "B. Basil",
        "C. Dill",
        "D. Mint"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The ancient city of Machu Picchu is located in Peru.",
      "conflict_prompt": "The ancient city of Machu Picchu is located in Peru and it is located in the Arctic Circle.",
      "question": "In which country is Machu Picchu located?",
      "options": [
        "A. Peru",
        "B. Mexico",
        "C. Spain",
        "D. Canada"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Penicillin was discovered by Alexander Fleming in 1928.",
      "conflict_prompt": "Penicillin was discovered by Alexander Fleming in 1928 and he simultaneously proved antibiotics do not work.",
      "question": "Who discovered penicillin in 1928?",
      "options": [
        "A. Louis Pasteur",
        "B. Alexander Fleming",
        "C. Jonas Salk",
        "D. Robert Koch"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Sphinx of Giza has the body of a lion and the head of a human.",
      "conflict_prompt": "The Great Sphinx of Giza has the body of a lion and the head of a human and it has the body of a human and the head of a lion simultaneously.",
      "question": "What features describe the Great Sphinx of Giza?",
      "options": [
        "A. Body of a human and head of a lion",
        "B. Entirely a human statue",
        "C. Body of a lion and head of a human",
        "D. A carved mountain"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The length of a marathon is 42.195 kilometers.",
      "conflict_prompt": "The length of a marathon is 42.195 kilometers and it is zero kilometers.",
      "question": "What is the official length of a marathon race?",
      "options": [
        "A. 10 km",
        "B. 26.2 miles (42.195 km)",
        "C. 5 km",
        "D. 100 km"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful vaccine (smallpox) was developed by Edward Jenner in the late 18th century.",
      "conflict_prompt": "The first successful vaccine (smallpox) was developed by Edward Jenner in the late 18th century and he argued vaccines could never work.",
      "question": "Who developed the first successful smallpox vaccine in the late 18th century?",
      "options": [
        "A. Louis Pasteur",
        "B. Edward Jenner",
        "C. Alexander Fleming",
        "D. Gregor Mendel"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of red blood cells is to transport oxygen throughout the body.",
      "conflict_prompt": "The primary function of red blood cells is to transport oxygen throughout the body and they transport no gases at all.",
      "question": "What is the main function of red blood cells?",
      "options": [
        "A. Store fat",
        "B. Transport oxygen",
        "C. Produce hormones",
        "D. Provide structural support"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Leaning Tower of Pisa is famous for its unintended tilt.",
      "conflict_prompt": "The Leaning Tower of Pisa is famous for its unintended tilt and it is perfectly vertical with no tilt.",
      "question": "What is the Leaning Tower of Pisa famous for?",
      "options": [
        "A. Being made of steel",
        "B. Its unintended tilt",
        "C. Being the tallest building in the world",
        "D. Being in Tokyo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Taj Mahal was built by the Mughal emperor Shah Jahan as a mausoleum for his wife Mumtaz Mahal.",
      "conflict_prompt": "The Taj Mahal was built by the Mughal emperor Shah Jahan as a mausoleum for his wife Mumtaz Mahal and it was built as a commercial shopping mall.",
      "question": "For whom was the Taj Mahal built as a mausoleum?",
      "options": [
        "A. Mumtaz Mahal",
        "B. Queen Elizabeth I",
        "C. Cleopatra",
        "D. Catherine the Great"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest day in terms of sunlight in the Northern Hemisphere occurs near the June solstice.",
      "conflict_prompt": "The longest day in terms of sunlight in the Northern Hemisphere occurs near the June solstice and it occurs near the December solstice at the same time.",
      "question": "When does the longest day of the year occur in the Northern Hemisphere?",
      "options": [
        "A. December solstice",
        "B. March equinox",
        "C. June solstice",
        "D. September equinox"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nile River flows northward into the Mediterranean Sea.",
      "conflict_prompt": "The Nile River flows northward into the Mediterranean Sea and it flows southward into the Pacific Ocean.",
      "question": "Into which body of water does the Nile River flow?",
      "options": [
        "A. Pacific Ocean",
        "B. Mediterranean Sea",
        "C. Arctic Ocean",
        "D. Indian Ocean"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary unit of currency in Japan is the yen (JPY).",
      "conflict_prompt": "The primary unit of currency in Japan is the yen (JPY) and the primary unit of currency is the rupee.",
      "question": "What is the currency of Japan?",
      "options": [
        "A. Dollar",
        "B. Euro",
        "C. Yen",
        "D. Rupee"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A cubic meter is a unit of volume equivalent to 1,000 liters.",
      "conflict_prompt": "A cubic meter is a unit of volume equivalent to 1,000 liters and it is equivalent to 1 liter.",
      "question": "How many liters are in one cubic meter?",
      "options": [
        "A. 1 liter",
        "B. 100 liters",
        "C. 1,000 liters",
        "D. 10,000 liters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first modern Olympic Games were held in Athens in 1896.",
      "conflict_prompt": "The first modern Olympic Games were held in Athens in 1896 and they were first held in 2000.",
      "question": "Where were the first modern Olympic Games held in 1896?",
      "options": [
        "A. Paris",
        "B. Athens",
        "C. London",
        "D. Sydney"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Saturn is known for its prominent ring system made of ice and rock particles.",
      "conflict_prompt": "Saturn is known for its prominent ring system made of ice and rock particles and it has no rings whatsoever.",
      "question": "What is a distinctive feature of Saturn?",
      "options": [
        "A. It has a solid surface like Earth",
        "B. It has a prominent ring system",
        "C. It is the smallest dwarf planet",
        "D. It orbits between Earth and Venus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest desert in the world by area is the Antarctic Desert.",
      "conflict_prompt": "The largest desert in the world by area is the Antarctic Desert and it is the smallest desert on Earth.",
      "question": "Which is the largest desert on Earth by area?",
      "options": [
        "A. Sahara Desert",
        "B. Gobi Desert",
        "C. Antarctic Desert",
        "D. Mojave Desert"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary organ of the human digestive system where most nutrient absorption occurs is the small intestine.",
      "conflict_prompt": "The primary organ of the human digestive system where most nutrient absorption occurs is the small intestine and it does not absorb nutrients at all.",
      "question": "Where does most nutrient absorption occur in the human digestive system?",
      "options": [
        "A. Stomach",
        "B. Large intestine",
        "C. Small intestine",
        "D. Esophagus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Union Jack is the national flag of the United Kingdom.",
      "conflict_prompt": "The Union Jack is the national flag of the United Kingdom and it is the national flag of Canada instead.",
      "question": "Which country's national flag is the Union Jack associated with?",
      "options": [
        "A. United Kingdom",
        "B. Canada",
        "C. Australia only",
        "D. Japan"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Greenland ice sheet covers most of the surface of Greenland.",
      "conflict_prompt": "The Greenland ice sheet covers most of the surface of Greenland and Greenland is completely ice-free.",
      "question": "What covers most of the surface of Greenland?",
      "options": [
        "A. Tropical rainforest",
        "B. Desert dunes",
        "C. Ice sheet",
        "D. Savanna"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest volcano on Earth (by volume) is Mauna Loa in Hawaii.",
      "conflict_prompt": "The largest volcano on Earth (by volume) is Mauna Loa in Hawaii and it is the smallest volcano in the world.",
      "question": "Which volcano is considered the largest on Earth by volume?",
      "options": [
        "A. Mount Fuji",
        "B. Mauna Loa",
        "C. Mount Vesuvius",
        "D. Krakatoa"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Australia is Canberra, not Sydney.",
      "conflict_prompt": "The capital of Australia is Canberra, not Sydney, and the capital is Sydney instead.",
      "question": "What is the capital city of Australia?",
      "options": [
        "A. Sydney",
        "B. Melbourne",
        "C. Canberra",
        "D. Brisbane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common cold is most often caused by rhinoviruses.",
      "conflict_prompt": "The common cold is most often caused by rhinoviruses and it is never caused by any viruses.",
      "question": "What type of virus most commonly causes the common cold?",
      "options": [
        "A. Rhinoviruses",
        "B. Influenza B only",
        "C. HIV",
        "D. Ebola"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest continent by land area is Asia.",
      "conflict_prompt": "The largest continent by land area is Asia and it is the smallest continent by land area.",
      "question": "Which continent is the largest by land area?",
      "options": [
        "A. Africa",
        "B. North America",
        "C. Asia",
        "D. Europe"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Northwest city of Seattle is home to the Space Needle.",
      "conflict_prompt": "The Pacific Northwest city of Seattle is home to the Space Needle and it has no such landmark.",
      "question": "Which city is home to the Space Needle?",
      "options": [
        "A. Portland",
        "B. Seattle",
        "C. Vancouver (Canada)",
        "D. San Francisco"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet in the solar system, Jupiter, has a strong magnetic field and many moons.",
      "conflict_prompt": "The largest planet in the solar system, Jupiter, has a strong magnetic field and many moons and it has no moons at all.",
      "question": "Which planet has a strong magnetic field and many moons and is the largest in the solar system?",
      "options": [
        "A. Earth",
        "B. Mars",
        "C. Jupiter",
        "D. Mercury"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A light year is a unit of distance equal to how far light travels in one year.",
      "conflict_prompt": "A light year is a unit of distance equal to how far light travels in one year and it is a unit of time equal to one day.",
      "question": "What does the term 'light year' measure?",
      "options": [
        "A. Time (one year)",
        "B. Distance (distance light travels in one year)",
        "C. Mass of a star",
        "D. Temperature of the Sun"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon River discharges more water than any other river in the world.",
      "conflict_prompt": "The Amazon River discharges more water than any other river in the world and it discharges no water at all.",
      "question": "Which river discharges the most water into the ocean?",
      "options": [
        "A. Nile River",
        "B. Yangtze River",
        "C. Amazon River",
        "D. Mississippi River"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning British monarch in history was Queen Victoria until recent times when Elizabeth II surpassed her.",
      "conflict_prompt": "The longest reigning British monarch in history was Queen Victoria until recent times when Elizabeth II surpassed her and no monarch has ever reigned for a long time.",
      "question": "Which British monarch reigned longer than Queen Victoria?",
      "options": [
        "A. George III",
        "B. Elizabeth II",
        "C. Henry VIII",
        "D. Edward VII"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element helium is a noble gas and is lighter than air.",
      "conflict_prompt": "The chemical element helium is a noble gas and is lighter than air and it is heavier than air.",
      "question": "Which of the following is true about helium?",
      "options": [
        "A. Helium is heavier than air",
        "B. Helium is a noble gas lighter than air",
        "C. Helium is a metal",
        "D. Helium is radioactive"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of France is Paris, home to the Louvre Museum.",
      "conflict_prompt": "The capital of France is Paris, home to the Louvre Museum, and it is not Paris at all.",
      "question": "Which city is the capital of France and home to the Louvre Museum?",
      "options": [
        "A. Lyon",
        "B. Marseille",
        "C. Paris",
        "D. Bordeaux"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Richter scale was historically used to quantify earthquake magnitude.",
      "conflict_prompt": "The Richter scale was historically used to quantify earthquake magnitude and earthquakes have no measurable magnitude.",
      "question": "What was the Richter scale used to measure?",
      "options": [
        "A. Temperature",
        "B. Earthquake magnitude",
        "C. Wind speed",
        "D. Ocean depth"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The kangaroo is a marsupial native to Australia that carries its young in a pouch.",
      "conflict_prompt": "The kangaroo is a marsupial native to Australia that carries its young in a pouch and it lays eggs like a platypus.",
      "question": "What reproductive trait is associated with kangaroos?",
      "options": [
        "A. Laying eggs",
        "B. Giving live birth and carrying young in a pouch",
        "C. Budding",
        "D. External fertilization in water"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The city of Istanbul straddles both Europe and Asia across the Bosporus Strait.",
      "conflict_prompt": "The city of Istanbul straddles both Europe and Asia across the Bosporus Strait and it is located entirely in South America.",
      "question": "Which city spans both Europe and Asia across the Bosporus Strait?",
      "options": [
        "A. Moscow",
        "B. Istanbul",
        "C. Cairo",
        "D. Tokyo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first element on the periodic table is hydrogen, atomic number 1.",
      "conflict_prompt": "The first element on the periodic table is hydrogen, atomic number 1, and it is element 118.",
      "question": "What is the atomic number of hydrogen?",
      "options": [
        "A. 118",
        "B. 1",
        "C. 8",
        "D. 26"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city by population in the United States is New York City.",
      "conflict_prompt": "The largest city by population in the United States is New York City and it is a tiny village with 100 residents.",
      "question": "Which city is the most populous in the United States?",
      "options": [
        "A. Los Angeles",
        "B. Chicago",
        "C. Houston",
        "D. New York City"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Photosynthetic organisms produce oxygen as a byproduct of photosynthesis.",
      "conflict_prompt": "Photosynthetic organisms produce oxygen as a byproduct of photosynthesis and they produce no oxygen at all.",
      "question": "What gas is commonly produced by photosynthetic organisms as a byproduct?",
      "options": [
        "A. Carbon dioxide",
        "B. Oxygen",
        "C. Methane",
        "D. Sulfur dioxide"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest freshwater lake by area is Lake Superior.",
      "conflict_prompt": "The largest freshwater lake by area is Lake Superior and it is the smallest freshwater lake by area.",
      "question": "Which is the largest freshwater lake by surface area?",
      "options": [
        "A. Lake Victoria",
        "B. Lake Superior",
        "C. Lake Baikal",
        "D. Caspian Sea"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Wright Flyer made the first controlled powered flight at Kitty Hawk, North Carolina.",
      "conflict_prompt": "The Wright Flyer made the first controlled powered flight at Kitty Hawk, North Carolina and it never flew anywhere.",
      "question": "Where did the Wright brothers achieve the first controlled powered flight?",
      "options": [
        "A. Kitty Hawk, North Carolina",
        "B. Kitty Hawk, Oregon",
        "C. Dayton, Ohio",
        "D. Paris, France"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest building in the world (as of the 21st century) is the Burj Khalifa in Dubai.",
      "conflict_prompt": "The tallest building in the world is the Burj Khalifa in Dubai and it is a one-story house.",
      "question": "Which building is currently the tallest in the world?",
      "options": [
        "A. Empire State Building",
        "B. Burj Khalifa",
        "C. Eiffel Tower",
        "D. Leaning Tower of Pisa"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The distance from the Earth to the Sun is about 1 astronomical unit (AU), approximately 150 million kilometers.",
      "conflict_prompt": "The distance from the Earth to the Sun is about 1 astronomical unit (AU), approximately 150 million kilometers and it is zero kilometers.",
      "question": "Approximately how far is the Earth from the Sun?",
      "options": [
        "A. 1 kilometer",
        "B. 150 million kilometers (1 AU)",
        "C. 1 billion kilometers",
        "D. 10 kilometers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest living land animal is the African elephant.",
      "conflict_prompt": "The largest living land animal is the African elephant and it is the size of a mouse.",
      "question": "Which animal is the largest living land animal?",
      "options": [
        "A. Blue whale",
        "B. African elephant",
        "C. Grizzly bear",
        "D. Giraffe"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Russia is Moscow.",
      "conflict_prompt": "The capital of Russia is Moscow and the capital is also Warsaw.",
      "question": "What is the capital city of Russia?",
      "options": [
        "A. Warsaw",
        "B. Moscow",
        "C. Saint Petersburg",
        "D. Kiev"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The hardest natural substance on Earth is diamond.",
      "conflict_prompt": "The hardest natural substance on Earth is diamond and it is the softest substance on Earth.",
      "question": "What is considered the hardest natural substance?",
      "options": [
        "A. Graphite",
        "B. Diamond",
        "C. Talc",
        "D. Wood"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Suez Canal connects the Mediterranean Sea to the Red Sea.",
      "conflict_prompt": "The Suez Canal connects the Mediterranean Sea to the Red Sea and it connects the Mediterranean Sea to the Pacific Ocean directly.",
      "question": "Which two seas does the Suez Canal connect?",
      "options": [
        "A. Black Sea and Caspian Sea",
        "B. Mediterranean Sea and Red Sea",
        "C. Atlantic Ocean and Indian Ocean",
        "D. Arctic Ocean and Southern Ocean"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Bermuda Triangle is roughly bounded by Miami, Bermuda, and Puerto Rico.",
      "conflict_prompt": "The Bermuda Triangle is roughly bounded by Miami, Bermuda, and Puerto Rico and it is bounded by Moscow, Beijing, and Paris.",
      "question": "Which three points roughly bound the Bermuda Triangle?",
      "options": [
        "A. Miami, Bermuda, and Puerto Rico",
        "B. Los Angeles, Honolulu, and Tokyo",
        "C. London, Madrid, and Rome",
        "D. Cairo, Nairobi, and Cape Town"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The domesticated horse has 64 chromosomes.",
      "conflict_prompt": "The domesticated horse has 64 chromosomes and it has 23 chromosomes.",
      "question": "How many chromosomes does a domestic horse typically have?",
      "options": [
        "A. 23",
        "B. 46",
        "C. 64",
        "D. 100"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Vatican City is the smallest independent state in the world by area.",
      "conflict_prompt": "The Vatican City is the smallest independent state in the world by area and it is the largest country in the world by area.",
      "question": "Which is the smallest independent state in the world by area?",
      "options": [
        "A. Russia",
        "B. Canada",
        "C. Vatican City",
        "D. China"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for carbon dioxide is CO2.",
      "conflict_prompt": "The chemical formula for carbon dioxide is CO2 and it is H2O.",
      "question": "What is the chemical formula for carbon dioxide?",
      "options": [
        "A. CO2",
        "B. H2O",
        "C. O2",
        "D. NaCl"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The South Pole is located on the continent of Antarctica.",
      "conflict_prompt": "The South Pole is located on the continent of Antarctica and it is located in the Sahara Desert.",
      "question": "On which continent is the South Pole located?",
      "options": [
        "A. Africa",
        "B. Antarctica",
        "C. South America",
        "D. Australia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The highest waterfall in the world is Angel Falls in Venezuela.",
      "conflict_prompt": "The highest waterfall in the world is Angel Falls in Venezuela and it is the shortest waterfall in the world.",
      "question": "Which waterfall is considered the highest in the world?",
      "options": [
        "A. Niagara Falls",
        "B. Angel Falls",
        "C. Victoria Falls",
        "D. Iguazu Falls"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest living reptile is the saltwater crocodile.",
      "conflict_prompt": "The largest living reptile is the saltwater crocodile and it is the smallest reptile.",
      "question": "Which species is the largest living reptile?",
      "options": [
        "A. Komodo dragon",
        "B. Saltwater crocodile",
        "C. Leopard gecko",
        "D. Garden snake"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official motto of the United States is 'In God We Trust'.",
      "conflict_prompt": "The official motto of the United States is 'In God We Trust' and the official motto is 'God is not trusted at all'.",
      "question": "What is the official motto of the United States?",
      "options": [
        "A. E pluribus unum",
        "B. In God We Trust",
        "C. Liberty and Justice for All",
        "D. United We Fall"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element iron has the symbol Fe.",
      "conflict_prompt": "The chemical element iron has the symbol Fe and its symbol is Au.",
      "question": "What is the chemical symbol for iron?",
      "options": [
        "A. Au",
        "B. Fe",
        "C. Ag",
        "D. Ir"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The international standard for paper size A4 measures 210 mm by 297 mm.",
      "conflict_prompt": "The international standard for paper size A4 measures 210 mm by 297 mm and it measures 1000 mm by 2000 mm.",
      "question": "What are the dimensions of A4 paper in millimeters?",
      "options": [
        "A. 210 x 297 mm",
        "B. 100 x 200 mm",
        "C. 500 x 700 mm",
        "D. 1000 x 2000 mm"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Egypt is Cairo.",
      "conflict_prompt": "The capital of Egypt is Cairo and the capital is Reykjavik.",
      "question": "What is the capital city of Egypt?",
      "options": [
        "A. Cairo",
        "B. Alexandria",
        "C. Giza",
        "D. Reykjavik"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mediterranean Sea is almost completely enclosed by land and connects to the Atlantic Ocean via the Strait of Gibraltar.",
      "conflict_prompt": "The Mediterranean Sea is almost completely enclosed by land and connects to the Atlantic Ocean via the Strait of Gibraltar and it connects directly to the Pacific Ocean with no strait.",
      "question": "Through which strait does the Mediterranean Sea connect to the Atlantic Ocean?",
      "options": [
        "A. Strait of Hormuz",
        "B. Strait of Gibraltar",
        "C. Bering Strait",
        "D. Strait of Malacca"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Black Death pandemic in the 14th century was caused by the bacterium Yersinia pestis.",
      "conflict_prompt": "The Black Death pandemic in the 14th century was caused by the bacterium Yersinia pestis and it was not caused by any pathogen.",
      "question": "What bacterium is believed to have caused the Black Death in the 14th century?",
      "options": [
        "A. Yersinia pestis",
        "B. Escherichia coli",
        "C. Staphylococcus aureus",
        "D. Bacillus anthracis"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest ocean by volume is the Pacific Ocean.",
      "conflict_prompt": "The largest ocean by volume is the Pacific Ocean and it is the smallest ocean by volume.",
      "question": "Which ocean is the largest by volume?",
      "options": [
        "A. Atlantic Ocean",
        "B. Pacific Ocean",
        "C. Indian Ocean",
        "D. Arctic Ocean"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary colors of light are red, green, and blue (RGB).",
      "conflict_prompt": "The primary colors of light are red, green, and blue (RGB) and primary colors of light are black, white, and gray.",
      "question": "What are the additive primary colors of light?",
      "options": [
        "A. Cyan, Magenta, Yellow",
        "B. Red, Green, Blue",
        "C. Black, White, Gray",
        "D. Brown, Orange, Pink"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous scientist Marie Curie won Nobel Prizes in both Physics and Chemistry.",
      "conflict_prompt": "The famous scientist Marie Curie won Nobel Prizes in both Physics and Chemistry and she never won any Nobel Prize.",
      "question": "In which fields did Marie Curie win Nobel Prizes?",
      "options": [
        "A. Literature and Peace",
        "B. Physics and Chemistry",
        "C. Medicine and Economics",
        "D. Architecture and Music"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency used in India is the Indian rupee (INR).",
      "conflict_prompt": "The currency used in India is the Indian rupee (INR) and the currency used is the US dollar exclusively.",
      "question": "What is the official currency of India?",
      "options": [
        "A. Indian rupee",
        "B. US dollar",
        "C. Euro",
        "D. Japanese yen"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Lakes of North America include Superior, Michigan, Huron, Erie, and Ontario.",
      "conflict_prompt": "The Great Lakes of North America include Superior, Michigan, Huron, Erie, and Ontario and they include none of those lakes.",
      "question": "Which of the following is one of the Great Lakes?",
      "options": [
        "A. Lake Tahoe",
        "B. Lake Superior",
        "C. Lake Como",
        "D. Lake Titicaca"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language spoken in Argentina is Spanish.",
      "conflict_prompt": "The primary language spoken in Argentina is Spanish and the primary language is Mandarin Chinese.",
      "question": "What is the primary language spoken in Argentina?",
      "options": [
        "A. Portuguese",
        "B. English",
        "C. Spanish",
        "D. Mandarin Chinese"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Aurora Borealis (Northern Lights) is caused by charged particles from the Sun interacting with Earth's magnetic field.",
      "conflict_prompt": "The Aurora Borealis (Northern Lights) is caused by charged particles from the Sun interacting with Earth's magnetic field and it is caused by city lights only.",
      "question": "What causes the Aurora Borealis?",
      "options": [
        "A. Car headlights",
        "B. Charged solar particles interacting with Earth's magnetic field",
        "C. Volcanic eruptions on Earth",
        "D. Reflections from the Moon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The smallest planet in the solar system is Mercury (by diameter).",
      "conflict_prompt": "The smallest planet in the solar system is Mercury (by diameter) and it is the largest planet.",
      "question": "Which planet is the smallest in our solar system by diameter?",
      "options": [
        "A. Earth",
        "B. Venus",
        "C. Mercury",
        "D. Jupiter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The United Nations was founded in 1945 after World War II.",
      "conflict_prompt": "The United Nations was founded in 1945 after World War II and it was founded in 1800 before the industrial revolution.",
      "question": "In what year was the United Nations founded?",
      "options": [
        "A. 1800",
        "B. 1914",
        "C. 1945",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 6 is carbon.",
      "conflict_prompt": "The chemical element with atomic number 6 is carbon and atomic number 6 corresponds to helium.",
      "question": "Which element has atomic number 6?",
      "options": [
        "A. Helium",
        "B. Carbon",
        "C. Oxygen",
        "D. Nitrogen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary destination for lunar samples returned by the Apollo missions was the United States (NASA).",
      "conflict_prompt": "The primary destination for lunar samples returned by the Apollo missions was the United States (NASA) and no samples were ever returned.",
      "question": "Which country primarily received lunar samples returned by the Apollo missions?",
      "options": [
        "A. Soviet Union",
        "B. United States",
        "C. China",
        "D. India"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The standard boiling point of pure water at 1 atm pressure is 100 degrees Celsius.",
      "conflict_prompt": "The standard boiling point of pure water at 1 atm pressure is 100 degrees Celsius and it boils at -50 degrees Celsius at the same pressure.",
      "question": "At standard atmospheric pressure (1 atm), what is the boiling point of water?",
      "options": [
        "A. -50 °C",
        "B. 0 °C",
        "C. 100 °C",
        "D. 373 °C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest organ in the human body is the skin.",
      "conflict_prompt": "The largest organ in the human body is the skin and it is the smallest organ.",
      "question": "What is the largest organ of the human body?",
      "options": [
        "A. Heart",
        "B. Skin",
        "C. Liver",
        "D. Brain"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The world's tallest tree species by average height is the coast redwood (Sequoia sempervirens).",
      "conflict_prompt": "The world's tallest tree species by average height is the coast redwood and it is the shortest tree species.",
      "question": "Which tree species is known for being the tallest on average?",
      "options": [
        "A. Baobab",
        "B. Coast redwood",
        "C. Bonsai pine",
        "D. Olive tree"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in guacamole is mashed avocado.",
      "conflict_prompt": "The primary ingredient in guacamole is mashed avocado and it is made mainly of steel.",
      "question": "What is the main ingredient in guacamole?",
      "options": [
        "A. Tomato",
        "B. Mashed avocado",
        "C. Cheese",
        "D. Steel"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning pope in modern history was Pope Pius IX, and more recently Pope John Paul II had one of the longer reigns in the 20th century.",
      "conflict_prompt": "The longest reigning pope in modern history was Pope Pius IX and Pope John Paul II, and no pope has ever reigned for long periods.",
      "question": "Which pope had a notably long reign in the 20th century?",
      "options": [
        "A. Pope John Paul II",
        "B. Pope Urban II",
        "C. Pope Alexander VI",
        "D. Pope Clement V"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mariana Trench is the deepest part of the world's oceans.",
      "conflict_prompt": "The Mariana Trench is the deepest part of the world's oceans and it is the highest mountain range above sea level.",
      "question": "What is the Mariana Trench known for?",
      "options": [
        "A. Being the deepest part of the oceans",
        "B. Being the longest river",
        "C. Being the largest desert",
        "D. Being the tallest mountain range above sea level"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize in Literature is awarded to an author for outstanding contributions in literature.",
      "conflict_prompt": "The Nobel Prize in Literature is awarded to an author for outstanding contributions in literature and it is awarded for achievements in automotive engineering only.",
      "question": "For what field is the Nobel Prize in Literature awarded?",
      "options": [
        "A. Physics",
        "B. Literature",
        "C. Automotive engineering",
        "D. Sports"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The instrument commonly used to measure atmospheric pressure is a barometer.",
      "conflict_prompt": "The instrument commonly used to measure atmospheric pressure is a barometer and atmospheric pressure is measured by a ruler.",
      "question": "What instrument is used to measure atmospheric pressure?",
      "options": [
        "A. Thermometer",
        "B. Barometer",
        "C. Speedometer",
        "D. Ruler"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main gas responsible for the greenhouse effect on Earth is carbon dioxide, among others like methane and water vapor.",
      "conflict_prompt": "The main gas responsible for the greenhouse effect on Earth is carbon dioxide among others and greenhouse gases have no effect on Earth's temperature.",
      "question": "Which gas is a major contributor to the greenhouse effect?",
      "options": [
        "A. Carbon dioxide",
        "B. Neon",
        "C. Argon",
        "D. Helium"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Beatles were a rock band formed in Liverpool, England.",
      "conflict_prompt": "The Beatles were a rock band formed in Liverpool, England and they were actually formed entirely in Mars.",
      "question": "Where were The Beatles formed?",
      "options": [
        "A. London",
        "B. Liverpool",
        "C. Manchester",
        "D. Mars"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest shark species is the whale shark, which is a filter feeder.",
      "conflict_prompt": "The largest shark species is the whale shark, which is a filter feeder and it is a tiny insect.",
      "question": "Which shark species is the largest and is a filter feeder?",
      "options": [
        "A. Great white shark",
        "B. Whale shark",
        "C. Hammerhead shark",
        "D. Tiger shark"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Spain is Madrid.",
      "conflict_prompt": "The capital of Spain is Madrid and the capital is Lisbon.",
      "question": "What is the capital of Spain?",
      "options": [
        "A. Lisbon",
        "B. Barcelona",
        "C. Madrid",
        "D. Seville"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Pyramid of Giza was constructed as a tomb for the Pharaoh Khufu.",
      "conflict_prompt": "The Great Pyramid of Giza was constructed as a tomb for the Pharaoh Khufu and it was constructed as a modern shopping center.",
      "question": "For whom was the Great Pyramid of Giza constructed as a tomb?",
      "options": [
        "A. Tutankhamun",
        "B. Ramses II",
        "C. Khufu",
        "D. Cleopatra"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human body's largest bone is the femur (thigh bone).",
      "conflict_prompt": "The human body's largest bone is the femur and the largest bone is the tiny stapes in the ear.",
      "question": "Which bone is the largest in the human body?",
      "options": [
        "A. Stapes",
        "B. Tibia",
        "C. Femur",
        "D. Ulna"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Taj Mahal is located in Agra, India.",
      "conflict_prompt": "The Taj Mahal is located in Agra, India and it is located in New York City.",
      "question": "In which city is the Taj Mahal located?",
      "options": [
        "A. Jaipur",
        "B. Agra",
        "C. Delhi",
        "D. New York City"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Pluto was reclassified from a planet to a dwarf planet by the IAU in 2006.",
      "conflict_prompt": "Pluto was reclassified from a planet to a dwarf planet by the IAU in 2006 and it remained classified as a planet with no reclassification.",
      "question": "How was Pluto reclassified by the IAU in 2006?",
      "options": [
        "A. As a comet",
        "B. As a dwarf planet",
        "C. As a star",
        "D. As an asteroid"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary producer of oxygen in Earth's atmosphere is phytoplankton in the oceans as well as terrestrial plants.",
      "conflict_prompt": "The primary producer of oxygen in Earth's atmosphere is phytoplankton in the oceans as well as terrestrial plants and oxygen is produced by rocks spontaneously without life.",
      "question": "Which organisms are major contributors to Earth's oxygen production?",
      "options": [
        "A. Rocks",
        "B. Phytoplankton and terrestrial plants",
        "C. Animals like humans",
        "D. Metals"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Siberian Tiger is the largest tiger subspecies by body size.",
      "conflict_prompt": "The Siberian Tiger is the largest tiger subspecies by body size and it is the smallest cat species.",
      "question": "Which tiger subspecies is the largest by body size?",
      "options": [
        "A. Bengal tiger",
        "B. Siberian (Amur) tiger",
        "C. Sumatran tiger",
        "D. Domestic cat"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Kilimanjaro is a dormant volcano and the highest peak in Africa.",
      "conflict_prompt": "Mount Kilimanjaro is a dormant volcano and the highest peak in Africa and it is an active geyser field with no elevation.",
      "question": "What is Mount Kilimanjaro known for?",
      "options": [
        "A. Being the lowest point in Africa",
        "B. Being a dormant volcano and the highest peak in Africa",
        "C. Being a large desert dune",
        "D. Being an underwater trench"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city in Brazil by population is São Paulo.",
      "conflict_prompt": "The largest city in Brazil by population is São Paulo and Brazil has no major cities.",
      "question": "Which city is the most populous in Brazil?",
      "options": [
        "A. Rio de Janeiro",
        "B. Brasília",
        "C. São Paulo",
        "D. Salvador"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas used in scuba tanks for recreational diving is compressed air (a mixture primarily of nitrogen and oxygen).",
      "conflict_prompt": "The primary gas used in scuba tanks for recreational diving is compressed air and tanks are filled with pure gold dust.",
      "question": "What gas mixture is commonly used in recreational scuba tanks?",
      "options": [
        "A. Pure neon",
        "B. Compressed air (mostly nitrogen and oxygen)",
        "C. Liquid water",
        "D. Pure helium only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty stands on Liberty Island in New York Harbor.",
      "conflict_prompt": "The Statue of Liberty stands on Liberty Island in New York Harbor and it stands on an island in the Pacific Ocean far from New York.",
      "question": "On which island does the Statue of Liberty stand?",
      "options": [
        "A. Ellis Island",
        "B. Liberty Island",
        "C. Alcatraz Island",
        "D. Santorini"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary distinguishing feature of mammals is that they have hair or fur and mammary glands that produce milk for their young.",
      "conflict_prompt": "The primary distinguishing feature of mammals is that they have hair or fur and mammary glands and they lay eggs exclusively like fish.",
      "question": "Which of the following is a distinguishing feature of mammals?",
      "options": [
        "A. Having scales and breathing underwater",
        "B. Having hair/fur and mammary glands producing milk",
        "C. Being unicellular",
        "D. Being photosynthetic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The S&P 500 is a stock market index that tracks 500 large-cap U.S. companies.",
      "conflict_prompt": "The S&P 500 is a stock market index that tracks 500 large-cap U.S. companies and it tracks no companies at all.",
      "question": "What does the S&P 500 index track?",
      "options": [
        "A. 500 small local businesses",
        "B. 500 large-cap U.S. companies",
        "C. 50 government bonds only",
        "D. The population count"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with the symbol 'Ag' is silver.",
      "conflict_prompt": "The chemical element with the symbol 'Ag' is silver and 'Ag' is the symbol for oxygen.",
      "question": "Which element has the chemical symbol 'Ag'?",
      "options": [
        "A. Gold",
        "B. Silver",
        "C. Oxygen",
        "D. Aluminum"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Golden Gate Bridge spans the Golden Gate strait connecting San Francisco Bay to the Pacific Ocean.",
      "conflict_prompt": "The Golden Gate Bridge spans the Golden Gate strait connecting San Francisco Bay to the Pacific Ocean and it spans an imaginary river in Europe.",
      "question": "What does the Golden Gate Bridge span?",
      "options": [
        "A. The Bosporus Strait",
        "B. The Golden Gate strait",
        "C. The River Thames",
        "D. The Nile River"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary diet of koalas consists almost exclusively of eucalyptus leaves.",
      "conflict_prompt": "The primary diet of koalas consists almost exclusively of eucalyptus leaves and they mainly eat metal scraps.",
      "question": "What do koalas primarily eat?",
      "options": [
        "A. Eucalyptus leaves",
        "B. Grass",
        "C. Fish",
        "D. Metal"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The equator divides the Earth into the Northern and Southern Hemispheres.",
      "conflict_prompt": "The equator divides the Earth into the Northern and Southern Hemispheres and it divides the Earth into no hemispheres at all.",
      "question": "What does the equator divide the Earth into?",
      "options": [
        "A. Eastern and Western Hemispheres only",
        "B. Northern and Southern Hemispheres",
        "C. Day and night permanently",
        "D. Continents"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest volcano in the solar system is Olympus Mons on Mars.",
      "conflict_prompt": "The largest volcano in the solar system is Olympus Mons on Mars and it is the smallest crater on the Moon.",
      "question": "Which mountain/volcano is the largest in the solar system?",
      "options": [
        "A. Mount Everest",
        "B. Mauna Kea",
        "C. Olympus Mons",
        "D. Mount Kilimanjaro"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary cause of tides on Earth is the gravitational pull of the Moon and Sun acting on Earth's oceans.",
      "conflict_prompt": "The primary cause of tides on Earth is the gravitational pull of the Moon and Sun acting on Earth's oceans and tides are caused by furniture moving on land.",
      "question": "What primarily causes tides on Earth?",
      "options": [
        "A. Wind only",
        "B. Gravitational pull of the Moon and Sun",
        "C. Tectonic plate movement",
        "D. Furniture shifting"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning monarch of the United Kingdom was Queen Elizabeth II.",
      "conflict_prompt": "The longest reigning monarch of the United Kingdom was Queen Elizabeth II and no monarch has ever reigned for a long time.",
      "question": "Who was the longest reigning monarch of the United Kingdom in modern history?",
      "options": [
        "A. Queen Victoria",
        "B. King George V",
        "C. Queen Elizabeth II",
        "D. King Henry VIII"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional hummus is chickpeas (garbanzo beans).",
      "conflict_prompt": "The primary ingredient in traditional hummus is chickpeas and it is made primarily from chocolate syrup.",
      "question": "What is the main ingredient in traditional hummus?",
      "options": [
        "A. Chickpeas",
        "B. Potatoes",
        "C. Chocolate syrup",
        "D. Rice"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest coral reef system in the world is the Great Barrier Reef in Australia.",
      "conflict_prompt": "The largest coral reef system in the world is the Great Barrier Reef in Australia and it is actually an alpine mountain range.",
      "question": "Where is the Great Barrier Reef located?",
      "options": [
        "A. Caribbean Sea",
        "B. Off the coast of Australia (Queensland)",
        "C. Mediterranean Sea",
        "D. Baltic Sea"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common unit for measuring electrical resistance is the ohm (Ω).",
      "conflict_prompt": "The common unit for measuring electrical resistance is the ohm and it is measured in liters.",
      "question": "What unit is used to measure electrical resistance?",
      "options": [
        "A. Volt",
        "B. Ampere",
        "C. Ohm",
        "D. Liter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island in the world is Greenland.",
      "conflict_prompt": "The largest island in the world is Greenland and it is the smallest island in the world.",
      "question": "Which is the largest island in the world (by area)?",
      "options": [
        "A. Australia",
        "B. Greenland",
        "C. Madagascar",
        "D. Borneo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Columbia River flows through the U.S. Pacific Northwest and empties into the Pacific Ocean.",
      "conflict_prompt": "The Columbia River flows through the U.S. Pacific Northwest and empties into the Pacific Ocean and it flows into a dry desert with no ocean connection.",
      "question": "Into which ocean does the Columbia River empty?",
      "options": [
        "A. Atlantic Ocean",
        "B. Pacific Ocean",
        "C. Indian Ocean",
        "D. Arctic Ocean"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary measure of central tendency that represents the middle value in a sorted dataset is the median.",
      "conflict_prompt": "The primary measure of central tendency that represents the middle value in a sorted dataset is the median and it represents the extreme outlier always.",
      "question": "Which statistic represents the middle value of a sorted dataset?",
      "options": [
        "A. Mean",
        "B. Mode",
        "C. Median",
        "D. Range"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official residence of the President of the United States is the White House in Washington, D.C.",
      "conflict_prompt": "The official residence of the President of the United States is the White House in Washington, D.C. and the President lives permanently in Buckingham Palace.",
      "question": "What is the official residence of the President of the United States?",
      "options": [
        "A. Buckingham Palace",
        "B. The White House",
        "C. The Élysée Palace",
        "D. The Kremlin"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary author of the U.S. Declaration of Independence was Thomas Jefferson.",
      "conflict_prompt": "The primary author of the U.S. Declaration of Independence was Thomas Jefferson and the document had no primary author.",
      "question": "Who is considered the primary author of the U.S. Declaration of Independence?",
      "options": [
        "A. John Adams",
        "B. Thomas Jefferson",
        "C. Benjamin Franklin",
        "D. George Washington"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest economy in the world by nominal GDP is the United States.",
      "conflict_prompt": "The largest economy in the world by nominal GDP is the United States and it has no economy at all.",
      "question": "Which country has the largest nominal GDP?",
      "options": [
        "A. China",
        "B. United States",
        "C. India",
        "D. Russia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest continuously inhabited city in the world is often cited as Damascus in Syria.",
      "conflict_prompt": "The longest continuously inhabited city in the world is often cited as Damascus in Syria and it has never been inhabited.",
      "question": "Which city is often cited as one of the longest continuously inhabited cities in the world?",
      "options": [
        "A. New York City",
        "B. Damascus",
        "C. Canberra",
        "D. Reykjavik"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary industry in the Middle East that significantly influences global markets is petroleum (oil) production.",
      "conflict_prompt": "The primary industry in the Middle East that significantly influences global markets is petroleum production and the region produces no oil at all.",
      "question": "Which industry significantly influences global markets in the Middle East?",
      "options": [
        "A. Petroleum (oil) production",
        "B. Deep-sea fishing",
        "C. Arctic tourism",
        "D. Alpine skiing resorts"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The organelle responsible for energy (ATP) production in eukaryotic cells is the mitochondrion.",
      "conflict_prompt": "The organelle responsible for energy (ATP) production in eukaryotic cells is the mitochondrion and cells have no organelles performing energy production.",
      "question": "Which organelle is primarily responsible for ATP production in eukaryotic cells?",
      "options": [
        "A. Ribosome",
        "B. Mitochondrion",
        "C. Golgi apparatus",
        "D. Lysosome"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Germany is Berlin.",
      "conflict_prompt": "The capital of Germany is Berlin and the capital is Vienna.",
      "question": "What is the capital city of Germany?",
      "options": [
        "A. Vienna",
        "B. Berlin",
        "C. Hamburg",
        "D. Munich"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest moon of Saturn is Titan.",
      "conflict_prompt": "The largest moon of Saturn is Titan and Saturn has no moons.",
      "question": "Which is the largest moon of Saturn?",
      "options": [
        "A. Europa",
        "B. Titan",
        "C. Ganymede",
        "D. Io"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary currency of China is the renminbi, commonly called the yuan (CNY).",
      "conflict_prompt": "The primary currency of China is the renminbi, commonly called the yuan (CNY) and China uses the Canadian dollar as its only currency.",
      "question": "What is the primary currency used in China?",
      "options": [
        "A. Japanese yen",
        "B. US dollar",
        "C. Renminbi (yuan)",
        "D. Canadian dollar"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The country of Iceland is known for its volcanic activity and geothermal energy resources.",
      "conflict_prompt": "The country of Iceland is known for its volcanic activity and geothermal energy resources and it has no volcanoes or geothermal energy.",
      "question": "What natural features is Iceland known for?",
      "options": [
        "A. Extensive deserts",
        "B. Volcanic activity and geothermal resources",
        "C. Tropical rainforests",
        "D. Large inland seas"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet in the solar system by mass is Jupiter.",
      "conflict_prompt": "The largest planet in the solar system by mass is Jupiter and it has no mass at all.",
      "question": "Which planet has the greatest mass in the solar system?",
      "options": [
        "A. Saturn",
        "B. Jupiter",
        "C. Uranus",
        "D. Neptune"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Taj Mahal is constructed primarily from white marble.",
      "conflict_prompt": "The Taj Mahal is constructed primarily from white marble and it is constructed entirely from plastic.",
      "question": "What material primarily composes the Taj Mahal?",
      "options": [
        "A. Red brick",
        "B. White marble",
        "C. Plastic",
        "D. Timber"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary advantage of electric vehicles is that they produce zero tailpipe emissions during operation.",
      "conflict_prompt": "The primary advantage of electric vehicles is that they produce zero tailpipe emissions during operation and they emit only smoke and pollutants while driving.",
      "question": "What is one primary environmental advantage of electric vehicles during operation?",
      "options": [
        "A. They produce zero tailpipe emissions",
        "B. They burn coal on the road",
        "C. They emit more CO2 while driving than gasoline cars",
        "D. They require no electricity"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Brazil is Brasília, purpose-built as the federal capital in the 1960s.",
      "conflict_prompt": "The capital of Brazil is Brasília, purpose-built as the federal capital in the 1960s and the capital is also São Paulo as the only capital.",
      "question": "What is the capital city of Brazil?",
      "options": [
        "A. Rio de Janeiro",
        "B. São Paulo",
        "C. Brasília",
        "D. Salvador"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The scientific method involves making observations, forming hypotheses, and testing them through experiments.",
      "conflict_prompt": "The scientific method involves making observations, forming hypotheses, and testing them through experiments and it forbids testing hypotheses at all.",
      "question": "Which of the following steps is part of the scientific method?",
      "options": [
        "A. Making observations and testing hypotheses",
        "B. Refusing to test anything",
        "C. Relying solely on superstition",
        "D. Avoiding experiments forever"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest bay in the world by area is the Bay of Bengal.",
      "conflict_prompt": "The largest bay in the world by area is the Bay of Bengal and it is a tiny pond.",
      "question": "Which is the largest bay in the world by area?",
      "options": [
        "A. Bay of Biscay",
        "B. Bay of Bengal",
        "C. Hudson Bay",
        "D. San Francisco Bay"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The principal author of the theory of evolution by natural selection is Charles Darwin.",
      "conflict_prompt": "The principal author of the theory of evolution by natural selection is Charles Darwin and he rejected natural selection entirely.",
      "question": "Who is the principal author of the theory of evolution by natural selection?",
      "options": [
        "A. Gregor Mendel",
        "B. Charles Darwin",
        "C. Louis Pasteur",
        "D. Michael Faraday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Mexico is Mexico City.",
      "conflict_prompt": "The capital of Mexico is Mexico City and the capital is Toronto.",
      "question": "What is the capital of Mexico?",
      "options": [
        "A. Mexico City",
        "B. Guadalajara",
        "C. Monterrey",
        "D. Toronto"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest active volcano in Europe is Mount Etna in Sicily.",
      "conflict_prompt": "The largest active volcano in Europe is Mount Etna in Sicily and it is a dormant valley with no volcanic activity.",
      "question": "Which volcano is the largest active volcano in Europe?",
      "options": [
        "A. Mount Vesuvius",
        "B. Mount Etna",
        "C. Eyjafjallajökull",
        "D. Mount St. Helens"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient of traditional Japanese sushi is raw or cooked fish combined with vinegared rice.",
      "conflict_prompt": "The primary ingredient of traditional Japanese sushi is raw or cooked fish combined with vinegared rice and sushi is made primarily from cardboard.",
      "question": "What commonly composes traditional sushi?",
      "options": [
        "A. Pasteurized milk",
        "B. Raw or cooked fish with vinegared rice",
        "C. Cardboard",
        "D. Roasted coffee beans"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The bridge construction began in 2005 and was completed in 2008.",
      "conflict_prompt": "The bridge construction began in 2005 and was completed in 2003.",
      "question": "In what year was the bridge completed?",
      "options": [
        "A. 2003",
        "B. 2005",
        "C. 2008",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She started her undergraduate studies in 2006 and graduated in 2010.",
      "conflict_prompt": "She started her undergraduate studies in 2006 and graduated in 2002.",
      "question": "In what year did she graduate from undergrad?",
      "options": [
        "A. 2002",
        "B. 2006",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company was founded in 1999 and went public in 2015.",
      "conflict_prompt": "The company was founded in 1999 and went public in 1990.",
      "question": "When did the company go public?",
      "options": [
        "A. 1990",
        "B. 1999",
        "C. 2010",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novel was written in 1987 and published in 1991.",
      "conflict_prompt": "The novel was written in 1987 and published in 1979.",
      "question": "In which year was the novel published?",
      "options": [
        "A. 1979",
        "B. 1987",
        "C. 1991",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He was appointed CEO in 2018 and resigned in 2022.",
      "conflict_prompt": "He was appointed CEO in 2018 and resigned in 2015.",
      "question": "When did he resign as CEO?",
      "options": [
        "A. 2015",
        "B. 2018",
        "C. 2020",
        "D. 2022"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra performed the premiere on March 10, 2014 and had rehearsals from March 1–9.",
      "conflict_prompt": "The orchestra performed the premiere on March 10, 2014 and had rehearsals from March 11–19.",
      "question": "When did the premiere performance take place?",
      "options": [
        "A. March 1, 2014",
        "B. March 9, 2014",
        "C. March 10, 2014",
        "D. March 19, 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The spacecraft launched on July 4, 2021 and landed back on Earth on July 18, 2021.",
      "conflict_prompt": "The spacecraft launched on July 4, 2021 and landed back on Earth on June 20, 2021.",
      "question": "On what date did the spacecraft land back on Earth?",
      "options": [
        "A. June 20, 2021",
        "B. July 4, 2021",
        "C. July 18, 2021",
        "D. August 1, 2021"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She was born in 1990 and obtained her medical license in 2016.",
      "conflict_prompt": "She was born in 1990 and obtained her medical license in 1988.",
      "question": "In what year did she obtain her medical license?",
      "options": [
        "A. 1988",
        "B. 1990",
        "C. 2005",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum opened to the public in 2002 after renovation work that began in 2000.",
      "conflict_prompt": "The museum opened to the public in 2002 after renovation work that began in 2005.",
      "question": "When did the renovation work begin?",
      "options": [
        "A. 2000",
        "B. 2002",
        "C. 2005",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete set a national record in 2016 and retired from competition in 2019.",
      "conflict_prompt": "The athlete set a national record in 2016 and retired from competition in 2010.",
      "question": "In which year did the athlete retire?",
      "options": [
        "A. 2010",
        "B. 2016",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference began on Monday, October 11, 2021 and concluded on Wednesday, October 13, 2021.",
      "conflict_prompt": "The conference began on Monday, October 11, 2021 and concluded on Wednesday, October 10, 2021.",
      "question": "On what date did the conference conclude?",
      "options": [
        "A. October 10, 2021",
        "B. October 11, 2021",
        "C. October 12, 2021",
        "D. October 13, 2021"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient underwent surgery on June 2, 2017 and was discharged on June 10, 2017.",
      "conflict_prompt": "The patient underwent surgery on June 2, 2017 and was discharged on May 28, 2017.",
      "question": "When was the patient discharged?",
      "options": [
        "A. May 28, 2017",
        "B. June 2, 2017",
        "C. June 10, 2017",
        "D. June 15, 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer finished the symphony in 1902 and it premiered in 1903.",
      "conflict_prompt": "The composer finished the symphony in 1902 and it premiered in 1898.",
      "question": "In which year did the symphony premiere?",
      "options": [
        "A. 1898",
        "B. 1902",
        "C. 1903",
        "D. 1910"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film's principal photography started in January 2018 and post-production ended in May 2019.",
      "conflict_prompt": "The film's principal photography started in January 2018 and post-production ended in November 2017.",
      "question": "When did post-production end?",
      "options": [
        "A. November 2017",
        "B. January 2018",
        "C. May 2019",
        "D. December 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They planted the orchard in spring 2010 and harvested fruit for the first time in 2013.",
      "conflict_prompt": "They planted the orchard in spring 2010 and harvested fruit for the first time in 2008.",
      "question": "When was the first harvest from the orchard?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research study was registered in 2014 and results were published in 2017.",
      "conflict_prompt": "The research study was registered in 2014 and results were published in 2010.",
      "question": "In what year were the study results published?",
      "options": [
        "A. 2010",
        "B. 2014",
        "C. 2016",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author completed the manuscript in December 2020 and its paperback edition was released in March 2022.",
      "conflict_prompt": "The author completed the manuscript in December 2020 and its paperback edition was released in January 2019.",
      "question": "When was the paperback edition released?",
      "options": [
        "A. January 2019",
        "B. December 2020",
        "C. March 2022",
        "D. July 2022"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The highway expansion started in 2012 and opened to traffic in 2016.",
      "conflict_prompt": "The highway expansion started in 2012 and opened to traffic in 2009.",
      "question": "When did the highway open to traffic?",
      "options": [
        "A. 2009",
        "B. 2012",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir rehearsed weekly from September 2015 and gave their first public concert in December 2015.",
      "conflict_prompt": "The choir rehearsed weekly from September 2015 and gave their first public concert in August 2015.",
      "question": "When was the choir's first public concert?",
      "options": [
        "A. August 2015",
        "B. September 2015",
        "C. November 2015",
        "D. December 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patent application was filed on April 3, 2007 and the patent was granted on October 12, 2012.",
      "conflict_prompt": "The patent application was filed on April 3, 2007 and the patent was granted on March 1, 2005.",
      "question": "On what date was the patent granted?",
      "options": [
        "A. March 1, 2005",
        "B. April 3, 2007",
        "C. October 12, 2012",
        "D. December 12, 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The team signed the coach on June 15, 2013 and he led his first match on August 20, 2013.",
      "conflict_prompt": "The team signed the coach on June 15, 2013 and he led his first match on May 2, 2013.",
      "question": "When did he lead his first match as coach?",
      "options": [
        "A. May 2, 2013",
        "B. June 15, 2013",
        "C. July 10, 2013",
        "D. August 20, 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The art exhibit opened in 1995 and closed in 1996 after a one-year run.",
      "conflict_prompt": "The art exhibit opened in 1995 and closed in 1993 after a one-year run.",
      "question": "In which year did the exhibit close?",
      "options": [
        "A. 1993",
        "B. 1995",
        "C. 1996",
        "D. 1997"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The volcanic monitoring program began in 2010 and recorded its first eruption in 2014.",
      "conflict_prompt": "The volcanic monitoring program began in 2010 and recorded its first eruption in 2006.",
      "question": "When did the program record its first eruption?",
      "options": [
        "A. 2006",
        "B. 2010",
        "C. 2012",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The skyscraper's construction commenced in 2007 and the building was topped out in 2011.",
      "conflict_prompt": "The skyscraper's construction commenced in 2007 and the building was topped out in 2005.",
      "question": "When was the skyscraper topped out?",
      "options": [
        "A. 2005",
        "B. 2007",
        "C. 2009",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra's conductor was hired in 2000 and celebrated a 20-year tenure in 2020.",
      "conflict_prompt": "The orchestra's conductor was hired in 2000 and celebrated a 20-year tenure in 2010.",
      "question": "In which year did the conductor celebrate 20 years with the orchestra?",
      "options": [
        "A. 2000",
        "B. 2010",
        "C. 2015",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity was established in 1988 and marked its 25th anniversary in 2013.",
      "conflict_prompt": "The charity was established in 1988 and marked its 25th anniversary in 2005.",
      "question": "When did the charity mark its 25th anniversary?",
      "options": [
        "A. 2005",
        "B. 1988",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He began working at the lab in 2011 and published his first paper from there in 2014.",
      "conflict_prompt": "He began working at the lab in 2011 and published his first paper from there in 2009.",
      "question": "When was his first paper from the lab published?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2013",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play premiered on April 2, 1998 after rehearsals earlier that month.",
      "conflict_prompt": "The play premiered on April 2, 1998 after rehearsals later that month on April 15.",
      "question": "On what date did the play premiere?",
      "options": [
        "A. April 2, 1998",
        "B. April 15, 1998",
        "C. March 30, 1998",
        "D. May 1, 1998"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The river ferry service resumed operations in May 2010 after being suspended since 2008.",
      "conflict_prompt": "The river ferry service resumed operations in May 2010 after being suspended since 2012.",
      "question": "Since which year had the ferry been suspended?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2012",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery opened its first shop in 1997 and expanded to a second location in 2005.",
      "conflict_prompt": "The bakery opened its first shop in 1997 and expanded to a second location in 1990.",
      "question": "When did the bakery open its second location?",
      "options": [
        "A. 1990",
        "B. 1997",
        "C. 2000",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The census was taken in 2010 and the report was published in 2012.",
      "conflict_prompt": "The census was taken in 2010 and the report was published in 2008.",
      "question": "When was the census report published?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The outbreak began in January 2003 and containment measures were implemented in March 2003.",
      "conflict_prompt": "The outbreak began in January 2003 and containment measures were implemented in November 2002.",
      "question": "When were containment measures implemented?",
      "options": [
        "A. November 2002",
        "B. January 2003",
        "C. February 2003",
        "D. March 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The manuscript was submitted on May 1, 2019 and accepted after peer review on August 12, 2019.",
      "conflict_prompt": "The manuscript was submitted on May 1, 2019 and accepted after peer review on March 3, 2018.",
      "question": "On what date was the manuscript accepted after peer review?",
      "options": [
        "A. March 3, 2018",
        "B. May 1, 2019",
        "C. August 12, 2019",
        "D. September 30, 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mountain lodge was built in 1965 and renovated in 1999.",
      "conflict_prompt": "The mountain lodge was built in 1965 and renovated in 1959.",
      "question": "When was the lodge renovated?",
      "options": [
        "A. 1959",
        "B. 1965",
        "C. 1985",
        "D. 1999"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon registration opened on January 1, 2020 and closed on March 31, 2020.",
      "conflict_prompt": "The marathon registration opened on January 1, 2020 and closed on December 31, 2019.",
      "question": "When did registration close?",
      "options": [
        "A. December 31, 2019",
        "B. January 1, 2020",
        "C. March 1, 2020",
        "D. March 31, 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historical society was incorporated in 1974 and published its first journal in 1976.",
      "conflict_prompt": "The historical society was incorporated in 1974 and published its first journal in 1968.",
      "question": "In what year did the society publish its first journal?",
      "options": [
        "A. 1968",
        "B. 1974",
        "C. 1975",
        "D. 1976"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new software update was released on September 10, 2022 and bug reports started coming in on September 12, 2022.",
      "conflict_prompt": "The new software update was released on September 10, 2022 and bug reports started coming in on September 8, 2022.",
      "question": "When did bug reports begin arriving?",
      "options": [
        "A. September 8, 2022",
        "B. September 10, 2022",
        "C. September 12, 2022",
        "D. September 20, 2022"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship set sail on June 1, 1890 and arrived at its destination on July 14, 1890.",
      "conflict_prompt": "The ship set sail on June 1, 1890 and arrived at its destination on May 20, 1890.",
      "question": "On what date did the ship arrive?",
      "options": [
        "A. May 20, 1890",
        "B. June 1, 1890",
        "C. July 14, 1890",
        "D. August 5, 1890"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The university admitted its first class in 1952 and celebrated its 50th anniversary in 2002.",
      "conflict_prompt": "The university admitted its first class in 1952 and celebrated its 50th anniversary in 1990.",
      "question": "When was the university's 50th anniversary?",
      "options": [
        "A. 1952",
        "B. 1990",
        "C. 2002",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The storm formed on August 9 and dissipated on August 13.",
      "conflict_prompt": "The storm formed on August 9 and dissipated on August 5.",
      "question": "On which date did the storm dissipate?",
      "options": [
        "A. August 5",
        "B. August 9",
        "C. August 12",
        "D. August 13"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer was born in 1878 and wrote his last piece in 1950.",
      "conflict_prompt": "The composer was born in 1878 and wrote his last piece in 1869.",
      "question": "When did the composer write his last piece?",
      "options": [
        "A. 1869",
        "B. 1878",
        "C. 1920",
        "D. 1950"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival ran annually from 2001 through 2010 and resumed again in 2015.",
      "conflict_prompt": "The festival ran annually from 2001 through 2010 and resumed again in 1998.",
      "question": "In which year did the festival resume after the break?",
      "options": [
        "A. 1998",
        "B. 2001",
        "C. 2010",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation permit was issued on March 2, 2016 and work began on April 1, 2016.",
      "conflict_prompt": "The renovation permit was issued on March 2, 2016 and work began on February 1, 2016.",
      "question": "When did renovation work begin?",
      "options": [
        "A. February 1, 2016",
        "B. March 2, 2016",
        "C. April 1, 2016",
        "D. May 1, 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The club was established in 1982 and its annual tournament started in 1985.",
      "conflict_prompt": "The club was established in 1982 and its annual tournament started in 1979.",
      "question": "When did the annual tournament begin?",
      "options": [
        "A. 1979",
        "B. 1982",
        "C. 1985",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The train departed at 07:30 and arrived at 10:45 the same morning.",
      "conflict_prompt": "The train departed at 07:30 and arrived at 06:15 the same morning.",
      "question": "At what time did the train arrive?",
      "options": [
        "A. 06:15",
        "B. 07:30",
        "C. 09:00",
        "D. 10:45"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibit was curated between February and April 2011 and opened in May 2011.",
      "conflict_prompt": "The exhibit was curated between February and April 2011 and opened in January 2011.",
      "question": "When did the exhibit open?",
      "options": [
        "A. January 2011",
        "B. February 2011",
        "C. April 2011",
        "D. May 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city's tram system began operation in 1903 and ceased regular service in 1968.",
      "conflict_prompt": "The city's tram system began operation in 1903 and ceased regular service in 1899.",
      "question": "When did the tram system cease regular service?",
      "options": [
        "A. 1899",
        "B. 1903",
        "C. 1955",
        "D. 1968"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research grant was awarded in 2004 and the laboratory opened in 2006.",
      "conflict_prompt": "The research grant was awarded in 2004 and the laboratory opened in 2002.",
      "question": "When did the laboratory open?",
      "options": [
        "A. 2002",
        "B. 2004",
        "C. 2005",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright completed Act I in 1910 and finished the full play in 1913.",
      "conflict_prompt": "The playwright completed Act I in 1910 and finished the full play in 1905.",
      "question": "In which year was the full play finished?",
      "options": [
        "A. 1905",
        "B. 1910",
        "C. 1913",
        "D. 1920"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lighthouse was constructed in 1875 and automated in 1960.",
      "conflict_prompt": "The lighthouse was constructed in 1875 and automated in 1868.",
      "question": "When was the lighthouse automated?",
      "options": [
        "A. 1868",
        "B. 1875",
        "C. 1950",
        "D. 1960"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new curriculum was implemented in September 2018 after pilot testing in 2017.",
      "conflict_prompt": "The new curriculum was implemented in September 2018 after pilot testing in 2019.",
      "question": "When was the curriculum implemented?",
      "options": [
        "A. 2017",
        "B. 2018",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The actor made his stage debut in 2001 and his film debut in 2004.",
      "conflict_prompt": "The actor made his stage debut in 2001 and his film debut in 1998.",
      "question": "In what year did he make his film debut?",
      "options": [
        "A. 1998",
        "B. 2001",
        "C. 2003",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cathedral's bell was cast in 1832 and first rung at its dedication in 1833.",
      "conflict_prompt": "The cathedral's bell was cast in 1832 and first rung at its dedication in 1829.",
      "question": "When was the bell first rung at the dedication?",
      "options": [
        "A. 1829",
        "B. 1832",
        "C. 1833",
        "D. 1840"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archive digitization project began in 2010 and the first batch went online in 2012.",
      "conflict_prompt": "The archive digitization project began in 2010 and the first batch went online in 2008.",
      "question": "When did the first batch of archives go online?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The concert tour started on May 3, 2015 and concluded on July 22, 2015.",
      "conflict_prompt": "The concert tour started on May 3, 2015 and concluded on April 10, 2015.",
      "question": "On what date did the tour conclude?",
      "options": [
        "A. April 10, 2015",
        "B. May 3, 2015",
        "C. June 15, 2015",
        "D. July 22, 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The construction crew broke ground in August 2014 and completed the project in June 2017.",
      "conflict_prompt": "The construction crew broke ground in August 2014 and completed the project in May 2012.",
      "question": "When was the construction project completed?",
      "options": [
        "A. May 2012",
        "B. August 2014",
        "C. June 2016",
        "D. June 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer premiered the quartet in 2009 and recorded it the next year.",
      "conflict_prompt": "The composer premiered the quartet in 2009 and recorded it in 2007.",
      "question": "When was the quartet recorded?",
      "options": [
        "A. 2007",
        "B. 2009",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery stopped production in December 2014 and reopened under new management in March 2016.",
      "conflict_prompt": "The bakery stopped production in December 2014 and reopened under new management in November 2013.",
      "question": "When did the bakery reopen under new management?",
      "options": [
        "A. November 2013",
        "B. December 2014",
        "C. January 2015",
        "D. March 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete was drafted in 2003 and made his professional debut in 2004.",
      "conflict_prompt": "The athlete was drafted in 2003 and made his professional debut in 2001.",
      "question": "When did he make his professional debut?",
      "options": [
        "A. 2001",
        "B. 2003",
        "C. 2004",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The foundation granted the scholarship in 2010 and the recipient began studying that fall.",
      "conflict_prompt": "The foundation granted the scholarship in 2010 and the recipient began studying in 2008.",
      "question": "When did the scholarship recipient begin studying?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The highway toll was introduced on January 1, 2009 and abolished on December 31, 2014.",
      "conflict_prompt": "The highway toll was introduced on January 1, 2009 and abolished on December 31, 2006.",
      "question": "When was the toll abolished?",
      "options": [
        "A. December 31, 2006",
        "B. January 1, 2009",
        "C. December 31, 2010",
        "D. December 31, 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeological dig began in June 1991 and yielded its most important find in August 1993.",
      "conflict_prompt": "The archaeological dig began in June 1991 and yielded its most important find in May 1989.",
      "question": "When was the dig's most important find made?",
      "options": [
        "A. May 1989",
        "B. June 1991",
        "C. July 1992",
        "D. August 1993"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite was launched on February 2, 2013 and entered orbit on February 3, 2013.",
      "conflict_prompt": "The satellite was launched on February 2, 2013 and entered orbit on January 28, 2013.",
      "question": "On what date did the satellite enter orbit?",
      "options": [
        "A. January 28, 2013",
        "B. February 2, 2013",
        "C. February 3, 2013",
        "D. February 10, 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novelist published her debut in 2005 and released her second book in 2009.",
      "conflict_prompt": "The novelist published her debut in 2005 and released her second book in 2001.",
      "question": "When was her second book released?",
      "options": [
        "A. 2001",
        "B. 2005",
        "C. 2007",
        "D. 2009"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir recorded the album in July 2012 and it was released in November 2012.",
      "conflict_prompt": "The choir recorded the album in July 2012 and it was released in May 2011.",
      "question": "When was the album released?",
      "options": [
        "A. May 2011",
        "B. July 2012",
        "C. September 2012",
        "D. November 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company hired its first employee in 2000 and hit profitability in 2006.",
      "conflict_prompt": "The company hired its first employee in 2000 and hit profitability in 1995.",
      "question": "In which year did the company become profitable?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2003",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The politician announced his candidacy in March 2017 and won the election in November 2018.",
      "conflict_prompt": "The politician announced his candidacy in March 2017 and won the election in June 2016.",
      "question": "When did he win the election?",
      "options": [
        "A. June 2016",
        "B. March 2017",
        "C. November 2017",
        "D. November 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer started sketching themes in 1899 and completed the opera in 1905.",
      "conflict_prompt": "The composer started sketching themes in 1899 and completed the opera in 1890.",
      "question": "In what year was the opera completed?",
      "options": [
        "A. 1890",
        "B. 1899",
        "C. 1900",
        "D. 1905"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The medical trial enrolled patients in 2012 and published final outcomes in 2016.",
      "conflict_prompt": "The medical trial enrolled patients in 2012 and published final outcomes in 2010.",
      "question": "When were the final outcomes published?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lighthouse keeper took the post in 1930 and retired in 1957.",
      "conflict_prompt": "The lighthouse keeper took the post in 1930 and retired in 1925.",
      "question": "When did he retire from the post?",
      "options": [
        "A. 1925",
        "B. 1930",
        "C. 1945",
        "D. 1957"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The TV series premiered in 2000 and its final episode aired in 2008.",
      "conflict_prompt": "The TV series premiered in 2000 and its final episode aired in 1996.",
      "question": "When did the final episode air?",
      "options": [
        "A. 1996",
        "B. 2000",
        "C. 2005",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra's founding year was 1922 and it celebrated its centenary in 2022.",
      "conflict_prompt": "The orchestra's founding year was 1922 and it celebrated its centenary in 2012.",
      "question": "In which year did the orchestra celebrate 100 years?",
      "options": [
        "A. 1922",
        "B. 2012",
        "C. 2022",
        "D. 2030"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot logged his first solo flight in 1994 and earned an instructor rating in 1998.",
      "conflict_prompt": "The pilot logged his first solo flight in 1994 and earned an instructor rating in 1990.",
      "question": "When did he earn his instructor rating?",
      "options": [
        "A. 1990",
        "B. 1994",
        "C. 1996",
        "D. 1998"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist joined the gallery in 2007 and held a retrospective there in 2017.",
      "conflict_prompt": "The artist joined the gallery in 2007 and held a retrospective there in 2001.",
      "question": "When was the retrospective held?",
      "options": [
        "A. 2001",
        "B. 2007",
        "C. 2015",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vaccine development started in 2015 and human trials began in 2018.",
      "conflict_prompt": "The vaccine development started in 2015 and human trials began in 2012.",
      "question": "When did human trials begin?",
      "options": [
        "A. 2012",
        "B. 2015",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play was written in 1943 and first performed in 1944.",
      "conflict_prompt": "The play was written in 1943 and first performed in 1939.",
      "question": "When was the first performance of the play?",
      "options": [
        "A. 1939",
        "B. 1943",
        "C. 1944",
        "D. 1950"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The observatory recorded the comet in November 2007 and tracked it until January 2008.",
      "conflict_prompt": "The observatory recorded the comet in November 2007 and tracked it until October 2006.",
      "question": "Until when did the observatory track the comet?",
      "options": [
        "A. October 2006",
        "B. November 2007",
        "C. December 2007",
        "D. January 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum acquired the painting in 1970 and put it on display in 1971.",
      "conflict_prompt": "The museum acquired the painting in 1970 and put it on display in 1965.",
      "question": "When was the painting put on display?",
      "options": [
        "A. 1965",
        "B. 1970",
        "C. 1971",
        "D. 1975"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software's beta phase started in January 2021 and the full launch occurred in September 2021.",
      "conflict_prompt": "The software's beta phase started in January 2021 and the full launch occurred in November 2020.",
      "question": "When was the software fully launched?",
      "options": [
        "A. November 2020",
        "B. January 2021",
        "C. June 2021",
        "D. September 2021"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vintage car was manufactured in 1954 and restored in 1994.",
      "conflict_prompt": "The vintage car was manufactured in 1954 and restored in 1949.",
      "question": "When was the car restored?",
      "options": [
        "A. 1949",
        "B. 1954",
        "C. 1980",
        "D. 1994"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The regional office opened in 2003 and became the headquarters in 2010.",
      "conflict_prompt": "The regional office opened in 2003 and became the headquarters in 1998.",
      "question": "When did it become the headquarters?",
      "options": [
        "A. 1998",
        "B. 2003",
        "C. 2008",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novelist was born in 1945 and published her memoir in 2015.",
      "conflict_prompt": "The novelist was born in 1945 and published her memoir in 1939.",
      "question": "When was the memoir published?",
      "options": [
        "A. 1939",
        "B. 1945",
        "C. 2000",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The restoration started in 2009 and the gallery reopened in 2011.",
      "conflict_prompt": "The restoration started in 2009 and the gallery reopened in 2005.",
      "question": "When did the gallery reopen?",
      "options": [
        "A. 2005",
        "B. 2009",
        "C. 2010",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference call took place at 2:00 PM and participants logged on at 1:45 PM.",
      "conflict_prompt": "The conference call took place at 2:00 PM and participants logged on at 2:30 PM.",
      "question": "When did participants log on for the call?",
      "options": [
        "A. 1:30 PM",
        "B. 1:45 PM",
        "C. 2:00 PM",
        "D. 2:30 PM"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The garden was planted in spring 1993 and the first blooms appeared in summer 1994.",
      "conflict_prompt": "The garden was planted in spring 1993 and the first blooms appeared in spring 1992.",
      "question": "When did the first blooms appear?",
      "options": [
        "A. Spring 1992",
        "B. Spring 1993",
        "C. Summer 1993",
        "D. Summer 1994"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company recorded its first annual loss in 2001 and returned to profit in 2004.",
      "conflict_prompt": "The company recorded its first annual loss in 2001 and returned to profit in 1998.",
      "question": "When did the company return to profit?",
      "options": [
        "A. 1998",
        "B. 2001",
        "C. 2003",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cathedral's stained glass was installed in 1920 and cleaned during a restoration in 1980.",
      "conflict_prompt": "The cathedral's stained glass was installed in 1920 and cleaned during a restoration in 1910.",
      "question": "When was the stained glass cleaned during restoration?",
      "options": [
        "A. 1910",
        "B. 1920",
        "C. 1950",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra toured Europe in the summer of 2002 and recorded an album that autumn.",
      "conflict_prompt": "The orchestra toured Europe in the summer of 2002 and recorded an album in spring 2001.",
      "question": "When did they record the album?",
      "options": [
        "A. Spring 2001",
        "B. Summer 2002",
        "C. Autumn 2002",
        "D. Winter 2003"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The singer released an EP in 2017 and her debut LP in 2019.",
      "conflict_prompt": "The singer released an EP in 2017 and her debut LP in 2015.",
      "question": "When was the debut LP released?",
      "options": [
        "A. 2015",
        "B. 2017",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge inspection took place on September 1, 2010 and repairs began on October 1, 2010.",
      "conflict_prompt": "The bridge inspection took place on September 1, 2010 and repairs began on August 15, 2010.",
      "question": "When did repairs begin?",
      "options": [
        "A. August 15, 2010",
        "B. September 1, 2010",
        "C. October 1, 2010",
        "D. November 1, 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research team formed in 2008 and published a review article in 2011.",
      "conflict_prompt": "The research team formed in 2008 and published a review article in 2006.",
      "question": "When was the review article published?",
      "options": [
        "A. 2006",
        "B. 2008",
        "C. 2009",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school year started on September 5 and ended on June 10.",
      "conflict_prompt": "The school year started on September 5 and ended on August 20.",
      "question": "When did the school year end?",
      "options": [
        "A. August 20",
        "B. September 5",
        "C. May 30",
        "D. June 10"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot program launched in 2013 and expanded nationwide in 2017.",
      "conflict_prompt": "The pilot program launched in 2013 and expanded nationwide in 2010.",
      "question": "When did it expand nationwide?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2015",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The monument was unveiled in 1955 and rededicated after restoration in 2005.",
      "conflict_prompt": "The monument was unveiled in 1955 and rededicated after restoration in 1948.",
      "question": "When was the monument rededicated after restoration?",
      "options": [
        "A. 1948",
        "B. 1955",
        "C. 1990",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The music festival was first held in 1978 and had its 40th edition in 2018.",
      "conflict_prompt": "The music festival was first held in 1978 and had its 40th edition in 2008.",
      "question": "In what year was the 40th edition held?",
      "options": [
        "A. 1978",
        "B. 2008",
        "C. 2018",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company introduced the product prototype in 2011 and mass production began in 2013.",
      "conflict_prompt": "The company introduced the product prototype in 2011 and mass production began in 2009.",
      "question": "When did mass production begin?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete broke the record in June 2000 and defended it successfully in 2004.",
      "conflict_prompt": "The athlete broke the record in June 2000 and defended it successfully in 1996.",
      "question": "When did he successfully defend the record?",
      "options": [
        "A. 1996",
        "B. 2000",
        "C. 2002",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibition catalog was printed in February 1999 and distributed in March 1999.",
      "conflict_prompt": "The exhibition catalog was printed in February 1999 and distributed in January 1998.",
      "question": "When was the catalog distributed?",
      "options": [
        "A. January 1998",
        "B. February 1999",
        "C. March 1999",
        "D. April 1999"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The training camp ran from July 1 to July 14, 2010 and players reported on June 30, 2010.",
      "conflict_prompt": "The training camp ran from July 1 to July 14, 2010 and players reported on July 20, 2010.",
      "question": "On what date did players report to camp?",
      "options": [
        "A. June 30, 2010",
        "B. July 1, 2010",
        "C. July 14, 2010",
        "D. July 20, 2010"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer sketched the theme in 1801 and published the final edition in 1806.",
      "conflict_prompt": "The composer sketched the theme in 1801 and published the final edition in 1798.",
      "question": "When was the final edition published?",
      "options": [
        "A. 1798",
        "B. 1801",
        "C. 1804",
        "D. 1806"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archival footage was digitized in 2014 and made available online in 2015.",
      "conflict_prompt": "The archival footage was digitized in 2014 and made available online in 2012.",
      "question": "When was the footage made available online?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot project was evaluated in 2011 and scaled up in 2013.",
      "conflict_prompt": "The pilot project was evaluated in 2011 and scaled up in 2009.",
      "question": "When was the project scaled up?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir was founded in 1960 and celebrated its 50th anniversary in 2010.",
      "conflict_prompt": "The choir was founded in 1960 and celebrated its 50th anniversary in 2000.",
      "question": "When did the choir celebrate 50 years?",
      "options": [
        "A. 1960",
        "B. 2000",
        "C. 2010",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery launched a gluten-free line in 2014 and launched a vegan line in 2016.",
      "conflict_prompt": "The bakery launched a gluten-free line in 2014 and launched a vegan line in 2010.",
      "question": "When was the vegan line launched?",
      "options": [
        "A. 2010",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The observatory installed the new telescope in March 2010 and achieved first light in April 2010.",
      "conflict_prompt": "The observatory installed the new telescope in March 2010 and achieved first light in January 2009.",
      "question": "When did the telescope achieve first light?",
      "options": [
        "A. January 2009",
        "B. March 2010",
        "C. April 2010",
        "D. June 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The newspaper launched a weekend edition in 1990 and ended it in 2005.",
      "conflict_prompt": "The newspaper launched a weekend edition in 1990 and ended it in 1985.",
      "question": "When did the weekend edition end?",
      "options": [
        "A. 1985",
        "B. 1990",
        "C. 2000",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The river dam was completed in 1962 and the reservoir reached full capacity in 1963.",
      "conflict_prompt": "The river dam was completed in 1962 and the reservoir reached full capacity in 1958.",
      "question": "When did the reservoir reach full capacity?",
      "options": [
        "A. 1958",
        "B. 1962",
        "C. 1963",
        "D. 1970"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The band formed in 1996 and released their debut album in 2000.",
      "conflict_prompt": "The band formed in 1996 and released their debut album in 1992.",
      "question": "When did they release their debut album?",
      "options": [
        "A. 1992",
        "B. 1996",
        "C. 1998",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The clinic opened in April 2000 and expanded services in 2007.",
      "conflict_prompt": "The clinic opened in April 2000 and expanded services in 1995.",
      "question": "When did the clinic expand services?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2005",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film festival accepted submissions starting January 1 and announced winners on March 15.",
      "conflict_prompt": "The film festival accepted submissions starting January 1 and announced winners on December 20 of the previous year.",
      "question": "When were winners announced?",
      "options": [
        "A. December 20 (previous year)",
        "B. January 1",
        "C. February 28",
        "D. March 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The courthouse was built in 1884 and renovated in 2004.",
      "conflict_prompt": "The courthouse was built in 1884 and renovated in 1878.",
      "question": "When was the courthouse renovated?",
      "options": [
        "A. 1878",
        "B. 1884",
        "C. 1999",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The seaside promenade was inaugurated in May 1960 and extended in 1990.",
      "conflict_prompt": "The seaside promenade was inaugurated in May 1960 and extended in 1955.",
      "question": "When was the promenade extended?",
      "options": [
        "A. 1955",
        "B. 1960",
        "C. 1975",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archive was established in 1987 and digitization efforts began in 2007.",
      "conflict_prompt": "The archive was established in 1987 and digitization efforts began in 1999.",
      "question": "When did digitization efforts begin?",
      "options": [
        "A. 1999",
        "B. 1987",
        "C. 2007",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The junior team formed in 2012 and won their first regional title in 2016.",
      "conflict_prompt": "The junior team formed in 2012 and won their first regional title in 2010.",
      "question": "When did they win their first regional title?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The electric tram was introduced in 1908 and withdrawn from service in 1952.",
      "conflict_prompt": "The electric tram was introduced in 1908 and withdrawn from service in 1899.",
      "question": "When was the tram withdrawn from service?",
      "options": [
        "A. 1899",
        "B. 1908",
        "C. 1940",
        "D. 1952"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The coach conducted training camps from 2011 to 2014 and accepted a new job in 2015.",
      "conflict_prompt": "The coach conducted training camps from 2011 to 2014 and accepted a new job in 2009.",
      "question": "When did the coach accept the new job?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2014",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The botanical garden planted a rare tree in 1986 and celebrated its 30th year in 2016.",
      "conflict_prompt": "The botanical garden planted a rare tree in 1986 and celebrated its 30th year in 2006.",
      "question": "When was the 30th year celebration?",
      "options": [
        "A. 1986",
        "B. 2006",
        "C. 2016",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot completed training in May 2005 and had his first commercial flight in August 2005.",
      "conflict_prompt": "The pilot completed training in May 2005 and had his first commercial flight in March 2005.",
      "question": "When did his first commercial flight occur?",
      "options": [
        "A. March 2005",
        "B. May 2005",
        "C. July 2005",
        "D. August 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town council passed the ordinance on April 10 and it took effect on May 1.",
      "conflict_prompt": "The town council passed the ordinance on April 10 and it took effect on March 1.",
      "question": "When did the ordinance take effect?",
      "options": [
        "A. March 1",
        "B. April 10",
        "C. April 30",
        "D. May 1"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeological site was discovered in 1971 and excavations began in 1973.",
      "conflict_prompt": "The archaeological site was discovered in 1971 and excavations began in 1968.",
      "question": "When did excavations begin?",
      "options": [
        "A. 1968",
        "B. 1971",
        "C. 1972",
        "D. 1973"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The newspaper celebrated 100 years of publication in 2009 after its founding in 1909.",
      "conflict_prompt": "The newspaper celebrated 100 years of publication in 2009 after its founding in 1915.",
      "question": "When was the newspaper founded?",
      "options": [
        "A. 1909",
        "B. 1915",
        "C. 1950",
        "D. 2009"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research team received IRB approval in July 2010 and began recruiting participants in August 2010.",
      "conflict_prompt": "The research team received IRB approval in July 2010 and began recruiting participants in June 2009.",
      "question": "When did participant recruitment begin?",
      "options": [
        "A. June 2009",
        "B. July 2010",
        "C. August 2010",
        "D. September 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city built a new library in 1980 and expanded it in 2000.",
      "conflict_prompt": "The city built a new library in 1980 and expanded it in 1975.",
      "question": "When was the library expansion?",
      "options": [
        "A. 1975",
        "B. 1980",
        "C. 1990",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer completed the opera in 1865 and its first performance was in 1866.",
      "conflict_prompt": "The composer completed the opera in 1865 and its first performance was in 1858.",
      "question": "When was the opera first performed?",
      "options": [
        "A. 1858",
        "B. 1865",
        "C. 1866",
        "D. 1870"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mayor took office on January 1, 2012 and left office on December 31, 2019.",
      "conflict_prompt": "The mayor took office on January 1, 2012 and left office on December 31, 2008.",
      "question": "When did the mayor leave office?",
      "options": [
        "A. December 31, 2008",
        "B. January 1, 2012",
        "C. December 31, 2016",
        "D. December 31, 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The volunteer program started in 1999 and reached 1,000 volunteers by 2004.",
      "conflict_prompt": "The volunteer program started in 1999 and reached 1,000 volunteers by 1995.",
      "question": "By what year had they reached 1,000 volunteers?",
      "options": [
        "A. 1995",
        "B. 1999",
        "C. 2002",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The summer internship ran from June to August 2018 and applications opened in March 2018.",
      "conflict_prompt": "The summer internship ran from June to August 2018 and applications opened in September 2018.",
      "question": "When did applications open for the internship?",
      "options": [
        "A. September 2017",
        "B. March 2018",
        "C. June 2018",
        "D. September 2018"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge inspection flagged concerns in 2011 and repairs were completed in 2013.",
      "conflict_prompt": "The bridge inspection flagged concerns in 2011 and repairs were completed in 2009.",
      "question": "When were the repairs completed?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school was established in 1892 and celebrated its bicentennial in 2092.",
      "conflict_prompt": "The school was established in 1892 and celebrated its bicentennial in 1992.",
      "question": "When was the school established?",
      "options": [
        "A. 1792",
        "B. 1892",
        "C. 1992",
        "D. 2092"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra recorded the soundtrack in April and released it in September of the same year.",
      "conflict_prompt": "The orchestra recorded the soundtrack in April and released it in January of the same year.",
      "question": "When was the soundtrack released?",
      "options": [
        "A. January (same year)",
        "B. April (same year)",
        "C. July (same year)",
        "D. September (same year)"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum's traveling exhibit arrived in July and departed in October 2014.",
      "conflict_prompt": "The museum's traveling exhibit arrived in July and departed in May 2014.",
      "question": "When did the traveling exhibit depart?",
      "options": [
        "A. May 2014",
        "B. July 2014",
        "C. September 2014",
        "D. October 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company introduced an acoustic version of the product in 2012 and a digital version in 2016.",
      "conflict_prompt": "The company introduced an acoustic version of the product in 2012 and a digital version in 2008.",
      "question": "When was the digital version introduced?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2012",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school board approved the bond measure in 2013 and construction began in 2014.",
      "conflict_prompt": "The school board approved the bond measure in 2013 and construction began in 2010.",
      "question": "When did construction begin?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company's research wing was established in 1990 and its first major discovery was published in 1997.",
      "conflict_prompt": "The company's research wing was established in 1990 and its first major discovery was published in 1985.",
      "question": "When was the first major discovery published?",
      "options": [
        "A. 1985",
        "B. 1990",
        "C. 1994",
        "D. 1997"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete fractured his ankle in March 2011 and returned to competition in September 2011.",
      "conflict_prompt": "The athlete fractured his ankle in March 2011 and returned to competition in January 2010.",
      "question": "When did he return to competition?",
      "options": [
        "A. January 2010",
        "B. March 2011",
        "C. June 2011",
        "D. September 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary was filmed in 2008 and premiered at a festival in 2009.",
      "conflict_prompt": "The documentary was filmed in 2008 and premiered at a festival in 2006.",
      "question": "When did the documentary premiere at a festival?",
      "options": [
        "A. 2006",
        "B. 2008",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The heritage trail was established in 1997 and extended by two miles in 2007.",
      "conflict_prompt": "The heritage trail was established in 1997 and extended by two miles in 1990.",
      "question": "When was the trail extended by two miles?",
      "options": [
        "A. 1990",
        "B. 1997",
        "C. 2000",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup pivoted its business model in late 2014 and secured Series A funding in 2016.",
      "conflict_prompt": "The startup pivoted its business model in late 2014 and secured Series A funding in 2012.",
      "question": "When did it secure Series A funding?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The monument was erected in 1910 and a plaque was added in 1950.",
      "conflict_prompt": "The monument was erected in 1910 and a plaque was added in 1902.",
      "question": "When was the plaque added?",
      "options": [
        "A. 1902",
        "B. 1910",
        "C. 1930",
        "D. 1950"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer premiered his concerto in 1938 and published the score in 1940.",
      "conflict_prompt": "The composer premiered his concerto in 1938 and published the score in 1932.",
      "question": "When was the score published?",
      "options": [
        "A. 1932",
        "B. 1938",
        "C. 1939",
        "D. 1940"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The adoption process began in May 2006 and the child officially joined the family in December 2007.",
      "conflict_prompt": "The adoption process began in May 2006 and the child officially joined the family in January 2005.",
      "question": "When did the child officially join the family?",
      "options": [
        "A. January 2005",
        "B. May 2006",
        "C. June 2007",
        "D. December 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The shipping route opened in 1845 and was closed for navigation in 1960.",
      "conflict_prompt": "The shipping route opened in 1845 and was closed for navigation in 1838.",
      "question": "When was the route closed for navigation?",
      "options": [
        "A. 1838",
        "B. 1845",
        "C. 1900",
        "D. 1960"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The registry began accepting entries on June 1 and closed on July 31.",
      "conflict_prompt": "The registry began accepting entries on June 1 and closed on May 31.",
      "question": "When did the registry close?",
      "options": [
        "A. May 31",
        "B. June 1",
        "C. July 1",
        "D. July 31"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The observatory logged a meteor shower peak on August 12 and continued observations through August 18.",
      "conflict_prompt": "The observatory logged a meteor shower peak on August 12 and continued observations through July 30.",
      "question": "Through what date did observations continue?",
      "options": [
        "A. July 30",
        "B. August 12",
        "C. August 15",
        "D. August 18"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The public theater opened in 1888 and hosted annual summer plays until 1939.",
      "conflict_prompt": "The public theater opened in 1888 and hosted annual summer plays until 1879.",
      "question": "Until what year did it host summer plays?",
      "options": [
        "A. 1879",
        "B. 1888",
        "C. 1920",
        "D. 1939"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The medical device received CE marking in 2009 and FDA clearance in 2011.",
      "conflict_prompt": "The medical device received CE marking in 2009 and FDA clearance in 2007.",
      "question": "When did it receive FDA clearance?",
      "options": [
        "A. 2007",
        "B. 2009",
        "C. 2010",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The sculpture was commissioned in 1966 and completed in 1970.",
      "conflict_prompt": "The sculpture was commissioned in 1966 and completed in 1960.",
      "question": "When was the sculpture completed?",
      "options": [
        "A. 1960",
        "B. 1966",
        "C. 1968",
        "D. 1970"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The youth orchestra started rehearsals in September 2005 and performed its first concert in December 2005.",
      "conflict_prompt": "The youth orchestra started rehearsals in September 2005 and performed its first concert in July 2005.",
      "question": "When was the first concert performed?",
      "options": [
        "A. July 2005",
        "B. September 2005",
        "C. November 2005",
        "D. December 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation was approved in 2017 and the building reopened in 2019.",
      "conflict_prompt": "The renovation was approved in 2017 and the building reopened in 2015.",
      "question": "When did the building reopen?",
      "options": [
        "A. 2015",
        "B. 2017",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film festival's submission window opened in January and closed in April 2020.",
      "conflict_prompt": "The film festival's submission window opened in January and closed in December 2019.",
      "question": "When did the submission window close?",
      "options": [
        "A. December 2019",
        "B. January 2020",
        "C. March 2020",
        "D. April 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity launched an emergency fund in March 2015 and distributed grants in April 2015.",
      "conflict_prompt": "The charity launched an emergency fund in March 2015 and distributed grants in February 2014.",
      "question": "When were grants distributed from the emergency fund?",
      "options": [
        "A. February 2014",
        "B. March 2015",
        "C. March 2016",
        "D. April 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film's editing was completed in May 2018 and the premiere occurred in June 2018.",
      "conflict_prompt": "The film's editing was completed in May 2018 and the premiere occurred in April 2017.",
      "question": "When did the film premiere?",
      "options": [
        "A. April 2017",
        "B. May 2018",
        "C. June 2018",
        "D. July 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The botanical symposium was held in March 2002 and proceedings were published in November 2002.",
      "conflict_prompt": "The botanical symposium was held in March 2002 and proceedings were published in January 2001.",
      "question": "When were the proceedings published?",
      "options": [
        "A. January 2001",
        "B. March 2002",
        "C. September 2002",
        "D. November 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The monument was commissioned in 1935 and dedicated in 1936.",
      "conflict_prompt": "The monument was commissioned in 1935 and dedicated in 1929.",
      "question": "When was the monument dedicated?",
      "options": [
        "A. 1929",
        "B. 1935",
        "C. 1936",
        "D. 1940"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The truck left the depot at 05:00 and delivered the cargo at 09:30.",
      "conflict_prompt": "The truck left the depot at 05:00 and delivered the cargo at 04:15.",
      "question": "At what time was the cargo delivered?",
      "options": [
        "A. 04:15",
        "B. 05:00",
        "C. 08:00",
        "D. 09:30"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The local chapter formed in 1993 and hosted its first conference in 1996.",
      "conflict_prompt": "The local chapter formed in 1993 and hosted its first conference in 1988.",
      "question": "When was the first conference hosted?",
      "options": [
        "A. 1988",
        "B. 1993",
        "C. 1995",
        "D. 1996"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The river patrol started nightly watches in 1977 and added daytime patrols in 1980.",
      "conflict_prompt": "The river patrol started nightly watches in 1977 and added daytime patrols in 1970.",
      "question": "When were daytime patrols added?",
      "options": [
        "A. 1970",
        "B. 1977",
        "C. 1978",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school's alumni association was founded in 1950 and launched a scholarship in 1975.",
      "conflict_prompt": "The school's alumni association was founded in 1950 and launched a scholarship in 1940.",
      "question": "When was the scholarship launched?",
      "options": [
        "A. 1940",
        "B. 1950",
        "C. 1960",
        "D. 1975"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The library's reading room opened in 1925 and electricity was installed in 1933.",
      "conflict_prompt": "The library's reading room opened in 1925 and electricity was installed in 1918.",
      "question": "When was electricity installed?",
      "options": [
        "A. 1918",
        "B. 1925",
        "C. 1930",
        "D. 1933"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film was shot during summer 2013 and the director completed post-production in winter 2014.",
      "conflict_prompt": "The film was shot during summer 2013 and the director completed post-production in spring 2012.",
      "question": "When was post-production completed?",
      "options": [
        "A. Spring 2012",
        "B. Summer 2013",
        "C. Autumn 2013",
        "D. Winter 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival committee formed in 2004 and staged its first event in 2005.",
      "conflict_prompt": "The festival committee formed in 2004 and staged its first event in 2002.",
      "question": "When was the first event staged?",
      "options": [
        "A. 2002",
        "B. 2004",
        "C. 2005",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum closed for refurbishment in January 2018 and reopened in September 2018.",
      "conflict_prompt": "The museum closed for refurbishment in January 2018 and reopened in December 2016.",
      "question": "When did the museum reopen?",
      "options": [
        "A. December 2016",
        "B. January 2018",
        "C. August 2018",
        "D. September 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author began research in 2001 and published a biography in 2007.",
      "conflict_prompt": "The author began research in 2001 and published a biography in 1999.",
      "question": "When was the biography published?",
      "options": [
        "A. 1999",
        "B. 2001",
        "C. 2005",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The science center opened an exhibit in May 1995 and retired it in 2000.",
      "conflict_prompt": "The science center opened an exhibit in May 1995 and retired it in 1990.",
      "question": "When was the exhibit retired?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 1998",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The poet published a collection in 1972 and received a prize in 1973.",
      "conflict_prompt": "The poet published a collection in 1972 and received a prize in 1969.",
      "question": "When did the poet receive the prize?",
      "options": [
        "A. 1969",
        "B. 1972",
        "C. 1973",
        "D. 1975"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The transit authority introduced express service in 1999 and extended hours in 2003.",
      "conflict_prompt": "The transit authority introduced express service in 1999 and extended hours in 1994.",
      "question": "When were hours extended?",
      "options": [
        "A. 1994",
        "B. 1999",
        "C. 2000",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The art school opened its doors in 1956 and moved to a new building in 1986.",
      "conflict_prompt": "The art school opened its doors in 1956 and moved to a new building in 1946.",
      "question": "When did it move to the new building?",
      "options": [
        "A. 1946",
        "B. 1956",
        "C. 1976",
        "D. 1986"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The streetcar line started service in 1911 and was electrified in 1920.",
      "conflict_prompt": "The streetcar line started service in 1911 and was electrified in 1905.",
      "question": "When was the line electrified?",
      "options": [
        "A. 1905",
        "B. 1911",
        "C. 1918",
        "D. 1920"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright staged a revival in 1979 and a commemorative edition was published in 1980.",
      "conflict_prompt": "The playwright staged a revival in 1979 and a commemorative edition was published in 1970.",
      "question": "When was the commemorative edition published?",
      "options": [
        "A. 1970",
        "B. 1979",
        "C. 1980",
        "D. 1985"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city's first subway line opened in 1935 and a second line was added in 1950.",
      "conflict_prompt": "The city's first subway line opened in 1935 and a second line was added in 1920.",
      "question": "When was the second line added?",
      "options": [
        "A. 1920",
        "B. 1935",
        "C. 1945",
        "D. 1950"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity's annual gala was held in November 2018 and raised funds through December 2018.",
      "conflict_prompt": "The charity's annual gala was held in November 2018 and raised funds through October 2017.",
      "question": "Through which month did fundraising continue?",
      "options": [
        "A. October 2017",
        "B. November 2018",
        "C. November 2019",
        "D. December 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vintage aircraft performed at airshows from 1995 until being retired in 2005.",
      "conflict_prompt": "The vintage aircraft performed at airshows from 1995 until being retired in 1990.",
      "question": "When was the aircraft retired?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 2000",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lighthouse underwent maintenance in 2012 and navigation was suspended for a week.",
      "conflict_prompt": "The lighthouse underwent maintenance in 2012 and navigation was suspended for two weeks in 2010.",
      "question": "When did the maintenance occur?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historical drama was set in 1789 and premiered in 1989.",
      "conflict_prompt": "The historical drama was set in 1789 and premiered in 1779.",
      "question": "When did the drama premiere?",
      "options": [
        "A. 1779",
        "B. 1789",
        "C. 1989",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon was scheduled for April 12 and postponed to May 17 due to weather.",
      "conflict_prompt": "The marathon was scheduled for April 12 and postponed to March 10 due to weather.",
      "question": "To what date was the marathon postponed?",
      "options": [
        "A. March 10",
        "B. April 12",
        "C. May 1",
        "D. May 17"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The house was constructed in 1899 and electricity was wired in during a 1920 renovation.",
      "conflict_prompt": "The house was constructed in 1899 and electricity was wired in during a 1885 renovation.",
      "question": "When was electricity added during renovation?",
      "options": [
        "A. 1885",
        "B. 1899",
        "C. 1920",
        "D. 1930"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The national park was designated in 1961 and visitor centers were built in the 1970s.",
      "conflict_prompt": "The national park was designated in 1961 and visitor centers were built in 1955.",
      "question": "When were visitor centers built?",
      "options": [
        "A. 1955",
        "B. 1961",
        "C. 1970s",
        "D. 1980s"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The radio station started broadcasting in 1947 and switched to FM in 1974.",
      "conflict_prompt": "The radio station started broadcasting in 1947 and switched to FM in 1939.",
      "question": "When did it switch to FM?",
      "options": [
        "A. 1939",
        "B. 1947",
        "C. 1965",
        "D. 1974"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The youth camp first opened in 1960 and celebrated 50 years of operation in 2010.",
      "conflict_prompt": "The youth camp first opened in 1960 and celebrated 50 years of operation in 2000.",
      "question": "When did the camp first open?",
      "options": [
        "A. 1950",
        "B. 1960",
        "C. 2000",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film adaptation was announced in 2008 and production wrapped in 2010.",
      "conflict_prompt": "The film adaptation was announced in 2008 and production wrapped in 2006.",
      "question": "When did production wrap?",
      "options": [
        "A. 2006",
        "B. 2008",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The coral reef survey was conducted in 1994 and published the following year.",
      "conflict_prompt": "The coral reef survey was conducted in 1994 and published the previous year, 1992.",
      "question": "When was the survey published?",
      "options": [
        "A. 1992",
        "B. 1994",
        "C. 1995",
        "D. 1996"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The community garden plots were allocated in March 2003 and gardeners planted in April 2003.",
      "conflict_prompt": "The community garden plots were allocated in March 2003 and gardeners planted in February 2003.",
      "question": "When did gardeners plant in the plots?",
      "options": [
        "A. February 2003",
        "B. March 2003",
        "C. April 2003",
        "D. May 2003"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author began a diary in 1990 and ended entries in 1999.",
      "conflict_prompt": "The author began a diary in 1990 and ended entries in 1985.",
      "question": "When did the diary entries end?",
      "options": [
        "A. 1985",
        "B. 1990",
        "C. 1995",
        "D. 1999"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity trip departed on June 7 and volunteers returned on June 21.",
      "conflict_prompt": "The charity trip departed on June 7 and volunteers returned on May 30.",
      "question": "When did volunteers return from the trip?",
      "options": [
        "A. May 30",
        "B. June 7",
        "C. June 14",
        "D. June 21"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra commissioned a new work in 2001 and premiered it in 2002.",
      "conflict_prompt": "The orchestra commissioned a new work in 2001 and premiered it in 1998.",
      "question": "When did the premiere occur?",
      "options": [
        "A. 1998",
        "B. 2001",
        "C. 2002",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city began planning the park in 2015 and opened it to the public in 2018.",
      "conflict_prompt": "The city began planning the park in 2015 and opened it to the public in 2012.",
      "question": "When was the park opened to the public?",
      "options": [
        "A. 2012",
        "B. 2015",
        "C. 2017",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The university launched an exchange program in 2004 and sent its first group abroad in 2005.",
      "conflict_prompt": "The university launched an exchange program in 2004 and sent its first group abroad in 2002.",
      "question": "When did the first group go abroad?",
      "options": [
        "A. 2002",
        "B. 2004",
        "C. 2005",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater troupe rehearsed for three weeks beginning March 10 and opened the production on March 31.",
      "conflict_prompt": "The theater troupe rehearsed for three weeks beginning March 10 and opened the production on March 5.",
      "question": "When did the production open?",
      "options": [
        "A. March 5",
        "B. March 10",
        "C. March 24",
        "D. March 31"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city's first public transit line launched in 1963 and celebrated its 50th anniversary in 2013.",
      "conflict_prompt": "The city's first public transit line launched in 1963 and celebrated its 50th anniversary in 2003.",
      "question": "When did the transit line launch?",
      "options": [
        "A. 1953",
        "B. 1963",
        "C. 2003",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film's storyboard was drafted in summer 2014 and animation began in winter 2014.",
      "conflict_prompt": "The film's storyboard was drafted in summer 2014 and animation began in spring 2013.",
      "question": "When did animation begin?",
      "options": [
        "A. Spring 2013",
        "B. Summer 2014",
        "C. Autumn 2014",
        "D. Winter 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival committee met monthly starting January and organized the event in September.",
      "conflict_prompt": "The festival committee met monthly starting January and organized the event in November of the previous year.",
      "question": "In which month was the event organized?",
      "options": [
        "A. November (previous year)",
        "B. January",
        "C. July",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Maria was born in 1985 and graduated from university in 2007.",
      "conflict_prompt": "Maria was born in 1985 and graduated from university in 1977.",
      "question": "In what year did Maria graduate from university?",
      "options": [
        "A. 1977",
        "B. 1995",
        "C. 2007",
        "D. 1989"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company was founded in 1999 and went public in 2005.",
      "conflict_prompt": "The company was founded in 1999 and went public in 1993.",
      "question": "In what year did the company go public?",
      "options": [
        "A. 2001",
        "B. 1993",
        "C. 1999",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novel was published in 2012 and adapted into a film in 2018.",
      "conflict_prompt": "The novel was published in 2012 and adapted into a film in 2008.",
      "question": "In what year was the novel adapted into a film?",
      "options": [
        "A. 2012",
        "B. 2008",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge was completed in 1995 and opened to traffic in 1996.",
      "conflict_prompt": "The bridge was completed in 1995 and opened to traffic in 1994.",
      "question": "In what year did the bridge open to traffic?",
      "options": [
        "A. 1994",
        "B. 1996",
        "C. 1995",
        "D. 1998"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Dr. Lee earned her medical degree in 2003 and started practicing medicine in 2004.",
      "conflict_prompt": "Dr. Lee earned her medical degree in 2003 and started practicing medicine in 2001.",
      "question": "In what year did Dr. Lee start practicing medicine?",
      "options": [
        "A. 2001",
        "B. 2003",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite was launched in 2015 and began transmitting data in 2016.",
      "conflict_prompt": "The satellite was launched in 2015 and began transmitting data in 2014.",
      "question": "In what year did the satellite begin transmitting data?",
      "options": [
        "A. 2013",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He was hired as CEO in 2010 and resigned in 2018.",
      "conflict_prompt": "He was hired as CEO in 2010 and resigned in 2008.",
      "question": "In what year did he resign as CEO?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra premiered the symphony in 1978 and recorded it in 1980.",
      "conflict_prompt": "The orchestra premiered the symphony in 1978 and recorded it in 1976.",
      "question": "In what year did the orchestra record the symphony?",
      "options": [
        "A. 1976",
        "B. 1978",
        "C. 1980",
        "D. 1982"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Their wedding took place in June 2014 and the honeymoon started in July 2014.",
      "conflict_prompt": "Their wedding took place in June 2014 and the honeymoon started in May 2014.",
      "question": "In what month did their honeymoon start?",
      "options": [
        "A. May",
        "B. June",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete set a national record in 2009 and retired from competition in 2012.",
      "conflict_prompt": "The athlete set a national record in 2009 and retired from competition in 2005.",
      "question": "In what year did the athlete retire from competition?",
      "options": [
        "A. 2005",
        "B. 2009",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation started in March 2017 and finished in November 2017.",
      "conflict_prompt": "The renovation started in March 2017 and finished in January 2016.",
      "question": "In what month did the renovation finish?",
      "options": [
        "A. January",
        "B. March",
        "C. July",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She published her research paper in 2011 and received a grant to continue the work in 2013.",
      "conflict_prompt": "She published her research paper in 2011 and received a grant to continue the work in 2009.",
      "question": "In what year did she receive the grant to continue the work?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival opened on August 10, 2020 and closed on August 15, 2020.",
      "conflict_prompt": "The festival opened on August 10, 2020 and closed on August 5, 2020.",
      "question": "On what date did the festival close?",
      "options": [
        "A. August 5, 2020",
        "B. August 10, 2020",
        "C. August 12, 2020",
        "D. August 15, 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The painting was created in 1889 and acquired by the museum in 1901.",
      "conflict_prompt": "The painting was created in 1889 and acquired by the museum in 1875.",
      "question": "In what year did the museum acquire the painting?",
      "options": [
        "A. 1875",
        "B. 1889",
        "C. 1901",
        "D. 1910"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Construction on the dam began in 1962 and the reservoir filled by 1966.",
      "conflict_prompt": "Construction on the dam began in 1962 and the reservoir filled by 1958.",
      "question": "By what year had the reservoir filled?",
      "options": [
        "A. 1958",
        "B. 1962",
        "C. 1964",
        "D. 1966"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The astronaut launched into orbit on April 3, 2012 and returned to Earth on April 18, 2012.",
      "conflict_prompt": "The astronaut launched into orbit on April 3, 2012 and returned to Earth on March 28, 2012.",
      "question": "On what date did the astronaut return to Earth?",
      "options": [
        "A. March 28, 2012",
        "B. April 3, 2012",
        "C. April 10, 2012",
        "D. April 18, 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer wrote the opera in 1850 and it premiered in 1853.",
      "conflict_prompt": "The composer wrote the opera in 1850 and it premiered in 1847.",
      "question": "In what year did the opera premiere?",
      "options": [
        "A. 1847",
        "B. 1850",
        "C. 1853",
        "D. 1860"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software update was released on January 8, 2021 and users reported bugs on January 10, 2021.",
      "conflict_prompt": "The software update was released on January 8, 2021 and users reported bugs on January 6, 2021.",
      "question": "On what date did users report bugs after the release?",
      "options": [
        "A. January 6, 2021",
        "B. January 8, 2021",
        "C. January 10, 2021",
        "D. January 12, 2021"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The species was discovered in 1992 and added to the endangered list in 2001.",
      "conflict_prompt": "The species was discovered in 1992 and added to the endangered list in 1988.",
      "question": "In what year was the species added to the endangered list?",
      "options": [
        "A. 1988",
        "B. 1992",
        "C. 1995",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He completed the marathon in 3 hours on May 5, 2010 and signed up for another race in September 2010.",
      "conflict_prompt": "He completed the marathon in 3 hours on May 5, 2010 and signed up for another race in March 2010.",
      "question": "In which month did he sign up for the other race?",
      "options": [
        "A. March",
        "B. May",
        "C. July",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school opened in September 1960 and celebrated its 50th anniversary in 2010.",
      "conflict_prompt": "The school opened in September 1960 and celebrated its 50th anniversary in 1990.",
      "question": "In what year did the school celebrate its 50th anniversary?",
      "options": [
        "A. 1990",
        "B. 2005",
        "C. 2010",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient underwent surgery on June 1, 2015 and was discharged on June 10, 2015.",
      "conflict_prompt": "The patient underwent surgery on June 1, 2015 and was discharged on May 25, 2015.",
      "question": "On what date was the patient discharged?",
      "options": [
        "A. May 25, 2015",
        "B. June 1, 2015",
        "C. June 10, 2015",
        "D. June 20, 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film was shot in 2017 and premiered at a festival in March 2018.",
      "conflict_prompt": "The film was shot in 2017 and premiered at a festival in March 2016.",
      "question": "When did the film premiere at the festival?",
      "options": [
        "A. March 2016",
        "B. March 2017",
        "C. March 2018",
        "D. March 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He bought the house in 2000 and renovated it in 2003.",
      "conflict_prompt": "He bought the house in 2000 and renovated it in 1998.",
      "question": "In what year did he renovate the house?",
      "options": [
        "A. 1998",
        "B. 2000",
        "C. 2002",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon began at 7:00 AM and the winner crossed the finish line at 9:35 AM.",
      "conflict_prompt": "The marathon began at 7:00 AM and the winner crossed the finish line at 6:45 AM.",
      "question": "At what time did the winner cross the finish line?",
      "options": [
        "A. 6:45 AM",
        "B. 7:00 AM",
        "C. 9:00 AM",
        "D. 9:35 AM"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The plant was planted in April and the first harvest occurred in October of the same year.",
      "conflict_prompt": "The plant was planted in April and the first harvest occurred in February of the same year.",
      "question": "In which month did the first harvest occur?",
      "options": [
        "A. February",
        "B. April",
        "C. August",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer was born in 1870 and composed his most famous piece in 1898.",
      "conflict_prompt": "The composer was born in 1870 and composed his most famous piece in 1865.",
      "question": "In what year did the composer write his most famous piece?",
      "options": [
        "A. 1865",
        "B. 1870",
        "C. 1898",
        "D. 1905"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship departed the harbor on July 2, 1994 and arrived at its destination on July 20, 1994.",
      "conflict_prompt": "The ship departed the harbor on July 2, 1994 and arrived at its destination on June 28, 1994.",
      "question": "On what date did the ship arrive at its destination?",
      "options": [
        "A. June 28, 1994",
        "B. July 2, 1994",
        "C. July 15, 1994",
        "D. July 20, 1994"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The architect completed the design in 2001 and construction began in 2002.",
      "conflict_prompt": "The architect completed the design in 2001 and construction began in 1999.",
      "question": "In what year did construction begin according to the statement?",
      "options": [
        "A. 1999",
        "B. 2000",
        "C. 2001",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete qualified for the finals on August 12, 2016 and competed in the final on August 14, 2016.",
      "conflict_prompt": "The athlete qualified for the finals on August 12, 2016 and competed in the final on August 10, 2016.",
      "question": "On what date did the athlete compete in the final?",
      "options": [
        "A. August 10, 2016",
        "B. August 12, 2016",
        "C. August 14, 2016",
        "D. August 16, 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patent was filed in 1990 and was granted in 1994.",
      "conflict_prompt": "The patent was filed in 1990 and was granted in 1986.",
      "question": "In what year was the patent granted?",
      "options": [
        "A. 1986",
        "B. 1990",
        "C. 1992",
        "D. 1994"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town installed the new sewer system in 1973 and connected all homes by 1975.",
      "conflict_prompt": "The town installed the new sewer system in 1973 and connected all homes by 1970.",
      "question": "By what year were all homes connected to the new sewer system?",
      "options": [
        "A. 1970",
        "B. 1973",
        "C. 1974",
        "D. 1975"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The actor filmed the scene in November 2011 and the movie released in March 2012.",
      "conflict_prompt": "The actor filmed the scene in November 2011 and the movie released in October 2010.",
      "question": "When was the movie released?",
      "options": [
        "A. October 2010",
        "B. November 2011",
        "C. January 2012",
        "D. March 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The election took place on November 6, 2018 and the new mayor was sworn in on January 2, 2019.",
      "conflict_prompt": "The election took place on November 6, 2018 and the new mayor was sworn in on December 15, 2018.",
      "question": "On what date was the new mayor sworn in according to the statement?",
      "options": [
        "A. November 6, 2018",
        "B. December 15, 2018",
        "C. January 2, 2019",
        "D. February 1, 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete broke the record on May 20, 2007 and subsequently set a personal best in June 2008.",
      "conflict_prompt": "The athlete broke the record on May 20, 2007 and subsequently set a personal best in March 2006.",
      "question": "In what month and year did the athlete set the later personal best?",
      "options": [
        "A. March 2006",
        "B. May 2007",
        "C. June 2008",
        "D. April 2009"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company hired its first employee in 2003 and opened a second office in 2010.",
      "conflict_prompt": "The company hired its first employee in 2003 and opened a second office in 1998.",
      "question": "In what year did the company open a second office?",
      "options": [
        "A. 1998",
        "B. 2003",
        "C. 2007",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The concert tour started in February 2014 and ended in September 2014.",
      "conflict_prompt": "The concert tour started in February 2014 and ended in January 2014.",
      "question": "In which month did the concert tour end?",
      "options": [
        "A. January",
        "B. February",
        "C. July",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The device was prototyped in 2016 and mass production began in 2018.",
      "conflict_prompt": "The device was prototyped in 2016 and mass production began in 2014.",
      "question": "In what year did mass production begin?",
      "options": [
        "A. 2014",
        "B. 2016",
        "C. 2017",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer was awarded a prize in 1932 and died in 1950.",
      "conflict_prompt": "The composer was awarded a prize in 1932 and died in 1925.",
      "question": "In what year did the composer die?",
      "options": [
        "A. 1925",
        "B. 1932",
        "C. 1945",
        "D. 1950"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The trail was blazed in 1890 and the first recorded crossing happened in 1892.",
      "conflict_prompt": "The trail was blazed in 1890 and the first recorded crossing happened in 1885.",
      "question": "In what year did the first recorded crossing happen?",
      "options": [
        "A. 1885",
        "B. 1890",
        "C. 1892",
        "D. 1900"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The teacher began working at the school in 2006 and earned tenure in 2012.",
      "conflict_prompt": "The teacher began working at the school in 2006 and earned tenure in 2004.",
      "question": "In what year did the teacher earn tenure?",
      "options": [
        "A. 2004",
        "B. 2006",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The manuscript was completed in 1997 and published in 1999.",
      "conflict_prompt": "The manuscript was completed in 1997 and published in 1994.",
      "question": "In what year was the manuscript published?",
      "options": [
        "A. 1994",
        "B. 1997",
        "C. 1998",
        "D. 1999"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The building's cornerstone was laid in 1920 and the dedication ceremony occurred in 1922.",
      "conflict_prompt": "The building's cornerstone was laid in 1920 and the dedication ceremony occurred in 1918.",
      "question": "In what year did the dedication ceremony occur?",
      "options": [
        "A. 1918",
        "B. 1920",
        "C. 1921",
        "D. 1922"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery opened on January 1, 2000 and hired a head baker in February 2000.",
      "conflict_prompt": "The bakery opened on January 1, 2000 and hired a head baker in December 1999.",
      "question": "In which month did the bakery hire its head baker according to the statement?",
      "options": [
        "A. December 1999",
        "B. January 2000",
        "C. February 2000",
        "D. March 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The expedition left base camp on June 7 and reached the summit on June 21.",
      "conflict_prompt": "The expedition left base camp on June 7 and reached the summit on May 30.",
      "question": "On what date did the expedition reach the summit?",
      "options": [
        "A. May 30",
        "B. June 7",
        "C. June 14",
        "D. June 21"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research project began in 2014 and produced final results in 2019.",
      "conflict_prompt": "The research project began in 2014 and produced final results in 2012.",
      "question": "In what year were the final results produced?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2017",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum acquired the collection in 1988 and opened a special exhibit in 1990.",
      "conflict_prompt": "The museum acquired the collection in 1988 and opened a special exhibit in 1985.",
      "question": "In what year did the museum open the special exhibit?",
      "options": [
        "A. 1985",
        "B. 1988",
        "C. 1989",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra scheduled rehearsals starting in January and performed the concert in April.",
      "conflict_prompt": "The orchestra scheduled rehearsals starting in January and performed the concert in December of the previous year.",
      "question": "In what month did the orchestra perform the concert?",
      "options": [
        "A. December",
        "B. January",
        "C. March",
        "D. April"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival's final day was October 11, 2015 after a week of events that started October 5.",
      "conflict_prompt": "The festival's final day was October 11, 2015 after a week of events that started October 20.",
      "question": "On what date did the festival start according to the clean statement?",
      "options": [
        "A. October 1, 2015",
        "B. October 5, 2015",
        "C. October 11, 2015",
        "D. October 20, 2015"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete's season ran from September 2013 to May 2014.",
      "conflict_prompt": "The athlete's season ran from September 2013 to July 2013.",
      "question": "In which month did the athlete's season end?",
      "options": [
        "A. July 2013",
        "B. September 2013",
        "C. April 2014",
        "D. May 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new policy took effect on July 1, 2022 and compliance reports were due by August 1, 2022.",
      "conflict_prompt": "The new policy took effect on July 1, 2022 and compliance reports were due by June 1, 2022.",
      "question": "By what date were compliance reports due?",
      "options": [
        "A. June 1, 2022",
        "B. July 1, 2022",
        "C. July 15, 2022",
        "D. August 1, 2022"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The telescope became operational in 2008 and recorded its first image in March 2009.",
      "conflict_prompt": "The telescope became operational in 2008 and recorded its first image in January 2007.",
      "question": "When did the telescope record its first image?",
      "options": [
        "A. January 2007",
        "B. 2008",
        "C. February 2008",
        "D. March 2009"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference call started at 2:00 PM and the keynote speech began at 2:30 PM.",
      "conflict_prompt": "The conference call started at 2:00 PM and the keynote speech began at 1:45 PM.",
      "question": "At what time did the keynote speech begin?",
      "options": [
        "A. 1:45 PM",
        "B. 2:00 PM",
        "C. 2:15 PM",
        "D. 2:30 PM"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novel was written in 2000 and the author announced a sequel in 2015.",
      "conflict_prompt": "The novel was written in 2000 and the author announced a sequel in 1995.",
      "question": "In what year did the author announce the sequel?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2010",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery sold its first loaf on April 4, 1993 and opened a second location in 2000.",
      "conflict_prompt": "The bakery sold its first loaf on April 4, 1993 and opened a second location in 1988.",
      "question": "In what year did the bakery open a second location?",
      "options": [
        "A. 1988",
        "B. 1993",
        "C. 1997",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The medical trial enrolled patients from 2010 through 2012 and published results in 2014.",
      "conflict_prompt": "The medical trial enrolled patients from 2010 through 2012 and published results in 2009.",
      "question": "In what year were the trial results published?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2012",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum renovated its west wing in 2005 and reopened it to the public in 2006.",
      "conflict_prompt": "The museum renovated its west wing in 2005 and reopened it to the public in 2003.",
      "question": "When did the museum reopen the west wing to the public?",
      "options": [
        "A. 2003",
        "B. 2005",
        "C. 2006",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright wrote the script in 1996 and its premiere occurred in 1999.",
      "conflict_prompt": "The playwright wrote the script in 1996 and its premiere occurred in 1993.",
      "question": "In what year did the play premiere?",
      "options": [
        "A. 1993",
        "B. 1996",
        "C. 1997",
        "D. 1999"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school year started on September 3 and classes ended on June 15.",
      "conflict_prompt": "The school year started on September 3 and classes ended on August 30 of the same school year.",
      "question": "On what date did classes end according to the clean statement?",
      "options": [
        "A. August 30",
        "B. September 3",
        "C. May 30",
        "D. June 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city announced the new park in 2011 and construction finished in 2013.",
      "conflict_prompt": "The city announced the new park in 2011 and construction finished in 2009.",
      "question": "In what year did construction finish?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2011",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The band recorded the album in August 2004 and released it in November 2004.",
      "conflict_prompt": "The band recorded the album in August 2004 and released it in July 2003.",
      "question": "When was the album released?",
      "options": [
        "A. July 2003",
        "B. August 2004",
        "C. October 2004",
        "D. November 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The census was conducted in 2010 and the official report was published in 2012.",
      "conflict_prompt": "The census was conducted in 2010 and the official report was published in 2008.",
      "question": "In what year was the official report published?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The road was closed for repairs on May 10 and reopened on May 24.",
      "conflict_prompt": "The road was closed for repairs on May 10 and reopened on May 5.",
      "question": "On what date did the road reopen according to the clean statement?",
      "options": [
        "A. May 5",
        "B. May 10",
        "C. May 20",
        "D. May 24"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra recorded in 1977 and released the LP in 1978.",
      "conflict_prompt": "The orchestra recorded in 1977 and released the LP in 1975.",
      "question": "In what year was the LP released?",
      "options": [
        "A. 1975",
        "B. 1976",
        "C. 1977",
        "D. 1978"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The greenhouse was built in 2002 and first produced tomatoes in 2003.",
      "conflict_prompt": "The greenhouse was built in 2002 and first produced tomatoes in 2000.",
      "question": "In what year did the greenhouse first produce tomatoes?",
      "options": [
        "A. 2000",
        "B. 2001",
        "C. 2002",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship was launched in 1874 and decommissioned in 1902.",
      "conflict_prompt": "The ship was launched in 1874 and decommissioned in 1868.",
      "question": "In what year was the ship decommissioned?",
      "options": [
        "A. 1868",
        "B. 1874",
        "C. 1895",
        "D. 1902"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film festival announced winners on September 12 and the awards ceremony was held on September 13.",
      "conflict_prompt": "The film festival announced winners on September 12 and the awards ceremony was held on September 5.",
      "question": "On what date was the awards ceremony held?",
      "options": [
        "A. September 5",
        "B. September 12",
        "C. September 13",
        "D. September 20"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The building permit was issued in 2019 and construction began in 2020.",
      "conflict_prompt": "The building permit was issued in 2019 and construction began in 2017.",
      "question": "In what year did construction begin according to the clean statement?",
      "options": [
        "A. 2017",
        "B. 2018",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete joined the national team in 2000 and won an international medal in 2004.",
      "conflict_prompt": "The athlete joined the national team in 2000 and won an international medal in 1996.",
      "question": "In what year did the athlete win the international medal?",
      "options": [
        "A. 1996",
        "B. 2000",
        "C. 2002",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity launched its program in January 2013 and reported outcomes in December 2013.",
      "conflict_prompt": "The charity launched its program in January 2013 and reported outcomes in November 2012.",
      "question": "When did the charity report outcomes?",
      "options": [
        "A. November 2012",
        "B. January 2013",
        "C. June 2013",
        "D. December 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist painted the mural in 2010 and later restored it in 2018.",
      "conflict_prompt": "The artist painted the mural in 2010 and later restored it in 2006.",
      "question": "In what year was the mural restored?",
      "options": [
        "A. 2006",
        "B. 2010",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The telescope observed the comet in November 1997 and published findings in February 1998.",
      "conflict_prompt": "The telescope observed the comet in November 1997 and published findings in October 1996.",
      "question": "When were the findings published?",
      "options": [
        "A. October 1996",
        "B. November 1997",
        "C. January 1998",
        "D. February 1998"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She enrolled in piano lessons in 2008 and performed her first recital in 2010.",
      "conflict_prompt": "She enrolled in piano lessons in 2008 and performed her first recital in 2006.",
      "question": "In what year did she perform her first recital?",
      "options": [
        "A. 2006",
        "B. 2008",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mayor announced the new transit plan in March 2020 and funding was approved in June 2020.",
      "conflict_prompt": "The mayor announced the new transit plan in March 2020 and funding was approved in January 2020.",
      "question": "In what month was funding approved according to the clean statement?",
      "options": [
        "A. January 2020",
        "B. March 2020",
        "C. April 2020",
        "D. June 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum hosted the exhibit from April through July 2011 and then traveled it to another city in September 2011.",
      "conflict_prompt": "The museum hosted the exhibit from April through July 2011 and then traveled it to another city in February 2011.",
      "question": "In which month did the museum travel the exhibit to another city?",
      "options": [
        "A. February 2011",
        "B. April 2011",
        "C. July 2011",
        "D. September 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school holiday began on December 20 and classes resumed on January 6.",
      "conflict_prompt": "The school holiday began on December 20 and classes resumed on December 15.",
      "question": "On what date did classes resume according to the clean statement?",
      "options": [
        "A. December 15",
        "B. December 20",
        "C. January 1",
        "D. January 6"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author completed the trilogy by finishing the third book in 2016 and released a collector's edition in 2019.",
      "conflict_prompt": "The author completed the trilogy by finishing the third book in 2016 and released a collector's edition in 2010.",
      "question": "In what year was the collector's edition released?",
      "options": [
        "A. 2010",
        "B. 2016",
        "C. 2017",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research study recruited volunteers in spring 2002 and closed enrollment in fall 2003.",
      "conflict_prompt": "The research study recruited volunteers in spring 2002 and closed enrollment in spring 2001.",
      "question": "When did enrollment close according to the clean statement?",
      "options": [
        "A. Spring 2001",
        "B. Spring 2002",
        "C. Summer 2002",
        "D. Fall 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibit's opening night was June 2 and the public viewing started June 3.",
      "conflict_prompt": "The exhibit's opening night was June 2 and the public viewing started May 28.",
      "question": "On what date did the public viewing start according to the clean statement?",
      "options": [
        "A. May 28",
        "B. June 2",
        "C. June 3",
        "D. June 10"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot completed training in 2011 and flew as captain for the first time in 2013.",
      "conflict_prompt": "The pilot completed training in 2011 and flew as captain for the first time in 2009.",
      "question": "In what year did the pilot first fly as captain according to the clean statement?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company announced layoffs in August and finalized the severance packages in September of the same year.",
      "conflict_prompt": "The company announced layoffs in August and finalized the severance packages in July of the same year.",
      "question": "In which month were severance packages finalized according to the clean statement?",
      "options": [
        "A. July",
        "B. August",
        "C. September",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeological dig began in 2004 and uncovered the main structure in 2006.",
      "conflict_prompt": "The archaeological dig began in 2004 and uncovered the main structure in 2001.",
      "question": "In what year was the main structure uncovered?",
      "options": [
        "A. 2001",
        "B. 2002",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The library opened its new wing on March 15, 2010 and held a dedication on March 20, 2010.",
      "conflict_prompt": "The library opened its new wing on March 15, 2010 and held a dedication on March 10, 2009.",
      "question": "When was the dedication held according to the clean statement?",
      "options": [
        "A. March 10, 2009",
        "B. March 15, 2010",
        "C. March 18, 2010",
        "D. March 20, 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer premiered a new concerto in 2000 and revised it in 2002.",
      "conflict_prompt": "The composer premiered a new concerto in 2000 and revised it in 1998.",
      "question": "In what year was the concerto revised according to the clean statement?",
      "options": [
        "A. 1998",
        "B. 2000",
        "C. 2001",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The baker started the sourdough culture in January and baked daily loaves starting in March.",
      "conflict_prompt": "The baker started the sourdough culture in January and baked daily loaves starting in December of the previous year.",
      "question": "In which month did the baker begin baking daily loaves according to the clean statement?",
      "options": [
        "A. December",
        "B. January",
        "C. February",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mayor launched the initiative in 2017 and reported progress in 2019.",
      "conflict_prompt": "The mayor launched the initiative in 2017 and reported progress in 2015.",
      "question": "In what year did the mayor report progress according to the clean statement?",
      "options": [
        "A. 2015",
        "B. 2016",
        "C. 2017",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The actor filmed the commercial in June and the advertisement aired in August.",
      "conflict_prompt": "The actor filmed the commercial in June and the advertisement aired in May.",
      "question": "In which month did the advertisement air according to the clean statement?",
      "options": [
        "A. May",
        "B. June",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The team's preseason began on August 1 and the regular season opener was on September 10.",
      "conflict_prompt": "The team's preseason began on August 1 and the regular season opener was on July 25.",
      "question": "On what date was the regular season opener according to the clean statement?",
      "options": [
        "A. July 25",
        "B. August 1",
        "C. August 30",
        "D. September 10"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge inspection occurred in May 2014 and repairs were completed in December 2014.",
      "conflict_prompt": "The bridge inspection occurred in May 2014 and repairs were completed in March 2013.",
      "question": "In what month and year were repairs completed according to the clean statement?",
      "options": [
        "A. March 2013",
        "B. May 2014",
        "C. September 2014",
        "D. December 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The sunscreen product launched in spring 2012 and became widely available in summer 2012.",
      "conflict_prompt": "The sunscreen product launched in spring 2012 and became widely available in winter 2011.",
      "question": "In which season did the product become widely available according to the clean statement?",
      "options": [
        "A. Winter 2011",
        "B. Spring 2012",
        "C. Summer 2012",
        "D. Fall 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author moved to the city in 1998 and published a memoir about the move in 2003.",
      "conflict_prompt": "The author moved to the city in 1998 and published a memoir about the move in 1993.",
      "question": "In what year was the memoir published according to the clean statement?",
      "options": [
        "A. 1993",
        "B. 1998",
        "C. 2000",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The harvest festival started on October 1 and continued until October 7.",
      "conflict_prompt": "The harvest festival started on October 1 and continued until September 24.",
      "question": "On what date did the harvest festival end according to the clean statement?",
      "options": [
        "A. September 24",
        "B. October 1",
        "C. October 5",
        "D. October 7"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research team published preliminary findings in 2015 and final conclusions in 2018.",
      "conflict_prompt": "The research team published preliminary findings in 2015 and final conclusions in 2012.",
      "question": "In what year were final conclusions published according to the clean statement?",
      "options": [
        "A. 2012",
        "B. 2015",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The family moved into the house in June 1994 and celebrated their first anniversary there in June 1995.",
      "conflict_prompt": "The family moved into the house in June 1994 and celebrated their first anniversary there in June 1993.",
      "question": "When did they celebrate their first anniversary in the house according to the clean statement?",
      "options": [
        "A. June 1993",
        "B. June 1994",
        "C. June 1995",
        "D. June 1996"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software beta testing began on March 1 and the full release occurred on May 15.",
      "conflict_prompt": "The software beta testing began on March 1 and the full release occurred on February 20.",
      "question": "On what date did the full release occur according to the clean statement?",
      "options": [
        "A. February 20",
        "B. March 1",
        "C. April 10",
        "D. May 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship set sail in 1802 and returned to port in 1804.",
      "conflict_prompt": "The ship set sail in 1802 and returned to port in 1799.",
      "question": "In what year did the ship return to port according to the clean statement?",
      "options": [
        "A. 1799",
        "B. 1800",
        "C. 1802",
        "D. 1804"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city ran a pilot program in 2016 and expanded it citywide in 2019.",
      "conflict_prompt": "The city ran a pilot program in 2016 and expanded it citywide in 2014.",
      "question": "In what year was the program expanded citywide according to the clean statement?",
      "options": [
        "A. 2014",
        "B. 2016",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The restaurant changed its menu in January 2017 and introduced seasonal dishes in March 2017.",
      "conflict_prompt": "The restaurant changed its menu in January 2017 and introduced seasonal dishes in November 2016.",
      "question": "When did the restaurant introduce seasonal dishes according to the clean statement?",
      "options": [
        "A. November 2016",
        "B. January 2017",
        "C. February 2017",
        "D. March 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vineyard planted new vines in spring 1999 and produced its first vintage from them in 2003.",
      "conflict_prompt": "The vineyard planted new vines in spring 1999 and produced its first vintage from them in 1995.",
      "question": "In what year was the first vintage from the new vines produced according to the clean statement?",
      "options": [
        "A. 1995",
        "B. 1999",
        "C. 2001",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The concert was announced in February and tickets went on sale in March.",
      "conflict_prompt": "The concert was announced in February and tickets went on sale in January.",
      "question": "In which month did tickets go on sale according to the clean statement?",
      "options": [
        "A. January",
        "B. February",
        "C. March",
        "D. April"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city celebrated its bicentennial in 2020 and published a commemorative book that same year.",
      "conflict_prompt": "The city celebrated its bicentennial in 2020 and published a commemorative book in 2016.",
      "question": "In what year was the commemorative book published according to the clean statement?",
      "options": [
        "A. 2016",
        "B. 2018",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete set a new personal best in April and then peaked at the national championship in July.",
      "conflict_prompt": "The athlete set a new personal best in April and then peaked at the national championship in March.",
      "question": "In what month did the athlete peak at the national championship according to the clean statement?",
      "options": [
        "A. March",
        "B. April",
        "C. May",
        "D. July"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The art fair ran from June 10 to June 16 and closed on June 16 with an awards ceremony.",
      "conflict_prompt": "The art fair ran from June 10 to June 16 and closed on June 5 with an awards ceremony.",
      "question": "On what date did the art fair close according to the clean statement?",
      "options": [
        "A. June 5",
        "B. June 10",
        "C. June 14",
        "D. June 16"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The election results were certified on December 1 and the new representatives took office on January 3.",
      "conflict_prompt": "The election results were certified on December 1 and the new representatives took office on November 20.",
      "question": "On what date did the new representatives take office according to the clean statement?",
      "options": [
        "A. November 20",
        "B. December 1",
        "C. January 3",
        "D. February 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film received distribution rights in 2005 and was released internationally in 2007.",
      "conflict_prompt": "The film received distribution rights in 2005 and was released internationally in 2003.",
      "question": "In what year was the film released internationally according to the clean statement?",
      "options": [
        "A. 2003",
        "B. 2005",
        "C. 2006",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The public library initiated the catalog digitization in 1998 and completed it in 2004.",
      "conflict_prompt": "The public library initiated the catalog digitization in 1998 and completed it in 1994.",
      "question": "In what year was the digitization completed according to the clean statement?",
      "options": [
        "A. 1994",
        "B. 1998",
        "C. 2001",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer premiered a chamber work in 1955 and recorded it in 1957.",
      "conflict_prompt": "The composer premiered a chamber work in 1955 and recorded it in 1953.",
      "question": "When was the work recorded according to the clean statement?",
      "options": [
        "A. 1953",
        "B. 1955",
        "C. 1956",
        "D. 1957"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright rehearsed the cast in October and the opening night was on November 5.",
      "conflict_prompt": "The playwright rehearsed the cast in October and the opening night was on September 28.",
      "question": "On what date was opening night according to the clean statement?",
      "options": [
        "A. September 28",
        "B. October 15",
        "C. November 1",
        "D. November 5"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The train service resumed on April 12 after maintenance that started April 1.",
      "conflict_prompt": "The train service resumed on April 12 after maintenance that started April 20.",
      "question": "On what date did train service resume according to the statement?",
      "options": [
        "A. April 1",
        "B. April 12",
        "C. April 20",
        "D. May 1"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new curriculum was piloted in three schools in 2013 and adopted district-wide in 2016.",
      "conflict_prompt": "The new curriculum was piloted in three schools in 2013 and adopted district-wide in 2010.",
      "question": "In what year was the curriculum adopted district-wide according to the clean statement?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum's restoration began in 1991 and reopened the gallery in 1993.",
      "conflict_prompt": "The museum's restoration began in 1991 and reopened the gallery in 1989.",
      "question": "In what year did the gallery reopen according to the clean statement?",
      "options": [
        "A. 1989",
        "B. 1991",
        "C. 1992",
        "D. 1993"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city introduced bike lanes in 2012 and reported a reduction in accidents by 2015.",
      "conflict_prompt": "The city introduced bike lanes in 2012 and reported a reduction in accidents by 2010.",
      "question": "By what year did the city report a reduction in accidents according to the clean statement?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater company formed in 1986 and staged its centennial production in 2086.",
      "conflict_prompt": "The theater company formed in 1986 and staged its centennial production in 1976.",
      "question": "In what year did the company stage its centennial production according to the clean statement?",
      "options": [
        "A. 1976",
        "B. 1986",
        "C. 2000",
        "D. 2086"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The scientist received a fellowship in 2007 and published a breakthrough paper in 2010.",
      "conflict_prompt": "The scientist received a fellowship in 2007 and published a breakthrough paper in 2004.",
      "question": "In what year did the scientist publish the breakthrough paper according to the clean statement?",
      "options": [
        "A. 2004",
        "B. 2007",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The community garden started planting beds in March and hosted its first harvest fair in October.",
      "conflict_prompt": "The community garden started planting beds in March and hosted its first harvest fair in February.",
      "question": "In which month did the community host its first harvest fair according to the clean statement?",
      "options": [
        "A. February",
        "B. March",
        "C. June",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic house was acquired by the trust in 1977 and opened for tours in 1980.",
      "conflict_prompt": "The historic house was acquired by the trust in 1977 and opened for tours in 1974.",
      "question": "In what year did the house open for tours according to the clean statement?",
      "options": [
        "A. 1974",
        "B. 1977",
        "C. 1979",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup secured seed funding in 2011 and launched its product in 2013.",
      "conflict_prompt": "The startup secured seed funding in 2011 and launched its product in 2009.",
      "question": "In what year did the startup launch its product according to the clean statement?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2011",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir formed in 1995 and released its debut album in 2000.",
      "conflict_prompt": "The choir formed in 1995 and released its debut album in 1990.",
      "question": "In what year did the choir release its debut album according to the clean statement?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 1998",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge was inspected annually starting in 2000 and a major overhaul took place in 2014.",
      "conflict_prompt": "The bridge was inspected annually starting in 2000 and a major overhaul took place in 1996.",
      "question": "In what year did the major overhaul occur according to the clean statement?",
      "options": [
        "A. 1996",
        "B. 2000",
        "C. 2010",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot flew solo for the first time in 2001 and earned a commercial license in 2005.",
      "conflict_prompt": "The pilot flew solo for the first time in 2001 and earned a commercial license in 1999.",
      "question": "In what year did the pilot earn a commercial license according to the clean statement?",
      "options": [
        "A. 1999",
        "B. 2001",
        "C. 2003",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The community center opened in November 1982 and hosted a grand reopening after renovations in November 1992.",
      "conflict_prompt": "The community center opened in November 1982 and hosted a grand reopening after renovations in November 1978.",
      "question": "When did the grand reopening after renovations occur according to the clean statement?",
      "options": [
        "A. November 1978",
        "B. November 1982",
        "C. November 1988",
        "D. November 1992"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software patch was issued on April 9 and users were advised to update by April 15.",
      "conflict_prompt": "The software patch was issued on April 9 and users were advised to update by March 31.",
      "question": "By what date were users advised to update according to the clean statement?",
      "options": [
        "A. March 31",
        "B. April 9",
        "C. April 12",
        "D. April 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra toured Europe in summer 2009 and gave its final concert on August 27.",
      "conflict_prompt": "The orchestra toured Europe in summer 2009 and gave its final concert on July 10 of the previous year.",
      "question": "On what date did the orchestra give its final concert according to the clean statement?",
      "options": [
        "A. July 10 (previous year)",
        "B. July 25",
        "C. August 1",
        "D. August 27"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school district approved the budget in June and disbursed funds to schools in July.",
      "conflict_prompt": "The school district approved the budget in June and disbursed funds to schools in May.",
      "question": "In what month were funds disbursed according to the clean statement?",
      "options": [
        "A. May",
        "B. June",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon registration opened January 1 and early-bird registration ended February 15.",
      "conflict_prompt": "The marathon registration opened January 1 and early-bird registration ended December 15 of the previous year.",
      "question": "When did early-bird registration end according to the clean statement?",
      "options": [
        "A. December 15 (previous year)",
        "B. January 1",
        "C. February 1",
        "D. February 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite failed during re-entry on September 2, 1998 and debris was recovered later that month.",
      "conflict_prompt": "The satellite failed during re-entry on September 2, 1998 and debris was recovered in August 1997.",
      "question": "When was the satellite's debris recovered according to the clean statement?",
      "options": [
        "A. August 1997",
        "B. September 2, 1998",
        "C. September 15, 1998",
        "D. October 1998"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The university admitted the first class in fall 1965 and graduated its first class in 1969.",
      "conflict_prompt": "The university admitted the first class in fall 1965 and graduated its first class in 1960.",
      "question": "In what year did the university graduate its first class according to the clean statement?",
      "options": [
        "A. 1960",
        "B. 1965",
        "C. 1967",
        "D. 1969"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company launched a pilot plant in 2014 and scaled to full production in 2017.",
      "conflict_prompt": "The company launched a pilot plant in 2014 and scaled to full production in 2012.",
      "question": "In what year did the company scale to full production according to the clean statement?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2015",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary was filmed over two summers, 2008 and 2009, and premiered in 2010.",
      "conflict_prompt": "The documentary was filmed over two summers, 2008 and 2009, and premiered in 2007.",
      "question": "In what year did the documentary premiere according to the clean statement?",
      "options": [
        "A. 2007",
        "B. 2008",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival committee formed in 2001 and organized the inaugural event in 2002.",
      "conflict_prompt": "The festival committee formed in 2001 and organized the inaugural event in 1999.",
      "question": "In what year was the inaugural event organized according to the clean statement?",
      "options": [
        "A. 1999",
        "B. 2000",
        "C. 2001",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete suffered an injury in February and returned to competition in September of the same year.",
      "conflict_prompt": "The athlete suffered an injury in February and returned to competition in January of the same year.",
      "question": "In which month did the athlete return to competition according to the clean statement?",
      "options": [
        "A. January",
        "B. February",
        "C. June",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater opened a new season in October and staged a revival in December.",
      "conflict_prompt": "The theater opened a new season in October and staged a revival in August.",
      "question": "In which month did the theater stage the revival according to the clean statement?",
      "options": [
        "A. August",
        "B. October",
        "C. November",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The restoration of the fresco began in 2000 and the public viewing resumed in 2002.",
      "conflict_prompt": "The restoration of the fresco began in 2000 and the public viewing resumed in 1998.",
      "question": "In what year did public viewing resume according to the clean statement?",
      "options": [
        "A. 1998",
        "B. 2000",
        "C. 2001",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir held auditions in June and announced selections in July.",
      "conflict_prompt": "The choir held auditions in June and announced selections in May.",
      "question": "When were audition selections announced according to the clean statement?",
      "options": [
        "A. May",
        "B. June",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The product entered beta testing in September and launched commercially in December.",
      "conflict_prompt": "The product entered beta testing in September and launched commercially in August.",
      "question": "In which month did the product launch commercially according to the clean statement?",
      "options": [
        "A. August",
        "B. September",
        "C. November",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The greenhouse experiment began in May and produced results by October of that year.",
      "conflict_prompt": "The greenhouse experiment began in May and produced results by April of that same year.",
      "question": "By what month were results produced according to the clean statement?",
      "options": [
        "A. April",
        "B. May",
        "C. July",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright completed rewrites in late 1994 and the play premiered in early 1995.",
      "conflict_prompt": "The playwright completed rewrites in late 1994 and the play premiered in early 1993.",
      "question": "When did the play premiere according to the clean statement?",
      "options": [
        "A. Early 1993",
        "B. Late 1994",
        "C. Late 1994 / Early 1995",
        "D. Early 1995"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new highway opened to traffic on June 30 and tolls were collected starting July 1.",
      "conflict_prompt": "The new highway opened to traffic on June 30 and tolls were collected starting May 1.",
      "question": "When did toll collection begin according to the clean statement?",
      "options": [
        "A. May 1",
        "B. June 30",
        "C. July 1",
        "D. August 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The university announced a new scholarship program in 2015 and awarded the first scholarships in 2016.",
      "conflict_prompt": "The university announced a new scholarship program in 2015 and awarded the first scholarships in 2013.",
      "question": "In what year were the first scholarships awarded according to the clean statement?",
      "options": [
        "A. 2013",
        "B. 2015",
        "C. 2016",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity event was scheduled for June 18 and actually took place on June 18 despite rain.",
      "conflict_prompt": "The charity event was scheduled for June 18 and actually took place on June 10 of the same year.",
      "question": "On what date did the charity event take place according to the clean statement?",
      "options": [
        "A. June 10",
        "B. June 15",
        "C. June 18",
        "D. June 20"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer released the album in 2003 and won an award for it in 2004.",
      "conflict_prompt": "The composer released the album in 2003 and won an award for it in 2001.",
      "question": "In what year did the composer win the award according to the clean statement?",
      "options": [
        "A. 2001",
        "B. 2002",
        "C. 2003",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The library hosted a lecture series from March through May and the final lecture was on May 28.",
      "conflict_prompt": "The library hosted a lecture series from March through May and the final lecture was on February 28.",
      "question": "On what date was the final lecture according to the clean statement?",
      "options": [
        "A. February 28",
        "B. March 1",
        "C. May 15",
        "D. May 28"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup moved into its first office in June 2012 and expanded into a larger space in 2015.",
      "conflict_prompt": "The startup moved into its first office in June 2012 and expanded into a larger space in 2010.",
      "question": "In what year did the startup expand into a larger space according to the clean statement?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary crew filmed interviews in 2006 and released the film in 2008.",
      "conflict_prompt": "The documentary crew filmed interviews in 2006 and released the film in 2004.",
      "question": "In what year was the documentary released according to the clean statement?",
      "options": [
        "A. 2004",
        "B. 2006",
        "C. 2007",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The institute opened a research lab in September 1990 and published its first study from that lab in 1992.",
      "conflict_prompt": "The institute opened a research lab in September 1990 and published its first study from that lab in 1988.",
      "question": "In what year was the first study published from the new lab according to the clean statement?",
      "options": [
        "A. 1988",
        "B. 1990",
        "C. 1991",
        "D. 1992"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cycling race started at dawn on June 14 and the podium ceremony was held later that evening.",
      "conflict_prompt": "The cycling race started at dawn on June 14 and the podium ceremony was held the previous evening on June 13.",
      "question": "When was the podium ceremony held according to the clean statement?",
      "options": [
        "A. June 13",
        "B. June 14 (evening)",
        "C. June 15",
        "D. June 16"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She was born in 1992 and graduated from university in 2014.",
      "conflict_prompt": "She was born in 1992 and graduated from university in 1984.",
      "question": "In what year did she graduate from university?",
      "options": [
        "A. 1998",
        "B. 2006",
        "C. 2014",
        "D. 1984"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company was founded in 2001 and went public in 2010.",
      "conflict_prompt": "The company was founded in 2001 and went public in 1995.",
      "question": "In what year did the company go public?",
      "options": [
        "A. 1995",
        "B. 2005",
        "C. 2010",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Construction on the library began in 2016 and it opened in 2019.",
      "conflict_prompt": "Construction on the library began in 2016 and it opened in 2013.",
      "question": "When did the library open?",
      "options": [
        "A. 2016",
        "B. 2013",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist painted the portrait in 1882 and exhibited it in 1885.",
      "conflict_prompt": "The artist painted the portrait in 1882 and exhibited it in 1879.",
      "question": "In what year was the portrait exhibited?",
      "options": [
        "A. 1879",
        "B. 1882",
        "C. 1885",
        "D. 1890"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He started working at the firm in 2012 and retired in 2022.",
      "conflict_prompt": "He started working at the firm in 2012 and retired in 2008.",
      "question": "When did he retire from the firm?",
      "options": [
        "A. 2008",
        "B. 2012",
        "C. 2020",
        "D. 2022"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The movie was filmed in 2018 and released in 2019.",
      "conflict_prompt": "The movie was filmed in 2018 and released in 2016.",
      "question": "In what year was the movie released?",
      "options": [
        "A. 2016",
        "B. 2017",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient was admitted on March 5, 2020, and discharged on March 12, 2020.",
      "conflict_prompt": "The patient was admitted on March 5, 2020, and discharged on March 1, 2020.",
      "question": "On what date was the patient discharged?",
      "options": [
        "A. March 1, 2020",
        "B. March 5, 2020",
        "C. March 12, 2020",
        "D. March 20, 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The book was written in 2007 and published in 2010.",
      "conflict_prompt": "The book was written in 2007 and published in 2003.",
      "question": "In what year was the book published?",
      "options": [
        "A. 2007",
        "B. 2003",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They got married in June 2011 and celebrated their tenth anniversary in 2021.",
      "conflict_prompt": "They got married in June 2011 and celebrated their tenth anniversary in 2015.",
      "question": "When did they celebrate their tenth anniversary?",
      "options": [
        "A. 2015",
        "B. 2016",
        "C. 2021",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pianist composed the sonata in 1798 and premiered it in 1801.",
      "conflict_prompt": "The pianist composed the sonata in 1798 and premiered it in 1790.",
      "question": "In what year was the sonata premiered?",
      "options": [
        "A. 1790",
        "B. 1798",
        "C. 1801",
        "D. 1805"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software project started in January 2017 and released version 1.0 in July 2018.",
      "conflict_prompt": "The software project started in January 2017 and released version 1.0 in July 2016.",
      "question": "When was version 1.0 released?",
      "options": [
        "A. July 2016",
        "B. January 2017",
        "C. July 2018",
        "D. December 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge construction began in 2009 and it opened to traffic in 2013.",
      "conflict_prompt": "The bridge construction began in 2009 and it opened to traffic in 2005.",
      "question": "When did the bridge open to traffic?",
      "options": [
        "A. 2005",
        "B. 2009",
        "C. 2013",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He enrolled in medical school in 2004 and completed his residency in 2011.",
      "conflict_prompt": "He enrolled in medical school in 2004 and completed his residency in 2000.",
      "question": "In what year did he complete his residency?",
      "options": [
        "A. 2000",
        "B. 2004",
        "C. 2011",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival began on July 10 and ended on July 14.",
      "conflict_prompt": "The festival began on July 10 and ended on July 6.",
      "question": "On which date did the festival end?",
      "options": [
        "A. July 6",
        "B. July 10",
        "C. July 14",
        "D. July 20"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research began in 2012 and concluded with a final report in 2018.",
      "conflict_prompt": "The research began in 2012 and concluded with a final report in 2009.",
      "question": "When was the final report published?",
      "options": [
        "A. 2009",
        "B. 2012",
        "C. 2018",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author finished the manuscript in September 2015 and the novel was released in January 2016.",
      "conflict_prompt": "The author finished the manuscript in September 2015 and the novel was released in January 2014.",
      "question": "When was the novel released?",
      "options": [
        "A. January 2014",
        "B. September 2015",
        "C. January 2016",
        "D. March 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete turned professional in 2010 and won his first major title in 2015.",
      "conflict_prompt": "The athlete turned professional in 2010 and won his first major title in 2007.",
      "question": "In what year did he win his first major title?",
      "options": [
        "A. 2007",
        "B. 2010",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She planted the orchard in Spring 2003 and harvested fruit for the first time in 2006.",
      "conflict_prompt": "She planted the orchard in Spring 2003 and harvested fruit for the first time in 2001.",
      "question": "When did she harvest fruit from the orchard for the first time?",
      "options": [
        "A. 2001",
        "B. 2003",
        "C. 2006",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite was built in 2014 and launched into orbit in 2016.",
      "conflict_prompt": "The satellite was built in 2014 and launched into orbit in 2012.",
      "question": "In what year was the satellite launched into orbit?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He graduated from high school in 2009 and enrolled in college that fall.",
      "conflict_prompt": "He graduated from high school in 2009 and enrolled in college in 2005.",
      "question": "When did he enroll in college?",
      "options": [
        "A. 2005",
        "B. 2009",
        "C. 2011",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The chef opened the restaurant in 2013 and received a Michelin star in 2017.",
      "conflict_prompt": "The chef opened the restaurant in 2013 and received a Michelin star in 2010.",
      "question": "In what year did the restaurant receive a Michelin star?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2017",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cruise ship was launched in 2005 and completed its maiden voyage in 2006.",
      "conflict_prompt": "The cruise ship was launched in 2005 and completed its maiden voyage in 2002.",
      "question": "When did the cruise ship complete its maiden voyage?",
      "options": [
        "A. 2002",
        "B. 2005",
        "C. 2006",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival's planning started in January and the first event took place in April.",
      "conflict_prompt": "The festival's planning started in January and the first event took place in November of the previous year.",
      "question": "In which month did the first event take place?",
      "options": [
        "A. November",
        "B. January",
        "C. April",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She bought the car in 2018 and sold it in 2021.",
      "conflict_prompt": "She bought the car in 2018 and sold it in 2010.",
      "question": "When did she sell the car?",
      "options": [
        "A. 2010",
        "B. 2018",
        "C. 2021",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town council approved the plan in 2014 and construction began in 2015.",
      "conflict_prompt": "The town council approved the plan in 2014 and construction began in 2010.",
      "question": "When did construction begin?",
      "options": [
        "A. 2010",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer completed the opera in 1870 and it premiered in 1873.",
      "conflict_prompt": "The composer completed the opera in 1870 and it premiered in 1865.",
      "question": "When did the opera premiere?",
      "options": [
        "A. 1865",
        "B. 1870",
        "C. 1873",
        "D. 1880"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The medication was approved by regulators in 2012 and went on sale in 2013.",
      "conflict_prompt": "The medication was approved by regulators in 2012 and went on sale in 2009.",
      "question": "In what year did the medication go on sale?",
      "options": [
        "A. 2009",
        "B. 2012",
        "C. 2013",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete set the world record in 1996 and retired from competition in 2000.",
      "conflict_prompt": "The athlete set the world record in 1996 and retired from competition in 1992.",
      "question": "When did the athlete retire from competition?",
      "options": [
        "A. 1992",
        "B. 1996",
        "C. 1998",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra rehearsed for three weeks starting on May 1 and performed on May 22.",
      "conflict_prompt": "The orchestra rehearsed for three weeks starting on May 1 and performed on April 18.",
      "question": "On what date did the orchestra perform?",
      "options": [
        "A. April 18",
        "B. May 1",
        "C. May 22",
        "D. June 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum acquired the artifact in 1999 and put it on display in 2000.",
      "conflict_prompt": "The museum acquired the artifact in 1999 and put it on display in 1995.",
      "question": "When was the artifact put on display?",
      "options": [
        "A. 1995",
        "B. 1999",
        "C. 2000",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon began at 7:00 AM and the winners finished shortly after 10:00 AM.",
      "conflict_prompt": "The marathon began at 7:00 AM and the winners finished shortly after 6:00 AM.",
      "question": "When did the winners finish?",
      "options": [
        "A. 6:00 AM",
        "B. 7:00 AM",
        "C. 10:00 AM",
        "D. Noon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The political party formed in 1993 and won its first election in 1998.",
      "conflict_prompt": "The political party formed in 1993 and won its first election in 1988.",
      "question": "When did the party win its first election?",
      "options": [
        "A. 1988",
        "B. 1993",
        "C. 1998",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He began learning French in 2010 and passed the proficiency exam in 2014.",
      "conflict_prompt": "He began learning French in 2010 and passed the proficiency exam in 2006.",
      "question": "In what year did he pass the proficiency exam?",
      "options": [
        "A. 2006",
        "B. 2010",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation started in August 2017 and finished in December 2018.",
      "conflict_prompt": "The renovation started in August 2017 and finished in May 2016.",
      "question": "When was the renovation finished?",
      "options": [
        "A. May 2016",
        "B. August 2017",
        "C. December 2018",
        "D. January 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research paper was submitted in March and accepted in September of the same year.",
      "conflict_prompt": "The research paper was submitted in March and accepted in January of the same year.",
      "question": "In which month was the paper accepted?",
      "options": [
        "A. January",
        "B. March",
        "C. September",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibit ran from October 1 to November 15.",
      "conflict_prompt": "The exhibit ran from October 1 to September 10.",
      "question": "When did the exhibit end?",
      "options": [
        "A. September 10",
        "B. October 1",
        "C. November 15",
        "D. December 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software patch was released after a beta phase that began in May and ended in August.",
      "conflict_prompt": "The software patch was released after a beta phase that began in May and ended the previous January.",
      "question": "When did the beta phase end?",
      "options": [
        "A. January",
        "B. May",
        "C. August",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright completed the script in 1990 and the play premiered in 1992.",
      "conflict_prompt": "The playwright completed the script in 1990 and the play premiered in 1985.",
      "question": "When did the play premiere?",
      "options": [
        "A. 1985",
        "B. 1990",
        "C. 1992",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater company rehearsed all summer and staged the production in September.",
      "conflict_prompt": "The theater company rehearsed all summer and staged the production the previous spring.",
      "question": "In which month did they stage the production?",
      "options": [
        "A. Spring",
        "B. Summer",
        "C. September",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The product was announced in February and shipped to customers in April.",
      "conflict_prompt": "The product was announced in February and shipped to customers in January.",
      "question": "When did the product ship to customers?",
      "options": [
        "A. January",
        "B. February",
        "C. March",
        "D. April"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city hosted the conference in 2012 and hosted it again in 2017.",
      "conflict_prompt": "The city hosted the conference in 2012 and hosted it again in 2008.",
      "question": "When did the city host the conference again?",
      "options": [
        "A. 2008",
        "B. 2012",
        "C. 2017",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The beekeeper installed the hives in March and harvested honey in September of the same year.",
      "conflict_prompt": "The beekeeper installed the hives in March and harvested honey in January of the same year.",
      "question": "When did the beekeeper harvest honey?",
      "options": [
        "A. January",
        "B. March",
        "C. September",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete signed with the team in 2014 and made his debut in 2015.",
      "conflict_prompt": "The athlete signed with the team in 2014 and made his debut in 2011.",
      "question": "In what year did he make his debut?",
      "options": [
        "A. 2011",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The dam project was approved in 1997 and completed in 2003.",
      "conflict_prompt": "The dam project was approved in 1997 and completed in 1990.",
      "question": "When was the dam project completed?",
      "options": [
        "A. 1990",
        "B. 1997",
        "C. 2003",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He learned to fly in 2000 and earned his commercial pilot license in 2005.",
      "conflict_prompt": "He learned to fly in 2000 and earned his commercial pilot license in 1996.",
      "question": "When did he earn his commercial pilot license?",
      "options": [
        "A. 1996",
        "B. 2000",
        "C. 2005",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery opened its doors in 1998 and expanded to a second location in 2004.",
      "conflict_prompt": "The bakery opened its doors in 1998 and expanded to a second location in 1992.",
      "question": "When did the bakery expand to a second location?",
      "options": [
        "A. 1992",
        "B. 1998",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The spacecraft launched on July 2, 2021 and entered orbit on July 3, 2021.",
      "conflict_prompt": "The spacecraft launched on July 2, 2021 and entered orbit on June 28, 2020.",
      "question": "On what date did the spacecraft enter orbit?",
      "options": [
        "A. June 28, 2020",
        "B. July 2, 2021",
        "C. July 3, 2021",
        "D. July 10, 2021"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The zoo acquired the elephant in 2010 and celebrated its tenth birthday in 2018.",
      "conflict_prompt": "The zoo acquired the elephant in 2010 and celebrated its tenth birthday in 2009.",
      "question": "When did the zoo celebrate the elephant's tenth birthday?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2018",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra's season opened in September and closed in May the following year.",
      "conflict_prompt": "The orchestra's season opened in September and closed the previous April.",
      "question": "When did the season close?",
      "options": [
        "A. The previous April",
        "B. September",
        "C. May",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The curators purchased the painting in 2002 and restored it in 2006.",
      "conflict_prompt": "The curators purchased the painting in 2002 and restored it in 1998.",
      "question": "In what year was the painting restored?",
      "options": [
        "A. 1998",
        "B. 2002",
        "C. 2006",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The team practiced all winter and won the championship in April.",
      "conflict_prompt": "The team practiced all winter and won the championship the previous summer.",
      "question": "In which month did they win the championship?",
      "options": [
        "A. Summer",
        "B. Winter",
        "C. April",
        "D. May"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cathedral was consecrated in 1220 and underwent renovation in 1890.",
      "conflict_prompt": "The cathedral was consecrated in 1220 and underwent renovation in 1150.",
      "question": "In what year did the renovation occur?",
      "options": [
        "A. 1150",
        "B. 1220",
        "C. 1890",
        "D. 1900"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The spacecraft completed its mission in 2010 and re-entered Earth's atmosphere in 2010 as planned.",
      "conflict_prompt": "The spacecraft completed its mission in 2010 and re-entered Earth's atmosphere in 2005.",
      "question": "When did the spacecraft re-enter Earth's atmosphere?",
      "options": [
        "A. 2005",
        "B. 2010",
        "C. 2012",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The singer released her debut album in 2003 and embarked on a world tour in 2004.",
      "conflict_prompt": "The singer released her debut album in 2003 and embarked on a world tour in 2000.",
      "question": "When did she embark on a world tour?",
      "options": [
        "A. 2000",
        "B. 2003",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The building permit was issued in June and construction began in July.",
      "conflict_prompt": "The building permit was issued in June and construction began the previous January.",
      "question": "When did construction begin?",
      "options": [
        "A. The previous January",
        "B. June",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The committee convened on Monday and voted to adjourn on Friday.",
      "conflict_prompt": "The committee convened on Monday and voted to adjourn the previous Friday.",
      "question": "On which day did they vote to adjourn?",
      "options": [
        "A. The previous Friday",
        "B. Monday",
        "C. Friday",
        "D. Saturday"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She published her memoir in 2011 and promoted it on a radio tour in 2012.",
      "conflict_prompt": "She published her memoir in 2011 and promoted it on a radio tour in 2009.",
      "question": "When did she promote the memoir on a radio tour?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school district approved the new curriculum in 2015 and implemented it in the 2016–2017 school year.",
      "conflict_prompt": "The school district approved the new curriculum in 2015 and implemented it in the 2012–2013 school year.",
      "question": "When was the curriculum implemented?",
      "options": [
        "A. 2012–2013",
        "B. 2015",
        "C. 2016–2017",
        "D. 2018–2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The architect completed the blueprints in March and construction started in June.",
      "conflict_prompt": "The architect completed the blueprints in March and construction started the previous December.",
      "question": "When did construction start?",
      "options": [
        "A. The previous December",
        "B. March",
        "C. June",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity was established in 1990 and celebrated its 30th anniversary in 2020.",
      "conflict_prompt": "The charity was established in 1990 and celebrated its 30th anniversary in 2015.",
      "question": "When did it celebrate its 30th anniversary?",
      "options": [
        "A. 2015",
        "B. 1990",
        "C. 2020",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The train service launched on May 1 and full service began on June 1.",
      "conflict_prompt": "The train service launched on May 1 and full service began on April 1 of the same year.",
      "question": "When did full service begin?",
      "options": [
        "A. April 1",
        "B. May 1",
        "C. June 1",
        "D. July 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The immunization campaign started in January and reached its peak in June.",
      "conflict_prompt": "The immunization campaign started in January and reached its peak the previous October.",
      "question": "When did the campaign reach its peak?",
      "options": [
        "A. The previous October",
        "B. January",
        "C. June",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The musician joined the band in 2008 and left in 2014.",
      "conflict_prompt": "The musician joined the band in 2008 and left in 2005.",
      "question": "When did the musician leave the band?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot received his license in 1999 and logged his 1,000th flight hour in 2006.",
      "conflict_prompt": "The pilot received his license in 1999 and logged his 1,000th flight hour in 1995.",
      "question": "In what year did he log his 1,000th flight hour?",
      "options": [
        "A. 1995",
        "B. 1999",
        "C. 2006",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ice rink opened for the season on November 15 and closed on March 30.",
      "conflict_prompt": "The ice rink opened for the season on November 15 and closed on October 20.",
      "question": "When did the rink close for the season?",
      "options": [
        "A. October 20",
        "B. November 15",
        "C. March 30",
        "D. April 15"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary was filmed over the summer of 2013 and premiered at a festival in 2014.",
      "conflict_prompt": "The documentary was filmed over the summer of 2013 and premiered at a festival in 2011.",
      "question": "When did the documentary premiere at a festival?",
      "options": [
        "A. 2011",
        "B. 2013",
        "C. 2014",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The baby's due date was April 10 and the child was born on April 12.",
      "conflict_prompt": "The baby's due date was April 10 and the child was born on March 30 of the same year.",
      "question": "On what date was the child born?",
      "options": [
        "A. March 30",
        "B. April 10",
        "C. April 12",
        "D. May 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup secured seed funding in 2013 and a Series A round in 2015.",
      "conflict_prompt": "The startup secured seed funding in 2013 and a Series A round in 2010.",
      "question": "When did the startup secure its Series A round?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer was born in 1840 and wrote his last symphony in 1902.",
      "conflict_prompt": "The composer was born in 1840 and wrote his last symphony in 1838.",
      "question": "In what year did he write his last symphony?",
      "options": [
        "A. 1838",
        "B. 1840",
        "C. 1902",
        "D. 1895"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The gardener sowed seeds in April and the flowers bloomed in June.",
      "conflict_prompt": "The gardener sowed seeds in April and the flowers bloomed in February.",
      "question": "When did the flowers bloom?",
      "options": [
        "A. February",
        "B. April",
        "C. June",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup team formed in 2016 and their app launched in 2019.",
      "conflict_prompt": "The startup team formed in 2016 and their app launched in 2012.",
      "question": "When did their app launch?",
      "options": [
        "A. 2012",
        "B. 2016",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cathedral's restoration began in 2001 and finished in 2004.",
      "conflict_prompt": "The cathedral's restoration began in 2001 and finished in 1997.",
      "question": "When did the restoration finish?",
      "options": [
        "A. 1997",
        "B. 2001",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The chef developed the recipe in 2010 and published it in his cookbook in 2013.",
      "conflict_prompt": "The chef developed the recipe in 2010 and published it in his cookbook in 2008.",
      "question": "When was the recipe published in his cookbook?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot took off at 09:00 and landed at 12:30.",
      "conflict_prompt": "The pilot took off at 09:00 and landed at 08:45.",
      "question": "At what time did the plane land?",
      "options": [
        "A. 08:45",
        "B. 09:00",
        "C. 12:30",
        "D. 13:00"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient received the diagnosis in April and began treatment in May.",
      "conflict_prompt": "The patient received the diagnosis in April and began treatment in February.",
      "question": "When did treatment begin?",
      "options": [
        "A. February",
        "B. April",
        "C. May",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibition opened on September 10 and traveled to a second museum on October 5.",
      "conflict_prompt": "The exhibition opened on September 10 and traveled to a second museum on August 1 of the same year.",
      "question": "When did the exhibition travel to the second museum?",
      "options": [
        "A. August 1",
        "B. September 10",
        "C. October 5",
        "D. November 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The policy was announced on January 1 and took effect on March 1.",
      "conflict_prompt": "The policy was announced on January 1 and took effect on December 1 of the previous year.",
      "question": "When did the policy take effect?",
      "options": [
        "A. December 1",
        "B. January 1",
        "C. March 1",
        "D. April 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The construction crew cleared the site in 2015 and poured the foundation in 2016.",
      "conflict_prompt": "The construction crew cleared the site in 2015 and poured the foundation in 2012.",
      "question": "When was the foundation poured?",
      "options": [
        "A. 2012",
        "B. 2015",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historian completed her dissertation in 1997 and published it as a book in 2001.",
      "conflict_prompt": "The historian completed her dissertation in 1997 and published it as a book in 1990.",
      "question": "When was the dissertation published as a book?",
      "options": [
        "A. 1990",
        "B. 1997",
        "C. 2001",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The band recorded the album in 2011 and released it in early 2012.",
      "conflict_prompt": "The band recorded the album in 2011 and released it in 2009.",
      "question": "When was the album released?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The project proposal was submitted on June 1 and approvals were granted on August 20.",
      "conflict_prompt": "The project proposal was submitted on June 1 and approvals were granted on May 10.",
      "question": "On what date were approvals granted?",
      "options": [
        "A. May 10",
        "B. June 1",
        "C. August 20",
        "D. September 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeologists discovered the site in 1994 and published their findings in 1999.",
      "conflict_prompt": "The archaeologists discovered the site in 1994 and published their findings in 1988.",
      "question": "When were the findings published?",
      "options": [
        "A. 1988",
        "B. 1994",
        "C. 1999",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He took office as mayor in January 2010 and left office in January 2018.",
      "conflict_prompt": "He took office as mayor in January 2010 and left office in January 2005.",
      "question": "When did he leave office as mayor?",
      "options": [
        "A. January 2005",
        "B. January 2010",
        "C. January 2018",
        "D. January 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The solar farm began feeding power into the grid in June and reached full capacity by September.",
      "conflict_prompt": "The solar farm began feeding power into the grid in June and reached full capacity the previous February.",
      "question": "In which month did it reach full capacity?",
      "options": [
        "A. February",
        "B. June",
        "C. September",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author started research in 2006 and completed the book in 2010.",
      "conflict_prompt": "The author started research in 2006 and completed the book in 2002.",
      "question": "When did the author complete the book?",
      "options": [
        "A. 2002",
        "B. 2006",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra conductor took the post in 2003 and conducted his farewell concert in 2013.",
      "conflict_prompt": "The orchestra conductor took the post in 2003 and conducted his farewell concert in 1998.",
      "question": "When was his farewell concert?",
      "options": [
        "A. 1998",
        "B. 2003",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery received its hygiene certification in April and opened to the public in May.",
      "conflict_prompt": "The bakery received its hygiene certification in April and opened to the public in March.",
      "question": "When did it open to the public?",
      "options": [
        "A. March",
        "B. April",
        "C. May",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The minister announced the reform in 2007 and the law took effect in 2008.",
      "conflict_prompt": "The minister announced the reform in 2007 and the law took effect in 2005.",
      "question": "When did the law take effect?",
      "options": [
        "A. 2005",
        "B. 2007",
        "C. 2008",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The award ceremony took place after the shortlist was announced on Tuesday and winners were declared on Saturday.",
      "conflict_prompt": "The award ceremony took place after the shortlist was announced on Tuesday and winners were declared the previous Monday.",
      "question": "On which day were the winners declared?",
      "options": [
        "A. The previous Monday",
        "B. Tuesday",
        "C. Saturday",
        "D. Sunday"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film premiered at the festival in 2017 and had a wide release in 2018.",
      "conflict_prompt": "The film premiered at the festival in 2017 and had a wide release in 2015.",
      "question": "When did the film have a wide release?",
      "options": [
        "A. 2015",
        "B. 2017",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The violinist gave her first solo concert in 2002 and released her debut album in 2004.",
      "conflict_prompt": "The violinist gave her first solo concert in 2002 and released her debut album in 1999.",
      "question": "When was her debut album released?",
      "options": [
        "A. 1999",
        "B. 2002",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research team collected samples in July and published their analysis in December.",
      "conflict_prompt": "The research team collected samples in July and published their analysis in April of the same year.",
      "question": "When was their analysis published?",
      "options": [
        "A. April",
        "B. July",
        "C. December",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company introduced the new model in 2019 and discontinued the old model in 2020.",
      "conflict_prompt": "The company introduced the new model in 2019 and discontinued the old model in 2015.",
      "question": "When was the old model discontinued?",
      "options": [
        "A. 2015",
        "B. 2019",
        "C. 2020",
        "D. 2021"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation permit was issued on August 15 and work commenced on September 1.",
      "conflict_prompt": "The renovation permit was issued on August 15 and work commenced on July 1 of the same year.",
      "question": "When did work commence?",
      "options": [
        "A. July 1",
        "B. August 15",
        "C. September 1",
        "D. October 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The journal accepted the article in June and published it in August.",
      "conflict_prompt": "The journal accepted the article in June and published it in April.",
      "question": "When was the article published?",
      "options": [
        "A. April",
        "B. June",
        "C. August",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival's headline act performed on Friday, and the event concluded on Sunday.",
      "conflict_prompt": "The festival's headline act performed on Friday, and the event concluded the previous Monday.",
      "question": "On which day did the event conclude?",
      "options": [
        "A. The previous Monday",
        "B. Friday",
        "C. Sunday",
        "D. Saturday"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright signed the publishing contract in 1995 and the script was published in 1997.",
      "conflict_prompt": "The playwright signed the publishing contract in 1995 and the script was published in 1990.",
      "question": "When was the script published?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 1997",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city celebrated its bicentennial in 2010 after being founded in 1810.",
      "conflict_prompt": "The city celebrated its bicentennial in 2010 after being founded in 1825.",
      "question": "When was the city founded according to the statement?",
      "options": [
        "A. 1810",
        "B. 1825",
        "C. 2010",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The tree was planted in 1995 and measured 20 feet tall by 2010.",
      "conflict_prompt": "The tree was planted in 1995 and measured 20 feet tall by 1990.",
      "question": "By what year had the tree measured 20 feet tall?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 2010",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company moved into the new headquarters in March and employees began working there in April.",
      "conflict_prompt": "The company moved into the new headquarters in March and employees began working there in January.",
      "question": "When did employees begin working in the new headquarters?",
      "options": [
        "A. January",
        "B. March",
        "C. April",
        "D. May"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The team hired a coach in 2001 and won the league title in 2004.",
      "conflict_prompt": "The team hired a coach in 2001 and won the league title in 1998.",
      "question": "When did they win the league title?",
      "options": [
        "A. 1998",
        "B. 2001",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The graduate completed her PhD in 2013 and accepted a faculty job in 2015.",
      "conflict_prompt": "The graduate completed her PhD in 2013 and accepted a faculty job in 2010.",
      "question": "When did she accept the faculty job?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new transit line opened on September 1 and ridership increased through November.",
      "conflict_prompt": "The new transit line opened on September 1 and ridership increased the previous May.",
      "question": "During which month did ridership increase through?",
      "options": [
        "A. The previous May",
        "B. September",
        "C. November",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historical society organized the symposium in 2005 and published proceedings in 2007.",
      "conflict_prompt": "The historical society organized the symposium in 2005 and published proceedings in 2002.",
      "question": "When were the proceedings published?",
      "options": [
        "A. 2002",
        "B. 2005",
        "C. 2007",
        "D. 2009"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The scientist began experiments in January and reported results in June.",
      "conflict_prompt": "The scientist began experiments in January and reported results the previous September.",
      "question": "When were the results reported?",
      "options": [
        "A. The previous September",
        "B. January",
        "C. June",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival's main stage was erected on Thursday and dismantled after the event on Sunday.",
      "conflict_prompt": "The festival's main stage was erected on Thursday and dismantled before the event on Wednesday.",
      "question": "When was the stage dismantled?",
      "options": [
        "A. Wednesday",
        "B. Thursday",
        "C. Sunday",
        "D. Monday"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lab received the grant in 2010 and published preliminary data in 2012.",
      "conflict_prompt": "The lab received the grant in 2010 and published preliminary data in 2008.",
      "question": "When was the preliminary data published?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2012",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient had surgery on June 15 and began rehabilitation on June 20.",
      "conflict_prompt": "The patient had surgery on June 15 and began rehabilitation on June 10.",
      "question": "When did rehabilitation begin?",
      "options": [
        "A. June 10",
        "B. June 15",
        "C. June 20",
        "D. June 25"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibition catalogue was printed in May and the show opened in June.",
      "conflict_prompt": "The exhibition catalogue was printed in May and the show opened in March.",
      "question": "When did the show open?",
      "options": [
        "A. March",
        "B. May",
        "C. June",
        "D. July"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer wrote the concerto in 1920 and it was performed publicly in 1922.",
      "conflict_prompt": "The composer wrote the concerto in 1920 and it was performed publicly in 1918.",
      "question": "When was the concerto performed publicly?",
      "options": [
        "A. 1918",
        "B. 1920",
        "C. 1922",
        "D. 1925"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The plant was propagated in spring and produced cuttings by late summer.",
      "conflict_prompt": "The plant was propagated in spring and produced cuttings in early spring before propagation.",
      "question": "When did the plant produce cuttings?",
      "options": [
        "A. Early spring",
        "B. Spring",
        "C. Late summer",
        "D. Winter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The art school opened its new studio in 2014 and hosted workshops starting in 2015.",
      "conflict_prompt": "The art school opened its new studio in 2014 and hosted workshops starting in 2012.",
      "question": "When did workshops start in the new studio?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historical archive was digitized beginning in 2008 and completed in 2011.",
      "conflict_prompt": "The historical archive was digitized beginning in 2008 and completed in 2005.",
      "question": "When was digitization completed?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2011",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra performed the new score in December and recorded it the following February.",
      "conflict_prompt": "The orchestra performed the new score in December and recorded it the previous October.",
      "question": "When did they record the score?",
      "options": [
        "A. The previous October",
        "B. December",
        "C. The following February",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The roadworks commenced in 2012 and were completed by early 2014.",
      "conflict_prompt": "The roadworks commenced in 2012 and were completed by late 2010.",
      "question": "By when were the roadworks completed?",
      "options": [
        "A. Late 2010",
        "B. 2012",
        "C. Early 2014",
        "D. Mid 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship set sail in May and reached the island in July.",
      "conflict_prompt": "The ship set sail in May and reached the island in March of the same year.",
      "question": "When did the ship reach the island?",
      "options": [
        "A. March",
        "B. May",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The language course began in September and participants completed it in December.",
      "conflict_prompt": "The language course began in September and participants completed it the previous June.",
      "question": "When did participants complete the course?",
      "options": [
        "A. The previous June",
        "B. September",
        "C. December",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lighthouse was built in 1850 and automated in 1960.",
      "conflict_prompt": "The lighthouse was built in 1850 and automated in 1825.",
      "question": "When was the lighthouse automated?",
      "options": [
        "A. 1825",
        "B. 1850",
        "C. 1960",
        "D. 1900"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir rehearsed for the holiday concert starting in October and performed in December.",
      "conflict_prompt": "The choir rehearsed for the holiday concert starting in October and performed the previous June.",
      "question": "When did the choir perform the holiday concert?",
      "options": [
        "A. The previous June",
        "B. October",
        "C. December",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist studied in Paris from 1901 to 1904 and returned home in 1905.",
      "conflict_prompt": "The artist studied in Paris from 1901 to 1904 and returned home in 1899.",
      "question": "When did the artist return home?",
      "options": [
        "A. 1899",
        "B. 1901",
        "C. 1904",
        "D. 1905"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater production rehearsed in August and opened its run in September.",
      "conflict_prompt": "The theater production rehearsed in August and opened its run in June.",
      "question": "When did the production open its run?",
      "options": [
        "A. June",
        "B. August",
        "C. September",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The foundation laid groundwork in 1999 and launched the fellowship program in 2002.",
      "conflict_prompt": "The foundation laid groundwork in 1999 and launched the fellowship program in 1995.",
      "question": "When did the foundation launch the fellowship program?",
      "options": [
        "A. 1995",
        "B. 1999",
        "C. 2002",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The gardener pruned the roses in early March and they bloomed by mid-April.",
      "conflict_prompt": "The gardener pruned the roses in early March and they bloomed in January.",
      "question": "By when did the roses bloom?",
      "options": [
        "A. January",
        "B. Early March",
        "C. Mid-April",
        "D. May"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference call was scheduled for 2:00 PM and wrapped up by 3:30 PM.",
      "conflict_prompt": "The conference call was scheduled for 2:00 PM and wrapped up at 1:45 PM.",
      "question": "By what time did the call wrap up?",
      "options": [
        "A. 1:45 PM",
        "B. 2:00 PM",
        "C. 3:30 PM",
        "D. 4:00 PM"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historian traced the dynasty from 1200 and wrote the monograph in 1250.",
      "conflict_prompt": "The historian traced the dynasty from 1200 and wrote the monograph in 1150.",
      "question": "When was the monograph written?",
      "options": [
        "A. 1150",
        "B. 1200",
        "C. 1250",
        "D. 1300"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist opened the gallery in 2000 and organized a retrospective in 2010.",
      "conflict_prompt": "The artist opened the gallery in 2000 and organized a retrospective in 1995.",
      "question": "When was the retrospective organized?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The team completed the prototype in April and entered beta testing in June.",
      "conflict_prompt": "The team completed the prototype in April and entered beta testing in March.",
      "question": "When did beta testing begin?",
      "options": [
        "A. March",
        "B. April",
        "C. June",
        "D. July"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The librarian cataloged the donations in January and made them available in February.",
      "conflict_prompt": "The librarian cataloged the donations in January and made them available in December of the previous year.",
      "question": "When did the donations become available?",
      "options": [
        "A. December",
        "B. January",
        "C. February",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The farmer planted corn in late April and harvested it in late September.",
      "conflict_prompt": "The farmer planted corn in late April and harvested it in early April of the same year.",
      "question": "When did he harvest the corn?",
      "options": [
        "A. Early April",
        "B. Late April",
        "C. Late September",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The director shot scenes in winter and premiered the film the following spring.",
      "conflict_prompt": "The director shot scenes in winter and premiered the film the previous summer.",
      "question": "When did the film premiere?",
      "options": [
        "A. The previous summer",
        "B. Winter",
        "C. The following spring",
        "D. Autumn"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity's donation drive started in November and concluded in January.",
      "conflict_prompt": "The charity's donation drive started in November and concluded in September.",
      "question": "When did the donation drive conclude?",
      "options": [
        "A. September",
        "B. November",
        "C. January",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The biologist discovered the species in 2007 and published the taxonomy in 2010.",
      "conflict_prompt": "The biologist discovered the species in 2007 and published the taxonomy in 2004.",
      "question": "When was the taxonomy published?",
      "options": [
        "A. 2004",
        "B. 2007",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot logged his first solo flight at age 16 and earned a commercial certificate at age 22.",
      "conflict_prompt": "The pilot logged his first solo flight at age 16 and earned a commercial certificate at age 12.",
      "question": "At what age did he earn his commercial certificate?",
      "options": [
        "A. 12",
        "B. 16",
        "C. 22",
        "D. 24"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum announced the acquisition in May and installed the piece in July.",
      "conflict_prompt": "The museum announced the acquisition in May and installed the piece in March.",
      "question": "When was the piece installed?",
      "options": [
        "A. March",
        "B. May",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company launched its loyalty program in 2011 and reached 1 million members in 2016.",
      "conflict_prompt": "The company launched its loyalty program in 2011 and reached 1 million members in 2008.",
      "question": "When did the program reach 1 million members?",
      "options": [
        "A. 2008",
        "B. 2011",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup was founded in 2012 and acquired by a larger company in 2019.",
      "conflict_prompt": "The startup was founded in 2012 and acquired by a larger company in 2009.",
      "question": "In what year was the startup acquired?",
      "options": [
        "A. 2009",
        "B. 2015",
        "C. 2019",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He started his residency in 2016 and completed it in 2020.",
      "conflict_prompt": "He started his residency in 2016 and completed it in 2014.",
      "question": "In what year did he complete his residency?",
      "options": [
        "A. 2014",
        "B. 2016",
        "C. 2018",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novel was written in 2007 and published in 2010.",
      "conflict_prompt": "The novel was written in 2007 and published in 2005.",
      "question": "In what year was the novel published?",
      "options": [
        "A. 2005",
        "B. 2007",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She enrolled in piano lessons in January 2014 and performed publicly in December 2017.",
      "conflict_prompt": "She enrolled in piano lessons in January 2014 and performed publicly in December 2012.",
      "question": "When did she perform publicly?",
      "options": [
        "A. December 2012",
        "B. January 2014",
        "C. December 2017",
        "D. June 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The highway was completed in 1998 and opened to traffic in March 1999.",
      "conflict_prompt": "The highway was completed in 1998 and opened to traffic in March 1997.",
      "question": "When did the highway open to traffic?",
      "options": [
        "A. March 1997",
        "B. March 1998",
        "C. March 1999",
        "D. March 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum exhibit was installed in June 2015 and unveiled to the public in August 2015.",
      "conflict_prompt": "The museum exhibit was installed in June 2015 and unveiled to the public in April 2015.",
      "question": "When was the exhibit unveiled to the public?",
      "options": [
        "A. April 2015",
        "B. June 2015",
        "C. August 2015",
        "D. October 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer finished the symphony in 1824 and premiered it in 1825.",
      "conflict_prompt": "The composer finished the symphony in 1824 and premiered it in 1819.",
      "question": "In what year was the symphony premiered?",
      "options": [
        "A. 1819",
        "B. 1820",
        "C. 1824",
        "D. 1825"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lake house was built in 1975 and sold to the family in 1980.",
      "conflict_prompt": "The lake house was built in 1975 and sold to the family in 1968.",
      "question": "In what year did the family buy the lake house?",
      "options": [
        "A. 1968",
        "B. 1975",
        "C. 1980",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient was admitted to the hospital on April 3 and discharged on April 10.",
      "conflict_prompt": "The patient was admitted to the hospital on April 3 and discharged on April 1.",
      "question": "On what date was the patient discharged?",
      "options": [
        "A. April 1",
        "B. April 3",
        "C. April 7",
        "D. April 10"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference registration opened on January 5 and early-bird pricing ended on February 20.",
      "conflict_prompt": "The conference registration opened on January 5 and early-bird pricing ended on January 1.",
      "question": "When did early-bird pricing end?",
      "options": [
        "A. January 1",
        "B. January 5",
        "C. February 20",
        "D. March 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He was elected mayor in November 2018 and sworn into office in January 2019.",
      "conflict_prompt": "He was elected mayor in November 2018 and sworn into office in September 2018.",
      "question": "When was he sworn into office?",
      "options": [
        "A. September 2018",
        "B. November 2018",
        "C. December 2018",
        "D. January 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The painting was completed in 1910 and sold at auction in 1920.",
      "conflict_prompt": "The painting was completed in 1910 and sold at auction in 1905.",
      "question": "When was the painting sold at auction?",
      "options": [
        "A. 1905",
        "B. 1910",
        "C. 1915",
        "D. 1920"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The construction company began building the bridge in 2001 and finished in 2004.",
      "conflict_prompt": "The construction company began building the bridge in 2001 and finished in 1998.",
      "question": "In what year was the bridge finished?",
      "options": [
        "A. 1998",
        "B. 2000",
        "C. 2001",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete joined the professional team in 2013 and retired in 2021.",
      "conflict_prompt": "The athlete joined the professional team in 2013 and retired in 2010.",
      "question": "When did the athlete retire?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2018",
        "D. 2021"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software version 2.0 was released on May 12, 2017, after beta testing in March 2017.",
      "conflict_prompt": "The software version 2.0 was released on May 12, 2017, after beta testing in June 2018.",
      "question": "When was version 2.0 officially released?",
      "options": [
        "A. March 2017",
        "B. May 12, 2017",
        "C. June 2018",
        "D. January 2017"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author moved to Paris in 1992 and began writing the novel there in 1993.",
      "conflict_prompt": "The author moved to Paris in 1992 and began writing the novel there in 1991.",
      "question": "When did the author begin writing the novel in Paris?",
      "options": [
        "A. 1991",
        "B. 1992",
        "C. 1993",
        "D. 1994"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary was filmed in 2018 and broadcast on television in 2019.",
      "conflict_prompt": "The documentary was filmed in 2018 and broadcast on television in 2016.",
      "question": "When was the documentary broadcast on television?",
      "options": [
        "A. 2016",
        "B. 2017",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research study began recruiting participants in July 2020 and concluded data collection in January 2021.",
      "conflict_prompt": "The research study began recruiting participants in July 2020 and concluded data collection in June 2019.",
      "question": "When did data collection conclude?",
      "options": [
        "A. June 2019",
        "B. July 2020",
        "C. December 2020",
        "D. January 2021"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater production rehearsed in September and opened on October 15, 2016.",
      "conflict_prompt": "The theater production rehearsed in September and opened on August 15, 2016.",
      "question": "On what date did the production open?",
      "options": [
        "A. August 15, 2016",
        "B. September 15, 2016",
        "C. October 1, 2016",
        "D. October 15, 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company hired its first employee in 2003 and registered as an LLC in 2004.",
      "conflict_prompt": "The company hired its first employee in 2003 and registered as an LLC in 2001.",
      "question": "When did the company register as an LLC?",
      "options": [
        "A. 2001",
        "B. 2003",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She began teaching at the school in September 2010 and received tenure in 2018.",
      "conflict_prompt": "She began teaching at the school in September 2010 and received tenure in 2006.",
      "question": "When did she receive tenure?",
      "options": [
        "A. 2006",
        "B. 2010",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The aircraft made its maiden flight on June 2, 2005 and entered commercial service in 2007.",
      "conflict_prompt": "The aircraft made its maiden flight on June 2, 2005 and entered commercial service in 2003.",
      "question": "When did the aircraft enter commercial service?",
      "options": [
        "A. 2003",
        "B. 2005",
        "C. 2006",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city declared a snow emergency on January 12 and lifted it on January 14.",
      "conflict_prompt": "The city declared a snow emergency on January 12 and lifted it on January 10.",
      "question": "On which date was the snow emergency lifted?",
      "options": [
        "A. January 10",
        "B. January 12",
        "C. January 13",
        "D. January 14"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete set a new record in 2009 and that record stood until 2016.",
      "conflict_prompt": "The athlete set a new record in 2009 and that record stood until 2005.",
      "question": "Until what year did the record stand?",
      "options": [
        "A. 2005",
        "B. 2009",
        "C. 2012",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Their wedding ceremony took place on June 21, 2014 and the reception was held the same evening.",
      "conflict_prompt": "Their wedding ceremony took place on June 21, 2014 and the reception was held the previous evening.",
      "question": "When was the reception held relative to the ceremony?",
      "options": [
        "A. The previous evening",
        "B. The same evening",
        "C. The following week",
        "D. The next morning"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite was launched in 2012 and completed its mission in 2018.",
      "conflict_prompt": "The satellite was launched in 2012 and completed its mission in 2009.",
      "question": "When did the satellite complete its mission?",
      "options": [
        "A. 2009",
        "B. 2012",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer wrote the opera in 1899 and it premiered in 1902.",
      "conflict_prompt": "The composer wrote the opera in 1899 and it premiered in 1890.",
      "question": "In what year did the opera premiere?",
      "options": [
        "A. 1890",
        "B. 1899",
        "C. 1902",
        "D. 1910"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The building permit was issued on March 1 and construction began on March 15.",
      "conflict_prompt": "The building permit was issued on March 1 and construction began on February 15.",
      "question": "When did construction begin?",
      "options": [
        "A. February 15",
        "B. March 1",
        "C. March 15",
        "D. April 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film adaptation was announced in 2010 and released in theaters in 2014.",
      "conflict_prompt": "The film adaptation was announced in 2010 and released in theaters in 2008.",
      "question": "When was the film released in theaters?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2012",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The gymnast qualified for the finals on July 23 and won a medal on July 27.",
      "conflict_prompt": "The gymnast qualified for the finals on July 23 and won a medal on July 20.",
      "question": "On what date did the gymnast win a medal?",
      "options": [
        "A. July 20",
        "B. July 23",
        "C. July 25",
        "D. July 27"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The parish was established in 1850 and its current church building completed in 1870.",
      "conflict_prompt": "The parish was established in 1850 and its current church building completed in 1835.",
      "question": "When was the current church building completed?",
      "options": [
        "A. 1835",
        "B. 1850",
        "C. 1860",
        "D. 1870"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company launched product A in 2011 and product B in 2014.",
      "conflict_prompt": "The company launched product A in 2011 and product B in 2009.",
      "question": "When did the company launch product B?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2013",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient started the new medication on February 1 and reported improvement by February 15.",
      "conflict_prompt": "The patient started the new medication on February 1 and reported improvement by January 15.",
      "question": "By what date did the patient report improvement?",
      "options": [
        "A. January 15",
        "B. February 1",
        "C. February 10",
        "D. February 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir rehearsed weekly beginning in September and performed at the festival in November.",
      "conflict_prompt": "The choir rehearsed weekly beginning in September and performed at the festival in August.",
      "question": "When did the choir perform at the festival?",
      "options": [
        "A. August",
        "B. September",
        "C. October",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic house was designated a landmark in 1978 and restored between 1980 and 1982.",
      "conflict_prompt": "The historic house was designated a landmark in 1978 and restored between 1965 and 1967.",
      "question": "When did the restoration occur according to the statement?",
      "options": [
        "A. 1965–1967",
        "B. 1978–1979",
        "C. 1980–1982",
        "D. 1990–1992"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra commissioned the piece in 2003 and premiered it in 2006.",
      "conflict_prompt": "The orchestra commissioned the piece in 2003 and premiered it in 2000.",
      "question": "In what year was the piece premiered?",
      "options": [
        "A. 2000",
        "B. 2003",
        "C. 2005",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge inspection occurred on August 10 and repairs were completed on August 25.",
      "conflict_prompt": "The bridge inspection occurred on August 10 and repairs were completed on August 5.",
      "question": "When were the repairs completed?",
      "options": [
        "A. August 5",
        "B. August 10",
        "C. August 20",
        "D. August 25"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The university accepted her application in March and she enrolled in September of the same year.",
      "conflict_prompt": "The university accepted her application in March and she enrolled the previous September.",
      "question": "When did she enroll?",
      "options": [
        "A. The previous September",
        "B. The same March",
        "C. The same September",
        "D. The following January"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary aired its first episode on May 4 and concluded the season on June 15.",
      "conflict_prompt": "The documentary aired its first episode on May 4 and concluded the season on April 15.",
      "question": "When did the season conclude?",
      "options": [
        "A. April 15",
        "B. May 4",
        "C. May 30",
        "D. June 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ice rink opened for the season in November and closed in March.",
      "conflict_prompt": "The ice rink opened for the season in November and closed the previous September.",
      "question": "When did the rink close according to the statement?",
      "options": [
        "A. The previous September",
        "B. November",
        "C. January",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The series pilot was filmed in 2015 and the season premiered in 2016.",
      "conflict_prompt": "The series pilot was filmed in 2015 and the season premiered in 2014.",
      "question": "When did the season premiere?",
      "options": [
        "A. 2014",
        "B. 2015",
        "C. 2016",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They planted the orchard in spring 2008 and harvested their first crop in autumn 2012.",
      "conflict_prompt": "They planted the orchard in spring 2008 and harvested their first crop in autumn 2006.",
      "question": "When did they harvest their first crop?",
      "options": [
        "A. Autumn 2006",
        "B. Spring 2008",
        "C. Autumn 2010",
        "D. Autumn 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete signed the contract on July 1 and reported to training camp on July 15.",
      "conflict_prompt": "The athlete signed the contract on July 1 and reported to training camp on June 15.",
      "question": "When did the athlete report to training camp?",
      "options": [
        "A. June 15",
        "B. July 1",
        "C. July 15",
        "D. August 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historian published a major paper in 1997 and presented it at a conference in 1998.",
      "conflict_prompt": "The historian published a major paper in 1997 and presented it at a conference in 1992.",
      "question": "When did the historian present the paper at a conference?",
      "options": [
        "A. 1992",
        "B. 1997",
        "C. 1998",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The singer released an EP in March and started a tour in May of the same year.",
      "conflict_prompt": "The singer released an EP in March and started a tour in January of the same year.",
      "question": "When did the singer start the tour?",
      "options": [
        "A. January",
        "B. March",
        "C. April",
        "D. May"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research grant was awarded in 2011 and the project ran from 2012 to 2015.",
      "conflict_prompt": "The research grant was awarded in 2011 and the project ran from 2008 to 2010.",
      "question": "When did the project run according to the statement?",
      "options": [
        "A. 2008–2010",
        "B. 2010–2011",
        "C. 2012–2015",
        "D. 2015–2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship launched in 1890 and was decommissioned in 1920.",
      "conflict_prompt": "The ship launched in 1890 and was decommissioned in 1880.",
      "question": "When was the ship decommissioned?",
      "options": [
        "A. 1880",
        "B. 1890",
        "C. 1905",
        "D. 1920"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery opened its doors in 2000 and expanded to a second location in 2008.",
      "conflict_prompt": "The bakery opened its doors in 2000 and expanded to a second location in 1995.",
      "question": "When did the bakery open its second location?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2005",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The actor filmed the sequel in 2011 and the movie premiered in 2013.",
      "conflict_prompt": "The actor filmed the sequel in 2011 and the movie premiered in 2009.",
      "question": "When did the movie premiere?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation started in June 2019 and was completed in December 2019.",
      "conflict_prompt": "The renovation started in June 2019 and was completed in March 2018.",
      "question": "When was the renovation completed?",
      "options": [
        "A. March 2018",
        "B. June 2019",
        "C. October 2019",
        "D. December 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The graduate program accepted applications until February 28 and announced decisions on April 15.",
      "conflict_prompt": "The graduate program accepted applications until February 28 and announced decisions on January 15.",
      "question": "When were admissions decisions announced?",
      "options": [
        "A. January 15",
        "B. February 28",
        "C. March 15",
        "D. April 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival reported attendance figures for 2016 and showed growth through 2019.",
      "conflict_prompt": "The festival reported attendance figures for 2016 and showed growth through 2010.",
      "question": "Through which year did the festival show growth?",
      "options": [
        "A. 2010",
        "B. 2016",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lighthouse was automated in 1972 and left unmanned since 1973.",
      "conflict_prompt": "The lighthouse was automated in 1972 and left unmanned since 1965.",
      "question": "Since what year was the lighthouse left unmanned according to the statement?",
      "options": [
        "A. 1965",
        "B. 1972",
        "C. 1973",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity was incorporated in 1999 and launched its flagship program in 2001.",
      "conflict_prompt": "The charity was incorporated in 1999 and launched its flagship program in 1995.",
      "question": "When did the charity launch its flagship program?",
      "options": [
        "A. 1995",
        "B. 1999",
        "C. 2000",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra toured Europe in 2004 and returned home in November 2004.",
      "conflict_prompt": "The orchestra toured Europe in 2004 and returned home in January 2003.",
      "question": "When did the orchestra return home?",
      "options": [
        "A. January 2003",
        "B. June 2004",
        "C. November 2004",
        "D. December 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new metro line began testing in 2010 and opened for passengers in 2013.",
      "conflict_prompt": "The new metro line began testing in 2010 and opened for passengers in 2008.",
      "question": "When did the metro line open to passengers?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The photographer took the series in summer 1995 and exhibited it in 1996.",
      "conflict_prompt": "The photographer took the series in summer 1995 and exhibited it in 1992.",
      "question": "When was the series exhibited?",
      "options": [
        "A. 1992",
        "B. 1995",
        "C. 1996",
        "D. 1998"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity campaign launched in January and reached its goal by March of that year.",
      "conflict_prompt": "The charity campaign launched in January and reached its goal the previous November.",
      "question": "By when did the campaign reach its goal?",
      "options": [
        "A. The previous November",
        "B. January",
        "C. February",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The politician announced the bill on June 1 and it was voted on June 20.",
      "conflict_prompt": "The politician announced the bill on June 1 and it was voted on May 15.",
      "question": "When was the bill voted on according to the statement?",
      "options": [
        "A. May 15",
        "B. June 1",
        "C. June 20",
        "D. July 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software's beta period started in October and ended in December 2019.",
      "conflict_prompt": "The software's beta period started in October and ended in September 2019.",
      "question": "When did the beta period end?",
      "options": [
        "A. September 2019",
        "B. October 2019",
        "C. November 2019",
        "D. December 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school year began on September 4 and dismissed for summer on June 18.",
      "conflict_prompt": "The school year began on September 4 and dismissed for summer on August 18.",
      "question": "On what date did school dismiss for summer?",
      "options": [
        "A. August 18",
        "B. September 4",
        "C. June 18",
        "D. May 30"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient underwent surgery on Monday and was discharged the following Saturday.",
      "conflict_prompt": "The patient underwent surgery on Monday and was discharged the previous Saturday.",
      "question": "When was the patient discharged relative to the surgery?",
      "options": [
        "A. The previous Saturday",
        "B. The same Monday",
        "C. The following Saturday",
        "D. The following Monday"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The monument was commissioned in 1888 and unveiled in 1892.",
      "conflict_prompt": "The monument was commissioned in 1888 and unveiled in 1875.",
      "question": "In what year was the monument unveiled?",
      "options": [
        "A. 1875",
        "B. 1888",
        "C. 1890",
        "D. 1892"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city completed the sewer upgrade in 2006 and tested water quality in 2007.",
      "conflict_prompt": "The city completed the sewer upgrade in 2006 and tested water quality in 2004.",
      "question": "When was water quality tested according to the statement?",
      "options": [
        "A. 2004",
        "B. 2006",
        "C. 2007",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play's rewrite was finished in April and the new run started in May.",
      "conflict_prompt": "The play's rewrite was finished in April and the new run started in March.",
      "question": "When did the new run start?",
      "options": [
        "A. March",
        "B. April",
        "C. May",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research lab moved into the new facility in January 2017 and began experiments in February.",
      "conflict_prompt": "The research lab moved into the new facility in January 2017 and began experiments in December 2016.",
      "question": "When did experiments begin according to the statement?",
      "options": [
        "A. December 2016",
        "B. January 2017",
        "C. February 2017",
        "D. March 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author accepted the award in 2001 and donated the prize money in 2002.",
      "conflict_prompt": "The author accepted the award in 2001 and donated the prize money in 1999.",
      "question": "When did the author donate the prize money?",
      "options": [
        "A. 1999",
        "B. 2001",
        "C. 2002",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conservation plan was approved in 2010 and implemented starting in 2011.",
      "conflict_prompt": "The conservation plan was approved in 2010 and implemented starting in 2008.",
      "question": "When did implementation start according to the statement?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival announced its lineup on May 1 and ticket sales began on May 10.",
      "conflict_prompt": "The festival announced its lineup on May 1 and ticket sales began on April 10.",
      "question": "When did ticket sales begin?",
      "options": [
        "A. April 10",
        "B. May 1",
        "C. May 10",
        "D. June 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery stopped offering the seasonal pastry on December 31 and reintroduced it the next November.",
      "conflict_prompt": "The bakery stopped offering the seasonal pastry on December 31 and reintroduced it the previous November.",
      "question": "When was the pastry reintroduced?",
      "options": [
        "A. The previous November",
        "B. December 31",
        "C. The next January",
        "D. The next November"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The earthquake struck at 3:10 AM and aftershocks continued into the afternoon of the same day.",
      "conflict_prompt": "The earthquake struck at 3:10 AM and aftershocks continued into the evening of the previous day.",
      "question": "When did aftershocks continue according to the statement?",
      "options": [
        "A. The previous evening",
        "B. The morning of the same day",
        "C. The afternoon of the same day",
        "D. The following week"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school celebrated its centennial in 2005 and opened a commemorative exhibit that year.",
      "conflict_prompt": "The school celebrated its centennial in 2005 and opened a commemorative exhibit in 1995.",
      "question": "When was the commemorative exhibit opened according to the statement?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2005",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity gala was scheduled for November 12 and was moved to November 19 due to weather.",
      "conflict_prompt": "The charity gala was scheduled for November 12 and was moved to November 5 due to weather.",
      "question": "To what date was the gala moved?",
      "options": [
        "A. November 5",
        "B. November 12",
        "C. November 19",
        "D. December 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mountain hut was completed in 1933 and first used by climbers that winter.",
      "conflict_prompt": "The mountain hut was completed in 1933 and first used by climbers in 1925.",
      "question": "When was the hut first used by climbers according to the statement?",
      "options": [
        "A. 1925",
        "B. 1930",
        "C. 1933",
        "D. 1940"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The public library opened a children’s wing in 1991 and expanded it in 2005.",
      "conflict_prompt": "The public library opened a children’s wing in 1991 and expanded it in 1985.",
      "question": "When was the children’s wing expanded?",
      "options": [
        "A. 1985",
        "B. 1991",
        "C. 2000",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The train route was inaugurated on June 10 and the daily schedule began on June 15.",
      "conflict_prompt": "The train route was inaugurated on June 10 and the daily schedule began on June 1.",
      "question": "When did the daily schedule begin?",
      "options": [
        "A. June 1",
        "B. June 10",
        "C. June 15",
        "D. July 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup raised seed funding in 2015 and a Series A in 2017.",
      "conflict_prompt": "The startup raised seed funding in 2015 and a Series A in 2013.",
      "question": "When did the startup raise its Series A according to the statement?",
      "options": [
        "A. 2013",
        "B. 2015",
        "C. 2016",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The remodeling permit was approved on May 5 and demolition began on May 20.",
      "conflict_prompt": "The remodeling permit was approved on May 5 and demolition began on April 20.",
      "question": "When did demolition begin?",
      "options": [
        "A. April 20",
        "B. May 5",
        "C. May 20",
        "D. June 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir member joined in 2002 and left the ensemble in 2010.",
      "conflict_prompt": "The choir member joined in 2002 and left the ensemble in 1998.",
      "question": "When did the choir member leave the ensemble?",
      "options": [
        "A. 1998",
        "B. 2002",
        "C. 2006",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author completed the manuscript in June and submitted it to the publisher in July.",
      "conflict_prompt": "The author completed the manuscript in June and submitted it to the publisher the previous May.",
      "question": "When was the manuscript submitted according to the statement?",
      "options": [
        "A. The previous May",
        "B. June",
        "C. July",
        "D. August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The road closure began on Monday night and reopened to traffic on Thursday morning.",
      "conflict_prompt": "The road closure began on Monday night and reopened to traffic on Sunday morning before the closure.",
      "question": "When did the road reopen according to the statement?",
      "options": [
        "A. Sunday morning",
        "B. Monday night",
        "C. Wednesday evening",
        "D. Thursday morning"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer received the commission in 2000 and completed the work in 2003.",
      "conflict_prompt": "The composer received the commission in 2000 and completed the work in 1995.",
      "question": "When did the composer complete the work?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2002",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon course was certified in May and the race was first held in October 2010.",
      "conflict_prompt": "The marathon course was certified in May and the race was first held in October 2005.",
      "question": "When was the race first held?",
      "options": [
        "A. October 2005",
        "B. May 2010",
        "C. October 2010",
        "D. November 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cellist joined the quartet in 1999 and their first recording was released in 2001.",
      "conflict_prompt": "The cellist joined the quartet in 1999 and their first recording was released in 1997.",
      "question": "When was the quartet's first recording released according to the statement?",
      "options": [
        "A. 1997",
        "B. 1999",
        "C. 2000",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot episode aired on September 9 and the series finale aired two years later on September 9.",
      "conflict_prompt": "The pilot episode aired on September 9 and the series finale aired two years earlier on September 9.",
      "question": "When did the series finale air relative to the pilot?",
      "options": [
        "A. Two years earlier",
        "B. The same day",
        "C. Two years later",
        "D. One year later"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The team announced their coach in January and played their first match in March.",
      "conflict_prompt": "The team announced their coach in January and played their first match the previous November.",
      "question": "When did they play their first match according to the statement?",
      "options": [
        "A. The previous November",
        "B. January",
        "C. February",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The herb garden was planted in early spring and reached full bloom by midsummer.",
      "conflict_prompt": "The herb garden was planted in early spring and reached full bloom the previous autumn.",
      "question": "When did the garden reach full bloom according to the statement?",
      "options": [
        "A. The previous autumn",
        "B. Early spring",
        "C. Late spring",
        "D. Midsummer"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company held its IPO in 2006 and quarterly reports began being published the following quarter.",
      "conflict_prompt": "The company held its IPO in 2006 and quarterly reports began being published the previous year.",
      "question": "When did quarterly reports begin being published according to the statement?",
      "options": [
        "A. The previous year",
        "B. 2006",
        "C. The following quarter",
        "D. Two years later"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The regional office opened in June 1994 and closed in December 2002.",
      "conflict_prompt": "The regional office opened in June 1994 and closed in March 1990.",
      "question": "When did the regional office close?",
      "options": [
        "A. March 1990",
        "B. June 1994",
        "C. December 2000",
        "D. December 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist began the series in 1987 and added the final painting in 1992.",
      "conflict_prompt": "The artist began the series in 1987 and added the final painting in 1980.",
      "question": "When was the final painting added?",
      "options": [
        "A. 1980",
        "B. 1987",
        "C. 1990",
        "D. 1992"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot completed flight tests in April and certification was granted in September.",
      "conflict_prompt": "The pilot completed flight tests in April and certification was granted the previous January.",
      "question": "When was certification granted according to the statement?",
      "options": [
        "A. The previous January",
        "B. April",
        "C. July",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The campsite opened for the season on May 1 and closed on October 1.",
      "conflict_prompt": "The campsite opened for the season on May 1 and closed on April 1.",
      "question": "When did the campsite close according to the statement?",
      "options": [
        "A. April 1",
        "B. May 1",
        "C. September 1",
        "D. October 1"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city introduced street cleaning on Mondays and began the program on March 3.",
      "conflict_prompt": "The city introduced street cleaning on Mondays and began the program on February 3 the previous year.",
      "question": "When did the street cleaning program begin according to the statement?",
      "options": [
        "A. February 3 (previous year)",
        "B. March 3",
        "C. April 3",
        "D. May 3"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historian dated the manuscript to 1420 and announced the discovery in 2004.",
      "conflict_prompt": "The historian dated the manuscript to 1420 and announced the discovery in 1990.",
      "question": "When was the discovery announced according to the statement?",
      "options": [
        "A. 1990",
        "B. 1420",
        "C. 2000",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The magazine started publishing the column in January 2013 and ended it in December 2018.",
      "conflict_prompt": "The magazine started publishing the column in January 2013 and ended it in December 2010.",
      "question": "When did the magazine end the column according to the statement?",
      "options": [
        "A. December 2010",
        "B. January 2013",
        "C. December 2016",
        "D. December 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot program ran from September 2014 through February 2015 and was later expanded.",
      "conflict_prompt": "The pilot program ran from September 2014 through February 2013 and was later expanded.",
      "question": "Through when did the pilot program run according to the statement?",
      "options": [
        "A. February 2013",
        "B. September 2014",
        "C. January 2015",
        "D. February 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The crop was planted on April 10 and irrigated for the first time on May 1.",
      "conflict_prompt": "The crop was planted on April 10 and irrigated for the first time on March 1.",
      "question": "When was the crop irrigated for the first time according to the statement?",
      "options": [
        "A. March 1",
        "B. April 10",
        "C. April 25",
        "D. May 1"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum acquired the artifact in 1960 and included it in an exhibition in 1961.",
      "conflict_prompt": "The museum acquired the artifact in 1960 and included it in an exhibition in 1955.",
      "question": "When was the artifact included in an exhibition according to the statement?",
      "options": [
        "A. 1955",
        "B. 1960",
        "C. 1961",
        "D. 1970"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The monastery was founded in 1102 and celebrated its 900th anniversary in 2002.",
      "conflict_prompt": "The monastery was founded in 1102 and celebrated its 900th anniversary in 1995.",
      "question": "When was the monastery's 900th anniversary celebrated according to the statement?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2002",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The board approved the budget in April and enacted spending cuts in June.",
      "conflict_prompt": "The board approved the budget in April and enacted spending cuts the previous December.",
      "question": "When were the spending cuts enacted according to the statement?",
      "options": [
        "A. The previous December",
        "B. April",
        "C. May",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lake froze over in late December and thawed in March the following year.",
      "conflict_prompt": "The lake froze over in late December and thawed in November of the same year.",
      "question": "When did the lake thaw according to the statement?",
      "options": [
        "A. November (same year)",
        "B. Late December",
        "C. January",
        "D. March (following year)"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The restaurant opened a summer terrace in June and closed it in September.",
      "conflict_prompt": "The restaurant opened a summer terrace in June and closed it the previous April.",
      "question": "When did the restaurant close the summer terrace according to the statement?",
      "options": [
        "A. The previous April",
        "B. June",
        "C. August",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright finished Act I in February and completed the full script in June.",
      "conflict_prompt": "The playwright finished Act I in February and completed the full script in January.",
      "question": "When was the full script completed according to the statement?",
      "options": [
        "A. January",
        "B. February",
        "C. May",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city's recycling initiative launched in 2010 and reached full implementation in 2014.",
      "conflict_prompt": "The city's recycling initiative launched in 2010 and reached full implementation in 2008.",
      "question": "When did the initiative reach full implementation according to the statement?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2012",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The hardware store opened a new woodworking department in 1998 and hired a manager in 1999.",
      "conflict_prompt": "The hardware store opened a new woodworking department in 1998 and hired a manager in 1995.",
      "question": "When was the woodworking department manager hired according to the statement?",
      "options": [
        "A. 1995",
        "B. 1998",
        "C. 1999",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The children's hospital wing was dedicated in 2007 and admitted its first patient in January 2008.",
      "conflict_prompt": "The children's hospital wing was dedicated in 2007 and admitted its first patient in 2005.",
      "question": "When was the first patient admitted according to the statement?",
      "options": [
        "A. 2005",
        "B. 2007",
        "C. January 2008",
        "D. March 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company introduced a four-day workweek in 2019 and fully implemented it in 2020.",
      "conflict_prompt": "The company introduced a four-day workweek in 2019 and fully implemented it in 2017.",
      "question": "When was the four-day workweek fully implemented according to the statement?",
      "options": [
        "A. 2017",
        "B. 2019",
        "C. 2020",
        "D. 2021"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery introduced gluten-free bread in March and saw sales increase by June.",
      "conflict_prompt": "The bakery introduced gluten-free bread in March and saw sales increase the previous January.",
      "question": "By when did sales increase according to the statement?",
      "options": [
        "A. The previous January",
        "B. March",
        "C. April",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cruise ship departed on July 2 and returned on July 16.",
      "conflict_prompt": "The cruise ship departed on July 2 and returned on June 30.",
      "question": "When did the ship return according to the statement?",
      "options": [
        "A. June 30",
        "B. July 2",
        "C. July 14",
        "D. July 16"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir recorded the album in December and released it the following March.",
      "conflict_prompt": "The choir recorded the album in December and released it the previous March.",
      "question": "When was the album released according to the statement?",
      "options": [
        "A. The previous March",
        "B. The following January",
        "C. The following March",
        "D. The same December"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town opened a new park in 1984 and added a playground in 1990.",
      "conflict_prompt": "The town opened a new park in 1984 and added a playground in 1978.",
      "question": "When was the playground added according to the statement?",
      "options": [
        "A. 1978",
        "B. 1984",
        "C. 1988",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novelist accepted a fellowship in 2006 and wrote her next book during 2007.",
      "conflict_prompt": "The novelist accepted a fellowship in 2006 and wrote her next book during 2005.",
      "question": "When did she write her next book according to the statement?",
      "options": [
        "A. 2005",
        "B. 2006",
        "C. 2007",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic train service resumed operations in 1993 and was electrified by 2000.",
      "conflict_prompt": "The historic train service resumed operations in 1993 and was electrified in 1988.",
      "question": "By what year was the service electrified according to the statement?",
      "options": [
        "A. 1988",
        "B. 1993",
        "C. 1997",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The tech conference took place on October 5 and workshops occurred the next two days.",
      "conflict_prompt": "The tech conference took place on October 5 and workshops occurred the previous two days.",
      "question": "When did the workshops occur according to the statement?",
      "options": [
        "A. The previous two days",
        "B. The same day",
        "C. The next two days",
        "D. The following week"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vineyard planted new vines in spring 2002 and produced its first vintage in 2007.",
      "conflict_prompt": "The vineyard planted new vines in spring 2002 and produced its first vintage in 1999.",
      "question": "When was the first vintage produced according to the statement?",
      "options": [
        "A. 1999",
        "B. 2002",
        "C. 2005",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The observatory installed a new telescope in 2010 and made its first observation in 2011.",
      "conflict_prompt": "The observatory installed a new telescope in 2010 and made its first observation in 2009.",
      "question": "When was the first observation made according to the statement?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir invited guest soloists in 2008 and recorded their album in 2009.",
      "conflict_prompt": "The choir invited guest soloists in 2008 and recorded their album in 2006.",
      "question": "When did the choir record the album according to the statement?",
      "options": [
        "A. 2006",
        "B. 2008",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bike lane was painted in April and enforcement began on May 1.",
      "conflict_prompt": "The bike lane was painted in April and enforcement began on March 1.",
      "question": "When did enforcement begin according to the statement?",
      "options": [
        "A. March 1",
        "B. April 1",
        "C. May 1",
        "D. June 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film festival accepted submissions until July 31 and announced winners on September 10.",
      "conflict_prompt": "The film festival accepted submissions until July 31 and announced winners on June 10.",
      "question": "When were the winners announced according to the statement?",
      "options": [
        "A. June 10",
        "B. July 31",
        "C. August 20",
        "D. September 10"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new tram line was proposed in 2009 and construction started in 2011.",
      "conflict_prompt": "The new tram line was proposed in 2009 and construction started in 2007.",
      "question": "When did construction start according to the statement?",
      "options": [
        "A. 2007",
        "B. 2009",
        "C. 2010",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mayor declared a state of emergency on April 8 and lifted it on April 22.",
      "conflict_prompt": "The mayor declared a state of emergency on April 8 and lifted it on April 1.",
      "question": "When was the state of emergency lifted according to the statement?",
      "options": [
        "A. April 1",
        "B. April 8",
        "C. April 15",
        "D. April 22"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vintage car was restored in 1990 and sold at auction in 1995.",
      "conflict_prompt": "The vintage car was restored in 1990 and sold at auction in 1985.",
      "question": "When was the car sold at auction according to the statement?",
      "options": [
        "A. 1985",
        "B. 1990",
        "C. 1992",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir's summer residency began on June 20 and ended on July 2.",
      "conflict_prompt": "The choir's summer residency began on June 20 and ended on June 10.",
      "question": "When did the residency end according to the statement?",
      "options": [
        "A. June 10",
        "B. June 20",
        "C. June 30",
        "D. July 2"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new bus schedule took effect on September 1 and adjustments were made in October.",
      "conflict_prompt": "The new bus schedule took effect on September 1 and adjustments were made in July.",
      "question": "When were adjustments made according to the statement?",
      "options": [
        "A. July",
        "B. September",
        "C. October",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The small theater staged a revival in November and closed the run in December.",
      "conflict_prompt": "The small theater staged a revival in November and closed the run the previous October.",
      "question": "When did the run close according to the statement?",
      "options": [
        "A. The previous October",
        "B. November",
        "C. December",
        "D. January"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research team collected samples in August and published results the following May.",
      "conflict_prompt": "The research team collected samples in August and published results the previous May.",
      "question": "When were the results published according to the statement?",
      "options": [
        "A. The previous May",
        "B. The following May",
        "C. August",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery started morning deliveries on June 1 and expanded to evenings on August 1.",
      "conflict_prompt": "The bakery started morning deliveries on June 1 and expanded to evenings on May 1.",
      "question": "When were evening deliveries started according to the statement?",
      "options": [
        "A. May 1",
        "B. June 1",
        "C. July 1",
        "D. August 1"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The suburban development was approved in 1996 and houses were built between 1997 and 2001.",
      "conflict_prompt": "The suburban development was approved in 1996 and houses were built between 1990 and 1994.",
      "question": "When were houses built according to the statement?",
      "options": [
        "A. 1990–1994",
        "B. 1996–1997",
        "C. 1997–2001",
        "D. 2002–2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school hosted exchange students starting in September 2003 and hosted them through June 2006.",
      "conflict_prompt": "The school hosted exchange students starting in September 2003 and hosted them through June 2001.",
      "question": "Through when did the school host exchange students according to the statement?",
      "options": [
        "A. June 2001",
        "B. September 2003",
        "C. June 2004",
        "D. June 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival's first year was 1986 and it was held annually until 2000.",
      "conflict_prompt": "The festival's first year was 1986 and it was held annually until 1975.",
      "question": "Until what year was the festival held annually according to the statement?",
      "options": [
        "A. 1975",
        "B. 1986",
        "C. 1995",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete returned from injury in March and competed at nationals in April.",
      "conflict_prompt": "The athlete returned from injury in March and competed at nationals the previous February.",
      "question": "When did the athlete compete at nationals according to the statement?",
      "options": [
        "A. The previous February",
        "B. March",
        "C. April",
        "D. May"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic fort was built in 1740 and reopened as a museum in 1970.",
      "conflict_prompt": "The historic fort was built in 1740 and reopened as a museum in 1960.",
      "question": "When did the fort reopen as a museum according to the statement?",
      "options": [
        "A. 1960",
        "B. 1740",
        "C. 1970",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir announced auditions in July and held them on August 10.",
      "conflict_prompt": "The choir announced auditions in July and held them on June 10.",
      "question": "When were auditions held according to the statement?",
      "options": [
        "A. June 10",
        "B. July 1",
        "C. August 10",
        "D. September 10"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conservation society began surveys in 1993 and published findings in 1997.",
      "conflict_prompt": "The conservation society began surveys in 1993 and published findings in 1990.",
      "question": "When were the findings published according to the statement?",
      "options": [
        "A. 1990",
        "B. 1993",
        "C. 1995",
        "D. 1997"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibit opened on February 2 and closed on March 31.",
      "conflict_prompt": "The exhibit opened on February 2 and closed on January 31.",
      "question": "When did the exhibit close according to the statement?",
      "options": [
        "A. January 31",
        "B. February 2",
        "C. March 1",
        "D. March 31"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The band recorded their album in summer 2000 and toured in spring 2001.",
      "conflict_prompt": "The band recorded their album in summer 2000 and toured in spring 1999.",
      "question": "When did the band tour according to the statement?",
      "options": [
        "A. Spring 1999",
        "B. Summer 2000",
        "C. Spring 2001",
        "D. Summer 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research center opened a new lab in 2015 and hosted an open day in 2016.",
      "conflict_prompt": "The research center opened a new lab in 2015 and hosted an open day in 2013.",
      "question": "When was the open day hosted according to the statement?",
      "options": [
        "A. 2013",
        "B. 2015",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The hotel reopened after renovations in April and welcomed guests starting May 1.",
      "conflict_prompt": "The hotel reopened after renovations in April and welcomed guests starting March 1.",
      "question": "When did the hotel welcome guests according to the statement?",
      "options": [
        "A. March 1",
        "B. April 1",
        "C. May 1",
        "D. June 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The youth program began in 2012 and celebrated its 5th anniversary in 2017.",
      "conflict_prompt": "The youth program began in 2012 and celebrated its 5th anniversary in 2014.",
      "question": "When was the 5th anniversary celebrated according to the statement?",
      "options": [
        "A. 2014",
        "B. 2015",
        "C. 2017",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The newspaper ran the investigative series in June and won an award the following year.",
      "conflict_prompt": "The newspaper ran the investigative series in June and won an award the previous year.",
      "question": "When did the newspaper win the award relative to the series?",
      "options": [
        "A. The previous year",
        "B. The same month",
        "C. The following year",
        "D. Two years later"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historian published a new edition in 1988 and issued a corrected reprint in 1990.",
      "conflict_prompt": "The historian published a new edition in 1988 and issued a corrected reprint in 1982.",
      "question": "When was the corrected reprint issued according to the statement?",
      "options": [
        "A. 1982",
        "B. 1988",
        "C. 1989",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ferry service began seasonal operation in May and ended at the end of September.",
      "conflict_prompt": "The ferry service began seasonal operation in May and ended at the start of April.",
      "question": "When did the seasonal operation end according to the statement?",
      "options": [
        "A. The start of April",
        "B. May",
        "C. August",
        "D. The end of September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeological dig began in June and uncovered artifacts by late July.",
      "conflict_prompt": "The archaeological dig began in June and uncovered artifacts the previous March.",
      "question": "When were artifacts uncovered according to the statement?",
      "options": [
        "A. The previous March",
        "B. June",
        "C. Early July",
        "D. Late July"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The foundation granted scholarships in 2002 and increased the number awarded in 2005.",
      "conflict_prompt": "The foundation granted scholarships in 2002 and increased the number awarded in 1999.",
      "question": "When did the foundation increase the number of scholarships according to the statement?",
      "options": [
        "A. 1999",
        "B. 2002",
        "C. 2004",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibition was curated in 2018 and traveled to three cities in 2019.",
      "conflict_prompt": "The exhibition was curated in 2018 and traveled to three cities in 2016.",
      "question": "When did the exhibition travel to three cities according to the statement?",
      "options": [
        "A. 2016",
        "B. 2018",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge inspection found no major defects in 2011 and a follow-up inspection occurred in 2014.",
      "conflict_prompt": "The bridge inspection found no major defects in 2011 and a follow-up inspection occurred in 2008.",
      "question": "When was the follow-up inspection conducted according to the statement?",
      "options": [
        "A. 2008",
        "B. 2011",
        "C. 2013",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra scheduled rehearsals starting Monday and performed its first concert the next Saturday.",
      "conflict_prompt": "The orchestra scheduled rehearsals starting Monday and performed its first concert the previous Saturday.",
      "question": "When was the first concert performed relative to rehearsals according to the statement?",
      "options": [
        "A. The previous Saturday",
        "B. The same Monday",
        "C. The next Saturday",
        "D. Two weeks later"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The municipal pool opened for the summer on June 15 and closed on August 31.",
      "conflict_prompt": "The municipal pool opened for the summer on June 15 and closed on May 31.",
      "question": "When did the municipal pool close according to the statement?",
      "options": [
        "A. May 31",
        "B. June 15",
        "C. July 31",
        "D. August 31"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film festival ran for ten days beginning on March 20 and ending on March 29.",
      "conflict_prompt": "The film festival ran for ten days beginning on March 20 and ending on March 15.",
      "question": "When did the festival end according to the statement?",
      "options": [
        "A. March 15",
        "B. March 20",
        "C. March 29",
        "D. April 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The photographer started the project in 2014 and exhibited it publicly in 2016.",
      "conflict_prompt": "The photographer started the project in 2014 and exhibited it publicly in 2012.",
      "question": "When was the project exhibited publicly according to the statement?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The garden club organized planting days in April and hosted a harvest fair in September.",
      "conflict_prompt": "The garden club organized planting days in April and hosted a harvest fair the previous February.",
      "question": "When was the harvest fair held according to the statement?",
      "options": [
        "A. The previous February",
        "B. April",
        "C. July",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city held elections in November and the new council convened in January.",
      "conflict_prompt": "The city held elections in November and the new council convened in September.",
      "question": "When did the new council convene according to the statement?",
      "options": [
        "A. September",
        "B. November",
        "C. December",
        "D. January"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery launched a loyalty program in 2018 and celebrated its 1-year anniversary in 2019.",
      "conflict_prompt": "The bakery launched a loyalty program in 2018 and celebrated its 1-year anniversary in 2016.",
      "question": "When was the 1-year anniversary celebrated according to the statement?",
      "options": [
        "A. 2016",
        "B. 2017",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The photographer held a workshop in June and published a book based on it in December.",
      "conflict_prompt": "The photographer held a workshop in June and published a book based on it in May.",
      "question": "When was the book published according to the statement?",
      "options": [
        "A. May",
        "B. June",
        "C. November",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater group staged the play in 1994 and revived it twenty years later in 2014.",
      "conflict_prompt": "The theater group staged the play in 1994 and revived it twenty years earlier in 1974.",
      "question": "When was the revival staged according to the statement?",
      "options": [
        "A. 1974",
        "B. 1994",
        "C. 2004",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school launched a new curriculum in 2009 and updated it in 2013.",
      "conflict_prompt": "The school launched a new curriculum in 2009 and updated it in 2005.",
      "question": "When was the curriculum updated according to the statement?",
      "options": [
        "A. 2005",
        "B. 2009",
        "C. 2011",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The community center opened an after-school program in September and expanded hours in November.",
      "conflict_prompt": "The community center opened an after-school program in September and expanded hours the previous July.",
      "question": "When were hours expanded according to the statement?",
      "options": [
        "A. The previous July",
        "B. September",
        "C. October",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cycling race started at dawn on April 12 and finished at dusk on April 12.",
      "conflict_prompt": "The cycling race started at dawn on April 12 and finished at dawn on April 11.",
      "question": "When did the race finish according to the statement?",
      "options": [
        "A. The previous dawn",
        "B. Dawn on April 12",
        "C. Dusk on April 12",
        "D. Dawn on April 13"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The writer accepted a residency in 2010 and produced a collection that was published in 2012.",
      "conflict_prompt": "The writer accepted a residency in 2010 and produced a collection that was published in 2008.",
      "question": "When was the collection published according to the statement?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city implemented a noise ordinance in 2003 and reviews were held in 2006.",
      "conflict_prompt": "The city implemented a noise ordinance in 2003 and reviews were held in 2001.",
      "question": "When were reviews held according to the statement?",
      "options": [
        "A. 2001",
        "B. 2003",
        "C. 2005",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The sculptor began carving the statue in 1977 and installed it in 1980.",
      "conflict_prompt": "The sculptor began carving the statue in 1977 and installed it in 1970.",
      "question": "When was the statue installed according to the statement?",
      "options": [
        "A. 1970",
        "B. 1977",
        "C. 1979",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright submitted the script in March and rehearsals began in April.",
      "conflict_prompt": "The playwright submitted the script in March and rehearsals began in February.",
      "question": "When did rehearsals begin according to the statement?",
      "options": [
        "A. February",
        "B. March",
        "C. April",
        "D. May"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The community garden hosted monthly workdays beginning in April and continued through October.",
      "conflict_prompt": "The community garden hosted monthly workdays beginning in April and continued through March.",
      "question": "Through which month did the workdays continue according to the statement?",
      "options": [
        "A. March",
        "B. April",
        "C. August",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The railway timetables were updated in May and the changes took effect on June 1.",
      "conflict_prompt": "The railway timetables were updated in May and the changes took effect on April 1.",
      "question": "When did the timetable changes take effect according to the statement?",
      "options": [
        "A. April 1",
        "B. May 1",
        "C. June 1",
        "D. July 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town council approved the park renovation in 2013 and work began in spring 2014.",
      "conflict_prompt": "The town council approved the park renovation in 2013 and work began in spring 2012.",
      "question": "When did work begin according to the statement?",
      "options": [
        "A. Spring 2012",
        "B. 2013",
        "C. Spring 2014",
        "D. Summer 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The newspaper serialized the novel in weekly installments starting January and concluding in June.",
      "conflict_prompt": "The newspaper serialized the novel in weekly installments starting January and concluding the previous June.",
      "question": "When did the serialization conclude according to the statement?",
      "options": [
        "A. The previous June",
        "B. January",
        "C. May",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software patch was developed in February and deployed to users on March 3.",
      "conflict_prompt": "The software patch was developed in February and deployed to users on January 3.",
      "question": "When was the patch deployed according to the statement?",
      "options": [
        "A. January 3",
        "B. February 1",
        "C. March 3",
        "D. April 3"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir's spring concert was on April 9 and the summer concert on July 21.",
      "conflict_prompt": "The choir's spring concert was on April 9 and the summer concert on March 21.",
      "question": "When was the summer concert according to the statement?",
      "options": [
        "A. March 21",
        "B. April 9",
        "C. June 15",
        "D. July 21"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The art school admitted students in September and began classes that same month.",
      "conflict_prompt": "The art school admitted students in September and began classes the previous month.",
      "question": "When did classes begin according to the statement?",
      "options": [
        "A. The previous month",
        "B. September",
        "C. October",
        "D. November"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The college offered summer courses starting June and finished them by late July.",
      "conflict_prompt": "The college offered summer courses starting June and finished them by late May.",
      "question": "By when were the summer courses finished according to the statement?",
      "options": [
        "A. Late May",
        "B. Early June",
        "C. Late July",
        "D. Early August"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic festival celebrated its 50th anniversary in 2005 with events all year.",
      "conflict_prompt": "The historic festival celebrated its 50th anniversary in 2005 with events the previous year.",
      "question": "When did the 50th anniversary events take place according to the statement?",
      "options": [
        "A. The previous year",
        "B. 2005",
        "C. 2006",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum loaned the painting in 2010 and it returned to the collection in 2012.",
      "conflict_prompt": "The museum loaned the painting in 2010 and it returned to the collection in 2008.",
      "question": "When did the painting return to the collection according to the statement?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater company rehearsed in March and performed in April of the same year.",
      "conflict_prompt": "The theater company rehearsed in March and performed in February of the same year.",
      "question": "When did the company perform according to the statement?",
      "options": [
        "A. February",
        "B. March",
        "C. April",
        "D. May"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir's membership drive started in January and reached its goal by March.",
      "conflict_prompt": "The choir's membership drive started in January and reached its goal the previous November.",
      "question": "By when did the membership drive reach its goal according to the statement?",
      "options": [
        "A. The previous November",
        "B. January",
        "C. February",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novel was published in 1998 after the author finished writing it in 1996.",
      "conflict_prompt": "The novel was published in 1998 after the author finished writing it in 2000.",
      "question": "In what year did the author finish writing the novel?",
      "options": [
        "A. 1994",
        "B. 1998",
        "C. 1996",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new airport terminal opened in March 2015 following construction that began in 2012.",
      "conflict_prompt": "The new airport terminal opened in March 2015 following construction that began in 2017.",
      "question": "When did construction on the terminal begin?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She graduated from medical school in 2010 after starting her studies in 2006.",
      "conflict_prompt": "She graduated from medical school in 2010 after starting her studies in 2012.",
      "question": "In what year did she begin her medical studies?",
      "options": [
        "A. 2004",
        "B. 2008",
        "C. 2006",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic church was consecrated in 1905, four years after construction began in 1901.",
      "conflict_prompt": "The historic church was consecrated in 1905, four years after construction began in 1908.",
      "question": "When did construction of the church start?",
      "options": [
        "A. 1899",
        "B. 1901",
        "C. 1906",
        "D. 1908"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research team published their findings in 2021 after conducting experiments throughout 2019.",
      "conflict_prompt": "The research team published their findings in 2021 after conducting experiments throughout 2023.",
      "question": "During which year did the team conduct experiments?",
      "options": [
        "A. 2018",
        "B. 2019",
        "C. 2021",
        "D. 2023"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He was born in 1975 and completed his PhD in 2001.",
      "conflict_prompt": "He was born in 1975 and completed his PhD in 1969.",
      "question": "In which year did he complete his PhD?",
      "options": [
        "A. 1989",
        "B. 1975",
        "C. 2001",
        "D. 1969"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software's first release was in 2014 after beta testing occurred in 2013.",
      "conflict_prompt": "The software's first release was in 2014 after beta testing occurred in 2016.",
      "question": "When did beta testing take place?",
      "options": [
        "A. 2012",
        "B. 2013",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum exhibit opened on June 1, 2019, following a six-month preparation that began in December 2018.",
      "conflict_prompt": "The museum exhibit opened on June 1, 2019, following a six-month preparation that began in December 2020.",
      "question": "When did the exhibit's preparation begin?",
      "options": [
        "A. November 2018",
        "B. December 2018",
        "C. January 2019",
        "D. December 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite was launched in 2018 after assembly finished in April 2017.",
      "conflict_prompt": "The satellite was launched in 2018 after assembly finished in April 2019.",
      "question": "When was the satellite's assembly completed?",
      "options": [
        "A. April 2016",
        "B. April 2017",
        "C. April 2018",
        "D. April 2019"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They held the festival in September 2016, which was three years after the inaugural event in 2013.",
      "conflict_prompt": "They held the festival in September 2016, which was three years after the inaugural event in 2019.",
      "question": "In what year was the inaugural event held?",
      "options": [
        "A. 2012",
        "B. 2013",
        "C. 2014",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete set a national record in 2004 after competing internationally for the first time in 2002.",
      "conflict_prompt": "The athlete set a national record in 2004 after competing internationally for the first time in 2006.",
      "question": "When did the athlete first compete internationally?",
      "options": [
        "A. 2000",
        "B. 2002",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company was founded in 1999 and went public in 2007.",
      "conflict_prompt": "The company was founded in 1999 and went public in 1995.",
      "question": "In what year did the company go public?",
      "options": [
        "A. 1999",
        "B. 2000",
        "C. 2007",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer wrote the symphony in 1888 and it premiered in 1889.",
      "conflict_prompt": "The composer wrote the symphony in 1888 and it premiered in 1875.",
      "question": "When did the symphony premiere?",
      "options": [
        "A. 1880",
        "B. 1888",
        "C. 1889",
        "D. 1875"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation began in July 2020 and the building reopened in January 2021.",
      "conflict_prompt": "The renovation began in July 2020 and the building reopened in January 2019.",
      "question": "When did the building reopen after renovations?",
      "options": [
        "A. January 2019",
        "B. July 2020",
        "C. January 2021",
        "D. March 2022"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright wrote the script in 1992 and the play debuted in 1994.",
      "conflict_prompt": "The playwright wrote the script in 1992 and the play debuted in 1989.",
      "question": "In what year did the play debut?",
      "options": [
        "A. 1990",
        "B. 1992",
        "C. 1994",
        "D. 1989"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He started his first job in 1987 and retired in 2010.",
      "conflict_prompt": "He started his first job in 1987 and retired in 1979.",
      "question": "When did he retire?",
      "options": [
        "A. 1987",
        "B. 1995",
        "C. 2010",
        "D. 1979"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ferry service resumed in April 2022 after being suspended since November 2021.",
      "conflict_prompt": "The ferry service resumed in April 2022 after being suspended since June 2023.",
      "question": "Since when had the ferry service been suspended?",
      "options": [
        "A. October 2020",
        "B. November 2021",
        "C. January 2022",
        "D. June 2023"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vintage car was manufactured in 1955 and restored in 2005.",
      "conflict_prompt": "The vintage car was manufactured in 1955 and restored in 1948.",
      "question": "When was the car restored?",
      "options": [
        "A. 1948",
        "B. 1955",
        "C. 1995",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software update was released in August 2020 after a beta period in July 2020.",
      "conflict_prompt": "The software update was released in August 2020 after a beta period in September 2020.",
      "question": "When did the beta period occur?",
      "options": [
        "A. June 2020",
        "B. July 2020",
        "C. August 2020",
        "D. September 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The farmer planted the corn in April and harvested it in September of the same year.",
      "conflict_prompt": "The farmer planted the corn in April and harvested it in March of the same year.",
      "question": "When did the farmer harvest the corn?",
      "options": [
        "A. February",
        "B. April",
        "C. September",
        "D. March"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference took place on May 10, 2018, and registration opened on April 1, 2018.",
      "conflict_prompt": "The conference took place on May 10, 2018, and registration opened on June 1, 2019.",
      "question": "When did registration for the conference open?",
      "options": [
        "A. March 2018",
        "B. April 1, 2018",
        "C. May 10, 2018",
        "D. June 1, 2019"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The spacecraft completed its mission in 2023 after launching in 2020.",
      "conflict_prompt": "The spacecraft completed its mission in 2023 after launching in 2025.",
      "question": "When did the spacecraft launch?",
      "options": [
        "A. 2019",
        "B. 2020",
        "C. 2022",
        "D. 2025"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist painted the series between 2000 and 2003, and the final piece was sold in 2004.",
      "conflict_prompt": "The artist painted the series between 2000 and 2003, and the final piece was sold in 1998.",
      "question": "In what year was the final piece sold?",
      "options": [
        "A. 1998",
        "B. 2000",
        "C. 2003",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The medication was approved in 2011 after clinical trials ended in late 2010.",
      "conflict_prompt": "The medication was approved in 2011 after clinical trials ended in 2013.",
      "question": "When did the clinical trials end?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The family moved into the house in 1990 after purchasing it in December 1989.",
      "conflict_prompt": "The family moved into the house in 1990 after purchasing it in 1995.",
      "question": "When did they purchase the house?",
      "options": [
        "A. 1985",
        "B. 1989",
        "C. 1990",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra recorded the album in March 2012 and released it in July 2012.",
      "conflict_prompt": "The orchestra recorded the album in March 2012 and released it in January 2010.",
      "question": "When was the album released?",
      "options": [
        "A. March 2012",
        "B. May 2011",
        "C. July 2012",
        "D. January 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The policy was enacted in 1994 and updated ten years later in 2004.",
      "conflict_prompt": "The policy was enacted in 1994 and updated ten years later in 1984.",
      "question": "When was the policy updated ten years later?",
      "options": [
        "A. 1984",
        "B. 1994",
        "C. 2004",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The glacier reached its maximum measured size in 1920 and began retreating by 1930.",
      "conflict_prompt": "The glacier reached its maximum measured size in 1920 and began retreating by 1910.",
      "question": "By what year had the glacier begun retreating?",
      "options": [
        "A. 1900",
        "B. 1920",
        "C. 1930",
        "D. 1910"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They launched the marketing campaign in February 2017 and saw sales increase in April 2017.",
      "conflict_prompt": "They launched the marketing campaign in February 2017 and saw sales increase in January 2016.",
      "question": "When did sales increase following the campaign?",
      "options": [
        "A. December 2016",
        "B. February 2017",
        "C. April 2017",
        "D. January 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city completed the subway extension in 2013, three years after tunnel boring began in 2010.",
      "conflict_prompt": "The city completed the subway extension in 2013, three years after tunnel boring began in 2016.",
      "question": "When did tunnel boring begin?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cookbook was printed in 2007 after the author finished compiling recipes in 2006.",
      "conflict_prompt": "The cookbook was printed in 2007 after the author finished compiling recipes in 2010.",
      "question": "When did the author finish compiling the recipes?",
      "options": [
        "A. 2005",
        "B. 2006",
        "C. 2008",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge collapsed in 1985 after being inspected in early 1984.",
      "conflict_prompt": "The bridge collapsed in 1985 after being inspected in 1987.",
      "question": "When was the bridge inspected before the collapse?",
      "options": [
        "A. 1982",
        "B. 1984",
        "C. 1986",
        "D. 1987"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The graduate thesis was defended in June 2013 after two years of research starting in 2011.",
      "conflict_prompt": "The graduate thesis was defended in June 2013 after two years of research starting in 2015.",
      "question": "When did the research for the thesis begin?",
      "options": [
        "A. 2010",
        "B. 2011",
        "C. 2014",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship was launched in 1912 and sank after colliding with an iceberg in April of the same year.",
      "conflict_prompt": "The ship was launched in 1912 and sank after colliding with an iceberg in April 1910.",
      "question": "When did the ship collide with the iceberg?",
      "options": [
        "A. April 1910",
        "B. April 1912",
        "C. April 1914",
        "D. April 1908"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon took place on October 10, 2010, and registration closed on September 30, 2010.",
      "conflict_prompt": "The marathon took place on October 10, 2010, and registration closed on November 1, 2011.",
      "question": "When did registration for the marathon close?",
      "options": [
        "A. September 1, 2010",
        "B. September 30, 2010",
        "C. October 10, 2010",
        "D. November 1, 2011"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The astronaut completed training in 2009 and flew on her first mission in 2011.",
      "conflict_prompt": "The astronaut completed training in 2009 and flew on her first mission in 2007.",
      "question": "When did she fly on her first mission?",
      "options": [
        "A. 2005",
        "B. 2009",
        "C. 2011",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vintage wine was bottled in 1978 and finally opened at the reunion in 2008.",
      "conflict_prompt": "The vintage wine was bottled in 1978 and finally opened at the reunion in 1968.",
      "question": "When was the wine opened at the reunion?",
      "options": [
        "A. 1968",
        "B. 1978",
        "C. 1988",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary premiered in 2016 after filming took place in 2014 and 2015.",
      "conflict_prompt": "The documentary premiered in 2016 after filming took place in 2018 and 2019.",
      "question": "When did filming for the documentary occur?",
      "options": [
        "A. 2012-2013",
        "B. 2014-2015",
        "C. 2016-2017",
        "D. 2018-2019"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The band's debut album was recorded in 1991 and released in early 1992.",
      "conflict_prompt": "The band's debut album was recorded in 1991 and released in 1989.",
      "question": "When was the album released?",
      "options": [
        "A. 1989",
        "B. 1990",
        "C. 1991",
        "D. Early 1992"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient received the transplant in December 2017 after being listed in March 2016.",
      "conflict_prompt": "The patient received the transplant in December 2017 after being listed in January 2019.",
      "question": "When was the patient first listed for transplant?",
      "options": [
        "A. March 2016",
        "B. December 2017",
        "C. January 2018",
        "D. January 2019"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The highway project started in 2004 and the final lane opened in 2009.",
      "conflict_prompt": "The highway project started in 2004 and the final lane opened in 1999.",
      "question": "When did the final lane open?",
      "options": [
        "A. 1999",
        "B. 2004",
        "C. 2007",
        "D. 2009"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The detective solved the cold case in 2015 after reviewing files from 1995.",
      "conflict_prompt": "The detective solved the cold case in 2015 after reviewing files from 2018.",
      "question": "From which year were the files the detective reviewed?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete retired from competition in 2002 and later coached the national team in 2010.",
      "conflict_prompt": "The athlete retired from competition in 2002 and later coached the national team in 1998.",
      "question": "When did they begin coaching the national team?",
      "options": [
        "A. 1995",
        "B. 2002",
        "C. 2008",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The biography was completed in 2014 and published the following year in 2015.",
      "conflict_prompt": "The biography was completed in 2014 and published the following year in 2012.",
      "question": "When was the biography published?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school year began on September 3, 2018, and ended on June 12, 2019.",
      "conflict_prompt": "The school year began on September 3, 2018, and ended on June 12, 2017.",
      "question": "When did the school year end?",
      "options": [
        "A. June 12, 2017",
        "B. September 3, 2018",
        "C. December 20, 2018",
        "D. June 12, 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The scientist published the theory in 1950 and won the prize in 1962.",
      "conflict_prompt": "The scientist published the theory in 1950 and won the prize in 1942.",
      "question": "In which year did the scientist win the prize?",
      "options": [
        "A. 1942",
        "B. 1950",
        "C. 1958",
        "D. 1962"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The political party was formed in 1990 and first contested elections in 1992.",
      "conflict_prompt": "The political party was formed in 1990 and first contested elections in 1988.",
      "question": "When did the party first contest elections?",
      "options": [
        "A. 1988",
        "B. 1990",
        "C. 1992",
        "D. 1996"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchard trees were planted in 2001 and bore fruit by 2004.",
      "conflict_prompt": "The orchard trees were planted in 2001 and bore fruit by 1998.",
      "question": "By what year did the trees bear fruit?",
      "options": [
        "A. 1998",
        "B. 2001",
        "C. 2002",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary crew filmed in the region in 2009 and aired the film in 2010.",
      "conflict_prompt": "The documentary crew filmed in the region in 2009 and aired the film in 2007.",
      "question": "When did the documentary air?",
      "options": [
        "A. 2007",
        "B. 2009",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer completed the opera in 1870 and its first performance was in 1871.",
      "conflict_prompt": "The composer completed the opera in 1870 and its first performance was in 1860.",
      "question": "When was the opera first performed?",
      "options": [
        "A. 1860",
        "B. 1870",
        "C. 1871",
        "D. 1880"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author signed the book deal in 2003 and delivered the manuscript in 2004.",
      "conflict_prompt": "The author signed the book deal in 2003 and delivered the manuscript in 2001.",
      "question": "When did the author deliver the manuscript?",
      "options": [
        "A. 2001",
        "B. 2003",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The clinical guideline was issued in 2012 after a review conducted in 2011.",
      "conflict_prompt": "The clinical guideline was issued in 2012 after a review conducted in 2015.",
      "question": "When was the review conducted that preceded the guideline?",
      "options": [
        "A. 2010",
        "B. 2011",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theme park ride debuted in summer 2005 after two years of design work starting in 2003.",
      "conflict_prompt": "The theme park ride debuted in summer 2005 after two years of design work starting in 2007.",
      "question": "When did the design work for the ride begin?",
      "options": [
        "A. 2001",
        "B. 2003",
        "C. 2006",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival's first year was 2010 and it celebrated its tenth anniversary in 2019.",
      "conflict_prompt": "The festival's first year was 2010 and it celebrated its tenth anniversary in 2008.",
      "question": "In what year did the festival celebrate its tenth anniversary?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2018",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright completed revisions in late 2001 and the final script was printed in January 2002.",
      "conflict_prompt": "The playwright completed revisions in late 2001 and the final script was printed in January 2000.",
      "question": "When was the final script printed?",
      "options": [
        "A. January 2000",
        "B. Late 2001",
        "C. January 2002",
        "D. March 2003"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city installed the new traffic lights in May 2017 after ordering them in December 2016.",
      "conflict_prompt": "The city installed the new traffic lights in May 2017 after ordering them in July 2019.",
      "question": "When were the traffic lights ordered?",
      "options": [
        "A. November 2016",
        "B. December 2016",
        "C. January 2017",
        "D. July 2019"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The doctoral candidate began fieldwork in 2008 and defended the thesis in 2012.",
      "conflict_prompt": "The doctoral candidate began fieldwork in 2008 and defended the thesis in 2005.",
      "question": "When was the thesis defended?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer released a single in 2013 that was recorded in late 2012.",
      "conflict_prompt": "The composer released a single in 2013 that was recorded in 2015.",
      "question": "When was the single recorded?",
      "options": [
        "A. Early 2011",
        "B. Late 2012",
        "C. Mid 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The product warranty started on January 1, 2020, and the product was purchased on December 15, 2019.",
      "conflict_prompt": "The product warranty started on January 1, 2020, and the product was purchased on March 1, 2021.",
      "question": "When was the product purchased?",
      "options": [
        "A. December 15, 2019",
        "B. January 1, 2020",
        "C. June 1, 2020",
        "D. March 1, 2021"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The painter studied at the academy from 1983 to 1987 and held his first solo show in 1989.",
      "conflict_prompt": "The painter studied at the academy from 1983 to 1987 and held his first solo show in 1979.",
      "question": "When did he hold his first solo show?",
      "options": [
        "A. 1979",
        "B. 1983",
        "C. 1987",
        "D. 1989"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The TV series filmed its pilot episode in January 2010 and the season premiered in October 2010.",
      "conflict_prompt": "The TV series filmed its pilot episode in January 2010 and the season premiered in October 2009.",
      "question": "When did the season premiere?",
      "options": [
        "A. October 2009",
        "B. January 2010",
        "C. April 2010",
        "D. October 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The public art installation was commissioned in 2016 and unveiled in June 2018.",
      "conflict_prompt": "The public art installation was commissioned in 2016 and unveiled in June 2015.",
      "question": "When was the art installation unveiled?",
      "options": [
        "A. June 2015",
        "B. 2016",
        "C. 2017",
        "D. June 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient started chemotherapy in March 2014 and completed the regimen in December 2014.",
      "conflict_prompt": "The patient started chemotherapy in March 2014 and completed the regimen in January 2013.",
      "question": "When did the patient complete chemotherapy?",
      "options": [
        "A. January 2013",
        "B. March 2014",
        "C. June 2014",
        "D. December 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup raised seed funding in 2016 and launched its product in 2018.",
      "conflict_prompt": "The startup raised seed funding in 2016 and launched its product in 2014.",
      "question": "When did the startup launch its product?",
      "options": [
        "A. 2014",
        "B. 2016",
        "C. 2017",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer completed drafts of the concerto in 1999 and the world premiere was held in 2000.",
      "conflict_prompt": "The composer completed drafts of the concerto in 1999 and the world premiere was held in 1995.",
      "question": "When was the world premiere held?",
      "options": [
        "A. 1995",
        "B. 1998",
        "C. 1999",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The biologist published results in April after collecting samples in February of the same year.",
      "conflict_prompt": "The biologist published results in April after collecting samples in June of the same year.",
      "question": "When were the samples collected?",
      "options": [
        "A. January",
        "B. February",
        "C. April",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town's centennial celebration occurred in 2010 and planning started in mid-2009.",
      "conflict_prompt": "The town's centennial celebration occurred in 2010 and planning started in mid-2012.",
      "question": "When did planning for the centennial start?",
      "options": [
        "A. 2008",
        "B. Mid-2009",
        "C. 2010",
        "D. Mid-2012"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity hosted its annual gala on November 5, 2014, and ticket sales opened on August 1, 2014.",
      "conflict_prompt": "The charity hosted its annual gala on November 5, 2014, and ticket sales opened on December 1, 2015.",
      "question": "When did ticket sales open for the gala?",
      "options": [
        "A. July 1, 2014",
        "B. August 1, 2014",
        "C. November 5, 2014",
        "D. December 1, 2015"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The university admitted its first class in 1869 and celebrated its 150th anniversary in 2019.",
      "conflict_prompt": "The university admitted its first class in 1869 and celebrated its 150th anniversary in 2005.",
      "question": "In what year did the university celebrate its 150th anniversary?",
      "options": [
        "A. 2005",
        "B. 1869",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater troupe rehearsed the play for six weeks starting in April and opened on May 20, 2011.",
      "conflict_prompt": "The theater troupe rehearsed the play for six weeks starting in April and opened on March 15, 2011.",
      "question": "When did the play open?",
      "options": [
        "A. March 15, 2011",
        "B. April 1, 2011",
        "C. May 20, 2011",
        "D. June 30, 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The highway resurfacing began in June 2020 and was completed by September 2020.",
      "conflict_prompt": "The highway resurfacing began in June 2020 and was completed by May 2019.",
      "question": "By when was the resurfacing completed?",
      "options": [
        "A. May 2019",
        "B. June 2020",
        "C. August 2020",
        "D. September 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist began a residency in January 2017 and finished the commissioned works in December 2017.",
      "conflict_prompt": "The artist began a residency in January 2017 and finished the commissioned works in November 2015.",
      "question": "When did the artist finish the commissioned works?",
      "options": [
        "A. November 2015",
        "B. January 2017",
        "C. June 2017",
        "D. December 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city's bike-sharing program launched in May 2013 and added new stations in 2015.",
      "conflict_prompt": "The city's bike-sharing program launched in May 2013 and added new stations in 2010.",
      "question": "When were new stations added to the program?",
      "options": [
        "A. 2010",
        "B. May 2013",
        "C. 2014",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer finished scoring the film in August and the soundtrack was released the following month in September.",
      "conflict_prompt": "The composer finished scoring the film in August and the soundtrack was released the following month in July.",
      "question": "When was the soundtrack released?",
      "options": [
        "A. July",
        "B. August",
        "C. September",
        "D. October"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Production of the model car began in 1993 and the first units shipped in early 1994.",
      "conflict_prompt": "Production of the model car began in 1993 and the first units shipped in late 1992.",
      "question": "When did the first units ship?",
      "options": [
        "A. Late 1992",
        "B. 1993",
        "C. Early 1994",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation of the library started in March 2011 and the building reopened in August 2012.",
      "conflict_prompt": "The renovation of the library started in March 2011 and the building reopened in June 2010.",
      "question": "When did the library reopen after renovation?",
      "options": [
        "A. June 2010",
        "B. March 2011",
        "C. December 2011",
        "D. August 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The parade was scheduled for July 4, 2019, and permits were issued on June 1, 2019.",
      "conflict_prompt": "The parade was scheduled for July 4, 2019, and permits were issued on August 1, 2020.",
      "question": "When were the permits issued for the parade?",
      "options": [
        "A. May 1, 2019",
        "B. June 1, 2019",
        "C. July 4, 2019",
        "D. August 1, 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cultivar was first bred in 1972 and registered with the horticultural society in 1974.",
      "conflict_prompt": "The cultivar was first bred in 1972 and registered with the horticultural society in 1969.",
      "question": "When was the cultivar registered with the horticultural society?",
      "options": [
        "A. 1969",
        "B. 1972",
        "C. 1973",
        "D. 1974"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum acquired the painting in 1988 and added it to the permanent collection in 1990.",
      "conflict_prompt": "The museum acquired the painting in 1988 and added it to the permanent collection in 1975.",
      "question": "When was the painting added to the permanent collection?",
      "options": [
        "A. 1975",
        "B. 1988",
        "C. 1989",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The board appointed the new CEO in March 2015 and announced a strategic plan in June 2015.",
      "conflict_prompt": "The board appointed the new CEO in March 2015 and announced a strategic plan in January 2014.",
      "question": "When was the strategic plan announced?",
      "options": [
        "A. January 2014",
        "B. March 2015",
        "C. May 2015",
        "D. June 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research grant was awarded in 2007 and the project began in January 2008.",
      "conflict_prompt": "The research grant was awarded in 2007 and the project began in December 2006.",
      "question": "When did the project begin?",
      "options": [
        "A. December 2006",
        "B. 2007",
        "C. January 2008",
        "D. March 2009"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The housewarming party was held in April 2012 after the move-in on March 3, 2012.",
      "conflict_prompt": "The housewarming party was held in April 2012 after the move-in on May 1, 2013.",
      "question": "When did they move into the house?",
      "options": [
        "A. March 3, 2012",
        "B. April 2012",
        "C. May 1, 2013",
        "D. June 2014"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conservation project started in 1997 and the area was declared protected in 2000.",
      "conflict_prompt": "The conservation project started in 1997 and the area was declared protected in 1990.",
      "question": "When was the area declared protected?",
      "options": [
        "A. 1990",
        "B. 1997",
        "C. 1999",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The cyclist broke the course record in 2014 after training intensively throughout 2013.",
      "conflict_prompt": "The cyclist broke the course record in 2014 after training intensively throughout 2016.",
      "question": "Throughout which year did the cyclist train intensively?",
      "options": [
        "A. 2012",
        "B. 2013",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archival documents were digitized in 2010 and made publicly accessible in 2011.",
      "conflict_prompt": "The archival documents were digitized in 2010 and made publicly accessible in 2008.",
      "question": "When were the documents made publicly accessible?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2011",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The lighthouse was automated in 1976 and decommissioned in 2002.",
      "conflict_prompt": "The lighthouse was automated in 1976 and decommissioned in 1968.",
      "question": "When was the lighthouse decommissioned?",
      "options": [
        "A. 1968",
        "B. 1976",
        "C. 1985",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference paper was accepted in December 2011 and presented at the conference in March 2012.",
      "conflict_prompt": "The conference paper was accepted in December 2011 and presented at the conference in November 2010.",
      "question": "When was the paper presented at the conference?",
      "options": [
        "A. November 2010",
        "B. December 2011",
        "C. March 2012",
        "D. April 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery opened its doors in June 2006 and introduced gluten-free options in 2014.",
      "conflict_prompt": "The bakery opened its doors in June 2006 and introduced gluten-free options in 2000.",
      "question": "When did the bakery introduce gluten-free options?",
      "options": [
        "A. 2000",
        "B. 2006",
        "C. 2010",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The college admitted its first international students in 1970 and expanded exchange programs in 1980.",
      "conflict_prompt": "The college admitted its first international students in 1970 and expanded exchange programs in 1965.",
      "question": "When did the college expand its exchange programs?",
      "options": [
        "A. 1965",
        "B. 1970",
        "C. 1975",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge's safety inspection occurred in May 2018 and repairs were finished in November 2018.",
      "conflict_prompt": "The bridge's safety inspection occurred in May 2018 and repairs were finished in March 2017.",
      "question": "When were the repairs finished?",
      "options": [
        "A. March 2017",
        "B. May 2018",
        "C. September 2018",
        "D. November 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The researcher discovered the specimen in 1994 and published the description in 1996.",
      "conflict_prompt": "The researcher discovered the specimen in 1994 and published the description in 1991.",
      "question": "When was the species description published?",
      "options": [
        "A. 1991",
        "B. 1994",
        "C. 1995",
        "D. 1996"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity's program launched in January 2000 and saw community adoption by mid-2001.",
      "conflict_prompt": "The charity's program launched in January 2000 and saw community adoption by mid-1999.",
      "question": "By when did the community adopt the program?",
      "options": [
        "A. Mid-1999",
        "B. January 2000",
        "C. Early 2000",
        "D. Mid-2001"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic manuscript was dated to 1450 and conserved in 2003.",
      "conflict_prompt": "The historic manuscript was dated to 1450 and conserved in 1399.",
      "question": "When was the manuscript conserved?",
      "options": [
        "A. 1399",
        "B. 1450",
        "C. 1990",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school board approved the curriculum change in 2016 and implementation began in September 2017.",
      "conflict_prompt": "The school board approved the curriculum change in 2016 and implementation began in September 2015.",
      "question": "When did implementation of the curriculum change begin?",
      "options": [
        "A. September 2015",
        "B. 2016",
        "C. January 2017",
        "D. September 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The explorer mapped the coastline in 1780 and published his charts in 1782.",
      "conflict_prompt": "The explorer mapped the coastline in 1780 and published his charts in 1770.",
      "question": "When were the charts published?",
      "options": [
        "A. 1770",
        "B. 1780",
        "C. 1781",
        "D. 1782"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play was rehearsed during March and April and premiered on May 5, 1999.",
      "conflict_prompt": "The play was rehearsed during March and April and premiered on April 1, 1998.",
      "question": "When did the play premiere?",
      "options": [
        "A. April 1, 1998",
        "B. March 1, 1999",
        "C. May 5, 1999",
        "D. June 10, 1999"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite dish installation began at dawn and was completed by noon that same day.",
      "conflict_prompt": "The satellite dish installation began at dawn and was completed before dawn the previous day.",
      "question": "By what time was the installation completed?",
      "options": [
        "A. Before dawn the previous day",
        "B. Dawn",
        "C. Noon that same day",
        "D. Midnight"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film's screenplay was written in 2005 and principal photography took place in 2007.",
      "conflict_prompt": "The film's screenplay was written in 2005 and principal photography took place in 2003.",
      "question": "When did principal photography occur?",
      "options": [
        "A. 2003",
        "B. 2005",
        "C. 2006",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The public consultation ran from March to May 2010 and the final report was published in July 2010.",
      "conflict_prompt": "The public consultation ran from March to May 2010 and the final report was published in January 2009.",
      "question": "When was the final report published?",
      "options": [
        "A. January 2009",
        "B. May 2010",
        "C. June 2010",
        "D. July 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The castle was restored between 1980 and 1985 and opened to visitors in 1986.",
      "conflict_prompt": "The castle was restored between 1980 and 1985 and opened to visitors in 1979.",
      "question": "When did the castle open to visitors?",
      "options": [
        "A. 1979",
        "B. 1980",
        "C. 1985",
        "D. 1986"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot completed flight school in 2001 and logged her first solo flight in May 1999.",
      "conflict_prompt": "The pilot completed flight school in 2001 and logged her first solo flight in June 2003.",
      "question": "When did she log her first solo flight?",
      "options": [
        "A. May 1999",
        "B. 2001",
        "C. June 2002",
        "D. June 2003"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They discovered the archaeological site in 1976 and excavations continued through 1978.",
      "conflict_prompt": "They discovered the archaeological site in 1976 and excavations continued through 1974.",
      "question": "Through which year did excavations continue?",
      "options": [
        "A. 1974",
        "B. 1976",
        "C. 1977",
        "D. 1978"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer revised the sonata in 1810 and it was published posthumously in 1821.",
      "conflict_prompt": "The composer revised the sonata in 1810 and it was published posthumously in 1805.",
      "question": "When was the sonata published posthumously?",
      "options": [
        "A. 1805",
        "B. 1810",
        "C. 1815",
        "D. 1821"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The merger was announced in February 2012 and finalized in August 2012.",
      "conflict_prompt": "The merger was announced in February 2012 and finalized in December 2010.",
      "question": "When was the merger finalized?",
      "options": [
        "A. December 2010",
        "B. February 2012",
        "C. June 2012",
        "D. August 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The student enrolled in the program in 2017 and completed the internship requirement in summer 2018.",
      "conflict_prompt": "The student enrolled in the program in 2017 and completed the internship requirement in spring 2016.",
      "question": "When did the student complete the internship requirement?",
      "options": [
        "A. Spring 2016",
        "B. 2017",
        "C. Early 2018",
        "D. Summer 2018"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film festival accepted submissions until January 31, 2019, and screenings took place in March 2019.",
      "conflict_prompt": "The film festival accepted submissions until January 31, 2019, and screenings took place in February 2018.",
      "question": "When did screenings take place?",
      "options": [
        "A. February 2018",
        "B. January 31, 2019",
        "C. March 2019",
        "D. April 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot program started in autumn 2015 and expanded citywide in spring 2017.",
      "conflict_prompt": "The pilot program started in autumn 2015 and expanded citywide in summer 2014.",
      "question": "When did the program expand citywide?",
      "options": [
        "A. Summer 2014",
        "B. Autumn 2015",
        "C. Early 2016",
        "D. Spring 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The chef opened his first restaurant in 1998 and launched a cookbook in 2006.",
      "conflict_prompt": "The chef opened his first restaurant in 1998 and launched a cookbook in 1990.",
      "question": "When did the chef launch his cookbook?",
      "options": [
        "A. 1990",
        "B. 1998",
        "C. 2002",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The solar observatory began operations in 1979 and recorded a major flare in June 1982.",
      "conflict_prompt": "The solar observatory began operations in 1979 and recorded a major flare in May 1975.",
      "question": "When did the observatory record the major flare?",
      "options": [
        "A. May 1975",
        "B. 1979",
        "C. June 1982",
        "D. January 1980"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity launched its emergency fund in response to floods in 2013 and distributed aid by July 2013.",
      "conflict_prompt": "The charity launched its emergency fund in response to floods in 2013 and distributed aid by June 2010.",
      "question": "By when did the charity distribute aid for the 2013 floods?",
      "options": [
        "A. June 2010",
        "B. March 2013",
        "C. July 2013",
        "D. December 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The scientist's experiment started on January 1 and concluded on January 15 of the same month.",
      "conflict_prompt": "The scientist's experiment started on January 1 and concluded on December 31 of the previous year.",
      "question": "When did the experiment conclude?",
      "options": [
        "A. December 31 of the previous year",
        "B. January 1",
        "C. January 10",
        "D. January 15"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer began the opera in 1835 and completed it in 1838.",
      "conflict_prompt": "The composer began the opera in 1835 and completed it in 1830.",
      "question": "When did the composer complete the opera?",
      "options": [
        "A. 1830",
        "B. 1835",
        "C. 1837",
        "D. 1838"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play's first preview was staged on September 12 and the official opening night was on September 18.",
      "conflict_prompt": "The play's first preview was staged on September 12 and the official opening night was on August 20.",
      "question": "When was the official opening night?",
      "options": [
        "A. August 20",
        "B. September 12",
        "C. September 18",
        "D. October 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author was born in 1922 and published his memoir in 1995.",
      "conflict_prompt": "The author was born in 1922 and published his memoir in 1918.",
      "question": "In what year was his memoir published?",
      "options": [
        "A. 1918",
        "B. 1922",
        "C. 1980",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The farmer applied fertilizer in March and reported improved yields in the fall harvest of the same year.",
      "conflict_prompt": "The farmer applied fertilizer in March and reported improved yields in the spring harvest of the same year.",
      "question": "When did the farmer report improved yields?",
      "options": [
        "A. Spring",
        "B. Summer",
        "C. Fall",
        "D. Winter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir rehearsed weekly starting in September and performed their concert in December 2011.",
      "conflict_prompt": "The choir rehearsed weekly starting in September and performed their concert in June 2010.",
      "question": "When did the choir perform their concert?",
      "options": [
        "A. June 2010",
        "B. September 2011",
        "C. November 2011",
        "D. December 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The clinical trial recruitment began in 2014 and the final analysis was published in 2017.",
      "conflict_prompt": "The clinical trial recruitment began in 2014 and the final analysis was published in 2012.",
      "question": "When was the final analysis published?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2016",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer premiered his concerto in Vienna in 1901 after finishing orchestration in 1900.",
      "conflict_prompt": "The composer premiered his concerto in Vienna in 1901 after finishing orchestration in 1903.",
      "question": "When did he finish orchestrating the concerto?",
      "options": [
        "A. 1899",
        "B. 1900",
        "C. 1902",
        "D. 1903"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon registration opened on January 1 and the race was run on April 12.",
      "conflict_prompt": "The marathon registration opened on January 1 and the race was run on December 31 of the previous year.",
      "question": "When was the race run?",
      "options": [
        "A. December 31 of the previous year",
        "B. January 1",
        "C. March 10",
        "D. April 12"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeological team uncovered pottery shards in 2002 and carbon-dated them to around 800 CE in a 2003 report.",
      "conflict_prompt": "The archaeological team uncovered pottery shards in 2002 and carbon-dated them to around 800 CE in a 1999 report.",
      "question": "When was the carbon-dating report published?",
      "options": [
        "A. 1999",
        "B. 2000",
        "C. 2002",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer first sketched the melody in 1765 and finalized the quartets in 1770.",
      "conflict_prompt": "The composer first sketched the melody in 1765 and finalized the quartets in 1755.",
      "question": "When did he finalize the quartets?",
      "options": [
        "A. 1755",
        "B. 1765",
        "C. 1768",
        "D. 1770"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival's planning committee formed in November 2013 and the event took place in May 2014.",
      "conflict_prompt": "The festival's planning committee formed in November 2013 and the event took place in August 2012.",
      "question": "When did the event take place?",
      "options": [
        "A. August 2012",
        "B. November 2013",
        "C. April 2014",
        "D. May 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city introduced the new recycling program on January 1, 2015, and yard waste collection began in March 2015.",
      "conflict_prompt": "The city introduced the new recycling program on January 1, 2015, and yard waste collection began in November 2014.",
      "question": "When did yard waste collection begin?",
      "options": [
        "A. November 2014",
        "B. January 1, 2015",
        "C. February 2015",
        "D. March 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The biologist tagged the whales in 2006 and observed migration patterns over the next three years.",
      "conflict_prompt": "The biologist tagged the whales in 2006 and observed migration patterns over the next three years starting in 2002.",
      "question": "When did the biologist begin observing migration patterns after tagging?",
      "options": [
        "A. 2002",
        "B. 2006",
        "C. 2007",
        "D. Over the next three years after 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The station began broadcasting in 1954 and transitioned to stereo broadcasts in 1978.",
      "conflict_prompt": "The station began broadcasting in 1954 and transitioned to stereo broadcasts in 1949.",
      "question": "When did the station transition to stereo broadcasts?",
      "options": [
        "A. 1949",
        "B. 1954",
        "C. 1965",
        "D. 1978"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation permit was issued on February 2, 2010, and work commenced a week later on February 9, 2010.",
      "conflict_prompt": "The renovation permit was issued on February 2, 2010, and work commenced a week earlier on January 25, 2009.",
      "question": "When did the renovation work commence?",
      "options": [
        "A. January 25, 2009",
        "B. February 2, 2010",
        "C. February 9, 2010",
        "D. March 1, 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer rehearsed with the orchestra in June and recorded the performance in July 1992.",
      "conflict_prompt": "The composer rehearsed with the orchestra in June and recorded the performance in April 1990.",
      "question": "When was the performance recorded?",
      "options": [
        "A. April 1990",
        "B. June 1992",
        "C. July 1992",
        "D. August 1993"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The restoration of the fresco began in 2000 and was completed by 2003.",
      "conflict_prompt": "The restoration of the fresco began in 2000 and was completed in 1995.",
      "question": "By what year was the fresco restoration completed?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2002",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author conducted interviews during 2016 and released the article in January 2017.",
      "conflict_prompt": "The author conducted interviews during 2016 and released the article in December 2015.",
      "question": "When was the article released?",
      "options": [
        "A. December 2015",
        "B. 2016",
        "C. January 2017",
        "D. March 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater renovation started in 1995 and reopened to audiences in 1998.",
      "conflict_prompt": "The theater renovation started in 1995 and reopened to audiences in 1990.",
      "question": "When did the theater reopen to audiences?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 1996",
        "D. 1998"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery is open on Sundays. It bakes fresh croissants every Sunday morning.",
      "conflict_prompt": "The bakery is not open on Sundays. It bakes fresh croissants every Sunday morning.",
      "question": "Is the bakery open on Sundays?",
      "options": [
        "A. No",
        "B. Yes",
        "C. Only for pre-orders",
        "D. Only on holiday Sundays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Marcus is a certified electrician who inspects home wiring weekly.",
      "conflict_prompt": "Marcus is not a certified electrician who inspects home wiring weekly.",
      "question": "What is Marcus's profession?",
      "options": [
        "A. Plumber",
        "B. Carpenter",
        "C. Electrician",
        "D. Roofer"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city bus accepts contactless payment. Riders can tap their cards when boarding.",
      "conflict_prompt": "The city bus does not accept contactless payment. Riders can tap their cards when boarding.",
      "question": "Can riders use contactless payment when boarding?",
      "options": [
        "A. Yes",
        "B. No",
        "C. Only with cash",
        "D. Only with exact change"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Professor Kim teaches organic chemistry and holds office hours every Wednesday.",
      "conflict_prompt": "Professor Kim does not teach organic chemistry and holds office hours every Wednesday.",
      "question": "Which subject does Professor Kim teach?",
      "options": [
        "A. Organic chemistry",
        "B. Medieval literature",
        "C. Astrophysics",
        "D. Macroeconomics"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The library lends out bicycles to members. They must return them within 24 hours.",
      "conflict_prompt": "The library does not lend out bicycles to members. They must return them within 24 hours.",
      "question": "Does the library lend bicycles to members?",
      "options": [
        "A. No",
        "B. Yes",
        "C. Only during summer",
        "D. Only to staff"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Sofia owns a German shepherd that participates in agility competitions.",
      "conflict_prompt": "Sofia does not own a German shepherd that participates in agility competitions.",
      "question": "What kind of dog does Sofia own?",
      "options": [
        "A. Chihuahua",
        "B. Poodle",
        "C. German shepherd",
        "D. Beagle"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software automatically backs up user data every night at 02:00.",
      "conflict_prompt": "The software does not automatically back up user data every night at 02:00.",
      "question": "When does the software perform automatic backups?",
      "options": [
        "A. Every night at 02:00",
        "B. Every Friday at noon",
        "C. Every hour",
        "D. It never performs backups"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Olivia is a fluent Spanish speaker who translates documents for the firm.",
      "conflict_prompt": "Olivia is not a fluent Spanish speaker who translates documents for the firm.",
      "question": "What language is Olivia fluent in?",
      "options": [
        "A. French",
        "B. German",
        "C. Spanish",
        "D. Japanese"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hiking trail is marked with blue blazes and is suitable for beginners.",
      "conflict_prompt": "The hiking trail is not marked with blue blazes and is suitable for beginners.",
      "question": "What color blazes mark the hiking trail?",
      "options": [
        "A. Red",
        "B. Yellow",
        "C. Blue",
        "D. Green"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The concert hall enforces a strict no-photos policy during performances.",
      "conflict_prompt": "The concert hall does not enforce a strict no-photos policy during performances.",
      "question": "What is the concert hall's policy on photography during performances?",
      "options": [
        "A. No photos allowed",
        "B. Unlimited photos allowed",
        "C. Photos allowed only on weekdays",
        "D. Photos allowed from the balcony"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Dr. Patel administers COVID-19 vaccines at the clinic every Thursday.",
      "conflict_prompt": "Dr. Patel does not administer COVID-19 vaccines at the clinic every Thursday.",
      "question": "When does Dr. Patel administer COVID-19 vaccines at the clinic?",
      "options": [
        "A. Every Thursday",
        "B. Every Monday",
        "C. Two Saturdays a month",
        "D. The clinic does not provide vaccines"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community garden provides free seedlings to residents each spring.",
      "conflict_prompt": "The community garden does not provide free seedlings to residents each spring.",
      "question": "When does the community garden provide free seedlings?",
      "options": [
        "A. Each spring",
        "B. Every autumn",
        "C. Only in winter",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new phone model supports wireless charging and has a glass back.",
      "conflict_prompt": "The new phone model does not support wireless charging and has a glass back.",
      "question": "Does the new phone model support wireless charging?",
      "options": [
        "A. Yes",
        "B. No",
        "C. Only with a special case",
        "D. Only via third-party accessories"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Captain Rossi commands a cargo ship that travels across the Atlantic every month.",
      "conflict_prompt": "Captain Rossi does not command a cargo ship that travels across the Atlantic every month.",
      "question": "What does Captain Rossi command?",
      "options": [
        "A. A cargo ship",
        "B. A passenger train",
        "C. An airplane",
        "D. A space shuttle"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The preschool teaches basic sign language to toddlers during morning sessions.",
      "conflict_prompt": "The preschool does not teach basic sign language to toddlers during morning sessions.",
      "question": "What do toddlers learn at the preschool's morning sessions?",
      "options": [
        "A. Basic sign language",
        "B. Advanced calculus",
        "C. Professional coding",
        "D. Adult yoga"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The airline allows one carry-on bag per passenger and enforces size limits.",
      "conflict_prompt": "The airline does not allow one carry-on bag per passenger and enforces size limits.",
      "question": "How many carry-on bags does the airline allow per passenger?",
      "options": [
        "A. One",
        "B. Zero",
        "C. Three",
        "D. Unlimited"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Chef Ramirez prepares a gluten-free menu for diners with allergies.",
      "conflict_prompt": "Chef Ramirez does not prepare a gluten-free menu for diners with allergies.",
      "question": "What special menu does Chef Ramirez prepare?",
      "options": [
        "A. Gluten-free",
        "B. Raw food only",
        "C. Only desserts",
        "D. No special menus"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The solar panels produce enough electricity to power the entire community center.",
      "conflict_prompt": "The solar panels do not produce enough electricity to power the entire community center.",
      "question": "How much electricity do the solar panels produce?",
      "options": [
        "A. Enough to power the entire community center",
        "B. Enough to power a single light bulb",
        "C. No electricity at all",
        "D. Only for decorative lighting"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Nadia runs a charity that donates winter coats to homeless shelters each December.",
      "conflict_prompt": "Nadia does not run a charity that donates winter coats to homeless shelters each December.",
      "question": "What does Nadia's charity donate each December?",
      "options": [
        "A. Winter coats",
        "B. Beach umbrellas",
        "C. Gardening tools",
        "D. School textbooks"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The fitness tracker measures heart rate continuously during workouts.",
      "conflict_prompt": "The fitness tracker does not measure heart rate continuously during workouts.",
      "question": "What does the fitness tracker measure during workouts?",
      "options": [
        "A. Continuous heart rate",
        "B. Only steps",
        "C. Blood pressure only",
        "D. No physiological data"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The high school offers Advanced Placement calculus to juniors and seniors.",
      "conflict_prompt": "The high school does not offer Advanced Placement calculus to juniors and seniors.",
      "question": "Which AP course does the high school offer to upperclassmen?",
      "options": [
        "A. Calculus",
        "B. AP Ancient History",
        "C. AP Marine Biology",
        "D. No AP courses"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum displays a rare Ming dynasty vase in its Asian art wing.",
      "conflict_prompt": "The museum does not display a rare Ming dynasty vase in its Asian art wing.",
      "question": "What rare object is displayed in the Asian art wing?",
      "options": [
        "A. A Ming dynasty vase",
        "B. A Roman helmet",
        "C. A Mayan calendar",
        "D. A medieval tapestry"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood watch meets on the first Tuesday of every month at the community center.",
      "conflict_prompt": "The neighborhood watch does not meet on the first Tuesday of every month at the community center.",
      "question": "When does the neighborhood watch meet?",
      "options": [
        "A. First Tuesday of every month",
        "B. Every Saturday morning",
        "C. Only during summer",
        "D. They don't have meetings"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The online course includes weekly quizzes that contribute to the final grade.",
      "conflict_prompt": "The online course does not include weekly quizzes that contribute to the final grade.",
      "question": "What role do the weekly quizzes play in the online course?",
      "options": [
        "A. They contribute to the final grade",
        "B. They are optional and ungraded",
        "C. They are only for practice",
        "D. There are no quizzes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Julian volunteers as an emergency medical technician at the local ambulance service.",
      "conflict_prompt": "Julian does not volunteer as an emergency medical technician at the local ambulance service.",
      "question": "What volunteer role does Julian have?",
      "options": [
        "A. Emergency medical technician",
        "B. Park ranger",
        "C. Library assistant",
        "D. School bus driver"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The café serves a vegan breakfast menu every morning.",
      "conflict_prompt": "The café does not serve a vegan breakfast menu every morning.",
      "question": "What type of breakfast menu does the café serve every morning?",
      "options": [
        "A. Vegan",
        "B. Keto-only",
        "C. Seafood-based",
        "D. No breakfast menu"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software license grants users access to updates for one year.",
      "conflict_prompt": "The software license does not grant users access to updates for one year.",
      "question": "How long does the software license grant access to updates?",
      "options": [
        "A. One year",
        "B. One month",
        "C. Lifetime",
        "D. No updates included"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campsite provides potable water from a treated tap near the restrooms.",
      "conflict_prompt": "The campsite does not provide potable water from a treated tap near the restrooms.",
      "question": "Where does the campsite provide potable water?",
      "options": [
        "A. From a treated tap near the restrooms",
        "B. Only from a nearby river",
        "C. From bottled water vending machines only",
        "D. There is no potable water"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pension plan guarantees a fixed monthly benefit to retirees.",
      "conflict_prompt": "The pension plan does not guarantee a fixed monthly benefit to retirees.",
      "question": "What does the pension plan guarantee to retirees?",
      "options": [
        "A. A fixed monthly benefit",
        "B. No benefits at all",
        "C. Only medical insurance",
        "D. A one-time lump sum only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater sells discounted tickets to students with a valid ID.",
      "conflict_prompt": "The theater does not sell discounted tickets to students with a valid ID.",
      "question": "Who is eligible for discounted tickets at the theater?",
      "options": [
        "A. Students with a valid ID",
        "B. Only senior citizens",
        "C. Military personnel only",
        "D. No one; discounts aren't available"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The river cruise departs daily at 10:00 AM during summer months.",
      "conflict_prompt": "The river cruise does not depart daily at 10:00 AM during summer months.",
      "question": "When does the river cruise depart during summer months?",
      "options": [
        "A. Daily at 10:00 AM",
        "B. Only on weekends at noon",
        "C. Every other day at 08:00 AM",
        "D. It doesn't run during summer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum provides audio guides in English and French to visitors.",
      "conflict_prompt": "The museum does not provide audio guides in English and French to visitors.",
      "question": "Which languages are the museum's audio guides available in?",
      "options": [
        "A. English and French",
        "B. Only Mandarin",
        "C. German and Italian",
        "D. No audio guides are available"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The online banking app uses two-factor authentication to protect accounts.",
      "conflict_prompt": "The online banking app does not use two-factor authentication to protect accounts.",
      "question": "What security feature does the online banking app use?",
      "options": [
        "A. Two-factor authentication",
        "B. Single-factor only",
        "C. No authentication",
        "D. Hand-delivered passcodes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer harvests organic apples and sells them at the weekend market.",
      "conflict_prompt": "The farmer does not harvest organic apples and sells them at the weekend market.",
      "question": "What does the farmer sell at the weekend market?",
      "options": [
        "A. Organic apples",
        "B. Handmade furniture",
        "C. Fish",
        "D. Electronics"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The emergency exit is clearly marked and unlocked during business hours.",
      "conflict_prompt": "The emergency exit is not clearly marked and unlocked during business hours.",
      "question": "What is the status of the emergency exit during business hours?",
      "options": [
        "A. Clearly marked and unlocked",
        "B. Hidden and permanently locked",
        "C. Only accessible to staff",
        "D. Sealed off"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public pool requires swim caps to be worn by all swimmers.",
      "conflict_prompt": "The public pool does not require swim caps to be worn by all swimmers.",
      "question": "What is required of all swimmers at the public pool?",
      "options": [
        "A. Wearing swim caps",
        "B. Bringing their own towels",
        "C. Owning a membership",
        "D. No specific requirement"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The research lab stores biological samples at -80°C in ultra-low freezers.",
      "conflict_prompt": "The research lab does not store biological samples at -80°C in ultra-low freezers.",
      "question": "At what temperature does the lab store biological samples?",
      "options": [
        "A. -80°C in ultra-low freezers",
        "B. Room temperature",
        "C. 4°C in refrigerators",
        "D. Boiling point"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Hannah drives a hybrid vehicle that gets excellent fuel economy in the city.",
      "conflict_prompt": "Hannah does not drive a hybrid vehicle that gets excellent fuel economy in the city.",
      "question": "What type of vehicle does Hannah drive?",
      "options": [
        "A. A hybrid vehicle",
        "B. A heavy-duty truck",
        "C. A motorcycle",
        "D. A bicycle"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone app notifies users of severe weather alerts in their area.",
      "conflict_prompt": "The smartphone app does not notify users of severe weather alerts in their area.",
      "question": "What does the smartphone app notify users about?",
      "options": [
        "A. Severe weather alerts",
        "B. Stock prices only",
        "C. Local restaurant menus",
        "D. It sends no notifications"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The festival provides accessible seating for attendees with mobility impairments.",
      "conflict_prompt": "The festival does not provide accessible seating for attendees with mobility impairments.",
      "question": "What accommodation does the festival provide for attendees with mobility impairments?",
      "options": [
        "A. Accessible seating",
        "B. Free parking only",
        "C. Complimentary snacks",
        "D. No accommodations"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vineyard produces small-batch organic wine using sustainable practices.",
      "conflict_prompt": "The vineyard does not produce small-batch organic wine using sustainable practices.",
      "question": "What type of wine does the vineyard produce?",
      "options": [
        "A. Small-batch organic wine",
        "B. Mass-produced soda",
        "C. Distilled spirits only",
        "D. No alcoholic beverages"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's recycling program collects glass, paper, and plastic curbside every week.",
      "conflict_prompt": "The city's recycling program does not collect glass, paper, and plastic curbside every week.",
      "question": "Which materials does the city's recycling program collect curbside?",
      "options": [
        "A. Glass, paper, and plastic",
        "B. Food waste only",
        "C. Electronic waste only",
        "D. Hazardous chemicals"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The law firm offers pro bono legal services to low-income clients on Tuesdays.",
      "conflict_prompt": "The law firm does not offer pro bono legal services to low-income clients on Tuesdays.",
      "question": "When does the law firm offer pro bono services to low-income clients?",
      "options": [
        "A. Tuesdays",
        "B. Sundays",
        "C. Only by appointment",
        "D. They never offer pro bono services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The antique clock chimes every hour on the hour and has been running since 1920.",
      "conflict_prompt": "The antique clock does not chime every hour on the hour and has been running since 1920.",
      "question": "How often does the antique clock chime?",
      "options": [
        "A. Every hour on the hour",
        "B. Once a day",
        "C. Only on special occasions",
        "D. It doesn't chime at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare enforces a nut-free policy to protect children with allergies.",
      "conflict_prompt": "The daycare does not enforce a nut-free policy to protect children with allergies.",
      "question": "What policy does the daycare enforce to protect children with allergies?",
      "options": [
        "A. Nut-free policy",
        "B. No shoes indoors",
        "C. Mandatory uniforms",
        "D. No allergy protections"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The violinist performs a solo at the symphony's season opener next Saturday.",
      "conflict_prompt": "The violinist does not perform a solo at the symphony's season opener next Saturday.",
      "question": "What will the violinist do at the symphony's season opener?",
      "options": [
        "A. Perform a solo",
        "B. Conduct the orchestra",
        "C. Play only in the chorus",
        "D. Not attend"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum shop sells replica fossil casts for educational purposes.",
      "conflict_prompt": "The museum shop does not sell replica fossil casts for educational purposes.",
      "question": "What does the museum shop sell for educational purposes?",
      "options": [
        "A. Replica fossil casts",
        "B. Fresh produce",
        "C. Automotive parts",
        "D. Prescription medication"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city ordinance requires pet owners to register dogs and cats annually.",
      "conflict_prompt": "The city ordinance does not require pet owners to register dogs and cats annually.",
      "question": "What does the city ordinance require of pet owners?",
      "options": [
        "A. Register dogs and cats annually",
        "B. Only register exotic pets",
        "C. No registration at all",
        "D. Register plants instead"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers' market accepts SNAP benefits for eligible purchases.",
      "conflict_prompt": "The farmers' market does not accept SNAP benefits for eligible purchases.",
      "question": "What payment option does the farmers' market accept for eligible purchases?",
      "options": [
        "A. SNAP benefits",
        "B. Only credit cards",
        "C. Cryptocurrency only",
        "D. Cash only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden offers guided tours highlighting native plant species every Sunday.",
      "conflict_prompt": "The botanical garden does not offer guided tours highlighting native plant species every Sunday.",
      "question": "What do the botanical garden's Sunday tours highlight?",
      "options": [
        "A. Native plant species",
        "B. Urban architecture",
        "C. Marine life",
        "D. Local cuisine"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pediatric clinic maintains a separate waiting area for sick children.",
      "conflict_prompt": "The pediatric clinic does not maintain a separate waiting area for sick children.",
      "question": "What special arrangement does the pediatric clinic have for waiting children?",
      "options": [
        "A. A separate waiting area for sick children",
        "B. A shared waiting area only",
        "C. Virtual waiting room only",
        "D. No waiting area at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The subscription includes access to premium articles and an ad-free reading experience.",
      "conflict_prompt": "The subscription does not include access to premium articles and an ad-free reading experience.",
      "question": "What benefits does the subscription include?",
      "options": [
        "A. Access to premium articles and ad-free reading",
        "B. Only printed newsletters",
        "C. Free coffee every month",
        "D. No benefits"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recycling center accepts old electronics for proper disposal and recycling.",
      "conflict_prompt": "The recycling center does not accept old electronics for proper disposal and recycling.",
      "question": "What does the recycling center accept for proper disposal?",
      "options": [
        "A. Old electronics",
        "B. Live animals",
        "C. Hazardous nuclear waste",
        "D. Only paper"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university grants tenure to professors after a successful review process.",
      "conflict_prompt": "The university does not grant tenure to professors after a successful review process.",
      "question": "What happens to professors after a successful review process at the university?",
      "options": [
        "A. They are granted tenure",
        "B. They are immediately terminated",
        "C. They are reassigned to janitorial duties",
        "D. Nothing changes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The parking garage charges by the hour and offers a monthly flat-rate pass.",
      "conflict_prompt": "The parking garage does not charge by the hour and offers a monthly flat-rate pass.",
      "question": "How does the parking garage charge for parking?",
      "options": [
        "A. By the hour, and it offers a monthly flat-rate pass",
        "B. Only by the day with no monthly pass",
        "C. Free parking only",
        "D. By a donation system"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art class requires students to bring their own sketchbooks and pencils.",
      "conflict_prompt": "The art class does not require students to bring their own sketchbooks and pencils.",
      "question": "What materials are students required to bring to the art class?",
      "options": [
        "A. Sketchbooks and pencils",
        "B. Laptops and headphones",
        "C. Musical instruments",
        "D. No materials are required"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public park bans grilling on the grass and provides designated barbecue areas.",
      "conflict_prompt": "The public park does not ban grilling on the grass and provides designated barbecue areas.",
      "question": "What does the park provide for people who want to barbecue?",
      "options": [
        "A. Designated barbecue areas",
        "B. Free charcoal everywhere",
        "C. Indoor grilling halls",
        "D. No support for barbecuing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The courier delivers packages five days a week to residential addresses.",
      "conflict_prompt": "The courier does not deliver packages five days a week to residential addresses.",
      "question": "How often does the courier deliver to residential addresses?",
      "options": [
        "A. Five days a week",
        "B. Once a month",
        "C. Only on weekends",
        "D. It never delivers to residences"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone's camera captures 4K video at 60 frames per second.",
      "conflict_prompt": "The smartphone's camera does not capture 4K video at 60 frames per second.",
      "question": "What video capability does the smartphone's camera have?",
      "options": [
        "A. 4K at 60 fps",
        "B. 720p at 30 fps only",
        "C. No video recording",
        "D. Only time-lapse mode"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's health department provides free flu shots at community clinics each fall.",
      "conflict_prompt": "The city's health department does not provide free flu shots at community clinics each fall.",
      "question": "When does the health department provide free flu shots at community clinics?",
      "options": [
        "A. Each fall",
        "B. Only in spring",
        "C. Year-round daily",
        "D. They don't provide flu shots"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain rescue team trains weekly in rope rescue techniques.",
      "conflict_prompt": "The mountain rescue team does not train weekly in rope rescue techniques.",
      "question": "How often does the mountain rescue team train in rope rescue techniques?",
      "options": [
        "A. Weekly",
        "B. Once a year",
        "C. Never",
        "D. Only when called for a rescue"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bookstore hosts author readings every month in the evening.",
      "conflict_prompt": "The bookstore does not host author readings every month in the evening.",
      "question": "How frequently does the bookstore host author readings?",
      "options": [
        "A. Every month",
        "B. Every day",
        "C. Once every five years",
        "D. It never hosts readings"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant serves a seasonal tasting menu that changes quarterly.",
      "conflict_prompt": "The restaurant does not serve a seasonal tasting menu that changes quarterly.",
      "question": "How often does the restaurant change its seasonal tasting menu?",
      "options": [
        "A. Quarterly",
        "B. Daily",
        "C. Never",
        "D. Only during holidays"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pediatric dentist uses low-radiation X-rays for children's dental exams.",
      "conflict_prompt": "The pediatric dentist does not use low-radiation X-rays for children's dental exams.",
      "question": "What type of X-rays does the pediatric dentist use for children's exams?",
      "options": [
        "A. Low-radiation X-rays",
        "B. Full-body CT scans",
        "C. No imaging at all",
        "D. Only MRI"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry accommodates bicycles free of charge during weekday crossings.",
      "conflict_prompt": "The ferry does not accommodate bicycles free of charge during weekday crossings.",
      "question": "How does the ferry handle bicycles during weekday crossings?",
      "options": [
        "A. Accommodates bicycles free of charge",
        "B. Charges for bicycles only on weekends",
        "C. Bicycles are not allowed at all",
        "D. Only motorbikes are allowed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city offers free legal clinics to tenants facing eviction every Wednesday evening.",
      "conflict_prompt": "The city does not offer free legal clinics to tenants facing eviction every Wednesday evening.",
      "question": "When are free legal clinics for tenants facing eviction offered?",
      "options": [
        "A. Every Wednesday evening",
        "B. Only on the last day of the month",
        "C. Weekday mornings only",
        "D. They are not offered"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater company rehearses three times a week in preparation for the play.",
      "conflict_prompt": "The theater company does not rehearse three times a week in preparation for the play.",
      "question": "How often does the theater company rehearse for the play?",
      "options": [
        "A. Three times a week",
        "B. Once a month",
        "C. Only on opening night",
        "D. They don't rehearse"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city museum provides tactile exhibits for visitors who are blind or have low vision.",
      "conflict_prompt": "The city museum does not provide tactile exhibits for visitors who are blind or have low vision.",
      "question": "What accommodation does the city museum provide for visitors who are blind or have low vision?",
      "options": [
        "A. Tactile exhibits",
        "B. Visual-only displays",
        "C. No accommodations",
        "D. Free sunglasses"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The emergency shelter offers meals and temporary housing to families during extreme weather.",
      "conflict_prompt": "The emergency shelter does not offer meals and temporary housing to families during extreme weather.",
      "question": "What services does the emergency shelter provide during extreme weather?",
      "options": [
        "A. Meals and temporary housing for families",
        "B. Only medical care",
        "C. Long-term apartments",
        "D. No services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses locally sourced flour to bake artisanal bread daily.",
      "conflict_prompt": "The bakery does not use locally sourced flour to bake artisanal bread daily.",
      "question": "What type of flour does the bakery use to bake artisanal bread daily?",
      "options": [
        "A. Locally sourced flour",
        "B. Imported synthetic flour",
        "C. No flour at all",
        "D. Only cornmeal"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's bike-share program requires users to wear helmets while riding.",
      "conflict_prompt": "The city's bike-share program does not require users to wear helmets while riding.",
      "question": "What safety requirement does the city's bike-share program have?",
      "options": [
        "A. Users must wear helmets",
        "B. Users must have a driver's license",
        "C. No safety requirements",
        "D. Only children must wear helmets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The greenhouse maintains a humid environment suitable for tropical orchids year-round.",
      "conflict_prompt": "The greenhouse does not maintain a humid environment suitable for tropical orchids year-round.",
      "question": "What environment does the greenhouse maintain for orchids?",
      "options": [
        "A. Humid environment suitable for tropical orchids year-round",
        "B. Dry desert conditions",
        "C. Arctic tundra conditions",
        "D. No controlled environment"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local radio station broadcasts community news and emergency alerts 24 hours a day.",
      "conflict_prompt": "The local radio station does not broadcast community news and emergency alerts 24 hours a day.",
      "question": "What does the local radio station broadcast 24 hours a day?",
      "options": [
        "A. Community news and emergency alerts",
        "B. Only music with no news",
        "C. Advertisements only",
        "D. Silent signal only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The dog shelter vaccinates all dogs against rabies before adoption.",
      "conflict_prompt": "The dog shelter does not vaccinate all dogs against rabies before adoption.",
      "question": "What vaccination policy does the dog shelter have before adoption?",
      "options": [
        "A. Vaccinates all dogs against rabies",
        "B. Vaccinates only cats",
        "C. No vaccinations at all",
        "D. Vaccinates against measles"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater requires audience members to silence their phones during performances.",
      "conflict_prompt": "The theater does not require audience members to silence their phones during performances.",
      "question": "What does the theater require of audience members during performances?",
      "options": [
        "A. Silence their phones",
        "B. Record the entire show",
        "C. Bring flashlights",
        "D. Wear costumes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The swimming instructor gives private lessons to beginners twice a week.",
      "conflict_prompt": "The swimming instructor does not give private lessons to beginners twice a week.",
      "question": "How often does the swimming instructor give private lessons to beginners?",
      "options": [
        "A. Twice a week",
        "B. Once a month",
        "C. Every day",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local bakery offers a discount to customers who bring reusable containers.",
      "conflict_prompt": "The local bakery does not offer a discount to customers who bring reusable containers.",
      "question": "What incentive does the bakery offer to customers who bring reusable containers?",
      "options": [
        "A. A discount",
        "B. Free coffee only",
        "C. A raffle ticket",
        "D. No incentive"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hiking club schedules a family-friendly walk every first Saturday of the month.",
      "conflict_prompt": "The hiking club does not schedule a family-friendly walk every first Saturday of the month.",
      "question": "When does the hiking club schedule a family-friendly walk?",
      "options": [
        "A. First Saturday of the month",
        "B. Last Sunday of the year",
        "C. Daily at dawn",
        "D. They don't schedule family walks"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The aquarium feeds the sharks twice daily as part of its public demonstrations.",
      "conflict_prompt": "The aquarium does not feed the sharks twice daily as part of its public demonstrations.",
      "question": "How often does the aquarium feed the sharks during public demonstrations?",
      "options": [
        "A. Twice daily",
        "B. Once a month",
        "C. Only at night",
        "D. It doesn't feed them publicly"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum grants free admission to children under 12 every Sunday.",
      "conflict_prompt": "The museum does not grant free admission to children under 12 every Sunday.",
      "question": "Who gets free admission to the museum every Sunday?",
      "options": [
        "A. Children under 12",
        "B. Senior citizens only",
        "C. College students",
        "D. No one"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community center provides free Wi-Fi in its lobby for visitors.",
      "conflict_prompt": "The community center does not provide free Wi-Fi in its lobby for visitors.",
      "question": "What amenity does the community center provide in its lobby?",
      "options": [
        "A. Free Wi-Fi",
        "B. Free bicycles",
        "C. Free office space",
        "D. No amenities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city police department runs a youth mentorship program each summer.",
      "conflict_prompt": "The city police department does not run a youth mentorship program each summer.",
      "question": "What program does the city police department run each summer?",
      "options": [
        "A. A youth mentorship program",
        "B. A car auction",
        "C. An adult-only program",
        "D. No programs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pastry chef prepares a special gluten-free cake for weddings upon request.",
      "conflict_prompt": "The pastry chef does not prepare a special gluten-free cake for weddings upon request.",
      "question": "What special cake does the pastry chef prepare upon request for weddings?",
      "options": [
        "A. A gluten-free cake",
        "B. A raw vegan cake only",
        "C. A savory pie",
        "D. No special orders"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community theater offers discounted rehearsal space to student groups.",
      "conflict_prompt": "The community theater does not offer discounted rehearsal space to student groups.",
      "question": "Who receives discounted rehearsal space at the community theater?",
      "options": [
        "A. Student groups",
        "B. Corporations only",
        "C. No one",
        "D. Only visiting celebrities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic prescribes antibiotics only when bacterial infection is confirmed.",
      "conflict_prompt": "The clinic does not prescribe antibiotics only when bacterial infection is confirmed.",
      "question": "Under what condition does the clinic prescribe antibiotics?",
      "options": [
        "A. When a bacterial infection is confirmed",
        "B. For all viral colds",
        "C. For headaches only",
        "D. They never prescribe antibiotics"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry terminal posts real-time arrival information on digital boards for passengers.",
      "conflict_prompt": "The ferry terminal does not post real-time arrival information on digital boards for passengers.",
      "question": "What information does the ferry terminal post on digital boards?",
      "options": [
        "A. Real-time arrival information",
        "B. Stock market updates only",
        "C. Movie showtimes",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The park ranger enforces leash laws and issues citations when necessary.",
      "conflict_prompt": "The park ranger does not enforce leash laws and issues citations when necessary.",
      "question": "What does the park ranger enforce and issue when necessary?",
      "options": [
        "A. Leash laws and citations",
        "B. Building permits",
        "C. Fishing licenses only",
        "D. No enforcement actions"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local band performs at the waterfront amphitheater every Friday night during the season.",
      "conflict_prompt": "The local band does not perform at the waterfront amphitheater every Friday night during the season.",
      "question": "When does the local band perform at the waterfront amphitheater during the season?",
      "options": [
        "A. Every Friday night",
        "B. Every Monday morning",
        "C. Only on holidays",
        "D. They don't perform there"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city provides emergency generators to critical facilities during power outages.",
      "conflict_prompt": "The city does not provide emergency generators to critical facilities during power outages.",
      "question": "What does the city provide to critical facilities during power outages?",
      "options": [
        "A. Emergency generators",
        "B. Solar-powered bicycles",
        "C. Free heaters only",
        "D. Nothing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The state's DMV processes license renewals online and accepts electronic signatures.",
      "conflict_prompt": "The state's DMV does not process license renewals online and accept electronic signatures.",
      "question": "How does the state's DMV process license renewals?",
      "options": [
        "A. Online with electronic signatures",
        "B. Only in person with handwritten signatures",
        "C. By mail only",
        "D. They don't process renewals"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art museum loans works to other institutions for special exhibitions.",
      "conflict_prompt": "The art museum does not loan works to other institutions for special exhibitions.",
      "question": "What does the art museum do with some of its works for special exhibitions?",
      "options": [
        "A. Loans them to other institutions",
        "B. Destroys them",
        "C. Sells them at auctions weekly",
        "D. Keeps them locked permanently"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's volunteer firefighters respond to local emergencies and provide fire safety education.",
      "conflict_prompt": "The city's volunteer firefighters do not respond to local emergencies and provide fire safety education.",
      "question": "What roles do the city's volunteer firefighters perform?",
      "options": [
        "A. Respond to emergencies and provide fire safety education",
        "B. Only manage parking meters",
        "C. Produce local news broadcasts",
        "D. They have no public roles"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local grocer stocks fresh, sustainably sourced seafood in the deli case daily.",
      "conflict_prompt": "The local grocer does not stock fresh, sustainably sourced seafood in the deli case daily.",
      "question": "What does the local grocer stock in the deli case daily?",
      "options": [
        "A. Fresh, sustainably sourced seafood",
        "B. Only canned goods",
        "C. Electronics",
        "D. Furniture"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood association organizes a street cleanup twice a year in spring and fall.",
      "conflict_prompt": "The neighborhood association does not organize a street cleanup twice a year in spring and fall.",
      "question": "How often does the neighborhood association organize a street cleanup?",
      "options": [
        "A. Twice a year in spring and fall",
        "B. Weekly",
        "C. Once every decade",
        "D. They never organize cleanups"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The aquarium's education team leads school group workshops on marine conservation.",
      "conflict_prompt": "The aquarium's education team does not lead school group workshops on marine conservation.",
      "question": "What do the aquarium's education team workshops focus on for school groups?",
      "options": [
        "A. Marine conservation",
        "B. Automotive repair",
        "C. Culinary arts",
        "D. They don't lead workshops"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer's cooperative offers bulk purchasing discounts to member farms.",
      "conflict_prompt": "The farmer's cooperative does not offer bulk purchasing discounts to member farms.",
      "question": "What benefit does the farmer's cooperative offer to its member farms?",
      "options": [
        "A. Bulk purchasing discounts",
        "B. Free luxury vacations",
        "C. Individual tax exemptions",
        "D. No benefits"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic provides confidential counseling services to teens at no cost.",
      "conflict_prompt": "The clinic does not provide confidential counseling services to teens at no cost.",
      "question": "What service does the clinic provide to teens?",
      "options": [
        "A. Confidential counseling at no cost",
        "B. Driving lessons",
        "C. Paid cosmetic procedures only",
        "D. No teen services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city supports community gardens by distributing free compost to volunteers.",
      "conflict_prompt": "The city does not support community gardens by distributing free compost to volunteers.",
      "question": "How does the city support community gardens?",
      "options": [
        "A. By distributing free compost to volunteers",
        "B. By banning gardening activities",
        "C. By charging high fees for compost",
        "D. It provides no support"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood coffee shop closes at 6:00 PM on weekdays.",
      "conflict_prompt": "The neighborhood coffee shop does not close at 6:00 PM on weekdays.",
      "question": "When does the neighborhood coffee shop close on weekdays?",
      "options": [
        "A. 6:00 PM",
        "B. Midnight",
        "C. 9:00 AM",
        "D. It is open 24 hours"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's bike lanes are painted green to increase visibility to drivers.",
      "conflict_prompt": "The city's bike lanes are not painted green to increase visibility to drivers.",
      "question": "What color are the city's bike lanes painted to increase visibility?",
      "options": [
        "A. Green",
        "B. Blue",
        "C. Red",
        "D. Unpainted"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local animal shelter spays or neuters animals before adoption.",
      "conflict_prompt": "The local animal shelter does not spay or neuter animals before adoption.",
      "question": "What medical procedure does the shelter perform before adoption?",
      "options": [
        "A. Spays or neuters animals",
        "B. Full dental implants",
        "C. Cosmetic surgeries only",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university's registrar issues official transcripts electronically upon request.",
      "conflict_prompt": "The university's registrar does not issue official transcripts electronically upon request.",
      "question": "How does the registrar issue official transcripts upon request?",
      "options": [
        "A. Electronically",
        "B. Only by carrier pigeon",
        "C. Not at all",
        "D. Only hand-delivered"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool offers lap swimming hours early in the morning before public hours.",
      "conflict_prompt": "The community pool does not offer lap swimming hours early in the morning before public hours.",
      "question": "When does the community pool offer lap swimming hours?",
      "options": [
        "A. Early in the morning before public hours",
        "B. Only at midnight",
        "C. Never",
        "D. Only during winter"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public university provides need-based scholarships to eligible students each year.",
      "conflict_prompt": "The public university does not provide need-based scholarships to eligible students each year.",
      "question": "What type of scholarships does the public university provide to eligible students?",
      "options": [
        "A. Need-based scholarships",
        "B. Scholarships for lottery winners only",
        "C. No scholarships available",
        "D. Only athletic scholarships"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood pharmacy offers flu shots without an appointment during clinic hours.",
      "conflict_prompt": "The neighborhood pharmacy does not offer flu shots without an appointment during clinic hours.",
      "question": "How does the neighborhood pharmacy offer flu shots during clinic hours?",
      "options": [
        "A. Without an appointment",
        "B. Only by appointment months in advance",
        "C. It does not offer flu shots",
        "D. Only to staff"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The fire extinguisher in the lobby is inspected monthly and is fully charged.",
      "conflict_prompt": "The fire extinguisher in the lobby is not inspected monthly and is fully charged.",
      "question": "How often is the lobby fire extinguisher inspected?",
      "options": [
        "A. Monthly",
        "B. Once every five years",
        "C. Never",
        "D. Daily"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college runs a job placement center that helps graduates find employment.",
      "conflict_prompt": "The community college does not run a job placement center that helps graduates find employment.",
      "question": "What service does the community college provide to help graduates?",
      "options": [
        "A. A job placement center",
        "B. A daycare placement service",
        "C. A grocery delivery service",
        "D. No graduate services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater offers closed-captioned performances once a month for deaf or hard-of-hearing patrons.",
      "conflict_prompt": "The theater does not offer closed-captioned performances once a month for deaf or hard-of-hearing patrons.",
      "question": "How often does the theater offer closed-captioned performances?",
      "options": [
        "A. Once a month",
        "B. Every day",
        "C. Once a year",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community orchestra rehearses children's repertoire during Saturday morning sessions.",
      "conflict_prompt": "The community orchestra does not rehearse children's repertoire during Saturday morning sessions.",
      "question": "When does the community orchestra rehearse children's repertoire?",
      "options": [
        "A. Saturday morning sessions",
        "B. Weekday evenings only",
        "C. Late at night",
        "D. They don't rehearse children's repertoire"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hospital operates a 24-hour emergency department that accepts walk-in patients.",
      "conflict_prompt": "The hospital does not operate a 24-hour emergency department that accepts walk-in patients.",
      "question": "What type of department does the hospital operate that accepts walk-in patients?",
      "options": [
        "A. A 24-hour emergency department",
        "B. A weekend-only clinic",
        "C. An appointment-only surgery center",
        "D. No medical departments"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The food bank distributes nonperishable groceries to families every Saturday morning.",
      "conflict_prompt": "The food bank does not distribute nonperishable groceries to families every Saturday morning.",
      "question": "When does the food bank distribute nonperishable groceries to families?",
      "options": [
        "A. Every Saturday morning",
        "B. Only on public holidays",
        "C. Monthly on the 31st",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's public transportation system offers reduced fares to seniors and students.",
      "conflict_prompt": "The city's public transportation system does not offer reduced fares to seniors and students.",
      "question": "Who is eligible for reduced fares on the city's public transportation system?",
      "options": [
        "A. Seniors and students",
        "B. Only professional athletes",
        "C. No one",
        "D. Only city employees"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood hardware store cuts keys on-site for customers while they wait.",
      "conflict_prompt": "The neighborhood hardware store does not cut keys on-site for customers while they wait.",
      "question": "What service does the hardware store provide for customers while they wait?",
      "options": [
        "A. Key cutting on-site",
        "B. Car painting",
        "C. Tree felling",
        "D. No on-site services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The marine biology program tags sea turtles to monitor migration patterns.",
      "conflict_prompt": "The marine biology program does not tag sea turtles to monitor migration patterns.",
      "question": "What does the marine biology program do to monitor sea turtle migration?",
      "options": [
        "A. Tag sea turtles",
        "B. Track only sharks",
        "C. Plant coral reefs",
        "D. It does not conduct research"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local dentist offers free dental screenings for children during Health Week.",
      "conflict_prompt": "The local dentist does not offer free dental screenings for children during Health Week.",
      "question": "What service does the local dentist offer for children during Health Week?",
      "options": [
        "A. Free dental screenings",
        "B. Free haircuts",
        "C. Paid orthodontics only",
        "D. No services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community center organizes a free summer lunch program for kids under 18.",
      "conflict_prompt": "The community center does not organize a free summer lunch program for kids under 18.",
      "question": "Who benefits from the community center's summer lunch program?",
      "options": [
        "A. Kids under 18",
        "B. Only seniors",
        "C. College professors",
        "D. No one"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new library wing contains a dedicated teen study area with computers.",
      "conflict_prompt": "The new library wing does not contain a dedicated teen study area with computers.",
      "question": "What facility does the new library wing include for teens?",
      "options": [
        "A. A dedicated study area with computers",
        "B. A car repair workshop",
        "C. An animal barn",
        "D. No teen facilities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's streetlights are motion-activated in low-traffic residential areas to save energy.",
      "conflict_prompt": "The city's streetlights are not motion-activated in low-traffic residential areas to save energy.",
      "question": "How are the city's streetlights operated in low-traffic residential areas to save energy?",
      "options": [
        "A. Motion-activated",
        "B. Permanently on",
        "C. Solar lanterns only",
        "D. Candles"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local radio station plays indie music during its midday programming block.",
      "conflict_prompt": "The local radio station does not play indie music during its midday programming block.",
      "question": "What genre of music does the local radio station play during midday?",
      "options": [
        "A. Indie music",
        "B. Classical only",
        "C. Heavy metal exclusively",
        "D. No music at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The volunteer gardeners at the park plant native wildflowers to support pollinators.",
      "conflict_prompt": "The volunteer gardeners at the park do not plant native wildflowers to support pollinators.",
      "question": "What do volunteer gardeners plant in the park to support pollinators?",
      "options": [
        "A. Native wildflowers",
        "B. Non-flowering shrubs only",
        "C. Palm trees",
        "D. Concrete sculptures"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The post office offers passport photo services and processes applications by appointment.",
      "conflict_prompt": "The post office does not offer passport photo services and process applications by appointment.",
      "question": "What service related to passports does the post office offer?",
      "options": [
        "A. Passport photo services and application processing by appointment",
        "B. Only visa issuance",
        "C. No passport-related services",
        "D. Passport printing on demand"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local bookstore hosts a children's story hour every Wednesday afternoon.",
      "conflict_prompt": "The local bookstore does not host a children's story hour every Wednesday afternoon.",
      "question": "When does the local bookstore host a children's story hour?",
      "options": [
        "A. Every Wednesday afternoon",
        "B. Only on New Year's Day",
        "C. Twice a year",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers' cooperative provides cold storage facilities for member harvests during peak season.",
      "conflict_prompt": "The farmers' cooperative does not provide cold storage facilities for member harvests during peak season.",
      "question": "What facility does the farmers' cooperative provide during peak season?",
      "options": [
        "A. Cold storage facilities",
        "B. Sports arenas",
        "C. Office cubicles",
        "D. No facilities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The animal rescue organization rehabilitates injured birds and releases them when healthy.",
      "conflict_prompt": "The animal rescue organization does not rehabilitate injured birds and release them when healthy.",
      "question": "What does the animal rescue organization do with injured birds?",
      "options": [
        "A. Rehabilitates and releases them when healthy",
        "B. Keeps them permanently as pets",
        "C. Sells them immediately",
        "D. Ignores injured birds"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood watch posts emergency contact numbers on community bulletin boards.",
      "conflict_prompt": "The neighborhood watch does not post emergency contact numbers on community bulletin boards.",
      "question": "What information does the neighborhood watch post on community bulletin boards?",
      "options": [
        "A. Emergency contact numbers",
        "B. Restaurant menus only",
        "C. Classified ads exclusively",
        "D. No information at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community theater offers acting workshops for teens during school breaks.",
      "conflict_prompt": "The community theater does not offer acting workshops for teens during school breaks.",
      "question": "When does the community theater offer acting workshops for teens?",
      "options": [
        "A. During school breaks",
        "B. Only during finals week",
        "C. Every day of the year",
        "D. It doesn't offer workshops"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city enforces a curfew for minors after 11:00 PM on weekends to improve safety.",
      "conflict_prompt": "The city does not enforce a curfew for minors after 11:00 PM on weekends to improve safety.",
      "question": "What curfew does the city enforce for minors on weekends?",
      "options": [
        "A. After 11:00 PM",
        "B. After 6:00 AM",
        "C. There is no curfew",
        "D. Only on weekdays"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool offers swimming lessons for toddlers three times a week.",
      "conflict_prompt": "The community pool does not offer swimming lessons for toddlers three times a week.",
      "question": "How often does the community pool offer swimming lessons for toddlers?",
      "options": [
        "A. Three times a week",
        "B. Once a year",
        "C. Everyday at midnight",
        "D. It does not offer toddler lessons"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public library provides free notary services to cardholders on weekdays.",
      "conflict_prompt": "The public library does not provide free notary services to cardholders on weekdays.",
      "question": "What service does the public library provide free to cardholders on weekdays?",
      "options": [
        "A. Notary services",
        "B. Car repair",
        "C. Veterinary care",
        "D. No services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood bakery closes at 6 PM every day.",
      "conflict_prompt": "The neighborhood bakery does not close at 6 PM every day.",
      "question": "What time does the bakery close?",
      "options": [
        "A. 8 PM",
        "B. 6 PM",
        "C. 4 PM",
        "D. It stays open 24 hours"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Dr. Miller works full-time at the city hospital.",
      "conflict_prompt": "Dr. Miller does not work full-time at the city hospital.",
      "question": "What is Dr. Miller's employment status at the city hospital?",
      "options": [
        "A. Part-time contractor",
        "B. Works at a clinic, not the hospital",
        "C. Full-time employee",
        "D. Retired"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The apartment building has a rooftop garden.",
      "conflict_prompt": "The apartment building does not have a rooftop garden.",
      "question": "Which amenity does the apartment building include?",
      "options": [
        "A. Rooftop garden",
        "B. Indoor skating rink",
        "C. Private bowling alley",
        "D. None of the above"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Sofia's flight lands at 10:30 AM on Tuesday.",
      "conflict_prompt": "Sofia's flight does not land at 10:30 AM on Tuesday.",
      "question": "When does Sofia's flight arrive?",
      "options": [
        "A. 9:00 PM Tuesday",
        "B. 10:30 AM Tuesday",
        "C. 6:00 AM Wednesday",
        "D. Departure time only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The library opens at 9 AM on weekdays.",
      "conflict_prompt": "The library does not open at 9 AM on weekdays.",
      "question": "At what time does the library open on weekdays?",
      "options": [
        "A. 7 AM",
        "B. 9 AM",
        "C. 12 PM",
        "D. It is closed on weekdays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The company requires employees to complete safety training annually.",
      "conflict_prompt": "The company does not require employees to complete safety training annually.",
      "question": "How often must employees complete safety training?",
      "options": [
        "A. Monthly",
        "B. Annually",
        "C. Only during onboarding",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coffee shop uses organic beans in all its espresso drinks.",
      "conflict_prompt": "The coffee shop does not use organic beans in all its espresso drinks.",
      "question": "What type of beans does the coffee shop use for its espresso drinks?",
      "options": [
        "A. Organic beans for all espresso drinks",
        "B. Synthetic flavoring only",
        "C. Decaffeinated beans exclusively",
        "D. It uses instant coffee"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new highway bridge is toll-free for all vehicles.",
      "conflict_prompt": "The new highway bridge is not toll-free for all vehicles.",
      "question": "Is the new highway bridge free of tolls?",
      "options": [
        "A. Yes, toll-free for all vehicles",
        "B. Only free for motorcycles",
        "C. Only free for emergency vehicles",
        "D. No, tolls apply"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Ethan owns a black Labrador named Max.",
      "conflict_prompt": "Ethan does not own a black Labrador named Max.",
      "question": "What pet does Ethan own?",
      "options": [
        "A. A black Labrador named Max",
        "B. A goldfish",
        "C. No pets at all",
        "D. A parrot named Max"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conference room is equipped with a projector and video conferencing.",
      "conflict_prompt": "The conference room is not equipped with a projector and video conferencing.",
      "question": "Which equipment is available in the conference room?",
      "options": [
        "A. Projector and video conferencing",
        "B. Only a whiteboard",
        "C. A piano",
        "D. No equipment"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The river near town freezes every winter.",
      "conflict_prompt": "The river near town does not freeze every winter.",
      "question": "What happens to the river near town in winter?",
      "options": [
        "A. It dries up",
        "B. It freezes every winter",
        "C. It expands into a lake",
        "D. It becomes a waterfall"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public school offers Spanish as a foreign language course.",
      "conflict_prompt": "The public school does not offer Spanish as a foreign language course.",
      "question": "Which foreign language is taught at the public school?",
      "options": [
        "A. Mandarin",
        "B. Spanish",
        "C. Ancient Greek",
        "D. No foreign languages"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community center hosts yoga classes every Saturday morning.",
      "conflict_prompt": "The community center does not host yoga classes every Saturday morning.",
      "question": "When are the community center's yoga classes held?",
      "options": [
        "A. Every Friday night",
        "B. Every Saturday morning",
        "C. Only during summer",
        "D. They don't offer yoga"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone model includes a built-in heart rate monitor.",
      "conflict_prompt": "The smartphone model does not include a built-in heart rate monitor.",
      "question": "Which feature does the smartphone include?",
      "options": [
        "A. Built-in heart rate monitor",
        "B. Steam engine",
        "C. Rotating screen only",
        "D. No sensors at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Jackson paid his utilities bill online last week.",
      "conflict_prompt": "Jackson did not pay his utilities bill online last week.",
      "question": "How did Jackson settle his utilities bill last week?",
      "options": [
        "A. Paid online",
        "B. Paid in cash at the office",
        "C. Set up a payment plan",
        "D. He did not pay"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden opens free to visitors on the first Sunday of every month.",
      "conflict_prompt": "The botanical garden does not open free to visitors on the first Sunday of every month.",
      "question": "When is admission to the botanical garden free?",
      "options": [
        "A. First Sunday of every month",
        "B. Every Saturday",
        "C. Only on holidays",
        "D. It never offers free admission"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The electric car can travel 240 miles on a single charge.",
      "conflict_prompt": "The electric car cannot travel 240 miles on a single charge.",
      "question": "What is the electric car's single-charge range?",
      "options": [
        "A. 100 miles",
        "B. 240 miles",
        "C. 400 miles",
        "D. It runs on gasoline"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The historical exhibit displays artifacts from the 18th century.",
      "conflict_prompt": "The historical exhibit does not display artifacts from the 18th century.",
      "question": "What period do the artifacts in the exhibit come from?",
      "options": [
        "A. 18th century",
        "B. 21st century only",
        "C. Prehistoric era",
        "D. No artifacts are on display"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Maya cycles to work every weekday.",
      "conflict_prompt": "Maya does not cycle to work every weekday.",
      "question": "How does Maya commute to work on weekdays?",
      "options": [
        "A. Drives a car",
        "B. Rides public transit",
        "C. Cycles every weekday",
        "D. Works from home"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The town ordinance bans feeding wild deer in public parks.",
      "conflict_prompt": "The town ordinance does not ban feeding wild deer in public parks.",
      "question": "What does the town ordinance say about feeding wild deer in public parks?",
      "options": [
        "A. It is banned",
        "B. It is encouraged",
        "C. It is required by permit",
        "D. The ordinance is silent"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses walnuts in its banana bread recipe.",
      "conflict_prompt": "The bakery does not use walnuts in its banana bread recipe.",
      "question": "Which ingredient is included in the bakery's banana bread?",
      "options": [
        "A. Walnuts",
        "B. Almonds instead of walnuts",
        "C. No nuts ever",
        "D. Chocolate only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum requires visitors to book timed tickets in advance.",
      "conflict_prompt": "The museum does not require visitors to book timed tickets in advance.",
      "question": "What is required for visiting the museum?",
      "options": [
        "A. Timed tickets booked in advance",
        "B. Walk-in only with no tickets",
        "C. Membership card only",
        "D. There is no admission"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The office thermostat is set to 72 degrees Fahrenheit during work hours.",
      "conflict_prompt": "The office thermostat is not set to 72 degrees Fahrenheit during work hours.",
      "question": "What temperature is the office thermostat set to during work hours?",
      "options": [
        "A. 68°F",
        "B. 72°F",
        "C. 78°F",
        "D. It varies hourly"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Local buses run every 15 minutes on weekdays.",
      "conflict_prompt": "Local buses do not run every 15 minutes on weekdays.",
      "question": "How often do local buses run on weekdays?",
      "options": [
        "A. Every 15 minutes",
        "B. Once an hour",
        "C. Twice a day",
        "D. Only on weekends"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The textbook includes a chapter on climate change impacts.",
      "conflict_prompt": "The textbook does not include a chapter on climate change impacts.",
      "question": "Which topic is covered in the textbook?",
      "options": [
        "A. Climate change impacts",
        "B. Only ancient history",
        "C. Cooking recipes",
        "D. None of the above"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant's signature dish contains dairy.",
      "conflict_prompt": "The restaurant's signature dish does not contain dairy.",
      "question": "Which allergen is present in the restaurant's signature dish?",
      "options": [
        "A. Peanuts",
        "B. Shellfish",
        "C. Dairy",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hiking trail is closed to dogs.",
      "conflict_prompt": "The hiking trail is not closed to dogs.",
      "question": "Are dogs allowed on the hiking trail?",
      "options": [
        "A. Yes, dogs are allowed",
        "B. No, the trail is closed to dogs",
        "C. Only on leash",
        "D. Only service animals"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university requires a final thesis for the master's degree.",
      "conflict_prompt": "The university does not require a final thesis for the master's degree.",
      "question": "What is required to complete the master's degree at the university?",
      "options": [
        "A. Final thesis",
        "B. Only coursework",
        "C. Internship only",
        "D. No requirements"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The children's playground has a soft rubber surface under the swings.",
      "conflict_prompt": "The children's playground does not have a soft rubber surface under the swings.",
      "question": "What type of surface is under the playground swings?",
      "options": [
        "A. Concrete",
        "B. Soft rubber",
        "C. Gravel",
        "D. Water"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The printer in the office supports double-sided printing.",
      "conflict_prompt": "The printer in the office does not support double-sided printing.",
      "question": "Which printing capability does the office printer have?",
      "options": [
        "A. Single-sided only",
        "B. Double-sided printing support",
        "C. 3D printing",
        "D. No printing capability"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local bookstore stocks both new and used textbooks.",
      "conflict_prompt": "The local bookstore does not stock both new and used textbooks.",
      "question": "What types of textbooks does the local bookstore carry?",
      "options": [
        "A. New textbooks only",
        "B. Used textbooks only",
        "C. Both new and used textbooks",
        "D. No textbooks"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The swimming pool maintains a chlorine level within regulatory limits.",
      "conflict_prompt": "The swimming pool does not maintain a chlorine level within regulatory limits.",
      "question": "What is true about the swimming pool's chlorine level?",
      "options": [
        "A. It exceeds safe limits",
        "B. It is below detection",
        "C. It is within regulatory limits",
        "D. The pool uses salt instead of chlorine"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer grows organic tomatoes during the summer season.",
      "conflict_prompt": "The farmer does not grow organic tomatoes during the summer season.",
      "question": "What type of tomatoes does the farmer grow in summer?",
      "options": [
        "A. Organic tomatoes",
        "B. Genetically modified tomatoes",
        "C. No tomatoes in summer",
        "D. Only greenhouse tomatoes in winter"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train departs from platform 4 at 5:15 PM.",
      "conflict_prompt": "The train does not depart from platform 4 at 5:15 PM.",
      "question": "From which platform does the 5:15 PM train depart?",
      "options": [
        "A. Platform 1",
        "B. Platform 4",
        "C. Platform 7",
        "D. It doesn't depart at 5:15 PM"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone's warranty covers water damage for the first year.",
      "conflict_prompt": "The smartphone's warranty does not cover water damage for the first year.",
      "question": "What does the smartphone's warranty cover in the first year?",
      "options": [
        "A. Water damage coverage",
        "B. Screen cracks only",
        "C. Battery replacement only",
        "D. No coverage at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The charity distributes food to families every Thursday.",
      "conflict_prompt": "The charity does not distribute food to families every Thursday.",
      "question": "When does the charity distribute food to families?",
      "options": [
        "A. Every Thursday",
        "B. Every Monday",
        "C. Only during holidays",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's recycling program accepts glass bottles.",
      "conflict_prompt": "The city's recycling program does not accept glass bottles.",
      "question": "Which item is accepted by the city's recycling program?",
      "options": [
        "A. Glass bottles",
        "B. Hazardous waste",
        "C. Medical waste",
        "D. Electronics only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery sells a gluten-free loaf every morning.",
      "conflict_prompt": "The bakery does not sell a gluten-free loaf every morning.",
      "question": "What type of bread does the bakery sell each morning?",
      "options": [
        "A. Gluten-free loaf",
        "B. Only rye bread",
        "C. No bread in the morning",
        "D. Frozen bread only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software update adds a dark mode option to the app.",
      "conflict_prompt": "The software update does not add a dark mode option to the app.",
      "question": "Which new feature does the software update include?",
      "options": [
        "A. Dark mode option",
        "B. A built-in fax machine",
        "C. Physical keyboard support only",
        "D. No new features"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city hospital accepts walk-in patients at the urgent care clinic.",
      "conflict_prompt": "The city hospital does not accept walk-in patients at the urgent care clinic.",
      "question": "How does the urgent care clinic at the city hospital accept patients?",
      "options": [
        "A. Walk-in patients are accepted",
        "B. Only by appointment",
        "C. Only emergency referrals",
        "D. It is closed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new apartment includes a dishwasher in the kitchen.",
      "conflict_prompt": "The new apartment does not include a dishwasher in the kitchen.",
      "question": "Which appliance is included in the kitchen of the new apartment?",
      "options": [
        "A. Dishwasher",
        "B. Wood-burning stove",
        "C. No appliances",
        "D. Outdoor grill only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city park prohibits overnight camping.",
      "conflict_prompt": "The city park does not prohibit overnight camping.",
      "question": "Is overnight camping allowed in the city park?",
      "options": [
        "A. No, it is prohibited",
        "B. Yes, it is allowed",
        "C. Only with a permit",
        "D. Only on summer weekends"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery's blueberry muffins contain fresh blueberries.",
      "conflict_prompt": "The bakery's blueberry muffins do not contain fresh blueberries.",
      "question": "What ingredient is in the bakery's blueberry muffins?",
      "options": [
        "A. Fresh blueberries",
        "B. Chocolate chips",
        "C. No fruit at all",
        "D. Only artificial blueberry flavoring"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater enforces a no-refund policy for purchased tickets.",
      "conflict_prompt": "The theater does not enforce a no-refund policy for purchased tickets.",
      "question": "What is the theater's policy on refunds for purchased tickets?",
      "options": [
        "A. No refunds",
        "B. Full refunds anytime",
        "C. Refunds only for senior citizens",
        "D. Tickets are transferable but not refundable"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The heritage building retains its original wooden beams.",
      "conflict_prompt": "The heritage building does not retain its original wooden beams.",
      "question": "What structural feature does the heritage building keep?",
      "options": [
        "A. Original wooden beams",
        "B. Modern steel beams only",
        "C. No beams at all",
        "D. Glass beams"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local theater screens independent films on Wednesday nights.",
      "conflict_prompt": "The local theater does not screen independent films on Wednesday nights.",
      "question": "When does the local theater show independent films?",
      "options": [
        "A. Wednesday nights",
        "B. Sunday mornings",
        "C. Only during film festivals",
        "D. It never shows independent films"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The package requires a signature upon delivery.",
      "conflict_prompt": "The package does not require a signature upon delivery.",
      "question": "What is required when the package is delivered?",
      "options": [
        "A. A signature",
        "B. Just leaving it at the door",
        "C. Payment on delivery",
        "D. Returning to sender"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood watch meets on the first Tuesday of each month.",
      "conflict_prompt": "The neighborhood watch does not meet on the first Tuesday of each month.",
      "question": "When does the neighborhood watch meet?",
      "options": [
        "A. First Tuesday of each month",
        "B. Every Sunday",
        "C. Only during summer",
        "D. It never meets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare provides vegetarian meals for all children.",
      "conflict_prompt": "The daycare does not provide vegetarian meals for all children.",
      "question": "What kind of meals does the daycare provide for all children?",
      "options": [
        "A. Vegetarian meals",
        "B. Only meat-based meals",
        "C. No meals at all",
        "D. Meals upon parent request only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The science museum features a full-size dinosaur skeleton.",
      "conflict_prompt": "The science museum does not feature a full-size dinosaur skeleton.",
      "question": "Which exhibit is on display at the science museum?",
      "options": [
        "A. Full-size dinosaur skeleton",
        "B. A tiny fossil only",
        "C. No prehistoric exhibits",
        "D. A live dinosaur exhibit"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pharmacy fills prescriptions 7 days a week.",
      "conflict_prompt": "The pharmacy does not fill prescriptions 7 days a week.",
      "question": "How often does the pharmacy fill prescriptions?",
      "options": [
        "A. 5 days a week",
        "B. 7 days a week",
        "C. Weekends only",
        "D. By appointment only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The athletic club offers spin classes at 6 AM every weekday.",
      "conflict_prompt": "The athletic club does not offer spin classes at 6 AM every weekday.",
      "question": "When does the athletic club offer spin classes on weekdays?",
      "options": [
        "A. 6 AM",
        "B. Noon only",
        "C. 10 PM",
        "D. It doesn't offer spin classes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry accepts bicycles on board without an extra fee.",
      "conflict_prompt": "The ferry does not accept bicycles on board without an extra fee.",
      "question": "What is the ferry's policy regarding bicycles?",
      "options": [
        "A. Bicycles accepted without extra fee",
        "B. Bicycles banned entirely",
        "C. Bicycles accepted only with a large additional fee",
        "D. Only folding bikes allowed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The building's emergency exits are clearly marked and illuminated.",
      "conflict_prompt": "The building's emergency exits are not clearly marked and illuminated.",
      "question": "How are the building's emergency exits described?",
      "options": [
        "A. Clearly marked and illuminated",
        "B. Hidden and unlit",
        "C. Decorative only",
        "D. Sealed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The car's warranty includes roadside assistance for three years.",
      "conflict_prompt": "The car's warranty does not include roadside assistance for three years.",
      "question": "What does the car's warranty include regarding roadside assistance?",
      "options": [
        "A. No roadside assistance",
        "B. Roadside assistance for three years",
        "C. Roadside assistance for ten years",
        "D. Only towing vouchers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood grocery restocks fresh bread every morning.",
      "conflict_prompt": "The neighborhood grocery does not restock fresh bread every morning.",
      "question": "When does the neighborhood grocery restock fresh bread?",
      "options": [
        "A. Every morning",
        "B. Once a week",
        "C. Only on Sundays",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The piano in the concert hall is a Steinway grand.",
      "conflict_prompt": "The piano in the concert hall is not a Steinway grand.",
      "question": "What brand is the concert hall's piano?",
      "options": [
        "A. Yamaha upright",
        "B. Steinway grand",
        "C. Electric keyboard",
        "D. No piano"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The zoo feeds the penguins twice daily.",
      "conflict_prompt": "The zoo does not feed the penguins twice daily.",
      "question": "How often are the penguins fed at the zoo?",
      "options": [
        "A. Once a week",
        "B. Twice daily",
        "C. Never fed",
        "D. Only on weekends"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery closes on public holidays.",
      "conflict_prompt": "The bakery does not close on public holidays.",
      "question": "What happens to the bakery on public holidays?",
      "options": [
        "A. It closes",
        "B. It stays open 24 hours",
        "C. It operates reduced hours only",
        "D. It changes its menu"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum exhibit prohibits flash photography.",
      "conflict_prompt": "The museum exhibit does not prohibit flash photography.",
      "question": "What is the museum's rule regarding flash photography in the exhibit?",
      "options": [
        "A. Flash photography is prohibited",
        "B. Flash photography is required",
        "C. Only flash photography allowed",
        "D. Photography is banned completely"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school cafeteria serves lactose-free milk upon request.",
      "conflict_prompt": "The school cafeteria does not serve lactose-free milk upon request.",
      "question": "Which milk option is available at the school cafeteria upon request?",
      "options": [
        "A. Lactose-free milk",
        "B. Goat milk only",
        "C. No milk options",
        "D. Only powdered milk"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The rental agreement allows one pet per apartment.",
      "conflict_prompt": "The rental agreement does not allow one pet per apartment.",
      "question": "How many pets does the rental agreement allow per apartment?",
      "options": [
        "A. None",
        "B. One pet",
        "C. Three pets",
        "D. Unlimited"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bridge has weight restrictions for vehicles over 10 tons.",
      "conflict_prompt": "The bridge does not have weight restrictions for vehicles over 10 tons.",
      "question": "What is the bridge's policy on heavyweight vehicles?",
      "options": [
        "A. No restrictions",
        "B. Restricted for vehicles over 10 tons",
        "C. Restricted for vehicles under 10 tons",
        "D. Only pedestrians allowed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel's continental breakfast includes fresh fruit.",
      "conflict_prompt": "The hotel's continental breakfast does not include fresh fruit.",
      "question": "What item is included in the hotel's continental breakfast?",
      "options": [
        "A. Fresh fruit",
        "B. Only coffee",
        "C. No food",
        "D. Full hot breakfast only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city enforces a ban on single-use plastic bags.",
      "conflict_prompt": "The city does not enforce a ban on single-use plastic bags.",
      "question": "What regulation does the city have regarding single-use plastic bags?",
      "options": [
        "A. Ban enforced",
        "B. Encouraged use",
        "C. Tax on each bag",
        "D. No regulation"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant uses local seafood sourced from the nearby harbor.",
      "conflict_prompt": "The restaurant does not use local seafood sourced from the nearby harbor.",
      "question": "Where does the restaurant source its seafood?",
      "options": [
        "A. Nearby harbor (local)",
        "B. From inland freshwater only",
        "C. It doesn't serve seafood",
        "D. From outer space"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The swimming lessons are taught by certified instructors.",
      "conflict_prompt": "The swimming lessons are not taught by certified instructors.",
      "question": "Who teaches the swimming lessons?",
      "options": [
        "A. Certified instructors",
        "B. Untrained volunteers",
        "C. Robots",
        "D. No instructors"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vintage car's engine is original to the vehicle.",
      "conflict_prompt": "The vintage car's engine is not original to the vehicle.",
      "question": "What is true about the vintage car's engine?",
      "options": [
        "A. It is original to the vehicle",
        "B. It is a modern replacement",
        "C. There is no engine",
        "D. It runs on steam"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community garden plots are assigned on a first-come, first-served basis.",
      "conflict_prompt": "The community garden plots are not assigned on a first-come, first-served basis.",
      "question": "How are community garden plots assigned?",
      "options": [
        "A. Lottery only",
        "B. First-come, first-served",
        "C. By neighborhood only",
        "D. They are not assigned at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery's hours extend until midnight on Fridays.",
      "conflict_prompt": "The bakery's hours do not extend until midnight on Fridays.",
      "question": "How late is the bakery open on Fridays?",
      "options": [
        "A. Midnight",
        "B. 6 PM",
        "C. 10 AM",
        "D. Closed on Fridays"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The apartment complex provides on-site laundry facilities for residents.",
      "conflict_prompt": "The apartment complex does not provide on-site laundry facilities for residents.",
      "question": "What amenity does the apartment complex offer residents?",
      "options": [
        "A. On-site laundry facilities",
        "B. Free parking only",
        "C. Personal butlers",
        "D. No amenities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone camera captures 4K video at 60 frames per second.",
      "conflict_prompt": "The smartphone camera does not capture 4K video at 60 frames per second.",
      "question": "What video capability does the smartphone camera have?",
      "options": [
        "A. 720p at 30fps",
        "B. 4K at 60fps",
        "C. No video recording",
        "D. Only time-lapse"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college offers evening classes for working adults.",
      "conflict_prompt": "The community college does not offer evening classes for working adults.",
      "question": "Who benefits from the evening classes at the community college?",
      "options": [
        "A. Working adults",
        "B. Only children",
        "C. Tourists",
        "D. No one, there are no evening classes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local clinic administers flu vaccines during the fall season.",
      "conflict_prompt": "The local clinic does not administer flu vaccines during the fall season.",
      "question": "When does the local clinic administer flu vaccines?",
      "options": [
        "A. During the fall season",
        "B. Only in spring",
        "C. Never",
        "D. Only to staff"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campus bookstore accepts student ID as payment for store credit.",
      "conflict_prompt": "The campus bookstore does not accept student ID as payment for store credit.",
      "question": "What form of payment does the campus bookstore accept for store credit?",
      "options": [
        "A. Student ID",
        "B. Passport only",
        "C. Cash only",
        "D. No store credit available"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The historic theater retains its original marquee sign.",
      "conflict_prompt": "The historic theater does not retain its original marquee sign.",
      "question": "What feature does the historic theater still have?",
      "options": [
        "A. Original marquee sign",
        "B. Digital billboard only",
        "C. No signage",
        "D. A modern neon sign instead"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The dental clinic offers emergency appointments on weekends.",
      "conflict_prompt": "The dental clinic does not offer emergency appointments on weekends.",
      "question": "What service does the dental clinic provide on weekends?",
      "options": [
        "A. Emergency appointments",
        "B. Routine cleanings only",
        "C. Cosmetic consultations only",
        "D. Nothing, the clinic is closed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers market accepts credit cards at all vendor stalls.",
      "conflict_prompt": "The farmers market does not accept credit cards at all vendor stalls.",
      "question": "What is true about payment options at the farmers market?",
      "options": [
        "A. Credit cards accepted at all stalls",
        "B. Cash only",
        "C. Only mobile payments",
        "D. Barter system only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coastal path is accessible year-round for hikers.",
      "conflict_prompt": "The coastal path is not accessible year-round for hikers.",
      "question": "When is the coastal path accessible for hikers?",
      "options": [
        "A. Year-round",
        "B. Only in summer",
        "C. Only during low tide",
        "D. It is permanently closed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local museum's hours extend until 9 PM on Thursdays.",
      "conflict_prompt": "The local museum's hours do not extend until 9 PM on Thursdays.",
      "question": "How late is the local museum open on Thursdays?",
      "options": [
        "A. 5 PM",
        "B. 9 PM",
        "C. Midnight",
        "D. Closed on Thursdays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city permits street vending in designated zones only.",
      "conflict_prompt": "The city does not permit street vending in designated zones only.",
      "question": "Where is street vending permitted in the city?",
      "options": [
        "A. Only in designated zones",
        "B. Anywhere on public streets",
        "C. Only inside malls",
        "D. It is banned citywide"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The fitness center offers free towel service to members.",
      "conflict_prompt": "The fitness center does not offer free towel service to members.",
      "question": "Which amenity does the fitness center provide to members?",
      "options": [
        "A. Free towel service",
        "B. Free car wash",
        "C. Free hotel stays",
        "D. No amenities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare enforces a strict no-smoking policy on the premises.",
      "conflict_prompt": "The daycare does not enforce a strict no-smoking policy on the premises.",
      "question": "What is the daycare's policy on smoking on the premises?",
      "options": [
        "A. Strict no-smoking policy",
        "B. Smoking allowed in designated areas",
        "C. Encouraged during pickup",
        "D. No policy exists"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood pool requires swim caps for all bathers.",
      "conflict_prompt": "The neighborhood pool does not require swim caps for all bathers.",
      "question": "What is required for bathers at the neighborhood pool?",
      "options": [
        "A. Swim caps",
        "B. Street clothes",
        "C. Shoes only",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The airline provides complimentary snacks on domestic flights under two hours.",
      "conflict_prompt": "The airline does not provide complimentary snacks on domestic flights under two hours.",
      "question": "What does the airline offer on domestic flights under two hours?",
      "options": [
        "A. Complimentary snacks",
        "B. Full meals",
        "C. No service at all",
        "D. Paid-only snacks"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool closes for maintenance every October.",
      "conflict_prompt": "The community pool does not close for maintenance every October.",
      "question": "When does the community pool close for maintenance?",
      "options": [
        "A. Every October",
        "B. Every March",
        "C. Only during heat waves",
        "D. It never closes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new library app syncs reading lists across devices.",
      "conflict_prompt": "The new library app does not sync reading lists across devices.",
      "question": "What functionality does the new library app provide?",
      "options": [
        "A. Syncs reading lists across devices",
        "B. Only prints lists",
        "C. No synchronization features",
        "D. Voice-only interface"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local law requires businesses to display their hours at the entrance.",
      "conflict_prompt": "The local law does not require businesses to display their hours at the entrance.",
      "question": "What must businesses display at their entrance according to local law?",
      "options": [
        "A. Their hours",
        "B. Their favorite color",
        "C. Nothing required",
        "D. Only license information"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coffee shop's pastries are baked fresh every morning.",
      "conflict_prompt": "The coffee shop's pastries are not baked fresh every morning.",
      "question": "When are the coffee shop's pastries baked?",
      "options": [
        "A. Fresh every morning",
        "B. Once a week",
        "C. Never baked on-site",
        "D. Frozen and reheated"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campground provides potable water at the main pavilion.",
      "conflict_prompt": "The campground does not provide potable water at the main pavilion.",
      "question": "What amenity is available at the campground's main pavilion?",
      "options": [
        "A. Potable water",
        "B. A movie theater",
        "C. A shopping mall",
        "D. No amenities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's public Wi-Fi requires a one-time registration to access.",
      "conflict_prompt": "The city's public Wi-Fi does not require a one-time registration to access.",
      "question": "How does one access the city's public Wi-Fi?",
      "options": [
        "A. Requires one-time registration",
        "B. Requires paid subscription only",
        "C. No access available",
        "D. Only via invitation"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bus shelter has a bench that seats four people.",
      "conflict_prompt": "The bus shelter does not have a bench that seats four people.",
      "question": "What seating does the bus shelter provide?",
      "options": [
        "A. Bench seating for four",
        "B. Only standing room",
        "C. Luxury recliners",
        "D. No seating at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The grocery store stocks lactose-free ice cream in the frozen section.",
      "conflict_prompt": "The grocery store does not stock lactose-free ice cream in the frozen section.",
      "question": "Which product is found in the grocery store's frozen section?",
      "options": [
        "A. Lactose-free ice cream",
        "B. Fresh bread",
        "C. Live seafood tanks",
        "D. Electronics"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The athletic field is available for public use after 6 PM on weekdays.",
      "conflict_prompt": "The athletic field is not available for public use after 6 PM on weekdays.",
      "question": "When is the athletic field available for public use on weekdays?",
      "options": [
        "A. After 6 PM",
        "B. Only before 6 AM",
        "C. It is always closed",
        "D. Only on weekends"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's emergency alert system sends notifications via text messages.",
      "conflict_prompt": "The city's emergency alert system does not send notifications via text messages.",
      "question": "How does the city's emergency alert system send notifications?",
      "options": [
        "A. Via text messages",
        "B. Only via postal mail",
        "C. Only by billboard ads",
        "D. It doesn't send alerts"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery requires pre-orders for wedding cakes at least four weeks in advance.",
      "conflict_prompt": "The bakery does not require pre-orders for wedding cakes at least four weeks in advance.",
      "question": "How far in advance must wedding cakes be pre-ordered at the bakery?",
      "options": [
        "A. Four weeks",
        "B. One day",
        "C. No pre-order required",
        "D. One year"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city offers free composting workshops every spring.",
      "conflict_prompt": "The city does not offer free composting workshops every spring.",
      "question": "When are composting workshops offered by the city?",
      "options": [
        "A. Every spring",
        "B. Every fall",
        "C. Only biannually",
        "D. The city does not offer workshops"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry schedule includes a 7 AM departure on weekdays.",
      "conflict_prompt": "The ferry schedule does not include a 7 AM departure on weekdays.",
      "question": "What departure time is included in the ferry schedule on weekdays?",
      "options": [
        "A. 7 AM",
        "B. Midnight only",
        "C. 3 PM only",
        "D. No weekday departures"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's park maintenance team mows the lawns weekly.",
      "conflict_prompt": "The city's park maintenance team does not mow the lawns weekly.",
      "question": "How often does the park maintenance team mow the lawns?",
      "options": [
        "A. Weekly",
        "B. Monthly",
        "C. Only on demand",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pet shelter provides spay and neuter services to adopted animals.",
      "conflict_prompt": "The pet shelter does not provide spay and neuter services to adopted animals.",
      "question": "What veterinary service does the pet shelter provide to adopted animals?",
      "options": [
        "A. Spay and neuter services",
        "B. Only vaccinations",
        "C. No services",
        "D. Microchipping only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school auditorium seats up to 800 people.",
      "conflict_prompt": "The school auditorium does not seat up to 800 people.",
      "question": "What is the seating capacity of the school auditorium?",
      "options": [
        "A. 200 seats",
        "B. 800 seats",
        "C. 5,000 seats",
        "D. No fixed seating"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local theater sells discounted tickets for students.",
      "conflict_prompt": "The local theater does not sell discounted tickets for students.",
      "question": "What discount does the local theater offer?",
      "options": [
        "A. Student discounts",
        "B. Senior discounts only",
        "C. No discounts",
        "D. Discounts for pets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer's market opens at 7 AM on Saturdays.",
      "conflict_prompt": "The farmer's market does not open at 7 AM on Saturdays.",
      "question": "When does the farmer's market open on Saturdays?",
      "options": [
        "A. 7 AM",
        "B. 9 AM",
        "C. Noon",
        "D. It is closed on Saturdays"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum labels all artifacts with the year of origin.",
      "conflict_prompt": "The museum does not label all artifacts with the year of origin.",
      "question": "What information is included on all artifact labels at the museum?",
      "options": [
        "A. Year of origin",
        "B. Owner's name only",
        "C. Price tags",
        "D. No labels at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike rental shop includes helmets with every rental.",
      "conflict_prompt": "The bike rental shop does not include helmets with every rental.",
      "question": "What safety item is included with each bike rental?",
      "options": [
        "A. Helmet",
        "B. Life jacket",
        "C. Snow goggles",
        "D. No safety items"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train station has an ATM available inside the main concourse.",
      "conflict_prompt": "The train station does not have an ATM available inside the main concourse.",
      "question": "What facility is available inside the main concourse of the train station?",
      "options": [
        "A. ATM",
        "B. Indoor pool",
        "C. Post office only",
        "D. No facilities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campus Wi-Fi requires a student username and password to log in.",
      "conflict_prompt": "The campus Wi-Fi does not require a student username and password to log in.",
      "question": "What credentials are needed to access campus Wi-Fi?",
      "options": [
        "A. Student username and password",
        "B. Social media login only",
        "C. No credentials required",
        "D. Only a physical key"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public pool enforces a height requirement for children's slides.",
      "conflict_prompt": "The public pool does not enforce a height requirement for children's slides.",
      "question": "What safety rule applies to the children's slides at the public pool?",
      "options": [
        "A. Height requirement enforced",
        "B. No restrictions",
        "C. Age limit only",
        "D. Slides are closed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bookstore hosts author readings on the last Friday of every month.",
      "conflict_prompt": "The bookstore does not host author readings on the last Friday of every month.",
      "question": "When does the bookstore host author readings?",
      "options": [
        "A. Last Friday of every month",
        "B. First Monday only",
        "C. Sundays at noon",
        "D. It never hosts readings"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel provides complimentary shuttle service to the airport.",
      "conflict_prompt": "The hotel does not provide complimentary shuttle service to the airport.",
      "question": "What transport service does the hotel provide to guests?",
      "options": [
        "A. Complimentary shuttle to the airport",
        "B. Only paid taxi vouchers",
        "C. No transport services",
        "D. Free airline tickets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public market enforces vendor licenses for all sellers.",
      "conflict_prompt": "The public market does not enforce vendor licenses for all sellers.",
      "question": "What requirement does the public market enforce for sellers?",
      "options": [
        "A. Vendor licenses for all sellers",
        "B. Only a handshake agreement",
        "C. No registration required",
        "D. Mandatory uniforms"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The swimming club requires parental consent forms for minors.",
      "conflict_prompt": "The swimming club does not require parental consent forms for minors.",
      "question": "What paperwork is required for minors at the swimming club?",
      "options": [
        "A. Parental consent forms",
        "B. No paperwork",
        "C. A notarized affidavit only",
        "D. Photo ID of guardian only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's noise ordinance prohibits loud music after 10 PM.",
      "conflict_prompt": "The city's noise ordinance does not prohibit loud music after 10 PM.",
      "question": "What does the city's noise ordinance prohibit?",
      "options": [
        "A. Loud music after 10 PM",
        "B. Talking after 10 PM",
        "C. All music at any time",
        "D. Only construction noise"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ice rink offers public skating sessions on weekend afternoons.",
      "conflict_prompt": "The ice rink does not offer public skating sessions on weekend afternoons.",
      "question": "When does the ice rink offer public skating sessions?",
      "options": [
        "A. Weekend afternoons",
        "B. Weekday mornings only",
        "C. Only by reservation",
        "D. It is private membership only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer uses drip irrigation for all vegetable beds.",
      "conflict_prompt": "The farmer does not use drip irrigation for all vegetable beds.",
      "question": "What irrigation method does the farmer use for the vegetable beds?",
      "options": [
        "A. Drip irrigation",
        "B. Flood irrigation",
        "C. No irrigation",
        "D. Aerial sprinklers only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The print shop offers same-day printing for orders placed before noon.",
      "conflict_prompt": "The print shop does not offer same-day printing for orders placed before noon.",
      "question": "What service does the print shop provide for orders placed before noon?",
      "options": [
        "A. Same-day printing",
        "B. Two-week turnaround only",
        "C. No service at all",
        "D. Handwritten copies only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic's pediatrician sees patients on Tuesday afternoons.",
      "conflict_prompt": "The clinic's pediatrician does not see patients on Tuesday afternoons.",
      "question": "When does the clinic's pediatrician see patients?",
      "options": [
        "A. Tuesday afternoons",
        "B. Friday nights",
        "C. Only by telemedicine",
        "D. Never sees patients"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art supply store offers student discounts with a valid ID.",
      "conflict_prompt": "The art supply store does not offer student discounts with a valid ID.",
      "question": "What discount does the art supply store provide?",
      "options": [
        "A. Student discounts with valid ID",
        "B. Discounts for pets",
        "C. No discounts",
        "D. Discounts only during holidays"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden posts seasonal planting guides for visitors.",
      "conflict_prompt": "The botanical garden does not post seasonal planting guides for visitors.",
      "question": "What resource does the botanical garden provide to visitors?",
      "options": [
        "A. Seasonal planting guides",
        "B. Free plant giveaways",
        "C. No visitor resources",
        "D. A restaurant menu only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recycling center accepts used batteries for safe disposal.",
      "conflict_prompt": "The recycling center does not accept used batteries for safe disposal.",
      "question": "Which hazardous item does the recycling center accept?",
      "options": [
        "A. Used batteries",
        "B. Medical sharps",
        "C. Nuclear waste",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel's pool is heated year-round.",
      "conflict_prompt": "The hotel's pool is not heated year-round.",
      "question": "What is true about the hotel's pool?",
      "options": [
        "A. Heated year-round",
        "B. Only open in summer",
        "C. It is frozen permanently",
        "D. It is a saltwater lake"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community theater rehearses every Monday evening.",
      "conflict_prompt": "The community theater does not rehearse every Monday evening.",
      "question": "When does the community theater rehearse?",
      "options": [
        "A. Monday evenings",
        "B. Only during performances",
        "C. Thursday mornings",
        "D. It does not rehearse"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's flood warning sirens are tested monthly.",
      "conflict_prompt": "The city's flood warning sirens are not tested monthly.",
      "question": "How often are the city's flood warning sirens tested?",
      "options": [
        "A. Monthly",
        "B. Annually",
        "C. Never tested",
        "D. Daily"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school library offers interlibrary loan services to students.",
      "conflict_prompt": "The school library does not offer interlibrary loan services to students.",
      "question": "What service does the school library provide to students?",
      "options": [
        "A. Interlibrary loan services",
        "B. Only e-books",
        "C. No borrowing allowed",
        "D. Private tutoring only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer keeps honey bees in three hives on his property.",
      "conflict_prompt": "The farmer does not keep honey bees in three hives on his property.",
      "question": "How many honey bee hives does the farmer maintain?",
      "options": [
        "A. One hive",
        "B. Three hives",
        "C. None",
        "D. Fifty hives"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The laundromat offers free Wi-Fi for customers.",
      "conflict_prompt": "The laundromat does not offer free Wi-Fi for customers.",
      "question": "What amenity does the laundromat provide?",
      "options": [
        "A. Free Wi-Fi",
        "B. Free coffee only",
        "C. Free parking only",
        "D. No amenities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college library remains open through final exam week.",
      "conflict_prompt": "The community college library does not remain open through final exam week.",
      "question": "What is the library's schedule during final exam week?",
      "options": [
        "A. Remains open throughout",
        "B. Closes midweek",
        "C. Opens only for staff",
        "D. It is renovated then"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city requires bicycle riders to use lights after dusk.",
      "conflict_prompt": "The city does not require bicycle riders to use lights after dusk.",
      "question": "What does the city require of bicycle riders after dusk?",
      "options": [
        "A. Use lights",
        "B. Wear helmets only",
        "C. Ride on sidewalks",
        "D. No requirements"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood clinic participates in the national vaccination program.",
      "conflict_prompt": "The neighborhood clinic does not participate in the national vaccination program.",
      "question": "What public health program does the neighborhood clinic participate in?",
      "options": [
        "A. National vaccination program",
        "B. Private concierge health only",
        "C. No public programs",
        "D. Veterinary services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden sells annual memberships with free guest passes.",
      "conflict_prompt": "The botanical garden does not sell annual memberships with free guest passes.",
      "question": "What benefit comes with the botanical garden's annual membership?",
      "options": [
        "A. Free guest passes",
        "B. Free parking only",
        "C. Discounts on coffees only",
        "D. No benefits"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The historic tram runs along the waterfront every hour.",
      "conflict_prompt": "The historic tram does not run along the waterfront every hour.",
      "question": "How frequently does the historic tram run along the waterfront?",
      "options": [
        "A. Every hour",
        "B. Every five minutes",
        "C. Twice a day",
        "D. It doesn't run at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school requires students to complete community service as graduation credit.",
      "conflict_prompt": "The school does not require students to complete community service as graduation credit.",
      "question": "What graduation requirement does the school have for students?",
      "options": [
        "A. Community service hours",
        "B. Foreign exchange only",
        "C. No graduation requirements",
        "D. Mandatory military service"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pet store vaccinates puppies and kittens before adoption.",
      "conflict_prompt": "The pet store does not vaccinate puppies and kittens before adoption.",
      "question": "What veterinary care does the pet store provide before adoption?",
      "options": [
        "A. Vaccinations for puppies and kittens",
        "B. Microchipping only",
        "C. No veterinary care",
        "D. Spaying/neutering only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater's balcony seats offer unrestricted views of the stage.",
      "conflict_prompt": "The theater's balcony seats do not offer unrestricted views of the stage.",
      "question": "What is true about the balcony seats in the theater?",
      "options": [
        "A. They offer unrestricted views",
        "B. Views are obstructed",
        "C. There is no balcony",
        "D. Balcony is for display only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The concert venue enforces a clear bag policy for entry.",
      "conflict_prompt": "The concert venue does not enforce a clear bag policy for entry.",
      "question": "What policy does the concert venue enforce for bags at entry?",
      "options": [
        "A. Clear bag policy",
        "B. No bags allowed at all",
        "C. Only backpacks allowed",
        "D. Full luggage allowed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer sells free-range eggs at the weekend market.",
      "conflict_prompt": "The farmer does not sell free-range eggs at the weekend market.",
      "question": "Which product does the farmer sell at the weekend market?",
      "options": [
        "A. Free-range eggs",
        "B. Only frozen goods",
        "C. Electronics",
        "D. No products"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The library's summer reading program awards prizes to children who finish five books.",
      "conflict_prompt": "The library's summer reading program does not award prizes to children who finish five books.",
      "question": "What is the prize condition for the library's summer reading program?",
      "options": [
        "A. Finish five books",
        "B. Attend five events",
        "C. Donate five books",
        "D. No prizes awarded"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal parking garage charges an hourly rate for all vehicles.",
      "conflict_prompt": "The municipal parking garage does not charge an hourly rate for all vehicles.",
      "question": "How does the municipal parking garage charge for parking?",
      "options": [
        "A. Hourly rate for all vehicles",
        "B. Flat monthly rate only",
        "C. Free parking always",
        "D. By vehicle weight"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city library is open on Saturdays. It welcomes visitors between 10 AM and 4 PM.",
      "conflict_prompt": "The city library is not open on Saturdays. It welcomes visitors between 10 AM and 4 PM.",
      "question": "On which day is the library open for visitors between 10 AM and 4 PM?",
      "options": [
        "A. Monday",
        "B. Saturday",
        "C. Sunday",
        "D. Friday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Martin is a certified scuba instructor. He teaches open-water diving classes.",
      "conflict_prompt": "Martin is not a certified scuba instructor. He teaches open-water diving classes.",
      "question": "What is Martin's qualification for teaching open-water diving?",
      "options": [
        "A. Paramedic",
        "B. Lifeguard",
        "C. Certified scuba instructor",
        "D. Swim team coach"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses whole wheat flour in its bread. All loaves are made daily.",
      "conflict_prompt": "The bakery does not use whole wheat flour in its bread. All loaves are made daily.",
      "question": "What type of flour does the bakery use in its bread?",
      "options": [
        "A. Almond flour",
        "B. Cornmeal",
        "C. Whole wheat flour",
        "D. Rice flour"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Hannah owns a silver Honda sedan. She bought it last year.",
      "conflict_prompt": "Hannah does not own a silver Honda sedan. She bought it last year.",
      "question": "What type of car does Hannah own?",
      "options": [
        "A. Blue Toyota SUV",
        "B. Silver Honda sedan",
        "C. Red Ford truck",
        "D. Black motorcycle"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hiking trail is 8 miles long. It takes most hikers about five hours to complete.",
      "conflict_prompt": "The hiking trail is not 8 miles long. It takes most hikers about five hours to complete.",
      "question": "How long is the hiking trail?",
      "options": [
        "A. 8 miles",
        "B. 2 miles",
        "C. 12 miles",
        "D. 4 miles"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Dr. Patel is a cardiologist who performs heart valve repairs. She works at the downtown hospital.",
      "conflict_prompt": "Dr. Patel is not a cardiologist who performs heart valve repairs. She works at the downtown hospital.",
      "question": "What type of specialist is Dr. Patel?",
      "options": [
        "A. Dermatologist",
        "B. Cardiologist",
        "C. Orthopedic surgeon",
        "D. Neurologist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone model X has a 6.5-inch display. Its battery capacity is 4000 mAh.",
      "conflict_prompt": "The smartphone model X does not have a 6.5-inch display. Its battery capacity is 4000 mAh.",
      "question": "What is the display size of smartphone model X?",
      "options": [
        "A. 5.0 inches",
        "B. 6.5 inches",
        "C. 7.2 inches",
        "D. 4.7 inches"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum exhibit includes medieval armor. The exhibit runs through August.",
      "conflict_prompt": "The museum exhibit does not include medieval armor. The exhibit runs through August.",
      "question": "What type of artifacts are included in the exhibit?",
      "options": [
        "A. Modern sculptures",
        "B. Ancient pottery",
        "C. Medieval armor",
        "D. Contemporary paintings"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Olivia is fluent in Spanish. She provides translation services for medical appointments.",
      "conflict_prompt": "Olivia is not fluent in Spanish. She provides translation services for medical appointments.",
      "question": "Which language is Olivia fluent in for translation services?",
      "options": [
        "A. French",
        "B. German",
        "C. Spanish",
        "D. Mandarin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community garden allows dogs on leash. It has raised beds for vegetables.",
      "conflict_prompt": "The community garden does not allow dogs on leash. It has raised beds for vegetables.",
      "question": "Are dogs allowed in the community garden if on a leash?",
      "options": [
        "A. No",
        "B. Yes",
        "C. Only on Sundays",
        "D. Only service dogs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university offers a Master of Data Science program. Classes are available in the evening.",
      "conflict_prompt": "The university does not offer a Master of Data Science program. Classes are available in the evening.",
      "question": "Which graduate program does the university offer?",
      "options": [
        "A. Master of Fine Arts",
        "B. Master of Data Science",
        "C. Master of Architecture",
        "D. Master of Theology"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The flight to Tokyo departs at 10:30 PM. It is scheduled nonstop.",
      "conflict_prompt": "The flight to Tokyo does not depart at 10:30 PM. It is scheduled nonstop.",
      "question": "What time does the flight to Tokyo depart?",
      "options": [
        "A. 7:00 AM",
        "B. 10:30 PM",
        "C. 1:45 PM",
        "D. 5:20 PM"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new software update includes a dark mode. It improves battery life on phones.",
      "conflict_prompt": "The new software update does not include a dark mode. It improves battery life on phones.",
      "question": "Which display option is included in the new software update?",
      "options": [
        "A. Split-screen mode",
        "B. Dark mode",
        "C. 3D wallpapers",
        "D. Voice-activated themes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Captain Ruiz commands the cargo vessel. The ship carries refrigerated containers.",
      "conflict_prompt": "Captain Ruiz does not command the cargo vessel. The ship carries refrigerated containers.",
      "question": "What is Captain Ruiz's role on the cargo vessel?",
      "options": [
        "A. Chief engineer",
        "B. Deckhand",
        "C. Quartermaster",
        "D. Captain"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood has a weekly farmers market on Sundays. It sells organic produce.",
      "conflict_prompt": "The neighborhood does not have a weekly farmers market on Sundays. It sells organic produce.",
      "question": "When is the farmers market held each week?",
      "options": [
        "A. Monday",
        "B. Thursday",
        "C. Sunday",
        "D. Saturday"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The painting was created by a 19th-century French artist. It features a coastal landscape.",
      "conflict_prompt": "The painting was not created by a 19th-century French artist. It features a coastal landscape.",
      "question": "What school or origin is attributed to the painting's creator?",
      "options": [
        "A. 19th-century French",
        "B. 20th-century Japanese",
        "C. 18th-century Russian",
        "D. 21st-century American"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel provides complimentary breakfast to guests. The buffet opens at 7 AM.",
      "conflict_prompt": "The hotel does not provide complimentary breakfast to guests. The buffet opens at 7 AM.",
      "question": "What amenity does the hotel offer to guests?",
      "options": [
        "A. Paid valet parking only",
        "B. Complimentary breakfast",
        "C. Free smartphone rental",
        "D. Complimentary yoga classes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The orchestra rehearses on Wednesday evenings. Rehearsals begin at 7:30 PM.",
      "conflict_prompt": "The orchestra does not rehearse on Wednesday evenings. Rehearsals begin at 7:30 PM.",
      "question": "On which evening does the orchestra rehearse?",
      "options": [
        "A. Tuesday",
        "B. Wednesday",
        "C. Friday",
        "D. Sunday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coffee shop uses locally roasted beans. They serve single-origin pour-overs.",
      "conflict_prompt": "The coffee shop does not use locally roasted beans. They serve single-origin pour-overs.",
      "question": "What type of coffee beans does the shop use?",
      "options": [
        "A. Import blends",
        "B. Locally roasted beans",
        "C. Instant coffee",
        "D. Generic supermarket beans"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The children's book is illustrated by a well-known artist. It targets readers aged 4 to 8.",
      "conflict_prompt": "The children's book is not illustrated by a well-known artist. It targets readers aged 4 to 8.",
      "question": "Who illustrated the children's book?",
      "options": [
        "A. An unknown amateur",
        "B. A photography studio",
        "C. A well-known artist",
        "D. No illustrations"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train service runs every 30 minutes during peak hours. Tickets can be purchased at machines on the platform.",
      "conflict_prompt": "The train service does not run every 30 minutes during peak hours. Tickets can be purchased at machines on the platform.",
      "question": "How frequently does the train run during peak hours?",
      "options": [
        "A. Every hour",
        "B. Every 30 minutes",
        "C. Every 10 minutes",
        "D. Once a day"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone camera supports 4K video recording. The rear camera has optical image stabilization.",
      "conflict_prompt": "The smartphone camera does not support 4K video recording. The rear camera has optical image stabilization.",
      "question": "What video resolution does the smartphone camera support?",
      "options": [
        "A. 720p",
        "B. 1080p",
        "C. 4K",
        "D. 480p"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The nonprofit hosts a summer program for teenagers. The program runs for eight weeks.",
      "conflict_prompt": "The nonprofit does not host a summer program for teenagers. The program runs for eight weeks.",
      "question": "How long does the nonprofit's summer program run?",
      "options": [
        "A. Two weeks",
        "B. Four weeks",
        "C. Eight weeks",
        "D. One week"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Professor Nguyen teaches organic chemistry. Her office hours are on Mondays from 2 PM to 4 PM.",
      "conflict_prompt": "Professor Nguyen does not teach organic chemistry. Her office hours are on Mondays from 2 PM to 4 PM.",
      "question": "Which subject does Professor Nguyen teach?",
      "options": [
        "A. Organic chemistry",
        "B. Medieval history",
        "C. Microeconomics",
        "D. Music theory"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The apartment includes in-unit laundry. The lease requires a one-year commitment.",
      "conflict_prompt": "The apartment does not include in-unit laundry. The lease requires a one-year commitment.",
      "question": "What amenity is included in the apartment?",
      "options": [
        "A. In-unit laundry",
        "B. Shared laundry across the hall",
        "C. No laundry facilities",
        "D. Coin-operated laundry downstairs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conference room has video conferencing equipment. It seats up to 20 people.",
      "conflict_prompt": "The conference room does not have video conferencing equipment. It seats up to 20 people.",
      "question": "What technology is available in the conference room?",
      "options": [
        "A. No technology",
        "B. A chalkboard only",
        "C. Video conferencing equipment",
        "D. A single telephone"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local ordinance bans leaf burning year-round. Violators face a fine.",
      "conflict_prompt": "The local ordinance does not ban leaf burning year-round. Violators face a fine.",
      "question": "What does the local ordinance prohibit?",
      "options": [
        "A. Painting houses",
        "B. Leaf burning year-round",
        "C. Parking on streets",
        "D. Running for office"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartwatch tracks heart rate continuously. It alerts the user to irregular rhythms.",
      "conflict_prompt": "The smartwatch does not track heart rate continuously. It alerts the user to irregular rhythms.",
      "question": "What biometric does the smartwatch continually monitor?",
      "options": [
        "A. Blood sugar",
        "B. Heart rate",
        "C. Blood pressure",
        "D. Body temperature"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city park contains a children's playground. The playground has a rubber safety surface.",
      "conflict_prompt": "The city park does not contain a children's playground. The playground has a rubber safety surface.",
      "question": "What safety feature does the playground have?",
      "options": [
        "A. Sand pits",
        "B. Rubber safety surface",
        "C. Concrete flooring",
        "D. Wooden chips"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The documentary was released in 2019. It chronicles the history of jazz in New Orleans.",
      "conflict_prompt": "The documentary was not released in 2019. It chronicles the history of jazz in New Orleans.",
      "question": "When was the documentary released?",
      "options": [
        "A. 2005",
        "B. 2019",
        "C. 1990",
        "D. 2022"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software license permits academic use only. Commercial redistribution is prohibited.",
      "conflict_prompt": "The software license does not permit academic use only. Commercial redistribution is prohibited.",
      "question": "For which purpose does the software license permit use?",
      "options": [
        "A. Personal gaming",
        "B. Academic use only",
        "C. Commercial distribution",
        "D. Government-only use"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant's signature dish is a truffle pasta. It is available only during dinner service.",
      "conflict_prompt": "The restaurant's signature dish is not a truffle pasta. It is available only during dinner service.",
      "question": "What is the restaurant's signature dish?",
      "options": [
        "A. Truffle pasta",
        "B. Margherita pizza",
        "C. Beef stew",
        "D. Vegan curry"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum offers guided tours in English. Tours depart at 11 AM and 2 PM daily.",
      "conflict_prompt": "The museum does not offer guided tours in English. Tours depart at 11 AM and 2 PM daily.",
      "question": "Which language are the museum's guided tours offered in?",
      "options": [
        "A. Spanish",
        "B. Mandarin",
        "C. English",
        "D. German"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conference center provides free Wi-Fi for attendees. Network access requires a password provided at registration.",
      "conflict_prompt": "The conference center does not provide free Wi-Fi for attendees. Network access requires a password provided at registration.",
      "question": "What amenity does the conference center provide to attendees?",
      "options": [
        "A. Free Wi-Fi",
        "B. Free parking only",
        "C. Free meals",
        "D. Free childcare"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The play opens on March 12th. Performances run through April 5th.",
      "conflict_prompt": "The play does not open on March 12th. Performances run through April 5th.",
      "question": "On what date does the play open?",
      "options": [
        "A. March 12th",
        "B. April 12th",
        "C. February 1st",
        "D. May 3rd"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The company provides health insurance to full-time employees. Coverage begins after 60 days of employment.",
      "conflict_prompt": "The company does not provide health insurance to full-time employees. Coverage begins after 60 days of employment.",
      "question": "Who receives health insurance from the company?",
      "options": [
        "A. Part-time employees only",
        "B. Full-time employees",
        "C. Contractors only",
        "D. No one"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden features a native plant section. Signage explains local habitats.",
      "conflict_prompt": "The botanical garden does not feature a native plant section. Signage explains local habitats.",
      "question": "What special section does the botanical garden include?",
      "options": [
        "A. Tropical rainforest section",
        "B. Desert cactus section",
        "C. Native plant section",
        "D. Alpine garden"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike rental shop rents electric bikes. Rentals require a valid ID and deposit.",
      "conflict_prompt": "The bike rental shop does not rent electric bikes. Rentals require a valid ID and deposit.",
      "question": "What type of bikes does the rental shop offer?",
      "options": [
        "A. Manual road bikes only",
        "B. Electric bikes",
        "C. Children's tricycles only",
        "D. Unicycles"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The forecast predicts light snow on Thursday. Accumulation is expected to be less than an inch.",
      "conflict_prompt": "The forecast does not predict light snow on Thursday. Accumulation is expected to be less than an inch.",
      "question": "What weather is forecasted for Thursday according to the statement?",
      "options": [
        "A. Heavy rain",
        "B. Clear skies",
        "C. Light snow",
        "D. Thunderstorms"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater enforces a no-photography policy during performances. Ushers remind patrons at the start of the show.",
      "conflict_prompt": "The theater does not enforce a no-photography policy during performances. Ushers remind patrons at the start of the show.",
      "question": "What policy does the theater enforce during performances?",
      "options": [
        "A. No photography",
        "B. No talking",
        "C. No eating",
        "D. No late entry"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The cat is vaccinated against rabies. The owner keeps the vaccination records on file.",
      "conflict_prompt": "The cat is not vaccinated against rabies. The owner keeps the vaccination records on file.",
      "question": "Against which disease is the cat vaccinated?",
      "options": [
        "A. Distemper",
        "B. Rabies",
        "C. Parvovirus",
        "D. Kennel cough"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The historic house dates from 1875. It has been restored to its original Victorian interior.",
      "conflict_prompt": "The historic house does not date from 1875. It has been restored to its original Victorian interior.",
      "question": "From which year does the historic house date?",
      "options": [
        "A. 1800",
        "B. 1875",
        "C. 1920",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The charity supports adult literacy programs. Volunteers meet twice a week to tutor learners.",
      "conflict_prompt": "The charity does not support adult literacy programs. Volunteers meet twice a week to tutor learners.",
      "question": "What type of programs does the charity support?",
      "options": [
        "A. Youth sports",
        "B. Adult literacy",
        "C. Animal rescue",
        "D. Environmental cleanup"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vintage watch has a leather strap. It was manufactured in Switzerland.",
      "conflict_prompt": "The vintage watch does not have a leather strap. It was manufactured in Switzerland.",
      "question": "What type of strap does the vintage watch have?",
      "options": [
        "A. Metal bracelet",
        "B. Rubber strap",
        "C. Leather strap",
        "D. Nylon strap"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The rental cabin includes Wi-Fi access. Guests receive the network password upon arrival.",
      "conflict_prompt": "The rental cabin does not include Wi-Fi access. Guests receive the network password upon arrival.",
      "question": "What connectivity feature does the rental cabin include?",
      "options": [
        "A. Satellite TV only",
        "B. No connectivity",
        "C. Wi-Fi access",
        "D. Landline phone"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local deli is open seven days a week. It closes at 8 PM on weekdays.",
      "conflict_prompt": "The local deli is not open seven days a week. It closes at 8 PM on weekdays.",
      "question": "How many days per week is the deli open?",
      "options": [
        "A. Five days",
        "B. Six days",
        "C. Seven days",
        "D. Four days"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art class requires beginners to bring a sketchbook. Supplies are otherwise provided.",
      "conflict_prompt": "The art class does not require beginners to bring a sketchbook. Supplies are otherwise provided.",
      "question": "What item are beginners required to bring to the art class?",
      "options": [
        "A. Paints",
        "B. Sketchbook",
        "C. Easel",
        "D. Camera"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone's fingerprint sensor is located on the back panel. It unlocks the device instantly.",
      "conflict_prompt": "The smartphone's fingerprint sensor is not located on the back panel. It unlocks the device instantly.",
      "question": "Where is the smartphone's fingerprint sensor located?",
      "options": [
        "A. Under the screen",
        "B. On the side",
        "C. On the back panel",
        "D. Not present"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The company headquarters are located in Seattle. The office building overlooks the waterfront.",
      "conflict_prompt": "The company headquarters are not located in Seattle. The office building overlooks the waterfront.",
      "question": "In which city are the company's headquarters located?",
      "options": [
        "A. Portland",
        "B. Seattle",
        "C. San Francisco",
        "D. Denver"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The swim club practices in an indoor pool. Sessions are held year-round.",
      "conflict_prompt": "The swim club does not practice in an indoor pool. Sessions are held year-round.",
      "question": "Where does the swim club practice?",
      "options": [
        "A. Indoor pool",
        "B. Outdoor lake",
        "C. River",
        "D. Ocean"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recipe calls for two tablespoons of olive oil. It serves four people.",
      "conflict_prompt": "The recipe does not call for two tablespoons of olive oil. It serves four people.",
      "question": "How much olive oil does the recipe call for?",
      "options": [
        "A. One teaspoon",
        "B. Two tablespoons",
        "C. Half a cup",
        "D. Five tablespoons"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The science museum offers hands-on experiments for children. Workshops run on weekend afternoons.",
      "conflict_prompt": "The science museum does not offer hands-on experiments for children. Workshops run on weekend afternoons.",
      "question": "What type of activities does the science museum offer for children?",
      "options": [
        "A. Lectures only",
        "B. Hands-on experiments",
        "C. Movie screenings",
        "D. Static displays only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new appliance is energy-efficient. It has an Energy Star certification.",
      "conflict_prompt": "The new appliance is not energy-efficient. It has an Energy Star certification.",
      "question": "What certification does the appliance have?",
      "options": [
        "A. ISO 9001",
        "B. Energy Star",
        "C. USDA Organic",
        "D. CE Mark"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood watch meets on the first Tuesday of every month. Meetings begin at 7 PM.",
      "conflict_prompt": "The neighborhood watch does not meet on the first Tuesday of every month. Meetings begin at 7 PM.",
      "question": "When does the neighborhood watch meet each month?",
      "options": [
        "A. First Tuesday",
        "B. Last Friday",
        "C. Second Sunday",
        "D. Third Wednesday"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer's field is planted with corn this season. Harvest is expected in late September.",
      "conflict_prompt": "The farmer's field is not planted with corn this season. Harvest is expected in late September.",
      "question": "What crop is planted in the farmer's field this season?",
      "options": [
        "A. Wheat",
        "B. Corn",
        "C. Soybeans",
        "D. Cotton"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The fitness center has a sauna available to members. Sauna hours coincide with gym hours.",
      "conflict_prompt": "The fitness center does not have a sauna available to members. Sauna hours coincide with gym hours.",
      "question": "What amenity is available to fitness center members?",
      "options": [
        "A. Rock climbing wall",
        "B. Ice rink",
        "C. Sauna",
        "D. Bowling alley"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal pool is heated during the summer months. Lifeguards are on duty at all times the pool is open.",
      "conflict_prompt": "The municipal pool is not heated during the summer months. Lifeguards are on duty at all times the pool is open.",
      "question": "Is the municipal pool heated during summer according to the statement?",
      "options": [
        "A. Yes",
        "B. No",
        "C. Only weekends",
        "D. Only for competitions"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The electric car charges to 80% in 30 minutes on a fast charger. Its range is approximately 250 miles per charge.",
      "conflict_prompt": "The electric car does not charge to 80% in 30 minutes on a fast charger. Its range is approximately 250 miles per charge.",
      "question": "How long does it take the electric car to reach 80% charge on a fast charger?",
      "options": [
        "A. 10 minutes",
        "B. 30 minutes",
        "C. 2 hours",
        "D. 6 hours"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood elementary school has a bilingual Spanish-English program. Enrollment is open to all local students.",
      "conflict_prompt": "The neighborhood elementary school does not have a bilingual Spanish-English program. Enrollment is open to all local students.",
      "question": "What type of program does the elementary school offer?",
      "options": [
        "A. STEM-only curriculum",
        "B. Montessori program",
        "C. Bilingual Spanish-English program",
        "D. Foreign exchange program"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum café uses compostable cups. It sources pastries from a nearby bakery.",
      "conflict_prompt": "The museum café does not use compostable cups. It sources pastries from a nearby bakery.",
      "question": "What type of cups does the museum café use?",
      "options": [
        "A. Styrofoam cups",
        "B. Glassware only",
        "C. Compostable cups",
        "D. Ceramic mugs exclusively"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city bus accepts contactless payment. Riders can also pay with exact cash.",
      "conflict_prompt": "The city bus does not accept contactless payment. Riders can also pay with exact cash.",
      "question": "What modern payment method does the city bus accept?",
      "options": [
        "A. Checks",
        "B. Contactless payment",
        "C. Barter",
        "D. Gift cards only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hiking club requires participants to carry water. Leaders provide route maps before departure.",
      "conflict_prompt": "The hiking club does not require participants to carry water. Leaders provide route maps before departure.",
      "question": "What does the hiking club require participants to carry?",
      "options": [
        "A. Snowshoes",
        "B. Water",
        "C. Fishing gear",
        "D. Musical instruments"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local diner serves breakfast all day. The signature dish is a three-egg omelet.",
      "conflict_prompt": "The local diner does not serve breakfast all day. The signature dish is a three-egg omelet.",
      "question": "What is the diner's signature dish?",
      "options": [
        "A. Pancake stack",
        "B. Burgers",
        "C. Three-egg omelet",
        "D. Fish and chips"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community center offers free legal clinics on Thursdays. Appointments are required.",
      "conflict_prompt": "The community center does not offer free legal clinics on Thursdays. Appointments are required.",
      "question": "On which day are free legal clinics offered at the community center?",
      "options": [
        "A. Monday",
        "B. Thursday",
        "C. Saturday",
        "D. Wednesday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The laboratory uses nitrogen gas for the mass spectrometer. Tanks are stored in a ventilated cage.",
      "conflict_prompt": "The laboratory does not use nitrogen gas for the mass spectrometer. Tanks are stored in a ventilated cage.",
      "question": "What gas does the laboratory use for the mass spectrometer?",
      "options": [
        "A. Helium",
        "B. Nitrogen",
        "C. Argon",
        "D. Carbon dioxide"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The soccer club trains three times per week. Sessions are held on Tuesday, Thursday, and Saturday.",
      "conflict_prompt": "The soccer club does not train three times per week. Sessions are held on Tuesday, Thursday, and Saturday.",
      "question": "How many times per week does the soccer club train?",
      "options": [
        "A. Once",
        "B. Twice",
        "C. Three times",
        "D. Five times"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vineyard produces a dry rosé. Grapes are harvested in early September.",
      "conflict_prompt": "The vineyard does not produce a dry rosé. Grapes are harvested in early September.",
      "question": "What style of wine does the vineyard produce?",
      "options": [
        "A. Sweet dessert wine",
        "B. Dry rosé",
        "C. Sparkling champagne",
        "D. Fortified port"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The office copier is networked for printing from computers. It requires an access code for color prints.",
      "conflict_prompt": "The office copier is not networked for printing from computers. It requires an access code for color prints.",
      "question": "What feature does the office copier support?",
      "options": [
        "A. Only photocopying",
        "B. Networked printing from computers",
        "C. Faxing only",
        "D. Hand-crank operation"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local bookstore stocks rare first editions. Customers can request appraisals by appointment.",
      "conflict_prompt": "The local bookstore does not stock rare first editions. Customers can request appraisals by appointment.",
      "question": "What special items does the bookstore stock?",
      "options": [
        "A. Mass-market paperbacks only",
        "B. Rare first editions",
        "C. Discounted magazines only",
        "D. Textbooks only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater company's rehearsals are closed to the public. They hold open dress rehearsals the week before opening night.",
      "conflict_prompt": "The theater company's rehearsals are not closed to the public. They hold open dress rehearsals the week before opening night.",
      "question": "Are the theater company's regular rehearsals open to the public according to the statement?",
      "options": [
        "A. Yes",
        "B. No",
        "C. Only on weekends",
        "D. Only for donors"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery's croissants are made with European butter. They are baked fresh every morning.",
      "conflict_prompt": "The bakery's croissants are not made with European butter. They are baked fresh every morning.",
      "question": "What type of butter is used to make the bakery's croissants?",
      "options": [
        "A. Margarine",
        "B. European butter",
        "C. Vegan butter",
        "D. Peanut butter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coastal trail is wheelchair accessible. There are benches every quarter mile.",
      "conflict_prompt": "The coastal trail is not wheelchair accessible. There are benches every quarter mile.",
      "question": "What accessibility feature does the coastal trail have?",
      "options": [
        "A. Steps only",
        "B. Wheelchair accessible",
        "C. Narrow footpath only",
        "D. Rocky terrain only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum's audio guide is available in five languages. Headsets are provided at the entrance.",
      "conflict_prompt": "The museum's audio guide is not available in five languages. Headsets are provided at the entrance.",
      "question": "How many languages is the museum's audio guide available in?",
      "options": [
        "A. Two",
        "B. Three",
        "C. Five",
        "D. Ten"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public garden charges a small entrance fee. Children under six enter free.",
      "conflict_prompt": "The public garden does not charge a small entrance fee. Children under six enter free.",
      "question": "Who is exempt from the entrance fee at the public garden?",
      "options": [
        "A. Adults only",
        "B. Seniors only",
        "C. Children under six",
        "D. All visitors"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel's rooftop pool is heated year-round. Access is restricted to registered guests.",
      "conflict_prompt": "The hotel's rooftop pool is not heated year-round. Access is restricted to registered guests.",
      "question": "Who is allowed to use the hotel's rooftop pool?",
      "options": [
        "A. Registered guests",
        "B. The general public",
        "C. Only VIPs",
        "D. Local residents only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The photography workshop focuses on portrait lighting. Participants receive one-on-one critiques.",
      "conflict_prompt": "The photography workshop does not focus on portrait lighting. Participants receive one-on-one critiques.",
      "question": "What topic does the photography workshop focus on?",
      "options": [
        "A. Landscape composition",
        "B. Portrait lighting",
        "C. Wildlife tracking",
        "D. Darkroom development"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The summer camp provides canoeing lessons. Life jackets are provided to every camper.",
      "conflict_prompt": "The summer camp does not provide canoeing lessons. Life jackets are provided to every camper.",
      "question": "What water activity does the summer camp provide lessons for?",
      "options": [
        "A. Sailing",
        "B. Canoeing",
        "C. Scuba diving",
        "D. Surfing"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The antique shop specializes in mid-century furniture. Appointments are recommended for large purchases.",
      "conflict_prompt": "The antique shop does not specialize in mid-century furniture. Appointments are recommended for large purchases.",
      "question": "What type of items does the antique shop specialize in?",
      "options": [
        "A. Victorian antiques",
        "B. Mid-century furniture",
        "C. Contemporary art",
        "D. Electronics"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pharmacy offers a free medication disposal service. Drop-off is available during business hours.",
      "conflict_prompt": "The pharmacy does not offer a free medication disposal service. Drop-off is available during business hours.",
      "question": "What service does the pharmacy offer for unused medications?",
      "options": [
        "A. Mail-in return only",
        "B. Free disposal service",
        "C. Resale to customers",
        "D. Incineration by customers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college offers a free introductory coding workshop. The workshop is open to adults of all ages.",
      "conflict_prompt": "The community college does not offer a free introductory coding workshop. The workshop is open to adults of all ages.",
      "question": "Who is the introductory coding workshop open to?",
      "options": [
        "A. Children only",
        "B. College students only",
        "C. Adults of all ages",
        "D. Teachers only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical society meets monthly to discuss conservation. Meetings take place at the library auditorium.",
      "conflict_prompt": "The botanical society does not meet monthly to discuss conservation. Meetings take place at the library auditorium.",
      "question": "Where does the botanical society hold its meetings?",
      "options": [
        "A. City hall",
        "B. Library auditorium",
        "C. Outdoor park",
        "D. Private homes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school cafeteria offers vegetarian options daily. Menus are posted online each week.",
      "conflict_prompt": "The school cafeteria does not offer vegetarian options daily. Menus are posted online each week.",
      "question": "How often does the school cafeteria offer vegetarian options according to the statement?",
      "options": [
        "A. Once a month",
        "B. Daily",
        "C. Only Fridays",
        "D. Only during holidays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new commuter rail line connects the suburbs to downtown. Trains run every 20 minutes during rush hour.",
      "conflict_prompt": "The new commuter rail line does not connect the suburbs to downtown. Trains run every 20 minutes during rush hour.",
      "question": "What area does the new commuter rail line connect to downtown?",
      "options": [
        "A. Airports only",
        "B. Suburbs",
        "C. Rural farmlands",
        "D. Inter-state freight terminals"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The junior chess tournament is open to players under 14. Registration closes two weeks before the event.",
      "conflict_prompt": "The junior chess tournament is not open to players under 14. Registration closes two weeks before the event.",
      "question": "Which age group is eligible for the junior chess tournament?",
      "options": [
        "A. Under 18",
        "B. Over 16 only",
        "C. Under 14",
        "D. Adults only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery accepts pre-orders for holiday pies. Orders must be placed 48 hours in advance.",
      "conflict_prompt": "The bakery does not accept pre-orders for holiday pies. Orders must be placed 48 hours in advance.",
      "question": "How far in advance must customers place pre-orders for holiday pies?",
      "options": [
        "A. 24 hours",
        "B. 48 hours",
        "C. One week",
        "D. One month"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The film festival screens independent documentaries. Screenings begin at 6 PM each evening.",
      "conflict_prompt": "The film festival does not screen independent documentaries. Screenings begin at 6 PM each evening.",
      "question": "What genre does the film festival screen?",
      "options": [
        "A. Blockbuster action films",
        "B. Independent documentaries",
        "C. Silent films only",
        "D. Animated shorts only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public library offers free notary services. Notary appointments are available on weekdays.",
      "conflict_prompt": "The public library does not offer free notary services. Notary appointments are available on weekdays.",
      "question": "What free service does the public library offer?",
      "options": [
        "A. Free legal representation",
        "B. Free notary services",
        "C. Free computer repair",
        "D. Free translation services"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The airport shuttle runs every 15 minutes. The last shuttle leaves at midnight.",
      "conflict_prompt": "The airport shuttle does not run every 15 minutes. The last shuttle leaves at midnight.",
      "question": "How often does the airport shuttle run according to the statement?",
      "options": [
        "A. Every hour",
        "B. Every 15 minutes",
        "C. Every 45 minutes",
        "D. Twice a day"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery's sourdough starter is over five years old. Bakers feed it daily.",
      "conflict_prompt": "The bakery's sourdough starter is not over five years old. Bakers feed it daily.",
      "question": "How old is the bakery's sourdough starter according to the statement?",
      "options": [
        "A. Less than a year",
        "B. Over five years",
        "C. Ten years exactly",
        "D. One month"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art museum prohibits flash photography. Security staff enforce the rule during visiting hours.",
      "conflict_prompt": "The art museum does not prohibit flash photography. Security staff enforce the rule during visiting hours.",
      "question": "What photography rule does the art museum enforce?",
      "options": [
        "A. Flash photography prohibited",
        "B. No photography at all",
        "C. Cameras must be checked",
        "D. Only film cameras allowed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers market accepts SNAP benefits. There is a token booth near the entrance.",
      "conflict_prompt": "The farmers market does not accept SNAP benefits. There is a token booth near the entrance.",
      "question": "What payment assistance program does the farmers market accept?",
      "options": [
        "A. WIC only",
        "B. SNAP benefits",
        "C. None",
        "D. Food stamps only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community theater performs classic musicals. The current production runs for three weekends.",
      "conflict_prompt": "The community theater does not perform classic musicals. The current production runs for three weekends.",
      "question": "What genre of shows does the community theater perform?",
      "options": [
        "A. Stand-up comedy",
        "B. Classic musicals",
        "C. Puppet shows",
        "D. Magic acts"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The riverside trail is closed after heavy rain until inspected. Notices are posted at both trailheads.",
      "conflict_prompt": "The riverside trail is not closed after heavy rain until inspected. Notices are posted at both trailheads.",
      "question": "When is the riverside trail closed according to the statement?",
      "options": [
        "A. During winter only",
        "B. After heavy rain until inspected",
        "C. Only on holidays",
        "D. Never closed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The scholarship covers full tuition for two years. Recipients must maintain a 3.5 GPA.",
      "conflict_prompt": "The scholarship does not cover full tuition for two years. Recipients must maintain a 3.5 GPA.",
      "question": "What academic requirement must scholarship recipients maintain?",
      "options": [
        "A. 2.0 GPA",
        "B. 3.0 GPA",
        "C. 3.5 GPA",
        "D. No GPA requirement"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The monthly magazine is published on the first of every month. Subscriptions renew automatically.",
      "conflict_prompt": "The monthly magazine is not published on the first of every month. Subscriptions renew automatically.",
      "question": "On which day is the monthly magazine published?",
      "options": [
        "A. First of every month",
        "B. Last day of the month",
        "C. Every Monday",
        "D. Quarterly only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The music school offers private violin lessons. Teachers require students to practice at least 30 minutes daily.",
      "conflict_prompt": "The music school does not offer private violin lessons. Teachers require students to practice at least 30 minutes daily.",
      "question": "What type of private lessons does the music school offer?",
      "options": [
        "A. Guitar",
        "B. Violin",
        "C. Drums",
        "D. Piano only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden's conservatory houses tropical orchids. It maintains a humid, controlled climate.",
      "conflict_prompt": "The botanical garden's conservatory does not house tropical orchids. It maintains a humid, controlled climate.",
      "question": "What type of plants does the conservatory house?",
      "options": [
        "A. Cacti",
        "B. Tropical orchids",
        "C. Alpine mosses",
        "D. Tundra shrubs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter ferry allows bicycles on board. A small fee applies per bike.",
      "conflict_prompt": "The commuter ferry does not allow bicycles on board. A small fee applies per bike.",
      "question": "What item is permitted on the ferry for a small fee?",
      "options": [
        "A. Pets only",
        "B. Bicycles",
        "C. Cars",
        "D. Motorcycles only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city ordinance requires businesses to recycle glass. Inspections occur quarterly.",
      "conflict_prompt": "The city ordinance does not require businesses to recycle glass. Inspections occur quarterly.",
      "question": "What does the city ordinance require businesses to recycle?",
      "options": [
        "A. Electronics",
        "B. Glass",
        "C. Paper only",
        "D. Food waste only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater offers discounted student tickets. Valid student ID must be presented at the box office.",
      "conflict_prompt": "The theater does not offer discounted student tickets. Valid student ID must be presented at the box office.",
      "question": "What must students present to receive discounted tickets?",
      "options": [
        "A. Birth certificate",
        "B. Library card",
        "C. Valid student ID",
        "D. No ID required"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city hires seasonal park rangers in the summer. Rangers patrol the trails daily.",
      "conflict_prompt": "The city does not hire seasonal park rangers in the summer. Rangers patrol the trails daily.",
      "question": "When does the city hire seasonal park rangers?",
      "options": [
        "A. Winter",
        "B. Autumn",
        "C. Summer",
        "D. Spring"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new museum wing is dedicated to modern art. It opened to the public last month.",
      "conflict_prompt": "The new museum wing is not dedicated to modern art. It opened to the public last month.",
      "question": "What is the focus of the new museum wing?",
      "options": [
        "A. Natural history",
        "B. Modern art",
        "C. Ancient artifacts",
        "D. Children's exhibits"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local coffee roaster sources beans from Central America. They roast in small batches weekly.",
      "conflict_prompt": "The local coffee roaster does not source beans from Central America. They roast in small batches weekly.",
      "question": "From which region does the coffee roaster source beans according to the statement?",
      "options": [
        "A. East Asia",
        "B. Central America",
        "C. Scandinavia",
        "D. Australia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The speech therapy clinic treats children with articulation disorders. Sessions include parent training.",
      "conflict_prompt": "The speech therapy clinic does not treat children with articulation disorders. Sessions include parent training.",
      "question": "Which condition does the clinic treat?",
      "options": [
        "A. Diabetes",
        "B. Articulation disorders in children",
        "C. Broken bones",
        "D. Vision impairment"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The wildlife refuge prohibits feeding wild animals. Volunteers post educational signs along the trails.",
      "conflict_prompt": "The wildlife refuge does not prohibit feeding wild animals. Volunteers post educational signs along the trails.",
      "question": "What action is prohibited at the wildlife refuge?",
      "options": [
        "A. Hiking",
        "B. Feeding wild animals",
        "C. Birdwatching",
        "D. Photography"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local winery offers tastings by appointment. Tastings include five small pours.",
      "conflict_prompt": "The local winery does not offer tastings by appointment. Tastings include five small pours.",
      "question": "How many pours are included in the winery tasting according to the statement?",
      "options": [
        "A. Three",
        "B. Five",
        "C. Seven",
        "D. Ten"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community bulletin board posts announcements for neighborhood events. Submissions are accepted via email.",
      "conflict_prompt": "The community bulletin board does not post announcements for neighborhood events. Submissions are accepted via email.",
      "question": "How can residents submit announcements to the community bulletin board?",
      "options": [
        "A. In person only",
        "B. By fax only",
        "C. Via email",
        "D. By phone call only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel's concierge arranges city tours. Reservations can be made at the front desk.",
      "conflict_prompt": "The hotel's concierge does not arrange city tours. Reservations can be made at the front desk.",
      "question": "What service does the hotel's concierge provide according to the statement?",
      "options": [
        "A. Car repair",
        "B. Arranging city tours",
        "C. Medical care",
        "D. Home cleaning services"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The playground equipment is inspected monthly for safety. Inspection reports are available upon request.",
      "conflict_prompt": "The playground equipment is not inspected monthly for safety. Inspection reports are available upon request.",
      "question": "How often is the playground equipment inspected for safety?",
      "options": [
        "A. Annually",
        "B. Monthly",
        "C. Weekly",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer's co-op sells organic eggs at the market. They come from free-range hens.",
      "conflict_prompt": "The farmer's co-op does not sell organic eggs at the market. They come from free-range hens.",
      "question": "What type of eggs does the farmer's co-op sell at the market?",
      "options": [
        "A. Organic eggs",
        "B. Hatchery eggs only",
        "C. Synthetic eggs",
        "D. Ostrich eggs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant accepts reservations for private dining. Parties of more than ten require a deposit.",
      "conflict_prompt": "The restaurant does not accept reservations for private dining. Parties of more than ten require a deposit.",
      "question": "What is required for parties larger than ten for private dining?",
      "options": [
        "A. Nothing",
        "B. A deposit",
        "C. A government permit",
        "D. A membership card"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool offers lap swim in the morning. Recreational swim is scheduled in the afternoons.",
      "conflict_prompt": "The community pool does not offer lap swim in the morning. Recreational swim is scheduled in the afternoons.",
      "question": "When is lap swim offered at the community pool?",
      "options": [
        "A. Mornings",
        "B. Evenings",
        "C. Weekends only",
        "D. Not offered"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The crafts workshop teaches pottery wheel techniques. Materials are included in the fee.",
      "conflict_prompt": "The crafts workshop does not teach pottery wheel techniques. Materials are included in the fee.",
      "question": "What technique is taught in the crafts workshop?",
      "options": [
        "A. Glassblowing",
        "B. Pottery wheel techniques",
        "C. Wood carving",
        "D. Metal forging"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike lane runs along Oak Street for two miles. It connects to the city greenway near the park.",
      "conflict_prompt": "The bike lane does not run along Oak Street for two miles. It connects to the city greenway near the park.",
      "question": "Along which street does the two-mile bike lane run?",
      "options": [
        "A. Pine Street",
        "B. Oak Street",
        "C. Maple Avenue",
        "D. Elm Road"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university's alumni association holds an annual reunion in June. Alumni can register online.",
      "conflict_prompt": "The university's alumni association does not hold an annual reunion in June. Alumni can register online.",
      "question": "When is the alumni association's annual reunion held according to the statement?",
      "options": [
        "A. December",
        "B. June",
        "C. March",
        "D. September"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public transit map shows all bus routes. Timetables are printed at major stops.",
      "conflict_prompt": "The public transit map does not show all bus routes. Timetables are printed at major stops.",
      "question": "What information does the public transit map display according to the statement?",
      "options": [
        "A. Only subway lines",
        "B. All bus routes",
        "C. Taxi services only",
        "D. Bike repair shops"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood yearly clean-up is organized by volunteers. Tools and gloves are provided at the meeting point.",
      "conflict_prompt": "The neighborhood yearly clean-up is not organized by volunteers. Tools and gloves are provided at the meeting point.",
      "question": "Who organizes the neighborhood yearly clean-up according to the statement?",
      "options": [
        "A. City government only",
        "B. Private contractors only",
        "C. Volunteers",
        "D. A corporate sponsor"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local high school hosts a college fair each fall. Over 50 universities attend annually.",
      "conflict_prompt": "The local high school does not host a college fair each fall. Over 50 universities attend annually.",
      "question": "When is the college fair hosted at the high school?",
      "options": [
        "A. Spring",
        "B. Summer",
        "C. Fall",
        "D. Winter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community orchestra rehearses weekly at the municipal auditorium. Concerts occur three times a year.",
      "conflict_prompt": "The community orchestra does not rehearse weekly at the municipal auditorium. Concerts occur three times a year.",
      "question": "How often does the community orchestra rehearse according to the statement?",
      "options": [
        "A. Monthly",
        "B. Weekly",
        "C. Daily",
        "D. Quarterly"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The urban garden uses drip irrigation to conserve water. Volunteers maintain planting schedules by plot.",
      "conflict_prompt": "The urban garden does not use drip irrigation to conserve water. Volunteers maintain planting schedules by plot.",
      "question": "What irrigation method does the urban garden use according to the statement?",
      "options": [
        "A. Overhead sprinklers",
        "B. Drip irrigation",
        "C. Flood irrigation",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city offers free compost bins to residents. Applications are accepted online through the sanitation department.",
      "conflict_prompt": "The city does not offer free compost bins to residents. Applications are accepted online through the sanitation department.",
      "question": "How do residents apply for free compost bins according to the statement?",
      "options": [
        "A. At city hall in person",
        "B. By mail only",
        "C. Online through the sanitation department",
        "D. By phone only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local gym offers a beginner's yoga class. Mats are available for first-time attendees.",
      "conflict_prompt": "The local gym does not offer a beginner's yoga class. Mats are available for first-time attendees.",
      "question": "What class does the local gym offer for beginners according to the statement?",
      "options": [
        "A. Kickboxing",
        "B. Beginner's yoga",
        "C. Advanced bench pressing",
        "D. Marathon training"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The summer reading program awards prizes for reading 10 books. Registration is free for children under 12.",
      "conflict_prompt": "The summer reading program does not award prizes for reading 10 books. Registration is free for children under 12.",
      "question": "How many books must children read to be eligible for prizes in the summer program?",
      "options": [
        "A. 5",
        "B. 10",
        "C. 20",
        "D. 1"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The dog park is fenced and has separate areas for small and large breeds. Water stations are provided.",
      "conflict_prompt": "The dog park is not fenced and has separate areas for small and large breeds. Water stations are provided.",
      "question": "What feature separates areas for different dog sizes at the park?",
      "options": [
        "A. Fencing",
        "B. No separation",
        "C. Ropes only",
        "D. Signs only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public art installation features interactive lights. It operates nightly from dusk until midnight.",
      "conflict_prompt": "The public art installation does not feature interactive lights. It operates nightly from dusk until midnight.",
      "question": "What interactive element does the public art installation feature according to the statement?",
      "options": [
        "A. Water fountains",
        "B. Interactive lights",
        "C. Live performers",
        "D. Motionless sculptures"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers market is located in the town square on Saturdays. It starts at 8 AM and ends at 1 PM.",
      "conflict_prompt": "The farmers market is not located in the town square on Saturdays. It starts at 8 AM and ends at 1 PM.",
      "question": "Where is the farmers market located on Saturdays according to the statement?",
      "options": [
        "A. Industrial park",
        "B. Town square",
        "C. High school gym",
        "D. Private farm"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood association maintains a lending library box. Residents donate books and borrow freely.",
      "conflict_prompt": "The neighborhood association does not maintain a lending library box. Residents donate books and borrow freely.",
      "question": "What amenity does the neighborhood association maintain according to the statement?",
      "options": [
        "A. Free bike repair station",
        "B. Lending library box",
        "C. Free coffee kiosk",
        "D. Public piano"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The weekend art fair features local artisans. Booth applications are reviewed by a committee.",
      "conflict_prompt": "The weekend art fair does not feature local artisans. Booth applications are reviewed by a committee.",
      "question": "Who primarily features their work at the weekend art fair according to the statement?",
      "options": [
        "A. International corporations",
        "B. Local artisans",
        "C. Government agencies",
        "D. Fast-food vendors"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city pool offers adaptive swim lessons for people with disabilities. Registration must be completed online.",
      "conflict_prompt": "The city pool does not offer adaptive swim lessons for people with disabilities. Registration must be completed online.",
      "question": "What specialized lessons does the city pool offer according to the statement?",
      "options": [
        "A. Surfing lessons",
        "B. Adaptive swim lessons for people with disabilities",
        "C. Synchronized swimming only",
        "D. Water polo only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool is heated. It maintains a temperature of 82°F during swim season.",
      "conflict_prompt": "The community pool is not heated. It maintains a temperature of 82°F during swim season.",
      "question": "What is the community pool's temperature during swim season?",
      "options": [
        "A. 72°F",
        "B. 82°F",
        "C. 92°F",
        "D. 62°F"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Dr. Kim is a pediatrician who works at the downtown clinic.",
      "conflict_prompt": "Dr. Kim is not a pediatrician who works at the downtown clinic.",
      "question": "What is Dr. Kim's medical specialty?",
      "options": [
        "A. Cardiologist",
        "B. Dermatologist",
        "C. Pediatrician",
        "D. Neurologist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new electric bus runs on battery power and stops at all main transit hubs.",
      "conflict_prompt": "The new electric bus does not run on battery power and stops at all main transit hubs.",
      "question": "What powers the new electric bus?",
      "options": [
        "A. Diesel",
        "B. Human pedaling",
        "C. Battery power",
        "D. Natural gas"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Maria is fluent in Spanish and teaches literature classes in Spanish.",
      "conflict_prompt": "Maria is not fluent in Spanish and teaches literature classes in Spanish.",
      "question": "Which language does Maria teach literature in?",
      "options": [
        "A. French",
        "B. German",
        "C. Spanish",
        "D. Italian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city recycles glass at its central facility every Monday.",
      "conflict_prompt": "The city does not recycle glass at its central facility every Monday.",
      "question": "When does the city recycle glass at the central facility?",
      "options": [
        "A. Monday",
        "B. Wednesday",
        "C. Friday",
        "D. Sunday"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bookstore sells both new and used textbooks on the first floor.",
      "conflict_prompt": "The bookstore does not sell both new and used textbooks on the first floor.",
      "question": "What types of textbooks does the bookstore sell on the first floor?",
      "options": [
        "A. Antique only",
        "B. New and used",
        "C. Only digital",
        "D. None at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Our office supplies compostable cups in the break room.",
      "conflict_prompt": "Our office does not supply compostable cups in the break room.",
      "question": "What kind of cups are provided in the break room?",
      "options": [
        "A. Styrofoam",
        "B. Glass only",
        "C. Compostable",
        "D. No cups provided"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hiking trail is open year-round and is marked with blue blazes.",
      "conflict_prompt": "The hiking trail is not open year-round and is marked with blue blazes.",
      "question": "What color blazes mark the hiking trail?",
      "options": [
        "A. Red",
        "B. Blue",
        "C. Green",
        "D. Yellow"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum permits photography without flash in most galleries.",
      "conflict_prompt": "The museum does not permit photography without flash in most galleries.",
      "question": "What is the museum's photography policy in most galleries?",
      "options": [
        "A. Flash photography allowed",
        "B. No photography at all",
        "C. Photography allowed without flash",
        "D. Only tripod photography allowed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery sells sourdough loaves every morning starting at 7 AM.",
      "conflict_prompt": "The bakery does not sell sourdough loaves every morning starting at 7 AM.",
      "question": "When does the bakery begin selling sourdough loaves?",
      "options": [
        "A. 5 AM",
        "B. 7 AM",
        "C. 10 AM",
        "D. Noon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Jason owns a hybrid sedan that he drives to work weekdays.",
      "conflict_prompt": "Jason does not own a hybrid sedan that he drives to work weekdays.",
      "question": "What kind of vehicle does Jason own?",
      "options": [
        "A. Hybrid sedan",
        "B. Electric scooter",
        "C. Pickup truck",
        "D. Bicycle only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conference room has video conferencing equipment and a large projector screen.",
      "conflict_prompt": "The conference room does not have video conferencing equipment and a large projector screen.",
      "question": "What equipment is available in the conference room?",
      "options": [
        "A. Sewing machines",
        "B. Video conferencing equipment",
        "C. Stoves",
        "D. Treadmills"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university offers a scholarship for first-generation college students.",
      "conflict_prompt": "The university does not offer a scholarship for first-generation college students.",
      "question": "Who is eligible for the scholarship mentioned?",
      "options": [
        "A. First-generation college students",
        "B. Only graduate students",
        "C. Only faculty members",
        "D. International tourists"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic administers flu shots to patients during October and November.",
      "conflict_prompt": "The clinic does not administer flu shots to patients during October and November.",
      "question": "When does the clinic administer flu shots?",
      "options": [
        "A. October and November",
        "B. June and July",
        "C. January only",
        "D. Year-round every day"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry operates between the island and mainland every hour from 6 AM to 10 PM.",
      "conflict_prompt": "The ferry does not operate between the island and mainland every hour from 6 AM to 10 PM.",
      "question": "How frequently does the ferry operate during the day?",
      "options": [
        "A. Every hour",
        "B. Every 15 minutes",
        "C. Once a day",
        "D. Every other day"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Samantha keeps her passport in a locked safe at home.",
      "conflict_prompt": "Samantha does not keep her passport in a locked safe at home.",
      "question": "Where does Samantha keep her passport?",
      "options": [
        "A. In a locked safe at home",
        "B. In her desk drawer at work",
        "C. In a mailbox",
        "D. She doesn't have a passport"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The research lab follows a strict no-food policy to protect experiments.",
      "conflict_prompt": "The research lab does not follow a strict no-food policy to protect experiments.",
      "question": "What is the lab's policy regarding food?",
      "options": [
        "A. No-food policy",
        "B. Unlimited food allowed",
        "C. Only communal snacks allowed",
        "D. Only hot meals allowed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Oliver renovated the deck using pressure-treated lumber last summer.",
      "conflict_prompt": "Oliver did not renovate the deck using pressure-treated lumber last summer.",
      "question": "What material did Oliver use to renovate the deck?",
      "options": [
        "A. Marble tiles",
        "B. Pressure-treated lumber",
        "C. Glass panels",
        "D. Carpet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art class meets on Thursdays and focuses on watercolor techniques.",
      "conflict_prompt": "The art class does not meet on Thursdays and focuses on watercolor techniques.",
      "question": "Which day does the art class meet?",
      "options": [
        "A. Sunday",
        "B. Thursday",
        "C. Tuesday",
        "D. Saturday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The zoo feeds the penguins live fish every morning at 9 AM.",
      "conflict_prompt": "The zoo does not feed the penguins live fish every morning at 9 AM.",
      "question": "What time are the penguins fed live fish?",
      "options": [
        "A. 6 AM",
        "B. 9 AM",
        "C. 3 PM",
        "D. 8 PM"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The startup provides free lunches for employees on Wednesdays.",
      "conflict_prompt": "The startup does not provide free lunches for employees on Wednesdays.",
      "question": "On which day does the startup provide free lunches?",
      "options": [
        "A. Monday",
        "B. Wednesday",
        "C. Friday",
        "D. Sunday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The cathedral holds an organ recital every first Sunday of the month.",
      "conflict_prompt": "The cathedral does not hold an organ recital every first Sunday of the month.",
      "question": "How often does the cathedral hold the organ recital?",
      "options": [
        "A. Every first Sunday of the month",
        "B. Daily",
        "C. Once a year",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone comes with a one-year warranty from the manufacturer.",
      "conflict_prompt": "The smartphone does not come with a one-year warranty from the manufacturer.",
      "question": "How long is the manufacturer's warranty for the smartphone?",
      "options": [
        "A. One week",
        "B. One year",
        "C. Five years",
        "D. No warranty at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood garden is open to residents for planting during spring.",
      "conflict_prompt": "The neighborhood garden is not open to residents for planting during spring.",
      "question": "When can residents plant in the neighborhood garden?",
      "options": [
        "A. Winter",
        "B. Spring",
        "C. Late autumn",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train offers Wi-Fi to passengers in all carriages.",
      "conflict_prompt": "The train does not offer Wi-Fi to passengers in all carriages.",
      "question": "What amenity is offered to passengers on the train?",
      "options": [
        "A. Free laundry",
        "B. Wi-Fi",
        "C. In-car bowling alley",
        "D. Onboard helicopter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The piano tuner visits the school annually to tune all upright pianos.",
      "conflict_prompt": "The piano tuner does not visit the school annually to tune all upright pianos.",
      "question": "How often does the piano tuner visit the school?",
      "options": [
        "A. Annually",
        "B. Every six hours",
        "C. Every ten years",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The riverboat offers dinner cruises with a three-course meal.",
      "conflict_prompt": "The riverboat does not offer dinner cruises with a three-course meal.",
      "question": "What type of cruise does the riverboat offer?",
      "options": [
        "A. Dinner cruises with a three-course meal",
        "B. Only fishing tours",
        "C. Space travel excursions",
        "D. Underground subway tours"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software application saves documents automatically every five minutes.",
      "conflict_prompt": "The software application does not save documents automatically every five minutes.",
      "question": "How often does the application autosave documents?",
      "options": [
        "A. Every five minutes",
        "B. Every two hours",
        "C. Once a month",
        "D. It never saves automatically"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery offers vegan pastries labeled on the left-hand display case.",
      "conflict_prompt": "The bakery does not offer vegan pastries labeled on the left-hand display case.",
      "question": "Where are the vegan pastries located?",
      "options": [
        "A. Right-hand display case",
        "B. Left-hand display case",
        "C. On the rooftop",
        "D. In the basement storage"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The volunteer firefighters train twice a month at the station.",
      "conflict_prompt": "The volunteer firefighters do not train twice a month at the station.",
      "question": "How often do the volunteer firefighters train at the station?",
      "options": [
        "A. Twice a month",
        "B. Every day around the clock",
        "C. Once every five years",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The parking garage has spaces reserved for electric vehicles with charging stations.",
      "conflict_prompt": "The parking garage does not have spaces reserved for electric vehicles with charging stations.",
      "question": "What special parking spaces does the garage provide?",
      "options": [
        "A. Motorcycle only",
        "B. Electric vehicle charging spaces",
        "C. Helicopter landing pads",
        "D. Horse stables"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery's signature croissant is made with European butter.",
      "conflict_prompt": "The bakery's signature croissant is not made with European butter.",
      "question": "What type of butter is used for the bakery's signature croissant?",
      "options": [
        "A. Margarine",
        "B. European butter",
        "C. Olive oil",
        "D. Coconut oil"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The language tutor specializes in conversational Mandarin for adults.",
      "conflict_prompt": "The language tutor does not specialize in conversational Mandarin for adults.",
      "question": "What does the language tutor specialize in?",
      "options": [
        "A. Conversational Mandarin for adults",
        "B. Ancient Sumerian",
        "C. Advanced mathematics",
        "D. Aerospace engineering"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry terminal has a covered waiting area for passengers during bad weather.",
      "conflict_prompt": "The ferry terminal does not have a covered waiting area for passengers during bad weather.",
      "question": "What facility does the ferry terminal provide for passengers in bad weather?",
      "options": [
        "A. Covered waiting area",
        "B. Free scuba gear",
        "C. Open-air picnic tables only",
        "D. Underground tunnels to nearby cities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The grocery store stocks gluten-free pasta in the health foods aisle.",
      "conflict_prompt": "The grocery store does not stock gluten-free pasta in the health foods aisle.",
      "question": "Where is the gluten-free pasta stocked?",
      "options": [
        "A. Health foods aisle",
        "B. Pet supplies section",
        "C. Automotive aisle",
        "D. Outside the store"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The auditorium seats 1,200 people and has accessible seating areas.",
      "conflict_prompt": "The auditorium does not seat 1,200 people and has accessible seating areas.",
      "question": "What is the seating capacity of the auditorium?",
      "options": [
        "A. 500",
        "B. 1,200",
        "C. 10,000",
        "D. 12"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The volunteer program requires applicants to be at least 18 years old.",
      "conflict_prompt": "The volunteer program does not require applicants to be at least 18 years old.",
      "question": "What is the minimum age requirement for the volunteer program?",
      "options": [
        "A. 12 years",
        "B. 16 years",
        "C. 18 years",
        "D. No minimum age"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The cinema shows indie films on Tuesdays with discounted tickets.",
      "conflict_prompt": "The cinema does not show indie films on Tuesdays with discounted tickets.",
      "question": "What happens at the cinema on Tuesdays?",
      "options": [
        "A. Indie films with discounted tickets",
        "B. Science lectures",
        "C. Car repair demonstrations",
        "D. Roller-skating competitions"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden displays seasonal tulips in the east section during April.",
      "conflict_prompt": "The botanical garden does not display seasonal tulips in the east section during April.",
      "question": "What flowers are displayed in the east section during April?",
      "options": [
        "A. Cacti",
        "B. Tulips",
        "C. Pine trees",
        "D. Seaweed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The fitness studio offers prenatal yoga classes for expectant mothers.",
      "conflict_prompt": "The fitness studio does not offer prenatal yoga classes for expectant mothers.",
      "question": "Which class does the fitness studio offer?",
      "options": [
        "A. Prenatal yoga",
        "B. Rocket science",
        "C. Advanced welding",
        "D. None at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The deli prepares sandwiches to order using freshly sliced meats.",
      "conflict_prompt": "The deli does not prepare sandwiches to order using freshly sliced meats.",
      "question": "How are the deli's sandwiches prepared?",
      "options": [
        "A. Prepackaged only",
        "B. To order with freshly sliced meats",
        "C. Made of plastic",
        "D. They don't sell sandwiches"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The irrigation system waters the community garden every other morning at dawn.",
      "conflict_prompt": "The irrigation system does not water the community garden every other morning at dawn.",
      "question": "When does the irrigation system water the garden?",
      "options": [
        "A. Every night at midnight",
        "B. Every other morning at dawn",
        "C. Once a year",
        "D. It never waters"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The courier delivers packages to the building lobby before 5 PM on weekdays.",
      "conflict_prompt": "The courier does not deliver packages to the building lobby before 5 PM on weekdays.",
      "question": "By what time does the courier deliver packages to the lobby on weekdays?",
      "options": [
        "A. Before 5 PM",
        "B. After midnight only",
        "C. Only on weekends",
        "D. Once every decade"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The children's museum provides interactive science exhibits for ages 3 to 10.",
      "conflict_prompt": "The children's museum does not provide interactive science exhibits for ages 3 to 10.",
      "question": "What age range are the interactive science exhibits intended for?",
      "options": [
        "A. 3 to 10",
        "B. 18 to 25",
        "C. Senior citizens only",
        "D. Infants only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local bakery bakes gluten-free bread daily in a separate oven.",
      "conflict_prompt": "The local bakery does not bake gluten-free bread daily in a separate oven.",
      "question": "How often is gluten-free bread baked at the bakery?",
      "options": [
        "A. Daily",
        "B. Once a year",
        "C. Never",
        "D. Only on full moons"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel provides a complimentary shuttle to the airport every morning at 6:00 AM.",
      "conflict_prompt": "The hotel does not provide a complimentary shuttle to the airport every morning at 6:00 AM.",
      "question": "What time is the hotel's complimentary airport shuttle?",
      "options": [
        "A. 6:00 AM",
        "B. 9:00 PM",
        "C. Noon",
        "D. There is no shuttle"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The painter uses only non-toxic, water-based paints for interior work.",
      "conflict_prompt": "The painter does not use only non-toxic, water-based paints for interior work.",
      "question": "What type of paint does the painter use for interior work?",
      "options": [
        "A. Non-toxic, water-based paints",
        "B. Lead-based oil paints",
        "C. Molten metal",
        "D. No paint at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers' market accepts SNAP benefits every Saturday.",
      "conflict_prompt": "The farmers' market does not accept SNAP benefits every Saturday.",
      "question": "When does the farmers' market accept SNAP benefits?",
      "options": [
        "A. Every Saturday",
        "B. Only on weekdays",
        "C. Only on holidays",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The cruise ship has an onboard doctor available 24 hours a day.",
      "conflict_prompt": "The cruise ship does not have an onboard doctor available 24 hours a day.",
      "question": "How often is the onboard doctor available on the cruise ship?",
      "options": [
        "A. 24 hours a day",
        "B. Only during meal times",
        "C. Twice a year",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike rental shop provides helmets with every rental at no extra charge.",
      "conflict_prompt": "The bike rental shop does not provide helmets with every rental at no extra charge.",
      "question": "What does the bike rental shop include with each rental?",
      "options": [
        "A. Helmets at no extra charge",
        "B. Free flight tickets",
        "C. Live music",
        "D. A new bicycle every month"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public library catalogs new arrivals in the online system within 48 hours.",
      "conflict_prompt": "The public library does not catalog new arrivals in the online system within 48 hours.",
      "question": "How quickly are new arrivals cataloged in the online system?",
      "options": [
        "A. Within 48 hours",
        "B. After one year",
        "C. They are not cataloged",
        "D. Immediately in real time"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater requires patrons to silence phones during performances.",
      "conflict_prompt": "The theater does not require patrons to silence phones during performances.",
      "question": "What does the theater require patrons to do during performances?",
      "options": [
        "A. Silence phones",
        "B. Bring pets on stage",
        "C. Record the show",
        "D. Dance in the aisles"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare center serves organic snacks to children each afternoon.",
      "conflict_prompt": "The daycare center does not serve organic snacks to children each afternoon.",
      "question": "What kind of snacks does the daycare center serve each afternoon?",
      "options": [
        "A. Organic snacks",
        "B. Motor oil",
        "C. No snacks at all",
        "D. Only candy"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain lodge provides snowshoes for guests free of charge during winter.",
      "conflict_prompt": "The mountain lodge does not provide snowshoes for guests free of charge during winter.",
      "question": "What winter equipment does the lodge provide for guests?",
      "options": [
        "A. Snowshoes free of charge",
        "B. Surfboards",
        "C. Scuba tanks",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The orchestra rehearses every Wednesday evening at the community center.",
      "conflict_prompt": "The orchestra does not rehearse every Wednesday evening at the community center.",
      "question": "When does the orchestra rehearse?",
      "options": [
        "A. Every Wednesday evening",
        "B. Only on full moons",
        "C. Every hour on the hour",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pharmacist dispenses medication with counseling for new prescriptions.",
      "conflict_prompt": "The pharmacist does not dispense medication with counseling for new prescriptions.",
      "question": "What does the pharmacist provide when dispensing new prescriptions?",
      "options": [
        "A. Counseling",
        "B. A free car",
        "C. Swimming lessons",
        "D. No information at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The yacht club hosts sailing lessons for beginners every summer.",
      "conflict_prompt": "The yacht club does not host sailing lessons for beginners every summer.",
      "question": "When does the yacht club host sailing lessons for beginners?",
      "options": [
        "A. Every summer",
        "B. Only in winter",
        "C. On Mars",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The preschool requires vaccinations according to state guidelines for enrollment.",
      "conflict_prompt": "The preschool does not require vaccinations according to state guidelines for enrollment.",
      "question": "What does the preschool require for enrollment?",
      "options": [
        "A. Vaccinations per state guidelines",
        "B. A full marathon completion certificate",
        "C. Ownership of a horse",
        "D. No requirements"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city council meets publicly on the second Tuesday of each month.",
      "conflict_prompt": "The city council does not meet publicly on the second Tuesday of each month.",
      "question": "When does the city council hold public meetings each month?",
      "options": [
        "A. Second Tuesday",
        "B. Last day of the year only",
        "C. Every hour",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The music teacher assigns scales practice for students to do daily at home.",
      "conflict_prompt": "The music teacher does not assign scales practice for students to do daily at home.",
      "question": "What does the music teacher assign for daily home practice?",
      "options": [
        "A. Scales practice",
        "B. Rocket building",
        "C. No practice at all",
        "D. Only sleep"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The painter uses a protective primer before applying the final coat to outdoor furniture.",
      "conflict_prompt": "The painter does not use a protective primer before applying the final coat to outdoor furniture.",
      "question": "What does the painter apply before the final coat on outdoor furniture?",
      "options": [
        "A. Protective primer",
        "B. Sugar syrup",
        "C. Nothing at all",
        "D. Lava"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter rail allows bicycles on board during off-peak hours.",
      "conflict_prompt": "The commuter rail does not allow bicycles on board during off-peak hours.",
      "question": "When are bicycles allowed on the commuter rail?",
      "options": [
        "A. During off-peak hours",
        "B. Only during rush hour",
        "C. Never",
        "D. Only underwater"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The nursery plants native wildflowers in the meadow each spring.",
      "conflict_prompt": "The nursery does not plant native wildflowers in the meadow each spring.",
      "question": "What does the nursery plant in the meadow each spring?",
      "options": [
        "A. Native wildflowers",
        "B. Concrete blocks",
        "C. Satellite dishes",
        "D. No plants"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The landlord provides tenants with an annual parking permit for one vehicle.",
      "conflict_prompt": "The landlord does not provide tenants with an annual parking permit for one vehicle.",
      "question": "What parking benefit does the landlord provide tenants?",
      "options": [
        "A. Annual parking permit for one vehicle",
        "B. Lifetime valet service",
        "C. No parking allowed",
        "D. Parking for ten vehicles only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery donates leftover unsold bread to a local shelter every evening.",
      "conflict_prompt": "The bakery does not donate leftover unsold bread to a local shelter every evening.",
      "question": "What does the bakery do with leftover unsold bread?",
      "options": [
        "A. Donates it to a local shelter every evening",
        "B. Sells it at a luxury boutique",
        "C. Turns it into concrete",
        "D. Burns it daily"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The IT department enforces two-factor authentication for all employee accounts.",
      "conflict_prompt": "The IT department does not enforce two-factor authentication for all employee accounts.",
      "question": "What security measure does the IT department enforce for employee accounts?",
      "options": [
        "A. Two-factor authentication",
        "B. No login required",
        "C. Password written on a sticky note",
        "D. Verbal password only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The sculpture park prohibits climbing on sculptures to protect visitors and art.",
      "conflict_prompt": "The sculpture park does not prohibit climbing on sculptures to protect visitors and art.",
      "question": "What is the park's rule regarding climbing on sculptures?",
      "options": [
        "A. Prohibited",
        "B. Mandatory",
        "C. Encouraged",
        "D. Only for staff"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel restaurant sources seafood from sustainable fisheries.",
      "conflict_prompt": "The hotel restaurant does not source seafood from sustainable fisheries.",
      "question": "Where does the hotel restaurant source its seafood from?",
      "options": [
        "A. Sustainable fisheries",
        "B. Outer space",
        "C. Local toy stores",
        "D. Unknown, but not sustainable"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The volunteer literacy program matches tutors with adult learners for weekly sessions.",
      "conflict_prompt": "The volunteer literacy program does not match tutors with adult learners for weekly sessions.",
      "question": "How often do tutors and adult learners meet in the literacy program?",
      "options": [
        "A. Weekly",
        "B. Once every 50 years",
        "C. Every minute",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college offers night classes in computer networking for working adults.",
      "conflict_prompt": "The community college does not offer night classes in computer networking for working adults.",
      "question": "What type of evening classes does the community college offer for working adults?",
      "options": [
        "A. Computer networking",
        "B. Advanced basket weaving only",
        "C. None at all",
        "D. Baby-sitting for toddlers"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recycling program collects paper products on alternating Tuesdays.",
      "conflict_prompt": "The recycling program does not collect paper products on alternating Tuesdays.",
      "question": "When does the recycling program collect paper products?",
      "options": [
        "A. Alternating Tuesdays",
        "B. Every hour on the hour",
        "C. Only in February",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The photography club meets weekly to critique members' portfolios.",
      "conflict_prompt": "The photography club does not meet weekly to critique members' portfolios.",
      "question": "How often does the photography club meet?",
      "options": [
        "A. Weekly",
        "B. Once a decade",
        "C. Every seven minutes",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pharmacy keeps a supply of epinephrine auto-injectors for emergencies.",
      "conflict_prompt": "The pharmacy does not keep a supply of epinephrine auto-injectors for emergencies.",
      "question": "What emergency medication does the pharmacy stock?",
      "options": [
        "A. Epinephrine auto-injectors",
        "B. Chocolate bars only",
        "C. No medications at all",
        "D. Fireworks"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art supply store carries acid-free paper for archival framing projects.",
      "conflict_prompt": "The art supply store does not carry acid-free paper for archival framing projects.",
      "question": "What specialized paper does the art supply store carry?",
      "options": [
        "A. Acid-free paper",
        "B. Sandpaper only",
        "C. Only toilet paper",
        "D. Paper made of metal"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The piano in the recital hall is a Steinway grand that is tuned monthly.",
      "conflict_prompt": "The piano in the recital hall is not a Steinway grand that is tuned monthly.",
      "question": "What brand is the recital hall piano?",
      "options": [
        "A. Steinway",
        "B. Yamaha keyboard",
        "C. Electric organ",
        "D. No piano present"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school cafeteria offers a vegetarian entrée option every day.",
      "conflict_prompt": "The school cafeteria does not offer a vegetarian entrée option every day.",
      "question": "What type of meal option does the cafeteria offer daily?",
      "options": [
        "A. Vegetarian entrée",
        "B. Live animal roaming",
        "C. No food at all",
        "D. Only candy"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community theater sells season tickets that include all six productions.",
      "conflict_prompt": "The community theater does not sell season tickets that include all six productions.",
      "question": "What do the season tickets include at the community theater?",
      "options": [
        "A. All six productions",
        "B. Only one performance per year",
        "C. Free parking only",
        "D. A backstage cat"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The aquarium maintains a coral propagation program to restore reef habitats.",
      "conflict_prompt": "The aquarium does not maintain a coral propagation program to restore reef habitats.",
      "question": "What conservation program does the aquarium maintain?",
      "options": [
        "A. Coral propagation",
        "B. Desertification of reefs",
        "C. No conservation efforts",
        "D. Iceberg farming"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The deli keeps peanut-free options clearly labeled to protect customers with allergies.",
      "conflict_prompt": "The deli does not keep peanut-free options clearly labeled to protect customers with allergies.",
      "question": "Why does the deli label peanut-free options clearly?",
      "options": [
        "A. To protect customers with allergies",
        "B. To confuse customers",
        "C. For decorative purposes only",
        "D. There are no peanut-free options"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The shuttle bus runs on a fixed route that stops at the main plaza every 20 minutes.",
      "conflict_prompt": "The shuttle bus does not run on a fixed route that stops at the main plaza every 20 minutes.",
      "question": "How often does the shuttle bus stop at the main plaza?",
      "options": [
        "A. Every 20 minutes",
        "B. Once a week",
        "C. Every second",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pet clinic vaccinates puppies for rabies as part of standard care.",
      "conflict_prompt": "The pet clinic does not vaccinate puppies for rabies as part of standard care.",
      "question": "What standard vaccination does the pet clinic give to puppies?",
      "options": [
        "A. Rabies vaccine",
        "B. Human flu shots",
        "C. No vaccinations at all",
        "D. Only vitamin C"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public pool requires swimmers to shower before entering the water.",
      "conflict_prompt": "The public pool does not require swimmers to shower before entering the water.",
      "question": "What is required of swimmers before entering the public pool?",
      "options": [
        "A. Showering",
        "B. Bringing a boat",
        "C. Wearing a tuxedo",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The science museum offers guided tours for school groups by reservation.",
      "conflict_prompt": "The science museum does not offer guided tours for school groups by reservation.",
      "question": "How can school groups get guided tours at the science museum?",
      "options": [
        "A. By reservation",
        "B. Only by chance",
        "C. By climbing onto the roof",
        "D. They cannot get guided tours"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ice rink hosts public skating sessions every Saturday afternoon.",
      "conflict_prompt": "The ice rink does not host public skating sessions every Saturday afternoon.",
      "question": "When are public skating sessions held at the ice rink?",
      "options": [
        "A. Every Saturday afternoon",
        "B. Only on Tuesdays at dawn",
        "C. Daily at 3 AM",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses cage-free eggs in all its pastries.",
      "conflict_prompt": "The bakery does not use cage-free eggs in all its pastries.",
      "question": "What type of eggs does the bakery use in its pastries?",
      "options": [
        "A. Cage-free eggs",
        "B. Dinosaur eggs",
        "C. No eggs",
        "D. Only synthetic plastic eggs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city offers free mosquito prevention kits to households in high-risk areas.",
      "conflict_prompt": "The city does not offer free mosquito prevention kits to households in high-risk areas.",
      "question": "Who receives free mosquito prevention kits from the city?",
      "options": [
        "A. Households in high-risk areas",
        "B. Only downtown hotels",
        "C. No one",
        "D. Only city council members"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain guide requires climbers to have prior hiking experience for technical routes.",
      "conflict_prompt": "The mountain guide does not require climbers to have prior hiking experience for technical routes.",
      "question": "What does the mountain guide require for climbers attempting technical routes?",
      "options": [
        "A. Prior hiking experience",
        "B. A pilot's license",
        "C. No experience at all",
        "D. Ownership of a yacht"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The elementary school distributes laptops to students for remote learning.",
      "conflict_prompt": "The elementary school does not distribute laptops to students for remote learning.",
      "question": "What device does the elementary school distribute to students for remote learning?",
      "options": [
        "A. Laptops",
        "B. Typewriters",
        "C. Televisions only",
        "D. No devices"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vineyard produces a reserve Pinot Noir limited to 500 bottles each year.",
      "conflict_prompt": "The vineyard does not produce a reserve Pinot Noir limited to 500 bottles each year.",
      "question": "How many bottles are produced of the reserve Pinot Noir each year?",
      "options": [
        "A. 500",
        "B. 5,000,000",
        "C. 1",
        "D. Unlimited"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art museum loans pieces to partner galleries for six-month exhibitions.",
      "conflict_prompt": "The art museum does not loan pieces to partner galleries for six-month exhibitions.",
      "question": "For how long does the museum loan pieces to partner galleries?",
      "options": [
        "A. Six months",
        "B. Six days",
        "C. Six years",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The language center administers placement tests to new students before classes begin.",
      "conflict_prompt": "The language center does not administer placement tests to new students before classes begin.",
      "question": "What does the language center do for new students before classes begin?",
      "options": [
        "A. Administer placement tests",
        "B. Require building a car",
        "C. Host a concert",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers plant cover crops in the fields each fall to improve soil health.",
      "conflict_prompt": "The farmers do not plant cover crops in the fields each fall to improve soil health.",
      "question": "When do the farmers plant cover crops to improve soil health?",
      "options": [
        "A. Each fall",
        "B. In summer only",
        "C. At no time",
        "D. During earthquakes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coffee shop grinds beans fresh for each espresso shot upon order.",
      "conflict_prompt": "The coffee shop does not grind beans fresh for each espresso shot upon order.",
      "question": "How are beans prepared for each espresso shot at the coffee shop?",
      "options": [
        "A. Ground fresh upon order",
        "B. Pre-ground weeks in advance",
        "C. Not used at all",
        "D. Boiled into soup"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community choir performs at the spring festival every year.",
      "conflict_prompt": "The community choir does not perform at the spring festival every year.",
      "question": "When does the community choir perform at the festival?",
      "options": [
        "A. Every year at the spring festival",
        "B. Only during winter storms",
        "C. Once every millennium",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The preschool teacher posts weekly lesson plans on the classroom bulletin board.",
      "conflict_prompt": "The preschool teacher does not post weekly lesson plans on the classroom bulletin board.",
      "question": "Where are the preschool teacher's weekly lesson plans posted?",
      "options": [
        "A. On the classroom bulletin board",
        "B. Inside a vault in another country",
        "C. Deleted immediately after writing",
        "D. On the moon"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The library waives late fees for returned items during annual community drives.",
      "conflict_prompt": "The library does not waive late fees for returned items during annual community drives.",
      "question": "When does the library waive late fees for returned items?",
      "options": [
        "A. During annual community drives",
        "B. Only when the mayor visits",
        "C. Never",
        "D. Every hour on the hour"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The delivery service requires signatures for packages valued over $200.",
      "conflict_prompt": "The delivery service does not require signatures for packages valued over $200.",
      "question": "For what value threshold does the delivery service require signatures?",
      "options": [
        "A. Over $200",
        "B. Over $1,000,000",
        "C. For all postcards only",
        "D. No threshold at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The antique shop authenticates all items before offering them for sale.",
      "conflict_prompt": "The antique shop does not authenticate all items before offering them for sale.",
      "question": "What process does the antique shop perform on items before sale?",
      "options": [
        "A. Authentication",
        "B. Melting them down",
        "C. Shipping them to space",
        "D. No inspection"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community center offers free legal clinics once a month for residents.",
      "conflict_prompt": "The community center does not offer free legal clinics once a month for residents.",
      "question": "How often are free legal clinics offered at the community center?",
      "options": [
        "A. Once a month",
        "B. Daily",
        "C. Never",
        "D. Only on leap days"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The tennis court is resurfaced annually to maintain safe playing conditions.",
      "conflict_prompt": "The tennis court is not resurfaced annually to maintain safe playing conditions.",
      "question": "How often is the tennis court resurfaced?",
      "options": [
        "A. Annually",
        "B. Every century",
        "C. Never",
        "D. Hourly"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare provides nap mats and quiet time for toddlers after lunch.",
      "conflict_prompt": "The daycare does not provide nap mats and quiet time for toddlers after lunch.",
      "question": "What does the daycare provide for toddlers after lunch?",
      "options": [
        "A. Nap mats and quiet time",
        "B. Skydiving lessons",
        "C. Marathon training",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer's co-op offers bulk fertilizer discounts to members every spring.",
      "conflict_prompt": "The farmer's co-op does not offer bulk fertilizer discounts to members every spring.",
      "question": "When does the co-op offer bulk fertilizer discounts to members?",
      "options": [
        "A. Every spring",
        "B. During winter only",
        "C. Once per decade",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater company rehearses outdoors in good weather for the summer production.",
      "conflict_prompt": "The theater company does not rehearse outdoors in good weather for the summer production.",
      "question": "Where does the theater company rehearse for the summer production in good weather?",
      "options": [
        "A. Outdoors",
        "B. Underwater",
        "C. In a spaceship",
        "D. They do not rehearse"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pet adoption center requires spay/neuter documentation before finalizing adoptions.",
      "conflict_prompt": "The pet adoption center does not require spay/neuter documentation before finalizing adoptions.",
      "question": "What documentation does the pet adoption center require before finalizing adoptions?",
      "options": [
        "A. Spay/neuter documentation",
        "B. A passport",
        "C. A driver's license from another planet",
        "D. No documentation at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers market accepts vendor applications until the end of March each year.",
      "conflict_prompt": "The farmers market does not accept vendor applications until the end of March each year.",
      "question": "Until when does the farmers market accept vendor applications each year?",
      "options": [
        "A. End of March",
        "B. End of December only",
        "C. Every other century",
        "D. They never accept applications"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public restroom is cleaned and restocked hourly during business hours.",
      "conflict_prompt": "The public restroom is not cleaned and restocked hourly during business hours.",
      "question": "How often is the public restroom cleaned and restocked during business hours?",
      "options": [
        "A. Hourly",
        "B. Once a year",
        "C. Only on holidays",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery owner sources flour from a local mill within 50 miles.",
      "conflict_prompt": "The bakery owner does not source flour from a local mill within 50 miles.",
      "question": "How far away is the mill that supplies the bakery's flour?",
      "options": [
        "A. Within 50 miles",
        "B. Over 5,000 miles",
        "C. Next door only",
        "D. Unknown and irrelevant"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recycling center accepts electronics for safe disposal every Saturday.",
      "conflict_prompt": "The recycling center does not accept electronics for safe disposal every Saturday.",
      "question": "When can residents drop off electronics at the recycling center?",
      "options": [
        "A. Every Saturday",
        "B. Only on Tuesdays at dawn",
        "C. Never",
        "D. During solar eclipses only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mayor hosts a public Q&A at city hall once each quarter.",
      "conflict_prompt": "The mayor does not host a public Q&A at city hall once each quarter.",
      "question": "How often does the mayor host a public Q&A at city hall?",
      "options": [
        "A. Once each quarter",
        "B. Every minute",
        "C. Once in a lifetime",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater offers audio descriptions for visually impaired patrons during evening shows.",
      "conflict_prompt": "The theater does not offer audio descriptions for visually impaired patrons during evening shows.",
      "question": "What accommodation does the theater provide for visually impaired patrons?",
      "options": [
        "A. Audio descriptions during evening shows",
        "B. Free sunglasses",
        "C. Flashing lights only",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical society hosts monthly plant identification walks in the park.",
      "conflict_prompt": "The botanical society does not host monthly plant identification walks in the park.",
      "question": "How often does the botanical society host plant identification walks?",
      "options": [
        "A. Monthly",
        "B. Twice daily",
        "C. Only once per decade",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The grocery co-op sources produce from regional farms within the state.",
      "conflict_prompt": "The grocery co-op does not source produce from regional farms within the state.",
      "question": "Where does the co-op source its produce from?",
      "options": [
        "A. Regional farms within the state",
        "B. Outer space farms",
        "C. The international deep sea",
        "D. It does not source produce"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The laundromat charges a flat rate per wash and accepts card payments.",
      "conflict_prompt": "The laundromat does not charge a flat rate per wash and accepts card payments.",
      "question": "What payment method does the laundromat accept?",
      "options": [
        "A. Card payments",
        "B. Gold bars only",
        "C. No payment required",
        "D. Only magic beans"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community garden schedules volunteer workdays on the first Saturday of each month.",
      "conflict_prompt": "The community garden does not schedule volunteer workdays on the first Saturday of each month.",
      "question": "When are volunteer workdays scheduled at the community garden?",
      "options": [
        "A. First Saturday of each month",
        "B. Every Friday at midnight",
        "C. Once in 100 years",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The music venue enforces a no-reentry policy during performances.",
      "conflict_prompt": "The music venue does not enforce a no-reentry policy during performances.",
      "question": "What is the music venue's policy on reentry during performances?",
      "options": [
        "A. No reentry",
        "B. Unlimited reentry",
        "C. Reentry only with a spaceship",
        "D. Only reentry after donation of a car"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pediatric clinic schedules well-child visits at regular intervals during the first two years.",
      "conflict_prompt": "The pediatric clinic does not schedule well-child visits at regular intervals during the first two years.",
      "question": "When are well-child visits scheduled at the pediatric clinic?",
      "options": [
        "A. At regular intervals during the first two years",
        "B. Never",
        "C. Only after age 50",
        "D. Every minute"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The charter boat requires life jackets for all passengers while underway.",
      "conflict_prompt": "The charter boat does not require life jackets for all passengers while underway.",
      "question": "What safety item does the charter boat require for all passengers while underway?",
      "options": [
        "A. Life jackets",
        "B. Jetpacks",
        "C. Inflatable castles",
        "D. Nothing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university library offers extended study hours during final exams week.",
      "conflict_prompt": "The university library does not offer extended study hours during final exams week.",
      "question": "When does the university library offer extended study hours?",
      "options": [
        "A. During final exams week",
        "B. Only on national holidays",
        "C. Never",
        "D. Every Tuesday at noon"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers rotate crops annually to reduce pest pressure and maintain yield.",
      "conflict_prompt": "The farmers do not rotate crops annually to reduce pest pressure and maintain yield.",
      "question": "Why do the farmers rotate crops annually?",
      "options": [
        "A. To reduce pest pressure and maintain yield",
        "B. To encourage pest infestations",
        "C. To avoid harvesting ever",
        "D. For no particular reason"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community health center offers free blood pressure screenings every Wednesday.",
      "conflict_prompt": "The community health center does not offer free blood pressure screenings every Wednesday.",
      "question": "When are free blood pressure screenings offered at the community health center?",
      "options": [
        "A. Every Wednesday",
        "B. Once a decade",
        "C. Every hour",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The culinary school requires students to complete a safety course before using commercial equipment.",
      "conflict_prompt": "The culinary school does not require students to complete a safety course before using commercial equipment.",
      "question": "What must students complete before using commercial equipment at the culinary school?",
      "options": [
        "A. A safety course",
        "B. A skydiving certificate",
        "C. No prerequisites",
        "D. A marathon"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers market posts weekly vendor schedules on its website each Monday.",
      "conflict_prompt": "The farmers market does not post weekly vendor schedules on its website each Monday.",
      "question": "When does the farmers market post weekly vendor schedules on its website?",
      "options": [
        "A. Each Monday",
        "B. Every other century",
        "C. They never post schedules",
        "D. Only during thunderstorms"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The equestrian center requires helmets for riders under 18 during lessons.",
      "conflict_prompt": "The equestrian center does not require helmets for riders under 18 during lessons.",
      "question": "Who is required to wear helmets during lessons at the equestrian center?",
      "options": [
        "A. Riders under 18",
        "B. Only the horses",
        "C. No one",
        "D. Everyone must wear scuba gear"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The solar farm feeds excess electricity back to the grid during sunny afternoons.",
      "conflict_prompt": "The solar farm does not feed excess electricity back to the grid during sunny afternoons.",
      "question": "What does the solar farm do with excess electricity during sunny afternoons?",
      "options": [
        "A. Feed it back to the grid",
        "B. Pour it into the ocean",
        "C. Store it as bottled water",
        "D. It does nothing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The skating rink rents blades in a range of sizes for children and adults.",
      "conflict_prompt": "The skating rink does not rent blades in a range of sizes for children and adults.",
      "question": "Who can use the range of blade sizes rented at the skating rink?",
      "options": [
        "A. Children and adults",
        "B. Only robots",
        "C. Only garden gnomes",
        "D. No one"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college library offers interlibrary loan services for rare volumes.",
      "conflict_prompt": "The community college library does not offer interlibrary loan services for rare volumes.",
      "question": "What service does the community college library offer for rare volumes?",
      "options": [
        "A. Interlibrary loan services",
        "B. Free demolition",
        "C. Illegal replication",
        "D. No assistance"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The shelter provides emergency boarding for pets during natural disasters.",
      "conflict_prompt": "The shelter does not provide emergency boarding for pets during natural disasters.",
      "question": "What service does the shelter provide during natural disasters?",
      "options": [
        "A. Emergency boarding for pets",
        "B. Only plant rescue",
        "C. No services",
        "D. Exclusive rooftop parties"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local theater sells discounted student tickets with valid ID at the box office.",
      "conflict_prompt": "The local theater does not sell discounted student tickets with valid ID at the box office.",
      "question": "How can students get discounted tickets at the local theater?",
      "options": [
        "A. Show valid ID at the box office",
        "B. By singing opera in the lobby",
        "C. No discounts available",
        "D. Only if they are over 100 years old"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool schedules lap swimming only during designated morning hours.",
      "conflict_prompt": "The community pool does not schedule lap swimming only during designated morning hours.",
      "question": "When is lap swimming scheduled at the community pool?",
      "options": [
        "A. During designated morning hours",
        "B. All night every night",
        "C. Only on leap seconds",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers use drip irrigation to conserve water across vegetable plots.",
      "conflict_prompt": "The farmers do not use drip irrigation to conserve water across vegetable plots.",
      "question": "What irrigation method do the farmers use to conserve water?",
      "options": [
        "A. Drip irrigation",
        "B. Flooding the fields daily",
        "C. Spraying with champagne",
        "D. No irrigation method"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The concert venue provides closed-captioning for deaf patrons during performances.",
      "conflict_prompt": "The concert venue does not provide closed-captioning for deaf patrons during performances.",
      "question": "What accessibility feature does the concert venue provide?",
      "options": [
        "A. Closed-captioning for deaf patrons",
        "B. Free hot air balloon rides",
        "C. Only visual pyrotechnics",
        "D. No accessibility features"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campsite requires reservations for group sites during peak season.",
      "conflict_prompt": "The campsite does not require reservations for group sites during peak season.",
      "question": "What is required for group campsites during peak season?",
      "options": [
        "A. Reservations",
        "B. A secret handshake",
        "C. No requirements at all",
        "D. Payment in seashells only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The law firm provides pro bono consultations for qualifying low-income clients.",
      "conflict_prompt": "The law firm does not provide pro bono consultations for qualifying low-income clients.",
      "question": "Who can receive pro bono consultations from the law firm?",
      "options": [
        "A. Qualifying low-income clients",
        "B. Only corporate CEOs",
        "C. No one",
        "D. Only aliens"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The playground equipment is inspected monthly to ensure safety for children.",
      "conflict_prompt": "The playground equipment is not inspected monthly to ensure safety for children.",
      "question": "How often is the playground equipment inspected for safety?",
      "options": [
        "A. Monthly",
        "B. Every millennium",
        "C. Every second",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare maintains a staff-to-child ratio that meets state licensing requirements.",
      "conflict_prompt": "The daycare does not maintain a staff-to-child ratio that meets state licensing requirements.",
      "question": "What does the daycare maintain to comply with state licensing?",
      "options": [
        "A. Appropriate staff-to-child ratio",
        "B. A ratio of one staff to 100 children",
        "C. Only volunteer grandparents",
        "D. No staffing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The library offers a quiet study room that can be reserved online for two-hour blocks.",
      "conflict_prompt": "The library does not offer a quiet study room that can be reserved online for two-hour blocks.",
      "question": "How long can patrons reserve the quiet study room online?",
      "options": [
        "A. Two-hour blocks",
        "B. Eight-second intervals",
        "C. One year at a time",
        "D. They cannot reserve it at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers plant heirloom tomatoes in a dedicated greenhouse each season.",
      "conflict_prompt": "The farmers do not plant heirloom tomatoes in a dedicated greenhouse each season.",
      "question": "Where are the heirloom tomatoes planted each season?",
      "options": [
        "A. Dedicated greenhouse",
        "B. On the roof of city hall",
        "C. Inside refrigerators",
        "D. Nowhere"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community health clinic provides free contraceptive counseling to patients.",
      "conflict_prompt": "The community health clinic does not provide free contraceptive counseling to patients.",
      "question": "What counseling service does the community health clinic provide for free?",
      "options": [
        "A. Contraceptive counseling",
        "B. Investment advice",
        "C. None at all",
        "D. Only cooking classes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city requires building permits for exterior renovations to homes.",
      "conflict_prompt": "The city does not require building permits for exterior renovations to homes.",
      "question": "What does the city require for exterior home renovations?",
      "options": [
        "A. Building permits",
        "B. A formal dance",
        "C. No oversight at all",
        "D. Approval from neighboring planets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The summer camp provides lifeguards on duty during all waterfront activities.",
      "conflict_prompt": "The summer camp does not provide lifeguards on duty during all waterfront activities.",
      "question": "Who is on duty during waterfront activities at the summer camp?",
      "options": [
        "A. Lifeguards",
        "B. Construction workers only",
        "C. No supervision",
        "D. Only clowns"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum shop sells postcards of the permanent collection near the checkout counter.",
      "conflict_prompt": "The museum shop does not sell postcards of the permanent collection near the checkout counter.",
      "question": "Where are postcards of the permanent collection sold in the museum shop?",
      "options": [
        "A. Near the checkout counter",
        "B. In a locked vault underground",
        "C. They are not sold at all",
        "D. Only by subscription"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer uses organic compost to fertilize the vegetable fields each spring.",
      "conflict_prompt": "The farmer does not use organic compost to fertilize the vegetable fields each spring.",
      "question": "What does the farmer use to fertilize the vegetable fields each spring?",
      "options": [
        "A. Organic compost",
        "B. Industrial waste",
        "C. Radioactive material",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Emma is a chef. She prepares dinner for the restaurant every evening.",
      "conflict_prompt": "Emma is not a chef. She prepares dinner for the restaurant every evening.",
      "question": "What is Emma's job?",
      "options": [
        "A. Accountant",
        "B. Chef",
        "C. Receptionist",
        "D. Gardener"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bookstore is open on Sundays. It sells rare titles every Sunday.",
      "conflict_prompt": "The bookstore is not open on Sundays. It sells rare titles every Sunday.",
      "question": "Is the bookstore open on Sundays?",
      "options": [
        "A. No",
        "B. Only by appointment",
        "C. Yes",
        "D. Closed annually"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The package is fragile. The courier handles it with extra care during delivery.",
      "conflict_prompt": "The package is not fragile. The courier handles it with extra care during delivery.",
      "question": "How should the courier handle the package?",
      "options": [
        "A. With extra care",
        "B. Leave unattended",
        "C. Stack under heavy boxes",
        "D. Toss into the van"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Carlos drives a hybrid car. He charges it overnight at home.",
      "conflict_prompt": "Carlos does not drive a hybrid car. He charges it overnight at home.",
      "question": "What type of vehicle does Carlos drive?",
      "options": [
        "A. Motorcycle",
        "B. Hybrid car",
        "C. Bicycle",
        "D. SUV"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum has a climate-controlled gallery. Sensitive paintings are displayed there.",
      "conflict_prompt": "The museum does not have a climate-controlled gallery. Sensitive paintings are displayed there.",
      "question": "Where are the sensitive paintings displayed?",
      "options": [
        "A. In a climate-controlled gallery",
        "B. In an outdoor courtyard",
        "C. In a storage shed",
        "D. In the basement storage"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Dr. Nguyen is a pediatrician. She sees children at the clinic every afternoon.",
      "conflict_prompt": "Dr. Nguyen is not a pediatrician. She sees children at the clinic every afternoon.",
      "question": "What type of doctor is Dr. Nguyen?",
      "options": [
        "A. Cardiologist",
        "B. Pediatrician",
        "C. Dermatologist",
        "D. Neurologist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The office recycles paper. Employees place used documents in the blue bin.",
      "conflict_prompt": "The office does not recycle paper. Employees place used documents in the blue bin.",
      "question": "What do employees do with used paper documents?",
      "options": [
        "A. Place them in the blue recycling bin",
        "B. Burn them",
        "C. Dump them in the sink",
        "D. Take them home"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The concert starts at 8 PM. The opening act performs at 8:15 PM.",
      "conflict_prompt": "The concert does not start at 8 PM. The opening act performs at 8:15 PM.",
      "question": "When does the opening act perform?",
      "options": [
        "A. 7:00 PM",
        "B. 9:00 PM",
        "C. 8:15 PM",
        "D. Midnight"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "My neighbors own a golden retriever. It barks when the mail arrives.",
      "conflict_prompt": "My neighbors do not own a golden retriever. It barks when the mail arrives.",
      "question": "What kind of dog do my neighbors own?",
      "options": [
        "A. Bulldog",
        "B. Poodle",
        "C. Golden retriever",
        "D. Chihuahua"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery bakes sourdough daily. Fresh loaves are available each morning.",
      "conflict_prompt": "The bakery does not bake sourdough daily. Fresh loaves are available each morning.",
      "question": "What type of bread does the bakery bake every day?",
      "options": [
        "A. Brioche",
        "B. Sourdough",
        "C. Naan",
        "D. Pita"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The laboratory uses a fume hood for experiments. Hazardous fumes are vented outdoors.",
      "conflict_prompt": "The laboratory does not use a fume hood for experiments. Hazardous fumes are vented outdoors.",
      "question": "What equipment does the laboratory use to vent hazardous fumes?",
      "options": [
        "A. Open windows",
        "B. Portable fans",
        "C. Fume hood",
        "D. Humidifier"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Sofia studies marine biology. She conducts fieldwork on coral reefs during the summer.",
      "conflict_prompt": "Sofia does not study marine biology. She conducts fieldwork on coral reefs during the summer.",
      "question": "What subject does Sofia study?",
      "options": [
        "A. Astronomy",
        "B. Marine biology",
        "C. Linguistics",
        "D. Economics"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The company provides employee health insurance. Staff enroll during the open enrollment period.",
      "conflict_prompt": "The company does not provide employee health insurance. Staff enroll during the open enrollment period.",
      "question": "What benefit does the company provide?",
      "options": [
        "A. Free lunches",
        "B. Employee health insurance",
        "C. On-site daycare",
        "D. Gym memberships"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The laptop has a backlit keyboard. Typing in low light is easier because of it.",
      "conflict_prompt": "The laptop does not have a backlit keyboard. Typing in low light is easier because of it.",
      "question": "What feature helps with typing in low light?",
      "options": [
        "A. Backlit keyboard",
        "B. External mouse",
        "C. Solar charger",
        "D. Touchpad"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city operates a bike-sharing program. Stations are located every few blocks downtown.",
      "conflict_prompt": "The city does not operate a bike-sharing program. Stations are located every few blocks downtown.",
      "question": "What transportation program does the city operate downtown?",
      "options": [
        "A. Ferry service",
        "B. Bike-sharing program",
        "C. Monorail",
        "D. Cable car"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Liam is left-handed. He writes with his left hand at work.",
      "conflict_prompt": "Liam is not left-handed. He writes with his left hand at work.",
      "question": "Which hand does Liam use to write?",
      "options": [
        "A. Right hand",
        "B. Both hands equally",
        "C. Left hand",
        "D. He does not write"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater enforces a no-food policy. Audience members are asked to leave snacks outside.",
      "conflict_prompt": "The theater does not enforce a no-food policy. Audience members are asked to leave snacks outside.",
      "question": "What is the theater's policy on food?",
      "options": [
        "A. No-food policy",
        "B. Unlimited snacks allowed",
        "C. Only candy allowed",
        "D. Food allowed in certain sections"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school requires uniforms. Students wear a navy blazer and tie every weekday.",
      "conflict_prompt": "The school does not require uniforms. Students wear a navy blazer and tie every weekday.",
      "question": "What do students wear to school on weekdays?",
      "options": [
        "A. Casual clothes",
        "B. Sports jerseys",
        "C. Navy blazer and tie",
        "D. Costumes"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train runs express during rush hour. It skips local stops between Central and North Station.",
      "conflict_prompt": "The train does not run express during rush hour. It skips local stops between Central and North Station.",
      "question": "What does the train do between Central and North Station during rush hour?",
      "options": [
        "A. Stops at every local station",
        "B. Skips local stops (runs express)",
        "C. Operates as a shuttle",
        "D. Is out of service"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone supports wireless charging. Its back panel includes a coil for Qi charging.",
      "conflict_prompt": "The smartphone does not support wireless charging. Its back panel includes a coil for Qi charging.",
      "question": "Which charging feature does the smartphone support?",
      "options": [
        "A. Solar charging",
        "B. Wireless (Qi) charging",
        "C. Morse code charging",
        "D. No charging capability"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Nora volunteers at the animal shelter. She walks dogs every Saturday morning.",
      "conflict_prompt": "Nora does not volunteer at the animal shelter. She walks dogs every Saturday morning.",
      "question": "What activity does Nora do on Saturday mornings?",
      "options": [
        "A. Paints landscapes",
        "B. Walks dogs at the animal shelter",
        "C. Teaches piano",
        "D. Goes skiing"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The greenhouse uses LED grow lights. Tomatoes thrive under those lights year-round.",
      "conflict_prompt": "The greenhouse does not use LED grow lights. Tomatoes thrive under those lights year-round.",
      "question": "What lighting system does the greenhouse use to grow tomatoes?",
      "options": [
        "A. Incandescent bulbs",
        "B. LED grow lights",
        "C. Candlelight",
        "D. No lights at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The doctor prescribes antibiotics for the infection. The patient takes the medication for ten days.",
      "conflict_prompt": "The doctor does not prescribe antibiotics for the infection. The patient takes the medication for ten days.",
      "question": "How long does the patient take the prescribed medication?",
      "options": [
        "A. One day",
        "B. Ten days",
        "C. Six months",
        "D. Indefinitely"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry allows bicycles on board. Cyclists can roll their bikes onto the lower deck.",
      "conflict_prompt": "The ferry does not allow bicycles on board. Cyclists can roll their bikes onto the lower deck.",
      "question": "What can cyclists do on the ferry?",
      "options": [
        "A. Roll their bikes onto the lower deck",
        "B. Leave bikes at the dock only",
        "C. Not bring bikes at all",
        "D. Fly their bikes"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conference offers continuing education credits. Attendees receive certificates after sessions.",
      "conflict_prompt": "The conference does not offer continuing education credits. Attendees receive certificates after sessions.",
      "question": "What do attendees receive after sessions?",
      "options": [
        "A. Meal vouchers only",
        "B. Certificates for continuing education",
        "C. Free flight tickets",
        "D. Gym memberships"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The laboratory stores samples at -80°C. Specimens are kept in an ultra-low freezer.",
      "conflict_prompt": "The laboratory does not store samples at -80°C. Specimens are kept in an ultra-low freezer.",
      "question": "At what temperature are specimens stored?",
      "options": [
        "A. Room temperature",
        "B. -80°C",
        "C. 37°C",
        "D. 0°C"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain trail is marked red. Hikers follow red blazes to the summit.",
      "conflict_prompt": "The mountain trail is not marked red. Hikers follow red blazes to the summit.",
      "question": "Which trail markers should hikers follow to reach the summit?",
      "options": [
        "A. Blue arrows",
        "B. Red blazes",
        "C. Yellow flags",
        "D. No markers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university offers a scholarship for engineering students. Eligible applicants submit transcripts by March 1.",
      "conflict_prompt": "The university does not offer a scholarship for engineering students. Eligible applicants submit transcripts by March 1.",
      "question": "Who is the scholarship intended for?",
      "options": [
        "A. Medical students",
        "B. Engineering students",
        "C. Art students only",
        "D. Retirees"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant serves vegan options. The menu includes a chickpea-based burger.",
      "conflict_prompt": "The restaurant does not serve vegan options. The menu includes a chickpea-based burger.",
      "question": "Which vegan dish is listed on the menu?",
      "options": [
        "A. Beef steak",
        "B. Chicken salad",
        "C. Chickpea-based burger",
        "D. Fish tacos"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool uses chlorine. Water tests are conducted weekly to maintain levels.",
      "conflict_prompt": "The community pool does not use chlorine. Water tests are conducted weekly to maintain levels.",
      "question": "What chemical does the pool use for sanitation?",
      "options": [
        "A. Olive oil",
        "B. Chlorine",
        "C. Vinegar",
        "D. Baking soda"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The novel is written in first person. The narrator recounts events from their perspective.",
      "conflict_prompt": "The novel is not written in first person. The narrator recounts events from their perspective.",
      "question": "From which point of view is the novel written?",
      "options": [
        "A. Third person omniscient",
        "B. Second person",
        "C. First person",
        "D. Objective camera"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone camera records 4K video. Users can shoot high-resolution clips at 60 fps.",
      "conflict_prompt": "The smartphone camera does not record 4K video. Users can shoot high-resolution clips at 60 fps.",
      "question": "What video resolution can the smartphone record?",
      "options": [
        "A. 480p",
        "B. 720p",
        "C. 4K",
        "D. 120p only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Miguel studies French. He takes conversation classes twice a week.",
      "conflict_prompt": "Miguel does not study French. He takes conversation classes twice a week.",
      "question": "Which language does Miguel study?",
      "options": [
        "A. German",
        "B. Spanish",
        "C. French",
        "D. Mandarin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery closes at 6 PM. Staff begin cleaning after the last sale.",
      "conflict_prompt": "The bakery does not close at 6 PM. Staff begin cleaning after the last sale.",
      "question": "When does the bakery close?",
      "options": [
        "A. 10 PM",
        "B. 6 PM",
        "C. Open 24 hours",
        "D. 2 PM"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The playground is fenced. Children must use the gate to enter the area.",
      "conflict_prompt": "The playground is not fenced. Children must use the gate to enter the area.",
      "question": "How do children enter the playground?",
      "options": [
        "A. By climbing over the fence",
        "B. Using the gate",
        "C. Through underground tunnels",
        "D. They cannot enter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city collects organic waste separately. Residents place food scraps in the green bin.",
      "conflict_prompt": "The city does not collect organic waste separately. Residents place food scraps in the green bin.",
      "question": "Where should residents put food scraps?",
      "options": [
        "A. In the green bin for organics",
        "B. In the regular trash only",
        "C. Flush them down the toilet",
        "D. Burn them in the yard"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The play runs for two hours. There is a 15-minute intermission at the midpoint.",
      "conflict_prompt": "The play does not run for two hours. There is a 15-minute intermission at the midpoint.",
      "question": "How long is the intermission?",
      "options": [
        "A. 5 minutes",
        "B. 15 minutes",
        "C. 30 minutes",
        "D. No intermission"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The camera lens is weather-sealed. Photographers can shoot in light rain without worry.",
      "conflict_prompt": "The camera lens is not weather-sealed. Photographers can shoot in light rain without worry.",
      "question": "What feature allows photographers to shoot in light rain?",
      "options": [
        "A. Weather-sealed lens",
        "B. Paper lens cover",
        "C. No lens at all",
        "D. A film camera"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The nursery sells native plants. The staff can recommend local species for gardens.",
      "conflict_prompt": "The nursery does not sell native plants. The staff can recommend local species for gardens.",
      "question": "What kind of plants does the nursery sell?",
      "options": [
        "A. Tropical only",
        "B. Native plants",
        "C. Artificial plants",
        "D. Underwater coral"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The dentist uses digital X-rays. Patients receive a printout of images during the appointment.",
      "conflict_prompt": "The dentist does not use digital X-rays. Patients receive a printout of images during the appointment.",
      "question": "What type of X-rays does the dentist use?",
      "options": [
        "A. Film only",
        "B. Digital X-rays",
        "C. No X-rays at all",
        "D. Aerial X-rays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The supermarket accepts coupons. Shoppers can redeem them at checkout.",
      "conflict_prompt": "The supermarket does not accept coupons. Shoppers can redeem them at checkout.",
      "question": "What payment option is available at the supermarket?",
      "options": [
        "A. Coupons accepted at checkout",
        "B. Only cash",
        "C. Bartering only",
        "D. No payment necessary"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public library offers free Wi-Fi. Visitors connect to the network using their library card.",
      "conflict_prompt": "The public library does not offer free Wi-Fi. Visitors connect to the network using their library card.",
      "question": "How do visitors connect to the library's Wi-Fi?",
      "options": [
        "A. Using their library card",
        "B. By paying a fee",
        "C. They cannot connect",
        "D. Using dial-up modem"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses local flour. The bread has a distinct nutty flavor because of that mill.",
      "conflict_prompt": "The bakery does not use local flour. The bread has a distinct nutty flavor because of that mill.",
      "question": "What ingredient gives the bread its nutty flavor?",
      "options": [
        "A. Imported sugar",
        "B. Local flour from the mill",
        "C. Olive oil",
        "D. Cocoa powder"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art class meets on Wednesdays. Students bring sketchbooks to every session.",
      "conflict_prompt": "The art class does not meet on Wednesdays. Students bring sketchbooks to every session.",
      "question": "When does the art class meet?",
      "options": [
        "A. Sundays",
        "B. Wednesdays",
        "C. Fridays only",
        "D. Once a month"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The submarine operates at great depths. Its hull is constructed from titanium to withstand pressure.",
      "conflict_prompt": "The submarine does not operate at great depths. Its hull is constructed from titanium to withstand pressure.",
      "question": "What material is the submarine's hull made of?",
      "options": [
        "A. Wood",
        "B. Titanium",
        "C. Glass",
        "D. Cardboard"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The festival sells wristbands for entry. Attendees wear wristbands to access VIP areas.",
      "conflict_prompt": "The festival does not sell wristbands for entry. Attendees wear wristbands to access VIP areas.",
      "question": "What do attendees wear to access VIP areas?",
      "options": [
        "A. Wristbands",
        "B. Top hats",
        "C. Sunglasses",
        "D. Boots"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software includes a dark mode. Users can switch themes in the settings menu.",
      "conflict_prompt": "The software does not include a dark mode. Users can switch themes in the settings menu.",
      "question": "Where can users switch to dark mode?",
      "options": [
        "A. In the settings menu",
        "B. At the hardware level",
        "C. They cannot switch themes",
        "D. Only by reinstalling software"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farm raises heritage chickens. Eggs from those chickens are sold at the local market.",
      "conflict_prompt": "The farm does not raise heritage chickens. Eggs from those chickens are sold at the local market.",
      "question": "What type of eggs does the farm sell at the local market?",
      "options": [
        "A. Heritage chicken eggs",
        "B. Fish eggs",
        "C. Synthetic eggs",
        "D. No eggs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The company publishes an annual sustainability report. The report outlines emissions reductions for the year.",
      "conflict_prompt": "The company does not publish an annual sustainability report. The report outlines emissions reductions for the year.",
      "question": "What does the company's report outline?",
      "options": [
        "A. Emissions reductions for the year",
        "B. Recipes for soup",
        "C. Fictional short stories",
        "D. Movie schedules"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The park enforces leash laws. Dog owners must keep pets on a leash in designated areas.",
      "conflict_prompt": "The park does not enforce leash laws. Dog owners must keep pets on a leash in designated areas.",
      "question": "What must dog owners do in designated areas?",
      "options": [
        "A. Keep pets on a leash",
        "B. Let dogs run freely",
        "C. Leave dogs at home",
        "D. Only bring cats"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train ticket includes reserved seating. Passengers are assigned a seat number on boarding passes.",
      "conflict_prompt": "The train ticket does not include reserved seating. Passengers are assigned a seat number on boarding passes.",
      "question": "What does the train ticket provide?",
      "options": [
        "A. Reserved seating with a seat number",
        "B. Free meals only",
        "C. Bicycle rental",
        "D. No boarding information"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The research team publishes peer-reviewed articles. Their latest paper appears in an academic journal this month.",
      "conflict_prompt": "The research team does not publish peer-reviewed articles. Their latest paper appears in an academic journal this month.",
      "question": "Where does the team's latest paper appear?",
      "options": [
        "A. Academic journal",
        "B. Personal blog only",
        "C. Classified government files",
        "D. It is unpublished"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The café grinds coffee beans daily. Baristas prepare espresso from freshly ground beans.",
      "conflict_prompt": "The café does not grind coffee beans daily. Baristas prepare espresso from freshly ground beans.",
      "question": "When are the coffee beans ground?",
      "options": [
        "A. Daily",
        "B. Once a year",
        "C. Never",
        "D. Only on special holidays"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The emergency exit is marked in green. Employees are trained to follow that sign in evacuations.",
      "conflict_prompt": "The emergency exit is not marked in green. Employees are trained to follow that sign in evacuations.",
      "question": "What color marks the emergency exit sign?",
      "options": [
        "A. Red",
        "B. Green",
        "C. Blue",
        "D. Purple"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recipe calls for two cups of flour. Bakers measure the flour precisely before mixing.",
      "conflict_prompt": "The recipe does not call for two cups of flour. Bakers measure the flour precisely before mixing.",
      "question": "How much flour does the recipe call for?",
      "options": [
        "A. Half a teaspoon",
        "B. Two cups",
        "C. Five liters",
        "D. No flour"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The varsity team practices every weekday. Players attend drills from 4 to 6 PM.",
      "conflict_prompt": "The varsity team does not practice every weekday. Players attend drills from 4 to 6 PM.",
      "question": "When do players attend drills?",
      "options": [
        "A. 4 to 6 PM",
        "B. Midnight only",
        "C. 9 to 10 AM",
        "D. They don't practice"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel provides a complimentary breakfast. Guests can choose from hot and cold options each morning.",
      "conflict_prompt": "The hotel does not provide a complimentary breakfast. Guests can choose from hot and cold options each morning.",
      "question": "What meal does the hotel provide complimentary?",
      "options": [
        "A. Breakfast",
        "B. Dinner",
        "C. Lunch",
        "D. Midnight snacks only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The committee holds monthly meetings. Minutes are posted online after each meeting.",
      "conflict_prompt": "The committee does not hold monthly meetings. Minutes are posted online after each meeting.",
      "question": "How often does the committee meet?",
      "options": [
        "A. Monthly",
        "B. Once every five years",
        "C. Daily",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The parking garage has an electric-vehicle charging station. Drivers can park in the designated stall to charge.",
      "conflict_prompt": "The parking garage does not have an electric-vehicle charging station. Drivers can park in the designated stall to charge.",
      "question": "What facility does the parking garage offer for EV drivers?",
      "options": [
        "A. Electric-vehicle charging station",
        "B. Free carwash",
        "C. Tire inflation service",
        "D. Helicopter pad"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The policy requires two signatures for approval. Documents lacking signatures are returned to the sender.",
      "conflict_prompt": "The policy does not require two signatures for approval. Documents lacking signatures are returned to the sender.",
      "question": "What is required on documents for approval?",
      "options": [
        "A. Two signatures",
        "B. Fingerprints only",
        "C. No signatures",
        "D. A seal of wax"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery labels allergens on product tags. Customers can check labels for common allergens like nuts and dairy.",
      "conflict_prompt": "The bakery does not label allergens on product tags. Customers can check labels for common allergens like nuts and dairy.",
      "question": "What information is provided on the product tags?",
      "options": [
        "A. Allergen labels such as nuts and dairy",
        "B. The owner's favorite movie",
        "C. Hidden riddles",
        "D. No information at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The apartment includes on-site laundry. Residents use coin-operated machines in the basement.",
      "conflict_prompt": "The apartment does not include on-site laundry. Residents use coin-operated machines in the basement.",
      "question": "Where do residents do laundry?",
      "options": [
        "A. In on-site coin-operated machines in the basement",
        "B. At an offsite facility only",
        "C. They have no laundry access",
        "D. With a valet service only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The stadium enforces a bag-check policy. Fans must open bags for security inspection before entry.",
      "conflict_prompt": "The stadium does not enforce a bag-check policy. Fans must open bags for security inspection before entry.",
      "question": "What must fans do with their bags before entering the stadium?",
      "options": [
        "A. Open them for security inspection",
        "B. Mail them to the team",
        "C. Hide them under seats",
        "D. Leave them at home"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum prohibits flash photography. Visitors are asked to turn off camera flashes in galleries.",
      "conflict_prompt": "The museum does not prohibit flash photography. Visitors are asked to turn off camera flashes in galleries.",
      "question": "What are visitors asked to do with camera flashes?",
      "options": [
        "A. Turn them off",
        "B. Use only flash",
        "C. Replace them with strobe lights",
        "D. Point them at exhibits"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community garden allocates plots to residents. Plot assignments are renewed annually in April.",
      "conflict_prompt": "The community garden does not allocate plots to residents. Plot assignments are renewed annually in April.",
      "question": "When are plot assignments renewed?",
      "options": [
        "A. April each year",
        "B. December only",
        "C. Every ten years",
        "D. There is no renewal"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery offers gluten-free bread. Customers can request the gluten-free option at checkout.",
      "conflict_prompt": "The bakery does not offer gluten-free bread. Customers can request the gluten-free option at checkout.",
      "question": "What alternative does the bakery provide for customers with gluten intolerance?",
      "options": [
        "A. Gluten-free bread",
        "B. Raw meat",
        "C. Metal loaves",
        "D. Nothing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The motel provides complimentary towels. Guests find fresh towels in each room upon arrival.",
      "conflict_prompt": "The motel does not provide complimentary towels. Guests find fresh towels in each room upon arrival.",
      "question": "What do guests find in each room when they arrive?",
      "options": [
        "A. Fresh towels",
        "B. A toolbox",
        "C. A live chicken",
        "D. An empty box"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic accepts walk-in patients. Nurses triage arrivals in order of urgency.",
      "conflict_prompt": "The clinic does not accept walk-in patients. Nurses triage arrivals in order of urgency.",
      "question": "How are arriving patients triaged?",
      "options": [
        "A. In order of urgency",
        "B. By alphabetical order",
        "C. By shoe size",
        "D. Random lottery"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The building has a sprinkler system. The system activates automatically if a fire is detected.",
      "conflict_prompt": "The building does not have a sprinkler system. The system activates automatically if a fire is detected.",
      "question": "What happens if a fire is detected?",
      "options": [
        "A. Sprinkler system activates automatically",
        "B. Alarm sings a lullaby",
        "C. Building becomes invisible",
        "D. Nothing occurs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone app requires two-factor authentication. Users receive a verification code via SMS during login.",
      "conflict_prompt": "The smartphone app does not require two-factor authentication. Users receive a verification code via SMS during login.",
      "question": "How do users receive the verification code?",
      "options": [
        "A. Via SMS",
        "B. By carrier pigeon",
        "C. Through smoke signals",
        "D. They do not receive any code"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain lodge stocks firewood. Guests can purchase bundles at the front desk.",
      "conflict_prompt": "The mountain lodge does not stock firewood. Guests can purchase bundles at the front desk.",
      "question": "Where can guests buy firewood bundles?",
      "options": [
        "A. At the front desk of the lodge",
        "B. From a vending machine only",
        "C. At the summit",
        "D. It's illegal to buy firewood"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater requires advance ticket purchase for certain shows. Patrons are advised to buy tickets online ahead of time.",
      "conflict_prompt": "The theater does not require advance ticket purchase for certain shows. Patrons are advised to buy tickets online ahead of time.",
      "question": "What should patrons do for certain shows?",
      "options": [
        "A. Buy tickets online ahead of time",
        "B. Arrive without a ticket only",
        "C. Call the director directly",
        "D. Watch from outside"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The policy mandates wearing safety goggles in the shop. Workers wear eye protection while operating machinery.",
      "conflict_prompt": "The policy does not mandate wearing safety goggles in the shop. Workers wear eye protection while operating machinery.",
      "question": "What protective equipment must workers wear while operating machinery?",
      "options": [
        "A. Safety goggles (eye protection)",
        "B. Party hats",
        "C. Sunglasses for fashion",
        "D. No protective gear"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The map marks the trail as closed in winter. Hikers should not use it during heavy snow months.",
      "conflict_prompt": "The map does not mark the trail as closed in winter. Hikers should not use it during heavy snow months.",
      "question": "What does the map indicate about the trail in winter?",
      "options": [
        "A. It is closed in winter",
        "B. It is open for skiing only",
        "C. It is a roller coaster",
        "D. There is no trail"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pastry chef decorates cakes by hand. Each wedding cake receives custom piping and flowers.",
      "conflict_prompt": "The pastry chef does not decorate cakes by hand. Each wedding cake receives custom piping and flowers.",
      "question": "How are wedding cakes decorated?",
      "options": [
        "A. By hand with custom piping and flowers",
        "B. Printed on paper",
        "C. With stickers only",
        "D. Not decorated at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry departs hourly from the dock. Passengers should arrive 15 minutes before departure to board.",
      "conflict_prompt": "The ferry does not depart hourly from the dock. Passengers should arrive 15 minutes before departure to board.",
      "question": "How often does the ferry depart from the dock?",
      "options": [
        "A. Every ten minutes",
        "B. Hourly",
        "C. Once a day",
        "D. Irregularly with no schedule"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike shop tunes bicycles on request. Mechanics adjust brakes and gears during service appointments.",
      "conflict_prompt": "The bike shop does not tune bicycles on request. Mechanics adjust brakes and gears during service appointments.",
      "question": "What services do mechanics perform during appointments?",
      "options": [
        "A. Adjust brakes and gears",
        "B. Paint murals",
        "C. Wash carpets",
        "D. Stitch clothing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The airline allows carry-on luggage. Passengers may bring one carry-on and one personal item aboard.",
      "conflict_prompt": "The airline does not allow carry-on luggage. Passengers may bring one carry-on and one personal item aboard.",
      "question": "How many carry-on items may passengers bring aboard?",
      "options": [
        "A. One carry-on and one personal item",
        "B. Ten carry-ons",
        "C. No personal items allowed",
        "D. Only checked baggage"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art gallery rotates exhibitions quarterly. New shows open at the start of each season.",
      "conflict_prompt": "The art gallery does not rotate exhibitions quarterly. New shows open at the start of each season.",
      "question": "How often does the gallery rotate exhibitions?",
      "options": [
        "A. Quarterly (at the start of each season)",
        "B. Every century",
        "C. Weekly",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses pasture-raised eggs. The custard in their tarts has a rich yellow color as a result.",
      "conflict_prompt": "The bakery does not use pasture-raised eggs. The custard in their tarts has a rich yellow color as a result.",
      "question": "What ingredient contributes to the custard's rich yellow color?",
      "options": [
        "A. Pasture-raised eggs",
        "B. Charcoal",
        "C. Paint dye",
        "D. Water only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software autosaves documents every five minutes. Users can recover versions from the history panel.",
      "conflict_prompt": "The software does not autosave documents every five minutes. Users can recover versions from the history panel.",
      "question": "How often does the software autosave documents?",
      "options": [
        "A. Every five minutes",
        "B. Once a year",
        "C. Never",
        "D. Every second"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campground provides potable water. Campers fill containers at the central tap station.",
      "conflict_prompt": "The campground does not provide potable water. Campers fill containers at the central tap station.",
      "question": "Where do campers fill containers with drinking water?",
      "options": [
        "A. Central tap station with potable water",
        "B. Nearby swamp",
        "C. Rain puddles only",
        "D. They bring none"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone's battery lasts all day under normal use. Users can expect roughly 12 hours of mixed activity.",
      "conflict_prompt": "The smartphone's battery does not last all day under normal use. Users can expect roughly 12 hours of mixed activity.",
      "question": "How long can users expect the battery to last under normal use?",
      "options": [
        "A. Roughly 12 hours",
        "B. 2 minutes",
        "C. One month",
        "D. Indefinitely"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The dentist recommends flossing daily. Patients are shown proper flossing technique during visits.",
      "conflict_prompt": "The dentist does not recommend flossing daily. Patients are shown proper flossing technique during visits.",
      "question": "What habit does the dentist recommend to patients?",
      "options": [
        "A. Flossing daily",
        "B. Only eating candy",
        "C. Brushing once a year",
        "D. Never see a dentist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater sells matinee tickets at a discount for afternoon shows. Discounts apply to shows starting before 4 PM.",
      "conflict_prompt": "The theater does not sell matinee tickets at a discount for afternoon shows. Discounts apply to shows starting before 4 PM.",
      "question": "When do matinee discounts apply?",
      "options": [
        "A. For shows starting before 4 PM",
        "B. For midnight shows only",
        "C. For weekday mornings only",
        "D. Discounts never apply"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum offers guided tours on weekends. Docents lead groups at 11 AM and 2 PM.",
      "conflict_prompt": "The museum does not offer guided tours on weekends. Docents lead groups at 11 AM and 2 PM.",
      "question": "At what times do docents lead guided tours on weekends?",
      "options": [
        "A. 11 AM and 2 PM",
        "B. 3 AM only",
        "C. Noon only",
        "D. 8 PM and midnight"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone supports expandable storage via a microSD slot. Users can insert additional memory cards.",
      "conflict_prompt": "The smartphone does not support expandable storage via a microSD slot. Users can insert additional memory cards.",
      "question": "How can users expand the smartphone's storage?",
      "options": [
        "A. By inserting a microSD card",
        "B. By adding more RAM chips",
        "C. By painting the case",
        "D. There is no way to expand storage"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The emergency room operates 24/7. Patients can receive treatment at any hour of the day.",
      "conflict_prompt": "The emergency room does not operate 24/7. Patients can receive treatment at any hour of the day.",
      "question": "When is the emergency room open for patients?",
      "options": [
        "A. 24 hours a day, 7 days a week",
        "B. Only on weekdays",
        "C. Only during business hours",
        "D. Closed permanently"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school cafeteria serves hot meals. Students may choose a hot entree each lunch period.",
      "conflict_prompt": "The school cafeteria does not serve hot meals. Students may choose a hot entree each lunch period.",
      "question": "What type of meal option is available in the cafeteria?",
      "options": [
        "A. Hot entree each lunch period",
        "B. Only cold cereal",
        "C. Only vending machine snacks",
        "D. No food provided"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater mutes cell phones during performances. Ushers remind patrons to silence devices before the show.",
      "conflict_prompt": "The theater does not mute cell phones during performances. Ushers remind patrons to silence devices before the show.",
      "question": "What do ushers ask patrons to do before the show?",
      "options": [
        "A. Silence their cell phones",
        "B. Take photos with flash",
        "C. Dance in the aisles",
        "D. Bring pets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The cycling club meets every Tuesday evening. Riders depart from the community center at 6:30 PM.",
      "conflict_prompt": "The cycling club does not meet every Tuesday evening. Riders depart from the community center at 6:30 PM.",
      "question": "When do riders depart from the community center?",
      "options": [
        "A. 6:30 PM",
        "B. 10 AM",
        "C. Midnight",
        "D. There is no departure time"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry permits pets in carriers. Owners must keep animals contained during the trip.",
      "conflict_prompt": "The ferry does not permit pets in carriers. Owners must keep animals contained during the trip.",
      "question": "What must owners do with pets during the ferry trip?",
      "options": [
        "A. Keep them contained in carriers",
        "B. Let them roam the deck",
        "C. Leave them at home",
        "D. Release them into the water"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The library lends hotspot devices. Patrons may borrow a mobile hotspot for two-week loans.",
      "conflict_prompt": "The library does not lend hotspot devices. Patrons may borrow a mobile hotspot for two-week loans.",
      "question": "How long can patrons borrow a mobile hotspot?",
      "options": [
        "A. Two weeks",
        "B. Ten years",
        "C. One hour only",
        "D. They cannot borrow hotspots"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery displays calorie counts. Nutritional information is printed on labels beside each pastry.",
      "conflict_prompt": "The bakery does not display calorie counts. Nutritional information is printed on labels beside each pastry.",
      "question": "What information is printed on labels beside each pastry?",
      "options": [
        "A. Calorie counts and nutritional information",
        "B. The owner's autobiography",
        "C. Local weather forecast",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university requires a capstone project to graduate. Senior students present their projects in May.",
      "conflict_prompt": "The university does not require a capstone project to graduate. Senior students present their projects in May.",
      "question": "What requirement must senior students fulfill to graduate?",
      "options": [
        "A. Complete a capstone project",
        "B. Run a marathon",
        "C. Produce a film",
        "D. None of the above"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coffee shop roasts beans in-house. Fresh roast dates are posted on every bag.",
      "conflict_prompt": "The coffee shop does not roast beans in-house. Fresh roast dates are posted on every bag.",
      "question": "What information is posted on every bag of coffee?",
      "options": [
        "A. Fresh roast dates",
        "B. The owner's shoe size",
        "C. Stock market tips",
        "D. Nothing at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel enforces a no-smoking policy indoors. Guests must smoke in designated outdoor areas only.",
      "conflict_prompt": "The hotel does not enforce a no-smoking policy indoors. Guests must smoke in designated outdoor areas only.",
      "question": "Where are guests allowed to smoke?",
      "options": [
        "A. In designated outdoor areas only",
        "B. In all indoor rooms",
        "C. On the rooftop only",
        "D. Anywhere on the property"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic administers flu vaccines each fall. Patients can book appointments starting in September.",
      "conflict_prompt": "The clinic does not administer flu vaccines each fall. Patients can book appointments starting in September.",
      "question": "When can patients start booking flu vaccine appointments?",
      "options": [
        "A. September",
        "B. January",
        "C. July",
        "D. They cannot book appointments"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The airline provides in-flight entertainment on long-haul flights. Screens are available at every seat.",
      "conflict_prompt": "The airline does not provide in-flight entertainment on long-haul flights. Screens are available at every seat.",
      "question": "What amenity is available at every seat on long-haul flights?",
      "options": [
        "A. In-flight entertainment screens",
        "B. Free bicycles",
        "C. Personal butlers only",
        "D. No amenities"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum stores digital copies of its archives. Curators can retrieve scanned documents on request.",
      "conflict_prompt": "The museum does not store digital copies of its archives. Curators can retrieve scanned documents on request.",
      "question": "What can curators retrieve on request?",
      "options": [
        "A. Scanned digital documents from the archive",
        "B. Live animals",
        "C. Fresh produce",
        "D. The original Mona Lisa"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train station sells regional rail passes. Commuters can purchase multi-ride passes at the ticket window.",
      "conflict_prompt": "The train station does not sell regional rail passes. Commuters can purchase multi-ride passes at the ticket window.",
      "question": "Where can commuters buy multi-ride rail passes?",
      "options": [
        "A. At the ticket window of the train station",
        "B. Only in another city",
        "C. They cannot buy passes",
        "D. From street vendors"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recycling center accepts electronic waste. Residents can drop off old phones and computers on Saturdays.",
      "conflict_prompt": "The recycling center does not accept electronic waste. Residents can drop off old phones and computers on Saturdays.",
      "question": "When can residents drop off electronic waste like phones and computers?",
      "options": [
        "A. Saturdays",
        "B. Never",
        "C. Only on leap day",
        "D. Every hour on the hour"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The building offers handicap-accessible restrooms. Signs indicate accessible facilities on each floor.",
      "conflict_prompt": "The building does not offer handicap-accessible restrooms. Signs indicate accessible facilities on each floor.",
      "question": "What type of restroom facilities are indicated by signs on each floor?",
      "options": [
        "A. Handicap-accessible restrooms",
        "B. Pirate-themed restrooms",
        "C. No restrooms",
        "D. Only outdoor restrooms"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum rotates fragile items into storage for conservation. Certain artifacts are displayed only for short periods.",
      "conflict_prompt": "The museum does not rotate fragile items into storage for conservation. Certain artifacts are displayed only for short periods.",
      "question": "How often are fragile artifacts displayed?",
      "options": [
        "A. Only for short periods before returning to storage",
        "B. Permanently on display",
        "C. Never displayed",
        "D. Displayed daily without rotation"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant makes sauces from scratch daily. Chefs prepare fresh marinara each morning.",
      "conflict_prompt": "The restaurant does not make sauces from scratch daily. Chefs prepare fresh marinara each morning.",
      "question": "When is fresh marinara prepared?",
      "options": [
        "A. Each morning",
        "B. Once a decade",
        "C. Never",
        "D. Only when ordered by royalty"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum requires tickets for special exhibitions. Visitors can purchase timed-entry passes online.",
      "conflict_prompt": "The museum does not require tickets for special exhibitions. Visitors can purchase timed-entry passes online.",
      "question": "How can visitors obtain entry to special exhibitions?",
      "options": [
        "A. Purchase timed-entry passes online",
        "B. Walk in without any pass",
        "C. Only with VIP membership",
        "D. By calling the mayor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic performs routine blood tests onsite. Patients visit the lab in the morning for fasting tests.",
      "conflict_prompt": "The clinic does not perform routine blood tests onsite. Patients visit the lab in the morning for fasting tests.",
      "question": "When do patients visit the lab for fasting blood tests?",
      "options": [
        "A. In the morning",
        "B. Midnight only",
        "C. At lunchtime",
        "D. They do not take fasting tests"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school library lends laptops to students. Loans are available for three-day checkout periods.",
      "conflict_prompt": "The school library does not lend laptops to students. Loans are available for three-day checkout periods.",
      "question": "How long is the laptop checkout period?",
      "options": [
        "A. Three days",
        "B. One year",
        "C. Ten minutes",
        "D. Forever"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel provides complimentary shuttle service to the airport. Guests schedule pickups at the front desk.",
      "conflict_prompt": "The hotel does not provide complimentary shuttle service to the airport. Guests schedule pickups at the front desk.",
      "question": "How do guests arrange airport pickup?",
      "options": [
        "A. Schedule at the front desk for the complimentary shuttle",
        "B. Hitchhike",
        "C. Ride a tractor",
        "D. No pickup available"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city bans fireworks in residential areas. Notices warn that use is prohibited within city limits.",
      "conflict_prompt": "The city does not ban fireworks in residential areas. Notices warn that use is prohibited within city limits.",
      "question": "Where is the use of fireworks prohibited?",
      "options": [
        "A. Within city limits (residential areas)",
        "B. Only on the moon",
        "C. Everywhere except the city",
        "D. Nowhere"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike lane is painted green. Cyclists follow the green lane to remain separated from traffic.",
      "conflict_prompt": "The bike lane is not painted green. Cyclists follow the green lane to remain separated from traffic.",
      "question": "What color is the bike lane painted?",
      "options": [
        "A. Green",
        "B. Yellow",
        "C. Black",
        "D. Transparent"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses organic sugar. Pastries sweetened with it are labeled as organic on the display.",
      "conflict_prompt": "The bakery does not use organic sugar. Pastries sweetened with it are labeled as organic on the display.",
      "question": "What ingredient is used to sweeten the pastries labeled organic?",
      "options": [
        "A. Organic sugar",
        "B. Salt",
        "C. Motor oil",
        "D. Artificial paint"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater posts age recommendations for films. Parents are advised to heed the suggested ratings.",
      "conflict_prompt": "The theater does not post age recommendations for films. Parents are advised to heed the suggested ratings.",
      "question": "What should parents pay attention to when choosing films?",
      "options": [
        "A. Age recommendations (ratings)",
        "B. The color of the poster",
        "C. The snacks offered",
        "D. The cashier's mood"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The fitness center offers personal training sessions. Members book trainers through the front desk.",
      "conflict_prompt": "The fitness center does not offer personal training sessions. Members book trainers through the front desk.",
      "question": "How do members book a personal trainer?",
      "options": [
        "A. Through the front desk",
        "B. By skywriting",
        "C. By waiting at the pool",
        "D. Trainers are not bookable"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden hosts an orchid exhibit each spring. Visitors admire diverse orchid species in the conservatory.",
      "conflict_prompt": "The botanical garden does not host an orchid exhibit each spring. Visitors admire diverse orchid species in the conservatory.",
      "question": "Where do visitors view the orchid species?",
      "options": [
        "A. In the conservatory",
        "B. At the hardware store",
        "C. In a parking lot",
        "D. There are no orchids"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum prohibits food and drink in galleries. Signs remind visitors to finish snacks before entering exhibits.",
      "conflict_prompt": "The museum does not prohibit food and drink in galleries. Signs remind visitors to finish snacks before entering exhibits.",
      "question": "What do signs in the museum remind visitors regarding food?",
      "options": [
        "A. Finish snacks before entering exhibits (no food/drink allowed)",
        "B. Bring a full picnic into galleries",
        "C. Food is mandatory",
        "D. Only eat next to artifacts"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Liam is right-handed and wrote the note with his right hand.",
      "conflict_prompt": "Liam is right-handed and wrote the note with his left hand.",
      "question": "Which hand did Liam use to write the note?",
      "options": [
        "A. Left hand",
        "B. Both hands",
        "C. Right hand",
        "D. Neither hand"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aisha, who is married, introduced her spouse at the meeting.",
      "conflict_prompt": "Aisha, who is married, introduced herself as single at the meeting.",
      "question": "What is Aisha's marital status according to the statement?",
      "options": [
        "A. Single",
        "B. Engaged",
        "C. Married",
        "D. Divorced"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Diego, born in Argentina, holds Argentine citizenship.",
      "conflict_prompt": "Diego, born in Argentina, holds Canadian citizenship.",
      "question": "What citizenship does Diego have according to the statement?",
      "options": [
        "A. Canadian",
        "B. Mexican",
        "C. Argentine",
        "D. Spanish"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya is a vegetarian who never eats meat.",
      "conflict_prompt": "Maya is a vegetarian who often eats steak.",
      "question": "Which diet does Maya follow according to the statement?",
      "options": [
        "A. Keto",
        "B. Vegetarian",
        "C. Pescatarian",
        "D. Carnivore"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Thomas is an only child and has no siblings.",
      "conflict_prompt": "Thomas is an only child who grew up with three siblings.",
      "question": "Does Thomas have siblings according to the statement?",
      "options": [
        "A. Yes, three siblings",
        "B. No, he has none",
        "C. One sibling",
        "D. Unknown"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora is an organ donor who consented to donate her kidneys.",
      "conflict_prompt": "Nora is an organ donor who refused to donate her kidneys.",
      "question": "What did Nora consent to donate according to the statement?",
      "options": [
        "A. None",
        "B. Liver",
        "C. Kidneys",
        "D. Heart"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan is a licensed pilot who flies single-engine planes.",
      "conflict_prompt": "Ethan is a licensed pilot who has never flown a single-engine plane.",
      "question": "What type of planes does Ethan fly according to the statement?",
      "options": [
        "A. Helicopters",
        "B. Single-engine planes",
        "C. Commercial jets only",
        "D. He does not fly planes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olivia is fluent in Spanish and conducted the interview in Spanish.",
      "conflict_prompt": "Olivia is fluent in Spanish but conducted the interview in English because she doesn't speak Spanish.",
      "question": "In which language did Olivia conduct the interview according to the statement?",
      "options": [
        "A. French",
        "B. Spanish",
        "C. German",
        "D. English"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus is a vegetarian chef who prepares plant-based menus.",
      "conflict_prompt": "Marcus is a vegetarian chef who specializes in beef tasting menus.",
      "question": "What type of menus does Marcus prepare according to the statement?",
      "options": [
        "A. Plant-based menus",
        "B. Seafood menus",
        "C. Beef tasting menus",
        "D. Gluten-free menus"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia is a morning person who wakes up at 5:30 AM every day.",
      "conflict_prompt": "Sofia is a morning person who stays up late and never wakes before noon.",
      "question": "What time does Sofia wake up according to the statement?",
      "options": [
        "A. 5:30 AM",
        "B. Noon",
        "C. 9:00 PM",
        "D. 7:00 PM"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah is an only driver of his car and never lets others drive it.",
      "conflict_prompt": "Noah is an only driver of his car who frequently lends it to friends to drive.",
      "question": "Who drives Noah's car according to the statement?",
      "options": [
        "A. He lends it to friends frequently",
        "B. He never drives it",
        "C. Only Noah drives it",
        "D. His family shares driving duties"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah has a valid passport issued by the United Kingdom.",
      "conflict_prompt": "Hannah has a valid passport issued by the United States.",
      "question": "Which country's passport does Hannah hold according to the statement?",
      "options": [
        "A. Canada",
        "B. United Kingdom",
        "C. United States",
        "D. Australia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jacob owns a bicycle and rides it to work every day.",
      "conflict_prompt": "Jacob owns a bicycle but never rides it and drives to work every day.",
      "question": "How does Jacob commute to work according to the statement?",
      "options": [
        "A. He walks",
        "B. He takes the bus",
        "C. He rides his bicycle",
        "D. He drives"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Isabella is a non-smoker who has never used tobacco products.",
      "conflict_prompt": "Isabella is a non-smoker who smokes daily.",
      "question": "Does Isabella use tobacco according to the statement?",
      "options": [
        "A. Yes, daily",
        "B. Rarely",
        "C. No, she has never used tobacco",
        "D. She used to but quit"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lucas is a dual citizen of France and holds a French passport.",
      "conflict_prompt": "Lucas is a dual citizen of France and holds only a German passport.",
      "question": "Which passport does Lucas hold according to the statement?",
      "options": [
        "A. Italian passport",
        "B. German passport only",
        "C. French passport",
        "D. No passport"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Grace is legally blind and uses a white cane to get around.",
      "conflict_prompt": "Grace is legally blind and never uses mobility aids and navigates without a cane.",
      "question": "What mobility aid does Grace use according to the statement?",
      "options": [
        "A. A guide dog",
        "B. A wheelchair",
        "C. A white cane",
        "D. No mobility aid"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Owen is retired from the police force and no longer works in law enforcement.",
      "conflict_prompt": "Owen is retired from the police force and currently works full-time as an active police officer.",
      "question": "What is Owen's employment status according to the statement?",
      "options": [
        "A. Active police officer",
        "B. Retired from police",
        "C. Works in healthcare",
        "D. Unemployed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Chloe is a foster parent who currently cares for one foster child.",
      "conflict_prompt": "Chloe is a foster parent who has no children in her care.",
      "question": "How many foster children does Chloe currently care for according to the statement?",
      "options": [
        "A. None",
        "B. One",
        "C. Two",
        "D. Three"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Benjamin is an organ transplant recipient who received a heart transplant last year.",
      "conflict_prompt": "Benjamin is an organ transplant recipient who has never had a heart transplant.",
      "question": "Which organ transplant did Benjamin receive according to the statement?",
      "options": [
        "A. Kidney",
        "B. Liver",
        "C. Heart",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amelia is allergic to peanuts and carries an epinephrine injector.",
      "conflict_prompt": "Amelia is allergic to peanuts but does not carry any allergy medication.",
      "question": "What allergy medication does Amelia carry according to the statement?",
      "options": [
        "A. Antihistamine tablets only",
        "B. None",
        "C. An epinephrine injector",
        "D. Inhaler"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Daniel is the mayor and presided over the town council meeting.",
      "conflict_prompt": "Daniel is the mayor and was absent from the town council meeting.",
      "question": "What role does Daniel hold according to the statement?",
      "options": [
        "A. Governor",
        "B. Town council member",
        "C. Mayor",
        "D. City manager"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zoe is a vegan baker who uses no animal products in her recipes.",
      "conflict_prompt": "Zoe is a vegan baker who uses eggs and butter in all her recipes.",
      "question": "Which ingredient policy does Zoe follow according to the statement?",
      "options": [
        "A. Uses eggs and butter",
        "B. Uses dairy but no eggs",
        "C. Uses no animal products",
        "D. Uses only gluten-free ingredients"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Evan is left-handed and plays guitar with his left hand as the dominant hand.",
      "conflict_prompt": "Evan is left-handed and plays guitar using right-handed technique exclusively.",
      "question": "Which hand is Evan dominant in according to the statement?",
      "options": [
        "A. Ambidextrous",
        "B. Right hand",
        "C. Left hand",
        "D. Neither"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Luna serves as the head nurse at the clinic and manages the nursing staff.",
      "conflict_prompt": "Luna serves as the head nurse at the clinic but has no managerial duties over nursing staff.",
      "question": "What position does Luna hold at the clinic according to the statement?",
      "options": [
        "A. Receptionist",
        "B. Head nurse",
        "C. Laboratory technician",
        "D. Physician assistant"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar is a professional photographer who specializes in wedding photography.",
      "conflict_prompt": "Omar is a professional photographer who specializes exclusively in product photography and never shoots weddings.",
      "question": "What kind of photography does Omar specialize in according to the statement?",
      "options": [
        "A. Wildlife photography",
        "B. Food photography",
        "C. Wedding photography",
        "D. Sports photography"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya cycles to work every weekday and does not own a car.",
      "conflict_prompt": "Priya cycles to work every weekday but also owns and drives a car to work daily.",
      "question": "How does Priya typically commute to work according to the statement?",
      "options": [
        "A. By bus",
        "B. By bicycle",
        "C. By car",
        "D. By train"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gavin has a PhD in physics and teaches quantum mechanics at the university.",
      "conflict_prompt": "Gavin has a PhD in physics but teaches introductory literature courses only.",
      "question": "What subject does Gavin teach according to the statement?",
      "options": [
        "A. Quantum mechanics",
        "B. Introductory literature",
        "C. Calculus",
        "D. Biology"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mila is a homeowner who owns the house on Maple Street.",
      "conflict_prompt": "Mila is a homeowner who rents an apartment on Maple Street and does not own the house.",
      "question": "What is Mila's housing status on Maple Street according to the statement?",
      "options": [
        "A. Rents an apartment",
        "B. Owns the house",
        "C. Homeless",
        "D. Lives with parents"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Connor has a background in software engineering and works as a backend developer.",
      "conflict_prompt": "Connor has a background in software engineering and works exclusively as a UX designer.",
      "question": "What is Connor's role according to the statement?",
      "options": [
        "A. Backend developer",
        "B. UX designer",
        "C. Data scientist",
        "D. Product manager"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ava is an only passenger on the flight and had the entire row to herself.",
      "conflict_prompt": "Ava is an only passenger on the flight but the row was fully occupied by other passengers.",
      "question": "How many passengers were seated in Ava's row according to the statement?",
      "options": [
        "A. Several passengers",
        "B. Two passengers",
        "C. Ava alone",
        "D. The row was empty"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Henry is a full-time student and attends classes five days a week.",
      "conflict_prompt": "Henry is a full-time student who never attends classes and studies independently.",
      "question": "How often does Henry attend classes according to the statement?",
      "options": [
        "A. Never",
        "B. Once a week",
        "C. Three times a week",
        "D. Five days a week"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bella is a certified scuba diver who dove at the coral reef last summer.",
      "conflict_prompt": "Bella is a certified scuba diver who refuses to dive and avoided the coral reef last summer.",
      "question": "What activity did Bella do last summer according to the statement?",
      "options": [
        "A. Hiking",
        "B. Scuba diving at the coral reef",
        "C. Rock climbing",
        "D. She avoided outdoor activities"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noelle is a trained pianist who performed a Beethoven piece at the recital.",
      "conflict_prompt": "Noelle is a trained pianist who refused to perform at the recital.",
      "question": "What did Noelle do at the recital according to the statement?",
      "options": [
        "A. Performed a Beethoven piece",
        "B. Did not attend",
        "C. Sang instead of playing piano",
        "D. Organized the event"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael owns a dog named Bruno and takes him to the park daily.",
      "conflict_prompt": "Rafael owns a dog named Bruno but never takes him to the park.",
      "question": "What pet does Rafael own according to the statement?",
      "options": [
        "A. Cat",
        "B. Parrot",
        "C. Dog named Bruno",
        "D. No pets"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivy is a daytime security guard who works the morning shift from 6 AM to 2 PM.",
      "conflict_prompt": "Ivy is a daytime security guard who only works the night shift from 10 PM to 6 AM.",
      "question": "Which shift does Ivy work according to the statement?",
      "options": [
        "A. Morning shift from 6 AM to 2 PM",
        "B. Afternoon shift from 2 PM to 10 PM",
        "C. Night shift from 10 PM to 6 AM",
        "D. Rotating shifts"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Paul is a non-alcoholic who abstains from drinking alcohol.",
      "conflict_prompt": "Paul is a non-alcoholic who drinks wine every evening.",
      "question": "Does Paul drink alcohol according to the statement?",
      "options": [
        "A. Yes, daily",
        "B. Occasionally",
        "C. No, he abstains",
        "D. He used to but quit"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sara is a vegetarian nutritionist who recommends plant-based diets to clients.",
      "conflict_prompt": "Sara is a vegetarian nutritionist who recommends high-meat diets to clients.",
      "question": "What dietary recommendation does Sara provide according to the statement?",
      "options": [
        "A. High-meat diets",
        "B. Plant-based diets",
        "C. Low-carb diets only",
        "D. No dietary advice"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Victor is a published author who wrote three novels under his real name.",
      "conflict_prompt": "Victor is a published author who has never written a novel under his real name.",
      "question": "How many novels did Victor write under his real name according to the statement?",
      "options": [
        "A. None",
        "B. One",
        "C. Three",
        "D. Five"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya is diabetic and monitors her blood sugar daily with a glucose meter.",
      "conflict_prompt": "Maya is diabetic and never checks her blood sugar levels.",
      "question": "How does Maya manage her diabetes according to the statement?",
      "options": [
        "A. With daily glucose monitoring",
        "B. With only exercise",
        "C. With no monitoring",
        "D. With alternative medicine only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ollie works full time at the bakery and is employed there five days a week.",
      "conflict_prompt": "Ollie works full time at the bakery but is never scheduled to work during the week.",
      "question": "How often is Ollie employed at the bakery according to the statement?",
      "options": [
        "A. Five days a week",
        "B. Weekends only",
        "C. Not employed",
        "D. Seasonal"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rosa is a certified lifeguard who holds a current lifeguarding certification.",
      "conflict_prompt": "Rosa is a certified lifeguard who has an expired lifeguarding certification.",
      "question": "What is Rosa's certification status according to the statement?",
      "options": [
        "A. Expired certification",
        "B. No certification",
        "C. Current lifeguarding certification",
        "D. Certification pending"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leon is a vegan athlete who follows a strict plant-based training diet.",
      "conflict_prompt": "Leon is a vegan athlete who consumes dairy during training.",
      "question": "Which diet does Leon follow according to the statement?",
      "options": [
        "A. Paleo",
        "B. Plant-based (vegan)",
        "C. Dairy-heavy diet",
        "D. Carnivore"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hana is ambidextrous but prefers to write with her left hand.",
      "conflict_prompt": "Hana is ambidextrous but prefers to write with her right hand exclusively.",
      "question": "Which hand does Hana prefer to write with according to the statement?",
      "options": [
        "A. Right hand exclusively",
        "B. Both equally",
        "C. Left hand",
        "D. She does not write"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marco is a volunteer firefighter who responds to local emergencies.",
      "conflict_prompt": "Marco is a volunteer firefighter who never responds to any emergencies.",
      "question": "What role does Marco perform according to the statement?",
      "options": [
        "A. Full-time paramedic",
        "B. Volunteer firefighter",
        "C. Police officer",
        "D. Emergency room doctor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Keira drives an electric car and charges it at home every night.",
      "conflict_prompt": "Keira drives an electric car but never charges it and uses gasoline daily.",
      "question": "What type of car does Keira drive according to the statement?",
      "options": [
        "A. Gasoline car",
        "B. Hybrid car",
        "C. Electric car",
        "D. Diesel truck"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noel is the team's captain and led the players onto the field.",
      "conflict_prompt": "Noel is the team's captain but was removed from the captaincy and did not lead the team.",
      "question": "What leadership role does Noel have according to the statement?",
      "options": [
        "A. Assistant coach",
        "B. Team captain",
        "C. Team manager",
        "D. Fan representative"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Riley is a parent of two children and picked them up from school.",
      "conflict_prompt": "Riley is a parent of two children who has no children and never picks anyone up from school.",
      "question": "How many children does Riley have according to the statement?",
      "options": [
        "A. None",
        "B. One",
        "C. Two",
        "D. Three"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ariana is a morning jogger who runs five miles every morning.",
      "conflict_prompt": "Ariana is a morning jogger who never runs and avoids exercise entirely.",
      "question": "How far does Ariana run every morning according to the statement?",
      "options": [
        "A. She doesn't run",
        "B. One mile",
        "C. Five miles",
        "D. Ten miles"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felix is a licensed electrician who installs wiring in commercial buildings.",
      "conflict_prompt": "Felix is a licensed electrician who never installs wiring and works only in retail.",
      "question": "What type of work does Felix do according to the statement?",
      "options": [
        "A. Install wiring in commercial buildings",
        "B. Retail sales",
        "C. Plumbing",
        "D. Teaching"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia is a bilingual teacher who instructs classes in both English and French.",
      "conflict_prompt": "Nadia is a bilingual teacher who only teaches in Spanish and does not use English or French.",
      "question": "In which languages does Nadia teach according to the statement?",
      "options": [
        "A. Only Spanish",
        "B. English and French",
        "C. German only",
        "D. Mandarin and Japanese"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Quentin is an organ donor who agreed to donate his liver and kidneys.",
      "conflict_prompt": "Quentin is an organ donor who refused to donate his liver and kidneys.",
      "question": "Which organs did Quentin agree to donate according to the statement?",
      "options": [
        "A. Heart only",
        "B. Liver and kidneys",
        "C. Lungs only",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Harper is a professional skier who competed in the national downhill event.",
      "conflict_prompt": "Harper is a professional skier who skipped the national downhill event and did not compete.",
      "question": "What did Harper do in the national downhill event according to the statement?",
      "options": [
        "A. Competed",
        "B. Skipped it",
        "C. Was a spectator",
        "D. Coached the event"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivan owns a sailboat and spends summers sailing on the lake.",
      "conflict_prompt": "Ivan owns a sailboat but never sails it and keeps it docked year-round.",
      "question": "What activity does Ivan do in the summers according to the statement?",
      "options": [
        "A. Fishing from shore",
        "B. Sailing on the lake",
        "C. Camping inland",
        "D. He leaves town"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Selene is a classical violinist who performed in the orchestra last season.",
      "conflict_prompt": "Selene is a classical violinist who was not part of the orchestra and did not perform last season.",
      "question": "What did Selene do last season according to the statement?",
      "options": [
        "A. Performed in the orchestra",
        "B. Took a sabbatical",
        "C. Toured as a solo pop artist",
        "D. Did not perform at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dylan is a homeowner who pays mortgage payments every month.",
      "conflict_prompt": "Dylan is a homeowner who has no mortgage and pays no monthly mortgage payments.",
      "question": "What homeowner expense does Dylan have according to the statement?",
      "options": [
        "A. Rents his home",
        "B. Pays a mortgage monthly",
        "C. Pays monthly HOA only",
        "D. Lives mortgage-free"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bianca keeps a regular sleep schedule and goes to bed at 10 PM nightly.",
      "conflict_prompt": "Bianca keeps a regular sleep schedule but stays up until 3 AM every night.",
      "question": "What time does Bianca go to bed according to the statement?",
      "options": [
        "A. 10 PM",
        "B. Midnight",
        "C. 3 AM",
        "D. Varies widely"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael is a Christian who attends church every Sunday.",
      "conflict_prompt": "Rafael is a Christian who does not practice any religion and never attends church.",
      "question": "What religious practice does Rafael follow according to the statement?",
      "options": [
        "A. Muslim",
        "B. Jewish",
        "C. Christian and attends church every Sunday",
        "D. No religious practice"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jenna holds a valid driver's license and drives to work daily.",
      "conflict_prompt": "Jenna holds a valid driver's license but does not drive and relies on public transit daily.",
      "question": "How does Jenna commute to work according to the statement?",
      "options": [
        "A. Drives daily",
        "B. Uses public transit",
        "C. Walks only",
        "D. Bikes exclusively"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mateo is the youngest sibling and was born in 2002.",
      "conflict_prompt": "Mateo is the youngest sibling who was born in 1990.",
      "question": "When was Mateo born according to the statement?",
      "options": [
        "A. 1990",
        "B. 2002",
        "C. 1985",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Iris is a certified yoga instructor who teaches three classes each week.",
      "conflict_prompt": "Iris is a certified yoga instructor who no longer teaches any classes.",
      "question": "How many yoga classes does Iris teach each week according to the statement?",
      "options": [
        "A. None",
        "B. One",
        "C. Three",
        "D. Five"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Miles owns a motorcycle and rides it to work on sunny days.",
      "conflict_prompt": "Miles owns a motorcycle but never rides it and always drives a car to work.",
      "question": "What vehicle does Miles ride to work on sunny days according to the statement?",
      "options": [
        "A. Bicycle",
        "B. Motorcycle",
        "C. Car",
        "D. Scooter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora is a public school teacher who works in the district elementary school.",
      "conflict_prompt": "Nora is a public school teacher who works only in private tutoring and not in any district school.",
      "question": "Where does Nora work according to the statement?",
      "options": [
        "A. District elementary school",
        "B. Private tutoring only",
        "C. University",
        "D. Hospital education department"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Caleb is a licensed pharmacist who dispenses medications at the pharmacy.",
      "conflict_prompt": "Caleb is a licensed pharmacist who does not dispense medications and works in inventory only.",
      "question": "What job duty does Caleb perform according to the statement?",
      "options": [
        "A. Dispensing medications",
        "B. Working in inventory only",
        "C. Medical billing",
        "D. Sales representative"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leah keeps bees in her backyard and harvests honey each summer.",
      "conflict_prompt": "Leah keeps bees in her backyard but never harvests honey and does not handle them.",
      "question": "What does Leah do with her bees each summer according to the statement?",
      "options": [
        "A. Harvest honey",
        "B. Avoids them",
        "C. Releases them",
        "D. Sells hives"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Arjun is a vegetarian who avoids all seafood and meat.",
      "conflict_prompt": "Arjun is a vegetarian who eats fish regularly as part of his diet.",
      "question": "Which foods does Arjun avoid according to the statement?",
      "options": [
        "A. Only red meat",
        "B. All seafood and meat",
        "C. Dairy products only",
        "D. No restrictions"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Megan is a city resident and lives in apartment 4B downtown.",
      "conflict_prompt": "Megan is a city resident who lives in a rural farmhouse and not in apartment 4B downtown.",
      "question": "Where does Megan live according to the statement?",
      "options": [
        "A. Rural farmhouse",
        "B. Suburban house",
        "C. Apartment 4B downtown",
        "D. With relatives"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ramon is an undergraduate student majoring in biology at State University.",
      "conflict_prompt": "Ramon is an undergraduate student majoring in biology who is actually an MBA student at a different university.",
      "question": "What is Ramon's major according to the statement?",
      "options": [
        "A. Biology",
        "B. Business Administration",
        "C. Computer Science",
        "D. History"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "June is an experienced hiker who completed the Appalachian Trail last year.",
      "conflict_prompt": "June is an experienced hiker who has never hiked more than a mile from her home.",
      "question": "What major hike did June complete according to the statement?",
      "options": [
        "A. Pacific Crest Trail",
        "B. Appalachian Trail",
        "C. Inca Trail",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bryce is a licensed marriage counselor who holds weekly counseling sessions.",
      "conflict_prompt": "Bryce is a licensed marriage counselor who stopped counseling and does not see clients.",
      "question": "What service does Bryce provide according to the statement?",
      "options": [
        "A. Weekly marriage counseling sessions",
        "B. Financial advising",
        "C. Real estate brokering",
        "D. He does not provide services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Alina is a dog breeder who registered all her puppies with the kennel club.",
      "conflict_prompt": "Alina is a dog breeder who never registers her puppies with any kennel club.",
      "question": "What did Alina do with her puppies according to the statement?",
      "options": [
        "A. Registered them with the kennel club",
        "B. Put them up for adoption unregistered",
        "C. Sold them without papers",
        "D. Rehomed them to family only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sebastian is an emergency room doctor who worked the night shift last weekend.",
      "conflict_prompt": "Sebastian is an emergency room doctor who had no night shifts and was off duty last weekend.",
      "question": "What shift did Sebastian work last weekend according to the statement?",
      "options": [
        "A. Day shift",
        "B. Night shift",
        "C. He was off duty",
        "D. Weekend administrative shift"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dina is a homeowner who installed solar panels on her roof.",
      "conflict_prompt": "Dina is a homeowner who refuses to install solar panels and removed them from her roof.",
      "question": "What modification did Dina make to her roof according to the statement?",
      "options": [
        "A. Installed solar panels",
        "B. Installed a skylight",
        "C. Removed gutters",
        "D. Did nothing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Owen is a fluent Mandarin speaker who conducted the meeting in Mandarin.",
      "conflict_prompt": "Owen is a fluent Mandarin speaker who does not speak Mandarin and conducted the meeting in English.",
      "question": "In which language did Owen conduct the meeting according to the statement?",
      "options": [
        "A. Mandarin",
        "B. English",
        "C. Cantonese",
        "D. Japanese"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mira is a licensed lifeguard who supervised the pool during summer sessions.",
      "conflict_prompt": "Mira is a licensed lifeguard who did not supervise the pool and was on leave during summer sessions.",
      "question": "What did Mira do during summer sessions according to the statement?",
      "options": [
        "A. Supervised the pool",
        "B. Worked at a different facility",
        "C. Was on leave",
        "D. Taught swim lessons remotely"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jonah is a twin and is the older of two siblings.",
      "conflict_prompt": "Jonah is a twin and is the younger of two siblings.",
      "question": "What is Jonah's birth order among the twins according to the statement?",
      "options": [
        "A. Only child",
        "B. Older twin",
        "C. Younger twin",
        "D. Triplet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nina is a licensed therapist who sees clients in private practice.",
      "conflict_prompt": "Nina is a licensed therapist who is not licensed and does not see clients in private practice.",
      "question": "What is Nina's professional status according to the statement?",
      "options": [
        "A. Licensed therapist in private practice",
        "B. Unlicensed counselor",
        "C. Medical doctor",
        "D. Social worker"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Trevor is a non-smoker who spends evenings in smoke-free environments.",
      "conflict_prompt": "Trevor is a non-smoker who frequently smokes at social events.",
      "question": "What is Trevor's smoking habit according to the statement?",
      "options": [
        "A. Smokes frequently",
        "B. Vapes only",
        "C. Non-smoker",
        "D. Former smoker who quit last year"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Yara is a professional dancer who performed in three shows this season.",
      "conflict_prompt": "Yara is a professional dancer who did not perform at all this season.",
      "question": "How many shows did Yara perform in this season according to the statement?",
      "options": [
        "A. None",
        "B. One",
        "C. Two",
        "D. Three"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector is a homeowner and owns the property at 48 Elm Avenue.",
      "conflict_prompt": "Hector is a homeowner who rents the property at 48 Elm Avenue and does not own it.",
      "question": "What is Hector's relationship to the property at 48 Elm Avenue according to the statement?",
      "options": [
        "A. Rents it",
        "B. Owns it",
        "C. Manages it for the owner",
        "D. Vacant"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sasha volunteers at the animal shelter every Saturday morning.",
      "conflict_prompt": "Sasha volunteers at the animal shelter but never shows up on Saturdays.",
      "question": "When does Sasha volunteer at the shelter according to the statement?",
      "options": [
        "A. Friday nights",
        "B. Every Saturday morning",
        "C. Monthly only",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivan is a Republican who voted for the Republican candidate in the last election.",
      "conflict_prompt": "Ivan is a Republican who voted for the Democratic candidate in the last election.",
      "question": "Which candidate did Ivan vote for according to the statement?",
      "options": [
        "A. Libertarian candidate",
        "B. Republican candidate",
        "C. Democratic candidate",
        "D. He abstained"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carmen is a native New Yorker who was born and raised in Manhattan.",
      "conflict_prompt": "Carmen is a native New Yorker who was born and raised in rural Kansas.",
      "question": "Where was Carmen born and raised according to the statement?",
      "options": [
        "A. Rural Kansas",
        "B. Manhattan, New York",
        "C. Los Angeles, California",
        "D. Miami, Florida"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gus is a vegetarian chef who refuses to use any animal fat in his kitchen.",
      "conflict_prompt": "Gus is a vegetarian chef who uses animal fat in most of his dishes.",
      "question": "What ingredient policy does Gus follow in his kitchen according to the statement?",
      "options": [
        "A. Uses animal fat in most dishes",
        "B. Uses no animal fat",
        "C. Uses only butter and dairy",
        "D. Uses only soy-based fats"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Fiona is a high school principal who oversees three academic departments.",
      "conflict_prompt": "Fiona is a high school principal who does not oversee any academic departments.",
      "question": "How many academic departments does Fiona oversee according to the statement?",
      "options": [
        "A. None",
        "B. One",
        "C. Three",
        "D. Five"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Connor is colorblind and cannot distinguish red from green.",
      "conflict_prompt": "Connor is colorblind but accurately distinguishes red from green without difficulty.",
      "question": "What color vision issue does Connor have according to the statement?",
      "options": [
        "A. Cannot distinguish blue from yellow",
        "B. Cannot distinguish red from green",
        "C. Complete blindness",
        "D. Normal color vision"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zara is a vegetarian who avoids eating poultry as well as red meat.",
      "conflict_prompt": "Zara is a vegetarian who eats chicken several times a week.",
      "question": "Which foods does Zara avoid according to the statement?",
      "options": [
        "A. Poultry and red meat",
        "B. Only red meat",
        "C. Only dairy",
        "D. No animal products except fish"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Elias is an only child and inherited his parents' house alone.",
      "conflict_prompt": "Elias is an only child who shares inheritance equally with multiple siblings.",
      "question": "Who inherited the parents' house according to the statement?",
      "options": [
        "A. Elias alone",
        "B. Shared among siblings",
        "C. Sold to a third party",
        "D. Donated to charity"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bea has a green thumb and tends to her vegetable garden daily.",
      "conflict_prompt": "Bea has a green thumb but never tends to any garden and her plants die.",
      "question": "What gardening habit does Bea have according to the statement?",
      "options": [
        "A. Tends to her vegetable garden daily",
        "B. Occasionally waters plants",
        "C. Never gardens",
        "D. Hires a gardener"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar is a vegetarian who avoids all meat consumption.",
      "conflict_prompt": "Omar is a vegetarian who eats red meat multiple times per week.",
      "question": "What dietary restriction does Omar follow according to the statement?",
      "options": [
        "A. Avoids all meat",
        "B. Eats red meat weekly",
        "C. Avoids dairy only",
        "D. Eats only fish"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tessa is a full-time nurse who works three 12-hour shifts each week.",
      "conflict_prompt": "Tessa is a full-time nurse who works no shifts and is currently unemployed.",
      "question": "What is Tessa's work schedule according to the statement?",
      "options": [
        "A. Three 12-hour shifts each week",
        "B. Monday to Friday 9-5",
        "C. No shifts, unemployed",
        "D. Part-time evenings only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael is a vegetarian restaurant owner who serves only plant-based dishes.",
      "conflict_prompt": "Rafael is a vegetarian restaurant owner who serves primarily meat-based dishes.",
      "question": "What type of cuisine does Rafael's restaurant serve according to the statement?",
      "options": [
        "A. Plant-based dishes only",
        "B. Primarily meat-based dishes",
        "C. Seafood-only cuisine",
        "D. Desserts only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lena is a resident doctor specializing in pediatrics at the children's hospital.",
      "conflict_prompt": "Lena is a resident doctor specializing in pediatrics who works only in geriatric care.",
      "question": "What is Lena's medical specialty according to the statement?",
      "options": [
        "A. Pediatrics",
        "B. Geriatrics",
        "C. Cardiology",
        "D. Neurology"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samir commutes by subway and does not own a personal vehicle.",
      "conflict_prompt": "Samir commutes by subway but owns and uses a personal car every day.",
      "question": "How does Samir primarily commute according to the statement?",
      "options": [
        "A. Drives a personal car",
        "B. Commutes by subway",
        "C. Walks",
        "D. Bikes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Kira is a morning yoga instructor who teaches classes at 6 AM daily.",
      "conflict_prompt": "Kira is a morning yoga instructor who only teaches evening classes and never at 6 AM.",
      "question": "When does Kira teach yoga classes according to the statement?",
      "options": [
        "A. 6 AM daily",
        "B. Noon",
        "C. Evening only",
        "D. Weekends only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Daniela is a vegan activist who protested at the meatpacking plant.",
      "conflict_prompt": "Daniela is a vegan activist who campaigned for increased meat consumption.",
      "question": "What cause did Daniela protest according to the statement?",
      "options": [
        "A. Promoting meat consumption",
        "B. Environmental policy",
        "C. Vegan activism at a meatpacking plant",
        "D. Labor rights unrelated to food"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hugo is a certified lifeguard who saved a swimmer from drowning last summer.",
      "conflict_prompt": "Hugo is a certified lifeguard who was not at the pool last summer and did not save anyone.",
      "question": "What heroic act did Hugo perform last summer according to the statement?",
      "options": [
        "A. Saved a swimmer from drowning",
        "B. Performed CPR on a passerby",
        "C. Rescued a hiker",
        "D. Did not perform any rescue"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta is left-handed and uses left-handed scissors for crafts.",
      "conflict_prompt": "Marta is left-handed but always uses right-handed scissors exclusively.",
      "question": "Which scissors does Marta use for crafts according to the statement?",
      "options": [
        "A. Left-handed scissors",
        "B. Right-handed scissors",
        "C. Ambidextrous scissors",
        "D. She does not use scissors"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Eli is a vegetarian who declined the steak offered at the dinner party.",
      "conflict_prompt": "Eli is a vegetarian who accepted and ate the steak at the dinner party.",
      "question": "What did Eli do with the steak at the dinner party according to the statement?",
      "options": [
        "A. Declined it",
        "B. Ate only the salad",
        "C. Accepted and ate the steak",
        "D. Was not present"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia is a resident of Seattle and has lived there for ten years.",
      "conflict_prompt": "Nadia is a resident of Seattle who has lived in Tokyo for the last ten years.",
      "question": "How long has Nadia lived in Seattle according to the statement?",
      "options": [
        "A. Ten years",
        "B. Five years",
        "C. She lives in Tokyo",
        "D. One year"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pedro is a non-driver who does not possess a driver's license.",
      "conflict_prompt": "Pedro is a non-driver who holds a valid driver's license and drives daily.",
      "question": "Does Pedro have a driver's license according to the statement?",
      "options": [
        "A. Yes, valid license",
        "B. No, he does not possess one",
        "C. Suspended license",
        "D. Learner's permit"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Liam is the oldest sibling and cares for his younger brothers.",
      "conflict_prompt": "Liam is the oldest sibling who relies on his older sister for care.",
      "question": "What role does Liam have in his family according to the statement?",
      "options": [
        "A. Youngest sibling",
        "B. Oldest sibling",
        "C. Only child",
        "D. Middle child"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zoe is a morning commuter who usually boards the 7:05 AM train.",
      "conflict_prompt": "Zoe is a morning commuter who never takes morning trains and avoids the 7:05 AM train.",
      "question": "Which train does Zoe usually board according to the statement?",
      "options": [
        "A. 7:05 AM train",
        "B. 8:30 PM train",
        "C. Noon shuttle",
        "D. She drives instead"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Andre is a vegetarian who works at a vegan restaurant and never handles meat.",
      "conflict_prompt": "Andre is a vegetarian who works at a butcher shop and handles meat daily.",
      "question": "Where does Andre work according to the statement?",
      "options": [
        "A. Butcher shop",
        "B. Vegan restaurant",
        "C. Grocery produce section",
        "D. Bakery"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mina is a registered organ donor and indicated donation of corneas on her license.",
      "conflict_prompt": "Mina is a registered organ donor who explicitly declined cornea donation on her license.",
      "question": "Which organ donation did Mina consent to according to the statement?",
      "options": [
        "A. Corneas",
        "B. Heart",
        "C. Lungs",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar is a non-smoker and enforces a smoke-free home policy.",
      "conflict_prompt": "Omar is a non-smoker who allows smoking inside his home regularly.",
      "question": "What is Omar's home policy on smoking according to the statement?",
      "options": [
        "A. Allows smoking regularly",
        "B. Smoke-free policy",
        "C. Designated smoking room",
        "D. Unknown"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sandra is a vegetarian nutritionist who avoids recommending meat to clients.",
      "conflict_prompt": "Sandra is a vegetarian nutritionist who recommends high-meat diets to her clients.",
      "question": "What recommendation does Sandra give to clients according to the statement?",
      "options": [
        "A. High-meat diets",
        "B. Plant-based diets",
        "C. Low-fat diets only",
        "D. No recommendations"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tom is a full-time firefighter who is scheduled for shift work this week.",
      "conflict_prompt": "Tom is a full-time firefighter who is on unpaid leave and not scheduled this week.",
      "question": "What is Tom's work schedule this week according to the statement?",
      "options": [
        "A. Scheduled for shift work",
        "B. On unpaid leave",
        "C. Part-time shifts only",
        "D. Training only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leila is a college student majoring in chemistry at North College.",
      "conflict_prompt": "Leila is a college student majoring in chemistry who transferred and now studies literature at East College.",
      "question": "What is Leila's major according to the statement?",
      "options": [
        "A. Chemistry",
        "B. Literature",
        "C. Physics",
        "D. Mathematics"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Owen is a vegetarian who ordered a tofu dish at the restaurant.",
      "conflict_prompt": "Owen is a vegetarian who ordered a bacon cheeseburger at the restaurant.",
      "question": "What did Owen order at the restaurant according to the statement?",
      "options": [
        "A. Tofu dish",
        "B. Bacon cheeseburger",
        "C. Grilled salmon",
        "D. Pasta with meat sauce"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jules is a licensed commercial driver who operates a delivery truck for the company.",
      "conflict_prompt": "Jules is a licensed commercial driver who does not drive and only does warehouse work.",
      "question": "What is Jules's job role according to the statement?",
      "options": [
        "A. Warehouse worker",
        "B. Licensed commercial driver operating a delivery truck",
        "C. Office administrator",
        "D. Mechanic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Camila is a vegetarian who prepared vegetarian meals for the entire week.",
      "conflict_prompt": "Camila is a vegetarian who cooked meat-based meals for the entire week.",
      "question": "What type of meals did Camila prepare for the week according to the statement?",
      "options": [
        "A. Vegetarian meals",
        "B. Meat-based meals",
        "C. Raw food only",
        "D. No meals prepared"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hassan is a homeowner who installed a home office and works remotely from there.",
      "conflict_prompt": "Hassan is a homeowner who does not have a home office and never works remotely.",
      "question": "What home feature does Hassan use according to the statement?",
      "options": [
        "A. No home office",
        "B. Home office for remote work",
        "C. Separate rental unit",
        "D. Garage workshop"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivy volunteers as a docent at the museum and leads the tours every Sunday.",
      "conflict_prompt": "Ivy volunteers as a docent at the museum but never leads tours and avoids Sundays.",
      "question": "When does Ivy lead tours at the museum according to the statement?",
      "options": [
        "A. Every Saturday",
        "B. Every Sunday",
        "C. None, she avoids tours",
        "D. Occasionally on weekdays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pedro is a non-drinker who abstained from alcohol at the wedding.",
      "conflict_prompt": "Pedro is a non-drinker who drank multiple alcoholic beverages at the wedding.",
      "question": "What did Pedro do regarding alcohol at the wedding according to the statement?",
      "options": [
        "A. Drank multiple alcoholic beverages",
        "B. Abstained from alcohol",
        "C. Only had non-alcoholic beer",
        "D. Was not present"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rina is a certified scuba diver who logged dives in the Caribbean last winter.",
      "conflict_prompt": "Rina is a certified scuba diver who has never logged a dive and avoids the ocean.",
      "question": "What diving activity did Rina do last winter according to the statement?",
      "options": [
        "A. Logged dives in the Caribbean",
        "B. Avoided the ocean",
        "C. Snowboarding",
        "D. Scuba classes only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Khalid is a vegetarian who manages a plant-based cookbook blog.",
      "conflict_prompt": "Khalid is a vegetarian who runs a barbecue blog focused on grilled meats.",
      "question": "What type of blog does Khalid manage according to the statement?",
      "options": [
        "A. Barbecue and grilled meats blog",
        "B. Plant-based cookbook blog",
        "C. Travel photography blog",
        "D. Technology review blog"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta is a certified translator who translates documents from German to English professionally.",
      "conflict_prompt": "Marta is a certified translator who does not translate German and only works with Spanish texts.",
      "question": "Which language pair does Marta translate according to the statement?",
      "options": [
        "A. French to English",
        "B. German to English",
        "C. Spanish to Portuguese",
        "D. English to Italian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Oscar is an organ donor whose registry indicates donation of all organs.",
      "conflict_prompt": "Oscar is an organ donor whose registry indicates donation of no organs.",
      "question": "What does Oscar's organ donor registry indicate according to the statement?",
      "options": [
        "A. Donation of all organs",
        "B. Donation of only corneas",
        "C. Donation of only heart",
        "D. No organs donated"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia is a vegetarian who volunteers at the local vegan shelter.",
      "conflict_prompt": "Nadia is a vegetarian who volunteers at the local meat cooperative promoting meat products.",
      "question": "Where does Nadia volunteer according to the statement?",
      "options": [
        "A. Meat cooperative",
        "B. Local vegan shelter",
        "C. Homeless shelter",
        "D. Animal rescue clinic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ravi is a night-shift nurse who works from 11 PM to 7 AM at the hospital.",
      "conflict_prompt": "Ravi is a night-shift nurse who only works daytime hours and never nights.",
      "question": "What are Ravi's working hours according to the statement?",
      "options": [
        "A. 9 AM to 5 PM",
        "B. 11 PM to 7 AM",
        "C. 6 AM to 2 PM",
        "D. Rotating shifts"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aileen is a vegan who purchased only plant-based groceries this week.",
      "conflict_prompt": "Aileen is a vegan who bought meat and dairy products exclusively this week.",
      "question": "What kind of groceries did Aileen buy this week according to the statement?",
      "options": [
        "A. Meat and dairy exclusively",
        "B. Plant-based groceries only",
        "C. Snacks only",
        "D. Frozen meals only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah is an only child and had no siblings at family gatherings.",
      "conflict_prompt": "Noah is an only child who always attends family gatherings with multiple siblings.",
      "question": "What is Noah's sibling situation according to the statement?",
      "options": [
        "A. He has many siblings",
        "B. He is an only child",
        "C. He is a twin",
        "D. He is the middle child"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Celia is a trained paramedic who administered first aid at the accident scene.",
      "conflict_prompt": "Celia is a trained paramedic who did not provide any first aid at the accident scene.",
      "question": "What action did Celia take at the accident scene according to the statement?",
      "options": [
        "A. Administered first aid",
        "B. Called for help only",
        "C. Did nothing",
        "D. Documented the event"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ibrahim is a vegetarian and does not consume eggs.",
      "conflict_prompt": "Ibrahim is a vegetarian who eats eggs daily as part of his breakfast.",
      "question": "Does Ibrahim consume eggs according to the statement?",
      "options": [
        "A. Yes, daily",
        "B. Occasionally",
        "C. No, he does not consume eggs",
        "D. He eats only egg whites"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Vera is a full-time remote worker who works from home five days a week.",
      "conflict_prompt": "Vera is a full-time remote worker who works exclusively from the office and never from home.",
      "question": "Where does Vera work according to the statement?",
      "options": [
        "A. In the office only",
        "B. From home five days a week",
        "C. Hybrid schedule",
        "D. Freelance on the road"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rex is a vegetarian who runs a tofu production business.",
      "conflict_prompt": "Rex is a vegetarian who runs a steakhouse and specializes in meat dishes.",
      "question": "What business does Rex run according to the statement?",
      "options": [
        "A. Steakhouse specializing in meat",
        "B. Tofu production business",
        "C. Seafood restaurant",
        "D. Bakery"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ida is a licensed pilot who holds a current FAA certificate.",
      "conflict_prompt": "Ida is a licensed pilot whose FAA certificate is expired and invalid.",
      "question": "What is Ida's pilot certification status according to the statement?",
      "options": [
        "A. No certification",
        "B. Current FAA certificate",
        "C. Expired certificate",
        "D. Military-only certification"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Giselle is a vegetarian who avoids cheese and other dairy products.",
      "conflict_prompt": "Giselle is a vegetarian who includes cheese and dairy in every meal.",
      "question": "What dairy policy does Giselle follow according to the statement?",
      "options": [
        "A. Avoids cheese and dairy",
        "B. Includes cheese and dairy in every meal",
        "C. Uses only butter but no cheese",
        "D. Drinks milk only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael is an organ donor who agreed to donate his corneas upon death.",
      "conflict_prompt": "Rafael is an organ donor who specifically opted out of cornea donation.",
      "question": "What did Rafael agree to donate according to the statement?",
      "options": [
        "A. Heart only",
        "B. Corneas",
        "C. Kidneys only",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lena is an avid cyclist who participates in charity rides each spring.",
      "conflict_prompt": "Lena is an avid cyclist who avoids group rides and never participates in charity events.",
      "question": "What cycling activity does Lena partake in according to the statement?",
      "options": [
        "A. Avoids rides",
        "B. Participates in charity rides each spring",
        "C. Only commutes to work by bike",
        "D. Mountain biking exclusively"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Paulina is a vegetarian chef who refuses to use animal broth in her soups.",
      "conflict_prompt": "Paulina is a vegetarian chef who uses chicken and beef broth in all her soups.",
      "question": "What kind of broth does Paulina use in her soups according to the statement?",
      "options": [
        "A. Chicken and beef broth",
        "B. Vegetable or no animal broth",
        "C. Fish stock only",
        "D. No soups at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Kendrick is an only parent who raised his daughter by himself.",
      "conflict_prompt": "Kendrick is an only parent who was raised by both parents and did not raise a daughter himself.",
      "question": "What parental role does Kendrick have according to the statement?",
      "options": [
        "A. Raised by both parents",
        "B. Only parent who raised his daughter",
        "C. Foster parent",
        "D. Grandparent"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nell is a certified lifeguard who carries rescue equipment at all times while on duty.",
      "conflict_prompt": "Nell is a certified lifeguard who does not carry any rescue equipment while on duty.",
      "question": "What safety practice does Nell follow while on duty according to the statement?",
      "options": [
        "A. Carries rescue equipment at all times",
        "B. Carries no rescue equipment",
        "C. Uses only radios",
        "D. Delegates rescue responsibility"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zayn is a vegetarian who ordered a veggie burger at lunch.",
      "conflict_prompt": "Zayn is a vegetarian who ordered a double bacon burger at lunch.",
      "question": "What did Zayn order for lunch according to the statement?",
      "options": [
        "A. Veggie burger",
        "B. Double bacon burger",
        "C. Caesar salad with chicken",
        "D. Tuna sandwich"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya is a full-time student who attends morning lectures each weekday.",
      "conflict_prompt": "Maya is a full-time student who only takes evening online courses and never attends morning lectures.",
      "question": "When does Maya attend lectures according to the statement?",
      "options": [
        "A. Morning lectures each weekday",
        "B. Evening online courses only",
        "C. Weekend workshops only",
        "D. No lectures at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ruben is a vegetarian who prepares meat-free lunches for his coworkers.",
      "conflict_prompt": "Ruben is a vegetarian who regularly brings meat-heavy lunches for his coworkers.",
      "question": "What type of lunches does Ruben prepare for coworkers according to the statement?",
      "options": [
        "A. Meat-heavy lunches",
        "B. Meat-free lunches",
        "C. Seafood-only lunches",
        "D. No lunches prepared"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Faye is a resident of the coastal town and participates in beach cleanups monthly.",
      "conflict_prompt": "Faye is a resident of the coastal town who avoids the beach and never participates in cleanups.",
      "question": "What community activity does Faye participate in according to the statement?",
      "options": [
        "A. Avoids beach activities",
        "B. Participates in beach cleanups monthly",
        "C. Organizes inland park events",
        "D. Volunteer tutoring"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jordi is a vegetarian who does not keep any meat in his refrigerator.",
      "conflict_prompt": "Jordi is a vegetarian who always keeps cured meats and sausages in his refrigerator.",
      "question": "What does Jordi keep in his refrigerator according to the statement?",
      "options": [
        "A. Cured meats and sausages",
        "B. No meat at all",
        "C. Only dairy products",
        "D. Only beverages"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Clara is a full-time librarian who works at the central library every weekday.",
      "conflict_prompt": "Clara is a full-time librarian who left the library and now works in retail on weekdays.",
      "question": "Where does Clara work according to the statement?",
      "options": [
        "A. Central library every weekday",
        "B. Retail store on weekdays",
        "C. University archives",
        "D. From home"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan is a vegetarian who runs a community garden that grows vegetables, not meat.",
      "conflict_prompt": "Ethan is a vegetarian who runs a community farm raising livestock for meat production.",
      "question": "What does Ethan's community garden produce according to the statement?",
      "options": [
        "A. Livestock for meat",
        "B. Vegetables",
        "C. Ornamental flowers only",
        "D. Dairy products"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora is a licensed therapist who accepts new clients for weekly sessions.",
      "conflict_prompt": "Nora is a licensed therapist who is not accepting any new clients and has no weekly sessions available.",
      "question": "Is Nora accepting new clients according to the statement?",
      "options": [
        "A. Yes, accepts new clients",
        "B. No, not accepting new clients",
        "C. Only accepts referrals",
        "D. Accepts only emergency cases"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dante is a vegetarian who volunteers at a farm sanctuary for rescued animals.",
      "conflict_prompt": "Dante is a vegetarian who volunteers at a commercial slaughterhouse.",
      "question": "Where does Dante volunteer according to the statement?",
      "options": [
        "A. Commercial slaughterhouse",
        "B. Farm sanctuary for rescued animals",
        "C. Food bank",
        "D. Animal testing facility"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivy is a morning baker who starts baking at 3 AM for the bakery's opening.",
      "conflict_prompt": "Ivy is a morning baker who never bakes early and starts work only at noon.",
      "question": "When does Ivy start baking according to the statement?",
      "options": [
        "A. 3 AM",
        "B. Noon",
        "C. 6 PM",
        "D. Midnight"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ken is a vegetarian who celebrates Meatless Mondays and avoids meat every Monday.",
      "conflict_prompt": "Ken is a vegetarian who celebrates Meatless Mondays by eating a large steak on Mondays.",
      "question": "What does Ken do on Mondays according to the statement?",
      "options": [
        "A. Eats a large steak",
        "B. Avoids meat (Meatless Monday)",
        "C. Fast entirely",
        "D. Eats only seafood"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta is a volunteer EMT who assisted at the festival medical tent last weekend.",
      "conflict_prompt": "Marta is a volunteer EMT who did not attend the festival and provided no assistance.",
      "question": "What did Marta do at the festival last weekend according to the statement?",
      "options": [
        "A. Assisted at the medical tent",
        "B. Did not attend",
        "C. Performed as a musician",
        "D. Worked security"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ali is a vegetarian who subscribes to a plant-based meal service and receives weekly deliveries.",
      "conflict_prompt": "Ali is a vegetarian who subscribes to a meat subscription box and receives weekly meat deliveries.",
      "question": "What type of meal service does Ali subscribe to according to the statement?",
      "options": [
        "A. Meat subscription box",
        "B. Plant-based meal service",
        "C. Seafood delivery service",
        "D. No subscription"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya is a licensed dentist who practices general dentistry at the Midtown Clinic.",
      "conflict_prompt": "Maya is a licensed dentist who does not practice dentistry and works as a software engineer instead.",
      "question": "What is Maya's profession according to the statement?",
      "options": [
        "A. Software engineer",
        "B. Licensed dentist practicing at Midtown Clinic",
        "C. Dental hygienist",
        "D. Pharmacist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olivia is a vegetarian who does not eat meat.",
      "conflict_prompt": "Olivia is a vegetarian who eats steak regularly.",
      "question": "What is Olivia's diet?",
      "options": [
        "A. Vegan",
        "B. Pescatarian",
        "C. Vegetarian",
        "D. Carnivore"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus is a licensed pilot who flies commercial airplanes.",
      "conflict_prompt": "Marcus is a licensed pilot who has never completed pilot training.",
      "question": "What is Marcus qualified to do?",
      "options": [
        "A. Drive a train",
        "B. Fly commercial airplanes",
        "C. Perform heart surgery",
        "D. Teach kindergarten"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum's painting is an original work by Picasso from 1932.",
      "conflict_prompt": "The museum's painting is an original work by Picasso from 1932 but was painted in 2010.",
      "question": "When was the painting by Picasso created according to the statement?",
      "options": [
        "A. 1890",
        "B. 1932",
        "C. 2010",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dr. Chen is a cardiologist who performs heart surgeries.",
      "conflict_prompt": "Dr. Chen is a cardiologist who works exclusively as a criminal defense lawyer.",
      "question": "What is Dr. Chen's medical specialty?",
      "options": [
        "A. Neurology",
        "B. Cardiology",
        "C. Pediatrics",
        "D. Dermatology"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The cabin is located on the north shore of the lake.",
      "conflict_prompt": "The cabin is located on the north shore of the lake but sits on the south shore.",
      "question": "Where is the cabin located?",
      "options": [
        "A. East shore",
        "B. North shore",
        "C. West shore",
        "D. Downtown"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah is an only child who has no siblings.",
      "conflict_prompt": "Hannah is an only child who has three siblings.",
      "question": "How many siblings does Hannah have according to the statement?",
      "options": [
        "A. Three",
        "B. One",
        "C. None",
        "D. Ten"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The smartphone battery is non-removable and sealed inside the case.",
      "conflict_prompt": "The smartphone battery is non-removable but can be easily removed without tools.",
      "question": "What is true about the smartphone's battery?",
      "options": [
        "A. It is removable with a screwdriver",
        "B. It is non-removable and sealed",
        "C. It is solar-powered",
        "D. It is made of nickel-cadmium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carlos is a vegan who does not consume dairy products.",
      "conflict_prompt": "Carlos is a vegan who drinks milk every morning.",
      "question": "Which of these does Carlos avoid according to the statement?",
      "options": [
        "A. Legumes",
        "B. Dairy products",
        "C. Vegetables",
        "D. Grains"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The library closes at 8:00 PM on weekdays.",
      "conflict_prompt": "The library closes at 8:00 PM on weekdays but stays open 24 hours every weekday.",
      "question": "When does the library close on weekdays?",
      "options": [
        "A. 6:00 AM",
        "B. 8:00 PM",
        "C. Midnight",
        "D. It doesn't close"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora's apartment is on the tenth floor of the building.",
      "conflict_prompt": "Nora's apartment is on the tenth floor of the building but is located on the ground level.",
      "question": "On which floor is Nora's apartment located?",
      "options": [
        "A. Basement",
        "B. Ground level",
        "C. Tenth floor",
        "D. Roof"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The restaurant specializes in gluten-free cuisine and uses no wheat ingredients.",
      "conflict_prompt": "The restaurant specializes in gluten-free cuisine and serves wheat-based pasta daily.",
      "question": "What kind of cuisine does the restaurant specialize in?",
      "options": [
        "A. Gluten-free",
        "B. Traditional Italian with wheat",
        "C. Raw meat only",
        "D. Exclusive desserts"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jordan owns a hybrid car that runs on both electricity and gasoline.",
      "conflict_prompt": "Jordan owns a hybrid car that runs on both electricity and gasoline but has no gasoline tank.",
      "question": "What type of car does Jordan own?",
      "options": [
        "A. Electric-only car",
        "B. Diesel truck",
        "C. Hybrid car",
        "D. Horse-drawn carriage"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Emma is fluent in Spanish and speaks it at home.",
      "conflict_prompt": "Emma is fluent in Spanish but has never learned any Spanish words.",
      "question": "What language does Emma speak fluently?",
      "options": [
        "A. French",
        "B. German",
        "C. Spanish",
        "D. Mandarin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bridge can support vehicles up to 10 tons in weight.",
      "conflict_prompt": "The bridge can support vehicles up to 10 tons in weight but collapses under any weight.",
      "question": "What is the maximum vehicle weight the bridge can support?",
      "options": [
        "A. 1 ton",
        "B. 5 tons",
        "C. 10 tons",
        "D. 100 tons"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samuel is a certified scuba diver who dives to depths of 40 meters.",
      "conflict_prompt": "Samuel is a certified scuba diver who dives to depths of 40 meters but is certified only for surface swimming.",
      "question": "To what depth does Samuel dive according to the statement?",
      "options": [
        "A. 5 meters",
        "B. 20 meters",
        "C. 40 meters",
        "D. 100 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The conference starts on Monday at 9:00 AM.",
      "conflict_prompt": "The conference starts on Monday at 9:00 AM but actually begins on Sunday evening.",
      "question": "When does the conference begin according to the statement?",
      "options": [
        "A. Sunday evening",
        "B. Monday at 9:00 AM",
        "C. Tuesday at noon",
        "D. Saturday morning"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ava's passport expires in five years from today.",
      "conflict_prompt": "Ava's passport expires in five years from today but expired last month.",
      "question": "When does Ava's passport expire according to the statement?",
      "options": [
        "A. It expired last month",
        "B. In five years from today",
        "C. Tomorrow",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The theater is a historic Art Deco building built in 1929.",
      "conflict_prompt": "The theater is a historic Art Deco building built in 1929 but was constructed in 2019.",
      "question": "When was the theater originally built according to the statement?",
      "options": [
        "A. 1890",
        "B. 1929",
        "C. 2019",
        "D. 2050"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leo keeps his sailboat moored at Harbor D year-round.",
      "conflict_prompt": "Leo keeps his sailboat moored at Harbor D year-round but moves it daily to Harbor A.",
      "question": "Where is Leo's sailboat usually moored?",
      "options": [
        "A. Harbor A",
        "B. Harbor B",
        "C. Harbor C",
        "D. Harbor D"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The package requires a signature upon delivery.",
      "conflict_prompt": "The package requires a signature upon delivery but is automatically delivered without any signature.",
      "question": "What is required when the package is delivered?",
      "options": [
        "A. A signature",
        "B. Cash payment",
        "C. Return receipt",
        "D. A phone call"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya is legally married to Daniel and their marriage certificate is valid.",
      "conflict_prompt": "Maya is legally married to Daniel but they do not have a valid marriage certificate.",
      "question": "What is Maya's marital status according to the statement?",
      "options": [
        "A. Divorced",
        "B. Single",
        "C. Legally married to Daniel",
        "D. Widowed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The refrigerator is plugged in and currently powered on.",
      "conflict_prompt": "The refrigerator is plugged in and currently powered on but has no electricity supply.",
      "question": "What is the power state of the refrigerator according to the statement?",
      "options": [
        "A. Unplugged",
        "B. Powered on",
        "C. Battery-operated",
        "D. Solar-only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The chemistry lab contains only non-flammable solvents.",
      "conflict_prompt": "The chemistry lab contains only non-flammable solvents and stores large quantities of gasoline.",
      "question": "What type of solvents are in the chemistry lab according to the statement?",
      "options": [
        "A. Flammable solvents",
        "B. Non-flammable solvents",
        "C. Explosives",
        "D. Biological samples"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rachel is left-handed and writes with her left hand exclusively.",
      "conflict_prompt": "Rachel is left-handed and writes with her left hand exclusively but writes all notes with her right hand.",
      "question": "Which hand does Rachel use to write according to the statement?",
      "options": [
        "A. Right hand",
        "B. Both hands equally",
        "C. Left hand",
        "D. Neither hand"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The company headquarters is headquartered in Berlin, Germany.",
      "conflict_prompt": "The company headquarters is headquartered in Berlin, Germany but its main office is in Tokyo and not in Berlin.",
      "question": "Where is the company's headquarters located according to the statement?",
      "options": [
        "A. New York",
        "B. Tokyo",
        "C. Berlin",
        "D. Sydney"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The antique clock is an authentic 18th-century piece.",
      "conflict_prompt": "The antique clock is an authentic 18th-century piece but was manufactured last year.",
      "question": "From which century is the clock originally according to the statement?",
      "options": [
        "A. 16th century",
        "B. 17th century",
        "C. 18th century",
        "D. 21st century"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Oliver's dog is a purebred Labrador Retriever registered with the kennel club.",
      "conflict_prompt": "Oliver's dog is a purebred Labrador Retriever registered with the kennel club but is actually a mixed-breed stray.",
      "question": "What breed is Oliver's dog according to the statement?",
      "options": [
        "A. German Shepherd",
        "B. Purebred Labrador Retriever",
        "C. Cat",
        "D. Horse"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The software license is perpetual and never expires.",
      "conflict_prompt": "The software license is perpetual and never expires but it expires next month.",
      "question": "What type of software license is described?",
      "options": [
        "A. Time-limited subscription",
        "B. Perpetual license",
        "C. Trial license",
        "D. Open-source license"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The marathon route covers 26.2 miles through the city.",
      "conflict_prompt": "The marathon route covers 26.2 miles through the city but is only 5 kilometers long.",
      "question": "How long is the marathon route according to the statement?",
      "options": [
        "A. 5 kilometers",
        "B. 10 kilometers",
        "C. 26.2 miles",
        "D. 100 miles"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya is an organ donor and has registered her consent.",
      "conflict_prompt": "Priya is an organ donor and has registered her consent but has explicitly refused organ donation.",
      "question": "What is Priya's status regarding organ donation?",
      "options": [
        "A. Registered organ donor",
        "B. Refused organ donation",
        "C. Unknown",
        "D. Organ recipient only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The laptop's hard drive is encrypted and requires a password to access.",
      "conflict_prompt": "The laptop's hard drive is encrypted and requires a password to access but it is accessible without any password.",
      "question": "What is required to access the laptop's hard drive according to the statement?",
      "options": [
        "A. A fingerprint only",
        "B. A password due to encryption",
        "C. No authentication",
        "D. A physical key"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dylan is a high school teacher who teaches mathematics.",
      "conflict_prompt": "Dylan is a high school teacher who teaches mathematics but works full-time as a commercial pilot and never teaches.",
      "question": "What subject does Dylan teach?",
      "options": [
        "A. Biology",
        "B. Mathematics",
        "C. History",
        "D. Music"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The painting uses only natural pigments and no synthetic dyes.",
      "conflict_prompt": "The painting uses only natural pigments and no synthetic dyes but was created entirely with modern synthetic paints.",
      "question": "What kind of pigments does the painting use according to the statement?",
      "options": [
        "A. Synthetic dyes",
        "B. Oil-based synthetic paints",
        "C. Natural pigments",
        "D. Digital pixels"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Isabella's visa allows her to stay in the country for two years.",
      "conflict_prompt": "Isabella's visa allows her to stay in the country for two years but it was revoked yesterday making her deportable immediately.",
      "question": "How long does Isabella's visa permit her to stay according to the statement?",
      "options": [
        "A. 6 months",
        "B. 1 year",
        "C. 2 years",
        "D. Indefinitely"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The apartment is pet-friendly and allows dogs.",
      "conflict_prompt": "The apartment is pet-friendly and allows dogs but bans all pets including dogs.",
      "question": "What is the apartment's policy on dogs according to the statement?",
      "options": [
        "A. Dogs are not allowed",
        "B. Dogs are allowed",
        "C. Only cats are allowed",
        "D. No animals allowed except reptiles"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah is a vegetarian chef who never cooks meat.",
      "conflict_prompt": "Noah is a vegetarian chef who never cooks meat but specializes in steakhouse cuisine.",
      "question": "What type of food does Noah avoid cooking according to the statement?",
      "options": [
        "A. Desserts",
        "B. Meat",
        "C. Salads",
        "D. Pasta"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The telescope is calibrated for visible light observations only.",
      "conflict_prompt": "The telescope is calibrated for visible light observations only but is optimized exclusively for radio waves.",
      "question": "For which type of observations is the telescope calibrated according to the statement?",
      "options": [
        "A. Radio waves",
        "B. Infrared only",
        "C. Visible light",
        "D. Gamma rays"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The community center offers free Wi-Fi to all visitors.",
      "conflict_prompt": "The community center offers free Wi-Fi to all visitors but charges a daily fee for internet access.",
      "question": "What does the community center provide according to the statement?",
      "options": [
        "A. Paid internet only",
        "B. No internet access",
        "C. Free Wi-Fi to all visitors",
        "D. Internet available only to staff"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felix is a professional drummer who practices five hours daily.",
      "conflict_prompt": "Felix is a professional drummer who practices five hours daily but never touches a drum.",
      "question": "How much does Felix practice daily according to the statement?",
      "options": [
        "A. None",
        "B. One hour",
        "C. Five hours",
        "D. Twenty-four hours"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The battery pack is rated for 500 charge cycles before its capacity drops significantly.",
      "conflict_prompt": "The battery pack is rated for 500 charge cycles before its capacity drops significantly but loses capacity after a single charge.",
      "question": "After how many charge cycles is the battery pack expected to significantly drop in capacity according to the statement?",
      "options": [
        "A. 1 cycle",
        "B. 100 cycles",
        "C. 500 cycles",
        "D. 5000 cycles"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The office building uses a keycard access system for all employees.",
      "conflict_prompt": "The office building uses a keycard access system for all employees but has no locks and anyone can enter freely.",
      "question": "What security system does the office building use according to the statement?",
      "options": [
        "A. Keycard access",
        "B. No security",
        "C. Facial recognition only for visitors",
        "D. Guard dogs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aiden's flight is non-stop from New York to Los Angeles.",
      "conflict_prompt": "Aiden's flight is non-stop from New York to Los Angeles but includes multiple stops for boarding.",
      "question": "What type of flight does Aiden have according to the statement?",
      "options": [
        "A. Connecting flight with stops",
        "B. Non-stop flight",
        "C. Cargo flight only",
        "D. Private helicopter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school is coeducational and enrolls both boys and girls.",
      "conflict_prompt": "The school is coeducational and enrolls both boys and girls but admits only boys.",
      "question": "What is the gender admission policy of the school according to the statement?",
      "options": [
        "A. Boys only",
        "B. Girls only",
        "C. Coeducational (both boys and girls)",
        "D. No students admitted"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The sculpture is made entirely of bronze with no other metals mixed in.",
      "conflict_prompt": "The sculpture is made entirely of bronze with no other metals mixed in but is actually carved from wood.",
      "question": "What is the sculpture made of according to the statement?",
      "options": [
        "A. Plastic",
        "B. Wood",
        "C. Bronze",
        "D. Glass"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan holds a PhD in physics and teaches at the university.",
      "conflict_prompt": "Ethan holds a PhD in physics and teaches at the university but never attended any university.",
      "question": "What degree does Ethan have according to the statement?",
      "options": [
        "A. MD",
        "B. Bachelor's degree",
        "C. PhD in physics",
        "D. High school diploma"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The map indicates that the red area is a protected wildlife reserve.",
      "conflict_prompt": "The map indicates that the red area is a protected wildlife reserve but also shows it as an industrial zone.",
      "question": "What does the map label the red area as according to the statement?",
      "options": [
        "A. Industrial zone",
        "B. Residential area",
        "C. Protected wildlife reserve",
        "D. Shopping district"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia's watch is waterproof up to 50 meters underwater.",
      "conflict_prompt": "Sofia's watch is waterproof up to 50 meters underwater but cannot get wet at all.",
      "question": "How deep can Sofia's watch be submerged according to the statement?",
      "options": [
        "A. It cannot get wet",
        "B. 5 meters",
        "C. 50 meters",
        "D. 500 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The concert venue has a capacity of 5,000 attendees.",
      "conflict_prompt": "The concert venue has a capacity of 5,000 attendees but cannot accommodate more than 50 people.",
      "question": "What is the stated capacity of the concert venue?",
      "options": [
        "A. 50 attendees",
        "B. 500 attendees",
        "C. 5,000 attendees",
        "D. 50,000 attendees"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lucas is a city council member who represents District 7.",
      "conflict_prompt": "Lucas is a city council member who represents District 7 but represents District 2 instead.",
      "question": "Which district does Lucas represent according to the statement?",
      "options": [
        "A. District 2",
        "B. District 7",
        "C. District 10",
        "D. He is not a council member"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The tablet runs the latest operating system version released last month.",
      "conflict_prompt": "The tablet runs the latest operating system version released last month but is permanently stuck on the original factory firmware.",
      "question": "What software does the tablet run according to the statement?",
      "options": [
        "A. Original factory firmware only",
        "B. An outdated system from five years ago",
        "C. The latest operating system released last month",
        "D. A custom embedded OS for refrigerators"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The concert ticket grants access to the front row seating.",
      "conflict_prompt": "The concert ticket grants access to the front row seating but only to the balcony.",
      "question": "What area does the concert ticket grant access to according to the statement?",
      "options": [
        "A. Back row only",
        "B. Balcony",
        "C. Front row seating",
        "D. Outside the venue"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mason's bicycle has a functioning front brake and rear brake.",
      "conflict_prompt": "Mason's bicycle has a functioning front brake and rear brake but both brakes are broken.",
      "question": "Which brakes are functioning on Mason's bicycle according to the statement?",
      "options": [
        "A. Neither brake",
        "B. Only front brake",
        "C. Both front and rear brakes",
        "D. Only rear brake"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The document is marked confidential and should not be shared publicly.",
      "conflict_prompt": "The document is marked confidential and should not be shared publicly but is posted openly on the company website.",
      "question": "How is the document labeled according to the statement?",
      "options": [
        "A. Public",
        "B. For internal use only",
        "C. Confidential",
        "D. Classified top secret"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amir is an only registered driver on the car's insurance policy.",
      "conflict_prompt": "Amir is an only registered driver on the car's insurance policy but the policy lists three other drivers as primary.",
      "question": "Who is the sole registered driver on the car's insurance according to the statement?",
      "options": [
        "A. Amir",
        "B. Three other drivers",
        "C. No one",
        "D. The car is uninsured"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery uses no artificial preservatives in its bread.",
      "conflict_prompt": "The bakery uses no artificial preservatives in its bread but adds synthetic preservatives to every loaf.",
      "question": "What does the bakery avoid using in its bread according to the statement?",
      "options": [
        "A. Natural yeast",
        "B. Artificial preservatives",
        "C. Flour",
        "D. Salt"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The spacecraft is scheduled to launch in March of next year.",
      "conflict_prompt": "The spacecraft is scheduled to launch in March of next year but already launched last year.",
      "question": "When is the spacecraft scheduled to launch according to the statement?",
      "options": [
        "A. Last year",
        "B. This month",
        "C. March of next year",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nina's car is registered in California and displays California plates.",
      "conflict_prompt": "Nina's car is registered in California and displays California plates but is legally registered in another country only.",
      "question": "Where is Nina's car registered according to the statement?",
      "options": [
        "A. Texas",
        "B. Canada",
        "C. California",
        "D. Florida"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The warranty covers defects for three years from the date of purchase.",
      "conflict_prompt": "The warranty covers defects for three years from the date of purchase but offers no coverage at all.",
      "question": "How long does the warranty cover defects according to the statement?",
      "options": [
        "A. No coverage",
        "B. One year",
        "C. Three years",
        "D. Lifetime"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The swimming pool is heated and maintains a temperature of 28°C year-round.",
      "conflict_prompt": "The swimming pool is heated and maintains a temperature of 28°C year-round but is frozen solid throughout the year.",
      "question": "What temperature does the swimming pool maintain according to the statement?",
      "options": [
        "A. Frozen solid year-round",
        "B. 15°C",
        "C. 28°C",
        "D. 100°C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Grace is a dual citizen of Canada and the United Kingdom.",
      "conflict_prompt": "Grace is a dual citizen of Canada and the United Kingdom but holds citizenship in no country.",
      "question": "Which citizenships does Grace have according to the statement?",
      "options": [
        "A. Only Canadian",
        "B. Only British",
        "C. Canadian and United Kingdom (dual citizen)",
        "D. Stateless"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The old oak tree in the park is alive and still producing leaves each spring.",
      "conflict_prompt": "The old oak tree in the park is alive and still producing leaves each spring but is completely dead and leafless.",
      "question": "What is the condition of the oak tree according to the statement?",
      "options": [
        "A. Dead and leafless",
        "B. Alive and producing leaves each spring",
        "C. Always evergreen",
        "D. A plastic replica"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The laboratory mouse strain is genetically modified to lack the gene X.",
      "conflict_prompt": "The laboratory mouse strain is genetically modified to lack the gene X but carries multiple functional copies of gene X.",
      "question": "What genetic modification does the mouse strain have according to the statement?",
      "options": [
        "A. It lacks gene X",
        "B. It has extra copies of gene X",
        "C. It is wild-type with no modifications",
        "D. It is a plant, not a mouse"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The festival is alcohol-free and prohibits alcoholic beverages on the premises.",
      "conflict_prompt": "The festival is alcohol-free and prohibits alcoholic beverages on the premises but serves beer at every stall.",
      "question": "What is the festival's policy on alcohol according to the statement?",
      "options": [
        "A. Alcohol permitted everywhere",
        "B. Alcohol-free",
        "C. Alcohol only in VIP area",
        "D. Only wine permitted"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Eleanor works full-time from the office five days a week.",
      "conflict_prompt": "Eleanor works full-time from the office five days a week but works remotely from home every day.",
      "question": "Where does Eleanor work according to the statement?",
      "options": [
        "A. From a coffee shop",
        "B. From the office five days a week",
        "C. Exclusively remotely",
        "D. In a factory"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The exhibit features only original artifacts with no replicas on display.",
      "conflict_prompt": "The exhibit features only original artifacts with no replicas on display but consists entirely of replicas.",
      "question": "What type of items does the exhibit display according to the statement?",
      "options": [
        "A. Replicas only",
        "B. Original artifacts only",
        "C. Digital projections only",
        "D. Empty cases"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ben is allergic to peanuts and must avoid all peanut products.",
      "conflict_prompt": "Ben is allergic to peanuts and must avoid all peanut products but eats peanut butter daily without reaction.",
      "question": "What allergy does Ben have according to the statement?",
      "options": [
        "A. Shellfish allergy",
        "B. Peanut allergy",
        "C. Pollen allergy",
        "D. No allergies"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vintage wine is from the 1976 harvest and bottled that year.",
      "conflict_prompt": "The vintage wine is from the 1976 harvest and bottled that year but was produced in 2020 from new grapes.",
      "question": "From which year is the wine's harvest according to the statement?",
      "options": [
        "A. 2020",
        "B. 1976",
        "C. 1800",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector is right-handed and uses his right hand for all manual tasks.",
      "conflict_prompt": "Hector is right-handed and uses his right hand for all manual tasks but is ambidextrous and always uses his left hand instead.",
      "question": "Which hand does Hector primarily use according to the statement?",
      "options": [
        "A. Left hand",
        "B. Right hand",
        "C. Both hands equally",
        "D. Neither hand"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The seminar is intended for beginners and requires no prior experience.",
      "conflict_prompt": "The seminar is intended for beginners and requires no prior experience but mandates advanced-level prerequisites.",
      "question": "Who is the seminar designed for according to the statement?",
      "options": [
        "A. Experts only",
        "B. Those with advanced prerequisites",
        "C. Beginners with no prior experience",
        "D. Children under five"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The smartwatch is water-resistant up to 50 meters but not suitable for diving.",
      "conflict_prompt": "The smartwatch is water-resistant up to 50 meters but is not water-resistant at all.",
      "question": "To what depth is the smartwatch water-resistant according to the statement?",
      "options": [
        "A. Not water-resistant",
        "B. 5 meters",
        "C. 50 meters",
        "D. 500 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The nursery sells only organic, pesticide-free seedlings.",
      "conflict_prompt": "The nursery sells only organic, pesticide-free seedlings but treats every seedling with synthetic pesticides.",
      "question": "What kind of seedlings does the nursery sell according to the statement?",
      "options": [
        "A. Pesticide-treated seedlings",
        "B. Organic, pesticide-free seedlings",
        "C. Artificial plastic plants",
        "D. Factory-grown GMO trees"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The historical site is preserved and not open to commercial development.",
      "conflict_prompt": "The historical site is preserved and not open to commercial development but has been converted into a shopping mall.",
      "question": "What is the protection status of the historical site according to the statement?",
      "options": [
        "A. Converted to a shopping mall",
        "B. Preserved and not open to commercial development",
        "C. Demolished",
        "D. Unmarked public land"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The emergency exit is clearly marked and kept unobstructed at all times.",
      "conflict_prompt": "The emergency exit is clearly marked and kept unobstructed at all times but is permanently blocked by storage boxes.",
      "question": "What is true about the emergency exit according to the statement?",
      "options": [
        "A. It is blocked",
        "B. It is unmarked",
        "C. It is clearly marked and unobstructed",
        "D. It leads to a wall"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olive oil used in the restaurant is extra virgin with no additives.",
      "conflict_prompt": "Olive oil used in the restaurant is extra virgin with no additives but is actually mixed with industrial seed oils.",
      "question": "What type of olive oil does the restaurant use according to the statement?",
      "options": [
        "A. Light olive oil",
        "B. Extra virgin olive oil with no additives",
        "C. Pureed olives",
        "D. Margarine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The daycare center operates from 7:00 AM to 6:00 PM on weekdays.",
      "conflict_prompt": "The daycare center operates from 7:00 AM to 6:00 PM on weekdays but is open only overnight.",
      "question": "What are the operating hours of the daycare center according to the statement?",
      "options": [
        "A. 24 hours",
        "B. 7:00 AM to 6:00 PM on weekdays",
        "C. Weekends only",
        "D. Overnight only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amelia is a licensed attorney admitted to the bar in New York.",
      "conflict_prompt": "Amelia is a licensed attorney admitted to the bar in New York but is not permitted to practice law anywhere.",
      "question": "Where is Amelia admitted to practice law according to the statement?",
      "options": [
        "A. California",
        "B. New York",
        "C. Canada",
        "D. Nowhere"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The campground permits campfires only in designated fire rings.",
      "conflict_prompt": "The campground permits campfires only in designated fire rings but allows open fires anywhere.",
      "question": "Where are campfires allowed at the campground according to the statement?",
      "options": [
        "A. Anywhere in the campground",
        "B. Only in designated fire rings",
        "C. Not allowed at all",
        "D. Only in the parking lot"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The ferry operates between the islands every hour during daytime.",
      "conflict_prompt": "The ferry operates between the islands every hour during daytime but never runs during the day.",
      "question": "How often does the ferry operate during daytime according to the statement?",
      "options": [
        "A. Every 10 minutes",
        "B. Every hour",
        "C. Once a week",
        "D. It does not operate during daytime"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain trail is rated as moderate difficulty and suitable for hikers with basic experience.",
      "conflict_prompt": "The mountain trail is rated as moderate difficulty and suitable for hikers with basic experience but is actually extremely technical and requires professional climbing gear.",
      "question": "How difficult is the mountain trail according to the statement?",
      "options": [
        "A. Easy for beginners",
        "B. Moderate difficulty",
        "C. Extremely technical requiring professional gear",
        "D. Impossible to hike"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The toolbox contains only metric wrenches with no imperial sizes.",
      "conflict_prompt": "The toolbox contains only metric wrenches with no imperial sizes but is filled exclusively with imperial wrenches.",
      "question": "What type of wrenches does the toolbox contain according to the statement?",
      "options": [
        "A. Imperial wrenches",
        "B. Metric wrenches",
        "C. Plastic toy wrenches",
        "D. No wrenches at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The animal shelter houses only dogs and has no cats on the premises.",
      "conflict_prompt": "The animal shelter houses only dogs and has no cats on the premises but is exclusively a cat shelter.",
      "question": "Which animals does the shelter house according to the statement?",
      "options": [
        "A. Cats only",
        "B. Birds only",
        "C. Dogs only",
        "D. Reptiles only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The professional certification is valid for four years before renewal is required.",
      "conflict_prompt": "The professional certification is valid for four years before renewal is required but is permanently invalid immediately.",
      "question": "For how long is the certification valid according to the statement?",
      "options": [
        "A. Immediately invalid",
        "B. One year",
        "C. Four years",
        "D. Lifetime"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The electric kettle shuts off automatically when the water reaches boiling point.",
      "conflict_prompt": "The electric kettle shuts off automatically when the water reaches boiling point but never turns off and continues heating indefinitely.",
      "question": "What safety feature does the electric kettle have according to the statement?",
      "options": [
        "A. No automatic shutoff",
        "B. Shuts off at a preset temperature below boiling",
        "C. Shuts off automatically at boiling point",
        "D. Requires manual unplugging every minute"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The laboratory freezer maintains a temperature of -80°C for sample storage.",
      "conflict_prompt": "The laboratory freezer maintains a temperature of -80°C for sample storage but stores samples at room temperature.",
      "question": "What temperature does the laboratory freezer maintain according to the statement?",
      "options": [
        "A. Room temperature",
        "B. -20°C",
        "C. -80°C",
        "D. 0°C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bridge is pedestrian-only with no vehicle traffic allowed.",
      "conflict_prompt": "The bridge is pedestrian-only with no vehicle traffic allowed but is used exclusively for heavy truck traffic.",
      "question": "Who is permitted to use the bridge according to the statement?",
      "options": [
        "A. Heavy trucks only",
        "B. Trains only",
        "C. Pedestrians only",
        "D. Aircraft"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's signature loaf contains only whole grains and no white flour.",
      "conflict_prompt": "The bakery's signature loaf contains only whole grains and no white flour but is made entirely from bleached white flour.",
      "question": "What flour type does the bakery's signature loaf use according to the statement?",
      "options": [
        "A. Bleached white flour",
        "B. No flour at all",
        "C. Whole grains only",
        "D. Cornmeal"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The apartment lease prohibits subletting to other tenants.",
      "conflict_prompt": "The apartment lease prohibits subletting to other tenants but explicitly allows unlimited subletting.",
      "question": "What does the apartment lease say about subletting according to the statement?",
      "options": [
        "A. Subletting is allowed without limit",
        "B. Subletting is prohibited",
        "C. Subletting is required",
        "D. Subletting is only for pets"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The lake is classified as non-swimmable due to contamination.",
      "conflict_prompt": "The lake is classified as non-swimmable due to contamination but is completely safe and regularly used for swimming competitions.",
      "question": "How is the lake classified according to the statement?",
      "options": [
        "A. Safe for swimming",
        "B. Non-swimmable due to contamination",
        "C. A saltwater ocean",
        "D. A parking lot"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Harper's bicycle helmet meets the current safety standards and is certified.",
      "conflict_prompt": "Harper's bicycle helmet meets the current safety standards and is certified but is uncertified and unsafe.",
      "question": "What is true about Harper's helmet according to the statement?",
      "options": [
        "A. It is uncertified and unsafe",
        "B. It meets current safety standards and is certified",
        "C. It is decorative only",
        "D. It is made of glass"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The camera lens is weather-sealed and suitable for outdoor use in rain.",
      "conflict_prompt": "The camera lens is weather-sealed and suitable for outdoor use in rain but must never get wet.",
      "question": "What feature does the camera lens have according to the statement?",
      "options": [
        "A. It is not weather-sealed",
        "B. It is weather-sealed and suitable for rain",
        "C. It is only for studio use",
        "D. It is disposable"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The research paper has been peer-reviewed and accepted by the journal.",
      "conflict_prompt": "The research paper has been peer-reviewed and accepted by the journal but was rejected without review.",
      "question": "What is the publication status of the research paper according to the statement?",
      "options": [
        "A. Rejected without review",
        "B. Under review",
        "C. Peer-reviewed and accepted",
        "D. Withdrawn by the authors"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The emergency generator provides backup power for up to 72 hours.",
      "conflict_prompt": "The emergency generator provides backup power for up to 72 hours but stops working immediately during a power outage.",
      "question": "How long does the emergency generator provide backup power according to the statement?",
      "options": [
        "A. Immediately fails at outage",
        "B. 24 hours",
        "C. 72 hours",
        "D. One week"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The official language of the event is English and all announcements will be in English.",
      "conflict_prompt": "The official language of the event is English and all announcements will be in English but no English will be spoken at all.",
      "question": "What is the official language of the event according to the statement?",
      "options": [
        "A. Spanish",
        "B. French",
        "C. English",
        "D. Sign language only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vintage car runs on leaded gasoline as designed in the 1960s.",
      "conflict_prompt": "The vintage car runs on leaded gasoline as designed in the 1960s but uses unleaded fuel exclusively and cannot accept leaded gasoline.",
      "question": "What type of fuel is the vintage car designed to use according to the statement?",
      "options": [
        "A. Diesel",
        "B. Unleaded gasoline only",
        "C. Leaded gasoline as designed in the 1960s",
        "D. Electricity"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The swimming classes are for adults only and not open to children.",
      "conflict_prompt": "The swimming classes are for adults only and not open to children but accept only toddlers under five.",
      "question": "Who are the swimming classes intended for according to the statement?",
      "options": [
        "A. Toddlers under five",
        "B. Adults only",
        "C. Children only",
        "D. Everyone including infants"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The art gallery displays only contemporary art created after 2000.",
      "conflict_prompt": "The art gallery displays only contemporary art created after 2000 but showcases only Renaissance paintings.",
      "question": "What type of art does the gallery display according to the statement?",
      "options": [
        "A. Renaissance paintings",
        "B. Contemporary art created after 2000",
        "C. Ancient artifacts",
        "D. Photography from the 1800s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The factory's emissions are within legal limits and monitored continuously.",
      "conflict_prompt": "The factory's emissions are within legal limits and monitored continuously but exceed legal limits constantly.",
      "question": "What is stated about the factory's emissions according to the statement?",
      "options": [
        "A. They constantly exceed legal limits",
        "B. They are unmonitored",
        "C. They are within legal limits and monitored",
        "D. The factory has no emissions"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The patient tested negative for influenza after the lab analysis.",
      "conflict_prompt": "The patient tested negative for influenza after the lab analysis but tested positive on the same test.",
      "question": "What was the influenza test result for the patient according to the statement?",
      "options": [
        "A. Positive",
        "B. Negative",
        "C. Inconclusive",
        "D. Not tested"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The campsite is accessible by car with a paved road leading directly to it.",
      "conflict_prompt": "The campsite is accessible by car with a paved road leading directly to it but is reachable only by helicopter.",
      "question": "How can the campsite be accessed according to the statement?",
      "options": [
        "A. Only by helicopter",
        "B. By paved road directly by car",
        "C. Only by hiking for several days",
        "D. By boat only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum accepts donations of historical artifacts but not modern reproductions.",
      "conflict_prompt": "The museum accepts donations of historical artifacts but not modern reproductions and only seeks to collect reproductions.",
      "question": "What kind of donations does the museum accept according to the statement?",
      "options": [
        "A. Modern reproductions only",
        "B. Historical artifacts",
        "C. Electronic waste",
        "D. Living animals"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The rooftop garden is open to residents daily from dawn to dusk.",
      "conflict_prompt": "The rooftop garden is open to residents daily from dawn to dusk but is permanently closed and inaccessible.",
      "question": "When is the rooftop garden open according to the statement?",
      "options": [
        "A. Permanently closed",
        "B. Dawn to dusk daily",
        "C. Only on weekends",
        "D. Nighttime only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The garment is labeled as 100% cotton with no synthetic fibers.",
      "conflict_prompt": "The garment is labeled as 100% cotton with no synthetic fibers but is made entirely from polyester.",
      "question": "What is the fabric composition according to the statement?",
      "options": [
        "A. 100% polyester",
        "B. 50% cotton 50% polyester",
        "C. 100% cotton",
        "D. Silk"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The sail is made from lightweight nylon suitable for light winds.",
      "conflict_prompt": "The sail is made from lightweight nylon suitable for light winds but is constructed from heavy steel plates.",
      "question": "What material is the sail made of according to the statement?",
      "options": [
        "A. Steel plates",
        "B. Carbon fiber",
        "C. Lightweight nylon",
        "D. Wood"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The zoo's nocturnal exhibit features animals that are active at night.",
      "conflict_prompt": "The zoo's nocturnal exhibit features animals that are active at night but contains only diurnal species active during the day.",
      "question": "What type of animals does the nocturnal exhibit feature according to the statement?",
      "options": [
        "A. Diurnal animals active during the day",
        "B. No animals at all",
        "C. Nocturnal animals active at night",
        "D. Marine fish"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The scholarship is merit-based and awarded solely on academic achievement.",
      "conflict_prompt": "The scholarship is merit-based and awarded solely on academic achievement but is given only based on random lottery.",
      "question": "On what basis is the scholarship awarded according to the statement?",
      "options": [
        "A. Random lottery",
        "B. Athletic ability",
        "C. Need-based financial status",
        "D. Merit and academic achievement"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The electric fence is live and carries an active charge for animal containment.",
      "conflict_prompt": "The electric fence is live and carries an active charge for animal containment but is completely de-energized and harmless.",
      "question": "What is true about the electric fence according to the statement?",
      "options": [
        "A. It is de-energized and harmless",
        "B. It is live and carries an active charge",
        "C. It is a decorative rope",
        "D. It is made of wood"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The antique book is signed by its author on the inside cover.",
      "conflict_prompt": "The antique book is signed by its author on the inside cover but contains no signature anywhere.",
      "question": "What special feature does the antique book have according to the statement?",
      "options": [
        "A. A missing cover",
        "B. An author's signature on the inside cover",
        "C. Burn marks",
        "D. Loose pages with no writing"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The solar panels generate enough electricity to power the building during peak sunlight hours.",
      "conflict_prompt": "The solar panels generate enough electricity to power the building during peak sunlight hours but produce zero power even in full sun.",
      "question": "What do the solar panels do during peak sunlight according to the statement?",
      "options": [
        "A. Produce zero power",
        "B. Generate enough electricity to power the building",
        "C. Overheat and melt",
        "D. Power only the coffee machine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The infant car seat is approved for children up to 13 kilograms.",
      "conflict_prompt": "The infant car seat is approved for children up to 13 kilograms but is certified only for adults over 60 kilograms.",
      "question": "For what maximum child weight is the infant car seat approved according to the statement?",
      "options": [
        "A. 5 kilograms",
        "B. 13 kilograms",
        "C. 30 kilograms",
        "D. 60 kilograms"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The city park is dog-friendly but requires dogs to be leashed at all times.",
      "conflict_prompt": "The city park is dog-friendly but requires dogs to be leashed at all times and allows unleashed dogs to roam freely.",
      "question": "What is required for dogs in the city park according to the statement?",
      "options": [
        "A. Dogs must be leashed at all times",
        "B. Dogs are banned",
        "C. Dogs must be muzzled permanently",
        "D. Dogs must be painted green"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The medication must be stored refrigerated between 2°C and 8°C.",
      "conflict_prompt": "The medication must be stored refrigerated between 2°C and 8°C but should be kept at room temperature above 25°C.",
      "question": "How must the medication be stored according to the statement?",
      "options": [
        "A. At room temperature above 25°C",
        "B. Refrigerated between 2°C and 8°C",
        "C. Frozen at -20°C",
        "D. In direct sunlight"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The violin is strung with gut strings, traditional for baroque performance.",
      "conflict_prompt": "The violin is strung with gut strings, traditional for baroque performance but is strung with modern steel strings only.",
      "question": "What type of strings does the violin have according to the statement?",
      "options": [
        "A. Steel strings only",
        "B. Synthetic core strings",
        "C. Gut strings",
        "D. No strings at all"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The historic bridge is listed on the national register of historic places.",
      "conflict_prompt": "The historic bridge is listed on the national register of historic places but is not recognized by any historic registry.",
      "question": "What is the bridge's recognition status according to the statement?",
      "options": [
        "A. Not recognized by any registry",
        "B. Privately owned with no status",
        "C. Listed on the national register of historic places",
        "D. Recently demolished"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The company's policy is to offer equal pay for equal work regardless of gender.",
      "conflict_prompt": "The company's policy is to offer equal pay for equal work regardless of gender but explicitly pays men more than women for the same role.",
      "question": "What does the company's pay policy state according to the statement?",
      "options": [
        "A. Pay men more than women",
        "B. Offer equal pay for equal work regardless of gender",
        "C. Pay is based solely on height",
        "D. No pay is provided"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The emergency services number for the town is 911.",
      "conflict_prompt": "The emergency services number for the town is 911 but there is no emergency number at all.",
      "question": "What is the emergency services number for the town according to the statement?",
      "options": [
        "A. 112",
        "B. 000",
        "C. 911",
        "D. There is no emergency number"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The play runs for two hours with one intermission in the middle.",
      "conflict_prompt": "The play runs for two hours with one intermission in the middle but has no intermission and lasts five minutes.",
      "question": "How long does the play run according to the statement?",
      "options": [
        "A. Five minutes with no intermission",
        "B. Two hours with one intermission",
        "C. Eight hours continuously",
        "D. It was canceled"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bridge toll is collected only in the eastbound direction.",
      "conflict_prompt": "The bridge toll is collected only in the eastbound direction but is actually collected in both directions equally.",
      "question": "In which direction is the bridge toll collected according to the statement?",
      "options": [
        "A. Westbound only",
        "B. Eastbound only",
        "C. Both directions",
        "D. Toll-free in all directions"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The concert is acoustic with no amplified instruments allowed.",
      "conflict_prompt": "The concert is acoustic with no amplified instruments allowed but features a full electric band with amplifiers.",
      "question": "What type of performance is the concert according to the statement?",
      "options": [
        "A. Electric with amplifiers",
        "B. Acoustic with no amplification",
        "C. A recorded soundtrack only",
        "D. Silent mime show"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The retailer sells only new items and does not deal in used goods.",
      "conflict_prompt": "The retailer sells only new items and does not deal in used goods but operates solely as a second-hand shop.",
      "question": "What does the retailer sell according to the statement?",
      "options": [
        "A. Used goods only",
        "B. New items only",
        "C. Both new and used equally",
        "D. Services only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The power outlet in the laboratory is isolated from the building's main grid for safety.",
      "conflict_prompt": "The power outlet in the laboratory is isolated from the building's main grid for safety but is directly connected to the main grid without isolation.",
      "question": "How is the laboratory power outlet configured according to the statement?",
      "options": [
        "A. Directly connected to main grid without isolation",
        "B. Isolated from the building's main grid",
        "C. Powered by batteries only",
        "D. Not connected to power at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The smartphone is carrier-unlocked and can be used with any network provider.",
      "conflict_prompt": "The smartphone is carrier-unlocked and can be used with any network provider but is permanently locked to a single carrier.",
      "question": "What is the phone's carrier status according to the statement?",
      "options": [
        "A. Locked to one carrier",
        "B. Carrier-unlocked for any provider",
        "C. No cellular capability",
        "D. Only works on satellite networks"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The greenhouse maintains a controlled humidity of 60% for the orchids.",
      "conflict_prompt": "The greenhouse maintains a controlled humidity of 60% for the orchids but keeps the humidity at 10% constantly.",
      "question": "What humidity level does the greenhouse maintain according to the statement?",
      "options": [
        "A. 10%",
        "B. 40%",
        "C. 60%",
        "D. 90%"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The policy requires employees to take mandatory annual cybersecurity training.",
      "conflict_prompt": "The policy requires employees to take mandatory annual cybersecurity training but forbids any cybersecurity education.",
      "question": "What does the policy require for employees according to the statement?",
      "options": [
        "A. No training allowed",
        "B. Mandatory annual cybersecurity training",
        "C. Training only for executives",
        "D. Training once every decade"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The kayak rental is single-person only and does not provide tandem kayaks.",
      "conflict_prompt": "The kayak rental is single-person only and does not provide tandem kayaks but offers only tandem kayaks.",
      "question": "What type of kayak does the rental provide according to the statement?",
      "options": [
        "A. Tandem kayaks only",
        "B. Single-person kayaks only",
        "C. Canoes only",
        "D. Motorboats only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vaccine requires two doses administered four weeks apart.",
      "conflict_prompt": "The vaccine requires two doses administered four weeks apart but is a single-dose vaccine given once.",
      "question": "How many doses does the vaccine require according to the statement?",
      "options": [
        "A. One dose",
        "B. Two doses four weeks apart",
        "C. Three doses over a year",
        "D. Daily doses for a month"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The wireless router supports dual-band frequencies: 2.4 GHz and 5 GHz.",
      "conflict_prompt": "The wireless router supports dual-band frequencies: 2.4 GHz and 5 GHz but only operates on a single 900 MHz band.",
      "question": "Which frequency bands does the router support according to the statement?",
      "options": [
        "A. 900 MHz only",
        "B. 2.4 GHz and 5 GHz",
        "C. 60 GHz only",
        "D. No wireless capability"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school lunch program provides vegetarian meal options every day.",
      "conflict_prompt": "The school lunch program provides vegetarian meal options every day but forbids serving vegetables entirely.",
      "question": "What does the school lunch program provide according to the statement?",
      "options": [
        "A. Vegetarian meal options daily",
        "B. No vegetarian options",
        "C. Only fast food",
        "D. Meals only on weekends"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The battery indicator shows full charge after two hours of charging.",
      "conflict_prompt": "The battery indicator shows full charge after two hours of charging but always displays empty regardless of charge.",
      "question": "When does the battery indicator show a full charge according to the statement?",
      "options": [
        "A. After two hours of charging",
        "B. Immediately when unplugged",
        "C. Never, it always shows empty",
        "D. After one year"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The local ordinance bans smoking in all indoor public places.",
      "conflict_prompt": "The local ordinance bans smoking in all indoor public places but allows smoking inside every indoor public place.",
      "question": "What does the local ordinance ban according to the statement?",
      "options": [
        "A. Smoking outdoors only",
        "B. Smoking in all indoor public places",
        "C. Smoking only in designated indoor rooms",
        "D. Smoking is mandatory indoors"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vintage radio receives only AM and FM frequencies, not digital broadcasts.",
      "conflict_prompt": "The vintage radio receives only AM and FM frequencies, not digital broadcasts but is a modern digital-only receiver.",
      "question": "Which broadcasts does the vintage radio receive according to the statement?",
      "options": [
        "A. Digital broadcasts only",
        "B. AM and FM frequencies",
        "C. Satellite TV only",
        "D. Internet streaming exclusively"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The community garden plots are allotted to residents and cannot be leased to businesses.",
      "conflict_prompt": "The community garden plots are allotted to residents and cannot be leased to businesses but are rented exclusively to corporations.",
      "question": "Who is eligible for community garden plots according to the statement?",
      "options": [
        "A. Corporations only",
        "B. Residents only",
        "C. Tourists only",
        "D. Government agencies only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The library's rare books collection is restricted and accessible by appointment only.",
      "conflict_prompt": "The library's rare books collection is restricted and accessible by appointment only but is freely available on open shelves to everyone.",
      "question": "How can one access the library's rare books collection according to the statement?",
      "options": [
        "A. Freely on open shelves",
        "B. By appointment only",
        "C. Not accessible at all",
        "D. Only to staff members"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The electrical panel is labeled and disconnected during maintenance work.",
      "conflict_prompt": "The electrical panel is labeled and disconnected during maintenance work but is left energized and unlabeled.",
      "question": "What safety procedure is followed for the electrical panel during maintenance according to the statement?",
      "options": [
        "A. Left energized and unlabeled",
        "B. Labeled and disconnected",
        "C. Removed completely",
        "D. Replaced with a dummy panel"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The stadium enforces a bag size limit: only small clutches are permitted.",
      "conflict_prompt": "The stadium enforces a bag size limit: only small clutches are permitted but allows large suitcases of any size.",
      "question": "What bag size is permitted at the stadium according to the statement?",
      "options": [
        "A. Large suitcases only",
        "B. No bags allowed",
        "C. Small clutches only",
        "D. Backpacks only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain cabin has no electricity and relies on a wood stove for heat.",
      "conflict_prompt": "The mountain cabin has no electricity and relies on a wood stove for heat but is fully electrified with central heating.",
      "question": "What does the mountain cabin rely on for heat according to the statement?",
      "options": [
        "A. Central electric heating",
        "B. Wood stove with no electricity",
        "C. Solar-powered air conditioning",
        "D. Geothermal pumps"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The patient is a non-smoker with no history of tobacco use.",
      "conflict_prompt": "The patient is a non-smoker with no history of tobacco use but has smoked two packs daily for decades.",
      "question": "What is the patient's smoking history according to the statement?",
      "options": [
        "A. Smokes two packs daily",
        "B. Former smoker",
        "C. Non-smoker with no history of tobacco use",
        "D. Uses smokeless tobacco only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The rental car is fueled and returns should be made with a full tank.",
      "conflict_prompt": "The rental car is fueled and returns should be made with a full tank but must be returned empty.",
      "question": "What is the expected fuel state when returning the rental car according to the statement?",
      "options": [
        "A. Empty tank",
        "B. Full tank",
        "C. Half tank",
        "D. Fueled with diesel only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The postage stamp required for this envelope is $1.20.",
      "conflict_prompt": "The postage stamp required for this envelope is $1.20 but no postage is needed at all.",
      "question": "How much postage is required for the envelope according to the statement?",
      "options": [
        "A. $0.00",
        "B. $0.55",
        "C. $1.20",
        "D. $10.00"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The conference room is equipped with a projector and HDMI connection.",
      "conflict_prompt": "The conference room is equipped with a projector and HDMI connection but has no audiovisual equipment whatsoever.",
      "question": "What audiovisual equipment does the conference room have according to the statement?",
      "options": [
        "A. No audiovisual equipment",
        "B. A projector and HDMI connection",
        "C. Only cassette players",
        "D. Holographic display only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The lab requires protective eyewear and lab coats to be worn at all times.",
      "conflict_prompt": "The lab requires protective eyewear and lab coats to be worn at all times but forbids any protective clothing.",
      "question": "What is required attire in the lab according to the statement?",
      "options": [
        "A. No protective clothing allowed",
        "B. Protective eyewear and lab coats at all times",
        "C. Formal wear only",
        "D. Swimwear"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's croissants are baked fresh daily every morning.",
      "conflict_prompt": "The bakery's croissants are baked fresh daily every morning but are never baked and always stale.",
      "question": "When are the croissants baked according to the statement?",
      "options": [
        "A. Weekly on Sundays",
        "B. Fresh daily every morning",
        "C. Never baked",
        "D. Only at midnight"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school's graduation ceremony is scheduled for June 15th at 10:00 AM.",
      "conflict_prompt": "The school's graduation ceremony is scheduled for June 15th at 10:00 AM but will take place on May 1st at 8:00 PM instead.",
      "question": "When is the school's graduation ceremony scheduled according to the statement?",
      "options": [
        "A. May 1st at 8:00 PM",
        "B. June 15th at 10:00 AM",
        "C. December 31st at midnight",
        "D. It was canceled"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The airline allows one carry-on bag and one personal item per passenger.",
      "conflict_prompt": "The airline allows one carry-on bag and one personal item per passenger but forbids any carry-on items entirely.",
      "question": "What does the airline allow per passenger according to the statement?",
      "options": [
        "A. No carry-on items",
        "B. One carry-on bag and one personal item",
        "C. Three checked bags only",
        "D. Pets as carry-ons only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The kindergarten class size is limited to 20 children maximum.",
      "conflict_prompt": "The kindergarten class size is limited to 20 children maximum but actually enrolls 100 children in each class.",
      "question": "What is the maximum class size for the kindergarten according to the statement?",
      "options": [
        "A. 100 children",
        "B. 50 children",
        "C. 20 children",
        "D. No limit"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The theater prohibits flash photography during performances.",
      "conflict_prompt": "The theater prohibits flash photography during performances but encourages attendees to use flash photography throughout.",
      "question": "What is the theater's policy on flash photography according to the statement?",
      "options": [
        "A. Flash photography encouraged",
        "B. Flash photography prohibited",
        "C. Flash photography required",
        "D. Flash photography only allowed in lobby"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum's currency exhibit accepts only cash payments at the entrance.",
      "conflict_prompt": "The museum's currency exhibit accepts only cash payments at the entrance but takes no payments at all.",
      "question": "How does the museum accept payment for the currency exhibit according to the statement?",
      "options": [
        "A. Credit card only",
        "B. Cash only at the entrance",
        "C. Online prepayment only",
        "D. Free admission always"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The playground's surface is made of soft rubber to reduce injury risk.",
      "conflict_prompt": "The playground's surface is made of soft rubber to reduce injury risk but is covered in sharp gravel.",
      "question": "What material is the playground surface made of according to the statement?",
      "options": [
        "A. Sharp gravel",
        "B. Soft rubber",
        "C. Concrete",
        "D. Ice"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The hiking trail is seasonally closed during winter months for safety.",
      "conflict_prompt": "The hiking trail is seasonally closed during winter months for safety but remains open and unmonitored year-round.",
      "question": "When is the hiking trail closed according to the statement?",
      "options": [
        "A. Open year-round",
        "B. Closed during winter months",
        "C. Only open in winter",
        "D. Closed permanently"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The building's fire alarm system is fully functional and tested monthly.",
      "conflict_prompt": "The building's fire alarm system is fully functional and tested monthly but has never been tested and is nonfunctional.",
      "question": "How often is the fire alarm system tested according to the statement?",
      "options": [
        "A. Never tested",
        "B. Tested monthly",
        "C. Tested once a decade",
        "D. Tested every minute"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum requires visitors to wear masks indoors.",
      "conflict_prompt": "The museum requires visitors to wear masks indoors but explicitly forbids masks inside the building.",
      "question": "What is the museum's mask policy according to the statement?",
      "options": [
        "A. Masks required indoors",
        "B. Masks forbidden indoors",
        "C. Masks optional only for staff",
        "D. Masks required only outdoors"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The rental agreement prohibits smoking anywhere on the property.",
      "conflict_prompt": "The rental agreement prohibits smoking anywhere on the property but designates smoking as mandatory.",
      "question": "What does the rental agreement say about smoking according to the statement?",
      "options": [
        "A. Smoking is mandatory",
        "B. Smoking is prohibited anywhere on the property",
        "C. Smoking allowed only in the basement",
        "D. Smoking allowed only with prior notice"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The local farmer's market accepts only locally grown produce within a 50-mile radius.",
      "conflict_prompt": "The local farmer's market accepts only locally grown produce within a 50-mile radius but sells only imported goods.",
      "question": "What produce does the farmer's market accept according to the statement?",
      "options": [
        "A. Imported goods only",
        "B. Locally grown produce within 50 miles",
        "C. Only packaged snacks",
        "D. Manufactured electronics"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The laboratory animals are housed in climate-controlled rooms at 22°C.",
      "conflict_prompt": "The laboratory animals are housed in climate-controlled rooms at 22°C but are kept outdoors with no temperature control.",
      "question": "What temperature is maintained for the laboratory animals according to the statement?",
      "options": [
        "A. Outdoors with no control",
        "B. 15°C",
        "C. 22°C in climate-controlled rooms",
        "D. 40°C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The passport office processes routine renewals within four weeks.",
      "conflict_prompt": "The passport office processes routine renewals within four weeks but takes several years to process renewals.",
      "question": "How long does the passport office take to process routine renewals according to the statement?",
      "options": [
        "A. Several years",
        "B. Four weeks",
        "C. One day",
        "D. Ten years"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The smartphone camera has optical image stabilization (OIS) built into the lens.",
      "conflict_prompt": "The smartphone camera has optical image stabilization (OIS) built into the lens but has no stabilization features at all.",
      "question": "What stabilization feature does the smartphone camera have according to the statement?",
      "options": [
        "A. No stabilization",
        "B. Electronic stabilization only",
        "C. Optical image stabilization (OIS)",
        "D. Mechanical tripod-only stabilization"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The playground equipment meets current safety codes and has rounded edges.",
      "conflict_prompt": "The playground equipment meets current safety codes and has rounded edges but contains sharp exposed metal and fails safety codes.",
      "question": "What is the safety status of the playground equipment according to the statement?",
      "options": [
        "A. Fails safety codes with exposed metal",
        "B. Meets current safety codes with rounded edges",
        "C. Contains toxic materials",
        "D. Is non-existent"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The password policy requires a minimum of 12 characters and complexity.",
      "conflict_prompt": "The password policy requires a minimum of 12 characters and complexity but allows single-character passwords with no restrictions.",
      "question": "What does the password policy require according to the statement?",
      "options": [
        "A. Single-character passwords",
        "B. Minimum of 12 characters with complexity",
        "C. No passwords required",
        "D. Passwords must be 'password'"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The freight shipment requires hazardous materials training for handlers.",
      "conflict_prompt": "The freight shipment requires hazardous materials training for handlers but contains only harmless items and requires no training.",
      "question": "What training is required for handlers of the freight shipment according to the statement?",
      "options": [
        "A. No training required",
        "B. Hazardous materials training",
        "C. Lifeguard certification",
        "D. Piano tuning certification"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The swimming pool is chlorine-treated and maintained daily.",
      "conflict_prompt": "The swimming pool is chlorine-treated and maintained daily but is never cleaned or treated.",
      "question": "How is the swimming pool maintained according to the statement?",
      "options": [
        "A. Never cleaned or treated",
        "B. Chlorine-treated and maintained daily",
        "C. Saltwater only with no maintenance",
        "D. Filled with soda"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum's policy allows photography without flash for personal use.",
      "conflict_prompt": "The museum's policy allows photography without flash for personal use but forbids all photography, including without flash.",
      "question": "What photography policy does the museum have according to the statement?",
      "options": [
        "A. No photography allowed at all",
        "B. Photography allowed without flash for personal use",
        "C. Flash photography mandatory",
        "D. Only professional photographers allowed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The car's airbags are functional and have not been deployed in any accidents.",
      "conflict_prompt": "The car's airbags are functional and have not been deployed in any accidents but were deployed and removed permanently.",
      "question": "What is the condition of the car's airbags according to the statement?",
      "options": [
        "A. Deployed and removed",
        "B. Non-functional",
        "C. Functional and not deployed",
        "D. Inflated permanently"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The faucet supplies potable water that is safe to drink.",
      "conflict_prompt": "The faucet supplies potable water that is safe to drink but is connected exclusively to contaminated industrial runoff.",
      "question": "What kind of water does the faucet supply according to the statement?",
      "options": [
        "A. Contaminated industrial runoff",
        "B. Saltwater",
        "C. Potable water safe to drink",
        "D. Lava"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The volunteer is certified in first aid and CPR.",
      "conflict_prompt": "The volunteer is certified in first aid and CPR but has no training in either and is uncertified.",
      "question": "What certifications does the volunteer have according to the statement?",
      "options": [
        "A. No certifications",
        "B. Certified in first aid and CPR",
        "C. Certified only in scuba diving",
        "D. Certified pilot license"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The printer is connected to the office network and available to all staff.",
      "conflict_prompt": "The printer is connected to the office network and available to all staff but is physically disconnected and inaccessible.",
      "question": "What is the printer's availability according to the statement?",
      "options": [
        "A. Available to all staff on the network",
        "B. Available only to IT staff offline",
        "C. Physically disconnected and inaccessible",
        "D. Only prints in invisible ink"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The summit will be held virtually with no in-person attendees.",
      "conflict_prompt": "The summit will be held virtually with no in-person attendees but requires mandatory in-person attendance for all speakers.",
      "question": "How will the summit be held according to the statement?",
      "options": [
        "A. In-person only",
        "B. Hybrid with in-person and virtual",
        "C. Virtually with no in-person attendees",
        "D. Canceled"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dr. Miller, a surgeon, has performed hundreds of operations during his career.",
      "conflict_prompt": "Dr. Miller, a surgeon, has never performed any surgeries.",
      "question": "What is Dr. Miller's medical specialty?",
      "options": [
        "A. Cardiologist",
        "B. Surgeon",
        "C. Neurologist",
        "D. General practitioner"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Laura, an identical twin, always celebrates birthdays with her twin sister.",
      "conflict_prompt": "Laura, an identical twin, is an only child with no siblings.",
      "question": "Which sibling status is described for Laura?",
      "options": [
        "A. Only child",
        "B. Identical twin",
        "C. Adopted child",
        "D. Triplet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carlos, a vegetarian, ordered a large mixed salad at dinner.",
      "conflict_prompt": "Carlos, a vegetarian, ordered a porterhouse steak for dinner.",
      "question": "What dietary preference is attributed to Carlos?",
      "options": [
        "A. Omnivore",
        "B. Vegan",
        "C. Vegetarian",
        "D. Pescatarian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum curator, an art historian, wrote the catalog notes for the Baroque exhibit.",
      "conflict_prompt": "The museum curator, an art historian, has no formal training in art history.",
      "question": "What is the curator's professional specialty?",
      "options": [
        "A. Botanist",
        "B. Art historian",
        "C. Chemist",
        "D. Architect"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sam, a left-handed pianist, practices scales with his dominant hand every day.",
      "conflict_prompt": "Sam, a left-handed pianist, refuses to use his left hand and plays only with his right.",
      "question": "What handedness is Sam described as having?",
      "options": [
        "A. Right-handed",
        "B. Ambidextrous",
        "C. Left-handed",
        "D. No hands"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ellen, a fluent Spanish speaker, translated the legal contract into Spanish.",
      "conflict_prompt": "Ellen, a fluent Spanish speaker, cannot understand simple Spanish phrases.",
      "question": "Which language is Ellen fluent in according to the statement?",
      "options": [
        "A. French",
        "B. Mandarin",
        "C. Spanish",
        "D. German"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mayor Thompson, a Democrat, advocated for expanded social programs during the campaign.",
      "conflict_prompt": "Mayor Thompson, a Democrat, is registered as a Republican according to voter records.",
      "question": "What political party is Mayor Thompson identified with in the statement?",
      "options": [
        "A. Republican",
        "B. Independent",
        "C. Democrat",
        "D. Libertarian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia, a vegan chef, prepared only plant-based dishes for the dinner service.",
      "conflict_prompt": "Nadia, a vegan chef, served bacon-wrapped scallops as the appetizer.",
      "question": "What type of cuisine does Nadia specialize in?",
      "options": [
        "A. Meat-centric cuisine",
        "B. Vegan cuisine",
        "C. Seafood-only cuisine",
        "D. Gluten-free baking"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vintage car is a 1965 Ford Mustang, restored to its original specifications.",
      "conflict_prompt": "The vintage car is a 1965 Ford Mustang but it was manufactured in 2020.",
      "question": "What model year is given for the car?",
      "options": [
        "A. 2020",
        "B. 1975",
        "C. 1965",
        "D. 1983"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Professor Li, a tenured faculty member, holds a permanent appointment at the university.",
      "conflict_prompt": "Professor Li, a tenured faculty member, is currently on a temporary adjunct contract that will expire this term.",
      "question": "What is Professor Li's employment status as described?",
      "options": [
        "A. Visiting scholar",
        "B. Tenured faculty",
        "C. Graduate assistant",
        "D. Part-time lecturer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus, a certified lifeguard, supervised the pool during the swim meet.",
      "conflict_prompt": "Marcus, a certified lifeguard, has no lifeguard certification and failed the training.",
      "question": "What qualification does Marcus have?",
      "options": [
        "A. Certified lifeguard",
        "B. Firefighter",
        "C. Personal trainer",
        "D. EMT"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olivia, an only child, grew up without siblings and often visited cousins.",
      "conflict_prompt": "Olivia, an only child, has three younger siblings who live nearby.",
      "question": "What sibling situation is stated for Olivia?",
      "options": [
        "A. Twin",
        "B. Only child",
        "C. Oldest of five",
        "D. Middle child"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Captain Hassan, a naval officer, commanded the ship during the deployment.",
      "conflict_prompt": "Captain Hassan, a naval officer, was demoted and had no command authority during the deployment.",
      "question": "What role does Hassan hold in the military context?",
      "options": [
        "A. Sergeant",
        "B. Naval officer",
        "C. Civilian contractor",
        "D. Merchant sailor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rita, a practicing Catholic, attended mass every Sunday with her family.",
      "conflict_prompt": "Rita, a practicing Catholic, openly declares she is atheist and never attends religious services.",
      "question": "What religion is Rita described as practicing?",
      "options": [
        "A. Judaism",
        "B. Islam",
        "C. Catholicism",
        "D. Hinduism"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The smartphone model is waterproof, rated IP68 for water resistance.",
      "conflict_prompt": "The smartphone model is waterproof but cannot be submerged in water without damage.",
      "question": "What water-resistance rating is claimed for the smartphone?",
      "options": [
        "A. IP20",
        "B. IP68",
        "C. No rating",
        "D. IP44"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ben, a recovered alcoholic, has been sober for five years and attends support meetings.",
      "conflict_prompt": "Ben, a recovered alcoholic, drinks alcohol every weekend and never goes to meetings.",
      "question": "What recovery status is Ben described as having?",
      "options": [
        "A. Active alcoholic",
        "B. Recovered alcoholic",
        "C. Casual drinker",
        "D. Teetotaler since birth"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The device is battery-powered, operating on two AA batteries.",
      "conflict_prompt": "The device is battery-powered but requires a hardwired AC connection and no batteries.",
      "question": "What power source is specified for the device?",
      "options": [
        "A. Solar",
        "B. Two AA batteries",
        "C. Lithium-ion internal battery",
        "D. Hardwired AC only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia, a professional marathon runner, trained six days a week for the event.",
      "conflict_prompt": "Sofia, a professional marathon runner, has a sedentary lifestyle and rarely exercises.",
      "question": "What athletic profession is Sofia associated with?",
      "options": [
        "A. Sprinter",
        "B. Marathon runner",
        "C. Swimmer",
        "D. Cyclist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The apartment tenant, a non-smoker, signed a lease prohibiting smoking on the premises.",
      "conflict_prompt": "The apartment tenant, a non-smoker, smokes indoors every night despite the lease.",
      "question": "What clause did the tenant's lease include about smoking?",
      "options": [
        "A. Smoking allowed in common areas",
        "B. Smoking prohibited on the premises",
        "C. Smoking permitted with landlord approval",
        "D. No mention of smoking"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jordan, a certified accountant, prepared the company's financial statements this quarter.",
      "conflict_prompt": "Jordan, a certified accountant, has no accounting credentials and misfiled the financial statements.",
      "question": "What professional qualification does Jordan hold according to the statement?",
      "options": [
        "A. Certified public accountant",
        "B. Civil engineer",
        "C. Registered nurse",
        "D. Software developer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, a hearing-impaired student, uses a cochlear implant and attends mainstream classes.",
      "conflict_prompt": "Maya, a hearing-impaired student, has perfect natural hearing with no need for assistive devices.",
      "question": "What disability is mentioned regarding Maya?",
      "options": [
        "A. Visual impairment",
        "B. Mobility impairment",
        "C. Hearing impairment",
        "D. Cognitive impairment"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The package is fragile and should be handled with care to avoid breakage.",
      "conflict_prompt": "The package is fragile but contains only unbreakable metal parts that cannot be damaged.",
      "question": "How should handlers treat the package?",
      "options": [
        "A. Handle with care because it is fragile",
        "B. Toss it freely; contents are unbreakable",
        "C. It must be refrigerated",
        "D. It requires a signature by a supervisor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Daniel, a licensed pilot, completed the transatlantic flight as the captain.",
      "conflict_prompt": "Daniel, a licensed pilot, has never completed any flight training and cannot fly planes.",
      "question": "What aviation qualification does Daniel have?",
      "options": [
        "A. Air traffic controller",
        "B. Licensed pilot",
        "C. Flight attendant",
        "D. Aircraft mechanic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah, a certified scuba diver, explored the coral reef down to 30 meters.",
      "conflict_prompt": "Hannah, a certified scuba diver, is afraid of water and has never learned to swim.",
      "question": "What recreational qualification does Hannah hold?",
      "options": [
        "A. Certified scuba diver",
        "B. Ski instructor",
        "C. Paragliding pilot",
        "D. Rock climbing guide"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bookstore owner, an avid reader, recommended a dozen novels to customers.",
      "conflict_prompt": "The bookstore owner, an avid reader, never reads books and dislikes literature.",
      "question": "How is the bookstore owner described in relation to reading?",
      "options": [
        "A. Avid reader",
        "B. Uninterested in books",
        "C. Collector of stamps",
        "D. Amateur chef"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leo, a bilingual teacher fluent in French and English, taught both languages at the school.",
      "conflict_prompt": "Leo, a bilingual teacher fluent in French and English, speaks only one word of English and cannot communicate in French either.",
      "question": "Which languages is Leo fluent in according to the statement?",
      "options": [
        "A. German and Spanish",
        "B. French and English",
        "C. Mandarin and Cantonese",
        "D. Arabic and Hebrew"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The rescue dog is a three-year-old Labrador that was adopted from the shelter.",
      "conflict_prompt": "The rescue dog is a three-year-old Labrador but was born and raised by its current owner from birth.",
      "question": "What is the origin of the Labrador described in the statement?",
      "options": [
        "A. Purchased from breeder",
        "B. Adopted from shelter",
        "C. Stray found last week",
        "D. Imported from another country"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Inspector Gomez, a forensic scientist, analyzed the evidence in the lab.",
      "conflict_prompt": "Inspector Gomez, a forensic scientist, has no scientific training and refuses to enter the lab.",
      "question": "What profession is Inspector Gomez associated with?",
      "options": [
        "A. Forensic scientist",
        "B. Traffic officer",
        "C. Parole officer",
        "D. School counselor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The building is earthquake-resistant and was designed to withstand major seismic events.",
      "conflict_prompt": "The building is earthquake-resistant but is known to collapse during even minor tremors.",
      "question": "What structural feature is claimed for the building?",
      "options": [
        "A. Designed for high winds only",
        "B. Earthquake-resistant",
        "C. Built on stilts for flooding",
        "D. Not built to code"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ava, a college graduate with a master's degree, completed her thesis last spring.",
      "conflict_prompt": "Ava, a college graduate with a master's degree, never completed any college coursework.",
      "question": "What level of education is Ava reported to have?",
      "options": [
        "A. High school diploma",
        "B. Bachelor's degree only",
        "C. Master's degree",
        "D. Doctorate"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The forest ranger, experienced in wilderness survival, led the rescue party into the mountains.",
      "conflict_prompt": "The forest ranger, experienced in wilderness survival, is unfamiliar with outdoor skills and gets lost in a park.",
      "question": "What expertise does the forest ranger possess?",
      "options": [
        "A. Wilderness survival",
        "B. Urban planning",
        "C. Computer programming",
        "D. Medical billing"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Natalie, a non-smoker, lives in an apartment building where smoking indoors is prohibited.",
      "conflict_prompt": "Natalie, a non-smoker, lights cigarettes in her apartment every evening.",
      "question": "Does Natalie smoke according to the statement?",
      "options": [
        "A. Yes, she smokes daily",
        "B. She is a former smoker",
        "C. No, she is a non-smoker",
        "D. She only uses vaping devices"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Coach Rivera, a certified youth soccer coach, organized drills for the under-12 team.",
      "conflict_prompt": "Coach Rivera, a certified youth soccer coach, has no coaching certification and does not run practices.",
      "question": "What certification does Coach Rivera hold?",
      "options": [
        "A. Certified youth soccer coach",
        "B. Certified swim instructor",
        "C. Licensed electrician",
        "D. FAA pilot license"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The medication is prescription-only and requires a doctor's order to dispense.",
      "conflict_prompt": "The medication is prescription-only yet is sold over the counter without any prescription.",
      "question": "How is the medication regulated according to the statement?",
      "options": [
        "A. Over the counter",
        "B. Prescription-only",
        "C. Controlled substance with special license",
        "D. Dietary supplement"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah, a practicing Buddhist, attended the meditation retreat at the monastery.",
      "conflict_prompt": "Noah, a practicing Buddhist, publicly denounces Buddhism and refuses to meditate.",
      "question": "Which religion is Noah described as practicing?",
      "options": [
        "A. Christianity",
        "B. Buddhism",
        "C. Islam",
        "D. Sikhism"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The concert venue is wheelchair-accessible and provides ramps and elevators for patrons.",
      "conflict_prompt": "The concert venue is wheelchair-accessible but has no ramps, elevators, or accessible seating.",
      "question": "What accessibility feature does the venue offer?",
      "options": [
        "A. No accessibility features",
        "B. Wheelchair-accessible with ramps and elevators",
        "C. Only stairs and no seating",
        "D. Accessible for visually impaired only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan, a vegetarian food blogger, reviewed only plant-based restaurants in the city.",
      "conflict_prompt": "Ethan, a vegetarian food blogger, exclusively promotes meat-based barbecue joints and eats meat daily.",
      "question": "What type of cuisine does Ethan focus on reviewing?",
      "options": [
        "A. Barbecue and meat-heavy restaurants",
        "B. Plant-based and vegetarian restaurants",
        "C. Seafood markets",
        "D. Fast-food burger chains"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain lodge is off-grid, powered entirely by solar panels and a generator.",
      "conflict_prompt": "The mountain lodge is off-grid but is connected to the municipal electricity grid and depends on it.",
      "question": "How is the lodge powered according to the statement?",
      "options": [
        "A. Connected to municipal grid",
        "B. Off-grid using solar panels and a generator",
        "C. Powered exclusively by geothermal",
        "D. No power available"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, a registered organ donor, signed her donor card giving consent to organ donation.",
      "conflict_prompt": "Marta, a registered organ donor, explicitly refused to donate any organs under any circumstances.",
      "question": "What decision did Marta make regarding organ donation?",
      "options": [
        "A. Registered organ donor",
        "B. Objected to all organ donation",
        "C. Undecided about donation",
        "D. Plans to donate only corneas"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The painting is an original by the artist, authenticated by multiple experts.",
      "conflict_prompt": "The painting is an original by the artist but all experts agree it is a forgery.",
      "question": "What is claimed about the painting's authenticity?",
      "options": [
        "A. Forgery",
        "B. Authentic original by the artist",
        "C. Mass-produced print",
        "D. Created by an unknown student"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lucas, a diabetic patient, carries insulin and monitors his blood sugar daily.",
      "conflict_prompt": "Lucas, a diabetic patient, refuses to take insulin and never checks his blood sugar.",
      "question": "What condition does Lucas manage according to the statement?",
      "options": [
        "A. Hypertension",
        "B. Diabetes",
        "C. Asthma",
        "D. Epilepsy"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The research team, composed of neuroscientists, published a paper on brain imaging.",
      "conflict_prompt": "The research team, composed of neuroscientists, has no background in neuroscience and studies literature instead.",
      "question": "What is the research team's field of expertise?",
      "options": [
        "A. Marine biology",
        "B. Neuroscience",
        "C. Agricultural science",
        "D. Astrophysics"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya, a vegetarian bride, requested a fully vegetarian wedding menu for her reception.",
      "conflict_prompt": "Priya, a vegetarian bride, insisted that the reception serve exclusively roast beef.",
      "question": "What type of menu did Priya request for her wedding reception?",
      "options": [
        "A. Seafood buffet",
        "B. Fully vegetarian menu",
        "C. Steakhouse menu",
        "D. Raw-only cuisine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The laptop is equipped with a solid-state drive for faster performance.",
      "conflict_prompt": "The laptop is equipped with a solid-state drive but only contains a slow spinning HDD.",
      "question": "What type of storage is specified for the laptop?",
      "options": [
        "A. Solid-state drive (SSD)",
        "B. External tape drive",
        "C. Optical disc only",
        "D. No storage included"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Grace, a professional violinist, performed the solo concerto with the orchestra.",
      "conflict_prompt": "Grace, a professional violinist, has never touched a violin and cannot play music.",
      "question": "What musical profession is Grace associated with?",
      "options": [
        "A. Pianist",
        "B. Drummer",
        "C. Professional violinist",
        "D. Opera singer"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery owner, a certified food handler, complies with all health codes and inspections.",
      "conflict_prompt": "The bakery owner, a certified food handler, ignores health codes and fails all inspections repeatedly.",
      "question": "What certification does the bakery owner possess?",
      "options": [
        "A. Certified food handler",
        "B. Licensed pharmacist",
        "C. Certified mechanic",
        "D. Licensed real estate agent"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar, a retired firefighter, volunteers at the fire museum and speaks about his past service.",
      "conflict_prompt": "Omar, a retired firefighter, has never served as a firefighter and fabricated his stories.",
      "question": "What is Omar's former profession?",
      "options": [
        "A. Retired firefighter",
        "B. Airline pilot",
        "C. High school teacher",
        "D. Software engineer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The building's elevator is out of service, and residents must use the stairs until repairs are complete.",
      "conflict_prompt": "The building's elevator is out of service but has been operating normally without interruption.",
      "question": "What is the current status of the building's elevator?",
      "options": [
        "A. Operating normally",
        "B. Out of service requiring use of stairs",
        "C. Recently upgraded to high speed",
        "D. Scheduled for removal next month"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sara, a licensed cosmetologist, is trained in hair coloring and styling.",
      "conflict_prompt": "Sara, a licensed cosmetologist, has never completed cosmetology school and lacks training.",
      "question": "What professional training does Sara have?",
      "options": [
        "A. Licensed cosmetologist",
        "B. Licensed plumber",
        "C. Certified public accountant",
        "D. Dental hygienist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The committee chair, an impartial moderator, ensured every candidate received equal time to speak.",
      "conflict_prompt": "The committee chair, an impartial moderator, openly favored one candidate and silenced others.",
      "question": "How is the committee chair described in terms of neutrality?",
      "options": [
        "A. Biased toward one candidate",
        "B. Impartial moderator",
        "C. Absent from meetings",
        "D. Candidate themselves"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tim, a vegetarian farmer, grows only vegetables and raises no livestock for meat.",
      "conflict_prompt": "Tim, a vegetarian farmer, runs a commercial feedlot and raises animals exclusively for slaughter.",
      "question": "What type of farming does Tim practice according to the statement?",
      "options": [
        "A. Livestock feedlot owner",
        "B. Vegetable farmer who raises no livestock for meat",
        "C. Poultry breeder",
        "D. Dairy farmer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The software is open-source and its code is available on a public repository.",
      "conflict_prompt": "The software is open-source but the source code is kept private and not shared publicly.",
      "question": "What licensing or availability is claimed for the software?",
      "options": [
        "A. Proprietary closed-source",
        "B. Open-source with public repository",
        "C. Shareware with trial period",
        "D. Subscription-only SaaS"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rachel, a member of the chess club, competes in regional tournaments every month.",
      "conflict_prompt": "Rachel, a member of the chess club, has no interest in chess and has never played a tournament.",
      "question": "What activity does Rachel participate in as a club member?",
      "options": [
        "A. Marathon running",
        "B. Competitive chess",
        "C. Bird watching",
        "D. Pottery"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The hiking trail is dog-friendly and permits leashed pets along the path.",
      "conflict_prompt": "The hiking trail is dog-friendly but posts clearly indicate dogs are banned at all times.",
      "question": "What is the trail's policy regarding pets?",
      "options": [
        "A. Dogs are permitted on leash",
        "B. Dogs are banned entirely",
        "C. Only service animals allowed",
        "D. Pets must be left at home"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Victor, a widower, lives alone since his spouse passed away five years ago.",
      "conflict_prompt": "Victor, a widower, lives with his spouse who is very much alive.",
      "question": "What marital status is Victor described as having?",
      "options": [
        "A. Married",
        "B. Divorced",
        "C. Widower",
        "D. Single never married"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The product is biodegradable and breaks down naturally within six months under composting conditions.",
      "conflict_prompt": "The product is biodegradable but is made from non-degradable plastics that persist for decades.",
      "question": "What environmental property is claimed for the product?",
      "options": [
        "A. Non-degradable plastic",
        "B. Biodegradable within six months",
        "C. Radioactive",
        "D. Requires chemical recycling"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Anna, a licensed electrician, installed the home wiring according to code.",
      "conflict_prompt": "Anna, a licensed electrician, has no electrical license and wired the house dangerously.",
      "question": "What license does Anna hold in the statement?",
      "options": [
        "A. Licensed plumber",
        "B. Licensed electrician",
        "C. Certified teacher",
        "D. Licensed realtor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The classroom is equipped with a smart board and digital projector for lessons.",
      "conflict_prompt": "The classroom is equipped with a smart board but has no electricity or devices to power it.",
      "question": "What technology is mentioned as being in the classroom?",
      "options": [
        "A. Overhead projector only",
        "B. Chalkboard only",
        "C. Smart board and digital projector",
        "D. No teaching aids at all"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mikhail, a Russian citizen, renewed his passport at the embassy last month.",
      "conflict_prompt": "Mikhail, a Russian citizen, is not a citizen of any country and has no passport.",
      "question": "What nationality is Mikhail described as having?",
      "options": [
        "A. Russian",
        "B. Canadian",
        "C. Stateless",
        "D. Brazilian"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's croissants are made fresh daily using traditional French techniques.",
      "conflict_prompt": "The bakery's croissants are made fresh daily but are actually frozen mass-produced pastries reheated.",
      "question": "How are the bakery's croissants described as being made?",
      "options": [
        "A. Frozen and reheated",
        "B. Made fresh daily using traditional techniques",
        "C. Imported pre-baked",
        "D. Gluten-free only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priest Alvarez, a Roman Catholic priest, led the Sunday mass at the parish.",
      "conflict_prompt": "Priest Alvarez, a Roman Catholic priest, publicly renounced the Church and leads atheist meetings.",
      "question": "What religious office does Alvarez hold according to the statement?",
      "options": [
        "A. Imam",
        "B. Rabbi",
        "C. Roman Catholic priest",
        "D. Buddhist monk"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Connor, a left-handed archer, consistently shoots with left-hand dominance in competitions.",
      "conflict_prompt": "Connor, a left-handed archer, shoots exclusively with his right hand and cannot use his left.",
      "question": "Which hand does Connor predominantly use for archery?",
      "options": [
        "A. Right hand",
        "B. Left hand",
        "C. Both hands equally",
        "D. Neither hand"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The public park is open to visitors year-round from dawn until dusk.",
      "conflict_prompt": "The public park is open to visitors year-round, but gates remain locked and no one may enter.",
      "question": "When is the public park open according to the statement?",
      "options": [
        "A. Dawn until dusk year-round",
        "B. Only on weekends",
        "C. Closed year-round",
        "D. Open overnight only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amir, a computer science major, completed advanced courses in algorithms and data structures.",
      "conflict_prompt": "Amir, a computer science major, failed basic programming classes and never learned algorithms.",
      "question": "What field of study is Amir pursuing?",
      "options": [
        "A. Biology",
        "B. Computer science",
        "C. English literature",
        "D. Mechanical engineering"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The patient, a paraplegic, uses a wheelchair for mobility and requires accessible facilities.",
      "conflict_prompt": "The patient, a paraplegic, walks unassisted and refuses to use mobility aids.",
      "question": "What mobility impairment is reported for the patient?",
      "options": [
        "A. Paraplegia requiring wheelchair",
        "B. Full mobility with no aid",
        "C. Visual impairment",
        "D. Hearing impairment"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The camper stove is propane-fueled and requires a small threaded gas canister to operate.",
      "conflict_prompt": "The camper stove is propane-fueled but runs only on electricity and cannot use gas canisters.",
      "question": "What fuel type is specified for the camper stove?",
      "options": [
        "A. Electric only",
        "B. Wood-burning",
        "C. Propane gas canister",
        "D. Diesel"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aisha, a registered nurse, administered medications during the night shift at the hospital.",
      "conflict_prompt": "Aisha, a registered nurse, has no nursing credentials and is barred from administering medications.",
      "question": "What profession does Aisha hold at the hospital?",
      "options": [
        "A. Registered nurse",
        "B. Hospital security guard",
        "C. Hospital chef",
        "D. Janitorial staff"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The antique clock is mechanical and requires winding once a week to keep time.",
      "conflict_prompt": "The antique clock is mechanical but never needs winding and runs on a hidden battery.",
      "question": "How is the antique clock powered according to the statement?",
      "options": [
        "A. Wound weekly as a mechanical clock",
        "B. Solar-powered",
        "C. Electric plug-in",
        "D. GPS-synchronized atomic clock"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Evelyn, a professional photographer, shoots weddings and portraits for clients.",
      "conflict_prompt": "Evelyn, a professional photographer, has never taken a photograph and dislikes cameras.",
      "question": "What is Evelyn's profession?",
      "options": [
        "A. Professional photographer",
        "B. Ballet dancer",
        "C. Web developer",
        "D. Pharmacist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery offers gluten-free bread, baked separately to avoid cross-contamination.",
      "conflict_prompt": "The bakery offers gluten-free bread but bakes it in the same trays and mixes with wheat flour intentionally.",
      "question": "What accommodation does the bakery provide for customers with gluten intolerance?",
      "options": [
        "A. No gluten-free options",
        "B. Gluten-free bread prepared separately",
        "C. Only dairy-free items",
        "D. Nut-free bakery"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Trevor, a licensed driver, passed his driving test and holds a valid license.",
      "conflict_prompt": "Trevor, a licensed driver, does not have a driver's license and was banned from driving.",
      "question": "What driving status is Trevor reported to have?",
      "options": [
        "A. Valid driver's license",
        "B. Driving ban",
        "C. Learner's permit only",
        "D. Commercial truck license"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school lunch program is free for eligible students and provides nutritious meals daily.",
      "conflict_prompt": "The school lunch program is free for eligible students but provides no food at all.",
      "question": "What benefit does the school lunch program provide according to the statement?",
      "options": [
        "A. Provides nutritious meals daily for eligible students",
        "B. Charges high fees for all students",
        "C. Only offers snacks once a week",
        "D. Provides meals only during summer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, a certified yoga instructor, leads morning classes in Vinyasa flow.",
      "conflict_prompt": "Marta, a certified yoga instructor, refuses to teach and has never been certified.",
      "question": "What certification does Marta hold?",
      "options": [
        "A. Certified yoga instructor",
        "B. Personal trainer certification only",
        "C. No fitness qualifications",
        "D. Certified nutritionist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bookshelf is made of solid oak and can support heavy volumes without sagging.",
      "conflict_prompt": "The bookshelf is made of solid oak but collapses under the weight of a single paperback.",
      "question": "What material is the bookshelf constructed from according to the statement?",
      "options": [
        "A. Particleboard",
        "B. Solid oak",
        "C. Plastic",
        "D. Metal mesh"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noelle, a certified translator, translated the legal documents from German to English.",
      "conflict_prompt": "Noelle, a certified translator, mistranslated every legal document and has no certification.",
      "question": "What service did Noelle provide in the statement?",
      "options": [
        "A. Translated legal documents from German to English",
        "B. Interpreted a live courtroom proceeding",
        "C. Designed a legal website",
        "D. Served as a judge"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain goat is native to alpine regions and adapted to steep rocky terrain.",
      "conflict_prompt": "The mountain goat is native to alpine regions but cannot climb and lives only in swamps.",
      "question": "What habitat is the mountain goat adapted to according to the statement?",
      "options": [
        "A. Tropical rainforest",
        "B. Alpine rocky terrain",
        "C. Deep ocean",
        "D. Desert dunes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Harrison, a registered voter, cast his ballot in the municipal election last Tuesday.",
      "conflict_prompt": "Harrison, a registered voter, is not registered and has never been eligible to vote.",
      "question": "What civic status is Harrison said to have in the statement?",
      "options": [
        "A. Registered voter",
        "B. Non-registered resident",
        "C. Felon ineligible to vote",
        "D. Under 18 years old"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The concert pianist, a conservatory graduate, studied at the national music academy.",
      "conflict_prompt": "The concert pianist, a conservatory graduate, never attended any music school and cannot read music.",
      "question": "Where did the concert pianist study according to the statement?",
      "options": [
        "A. No formal education",
        "B. National music academy (conservatory)",
        "C. Medical school",
        "D. Culinary institute"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The charity is non-profit and donates 100% of proceeds to community programs.",
      "conflict_prompt": "The charity is non-profit but diverts all proceeds to private shareholders for profit.",
      "question": "What organizational status does the charity have according to the statement?",
      "options": [
        "A. For-profit corporation",
        "B. Non-profit organization",
        "C. Government agency",
        "D. Private trust"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ariel, a vegan activist, campaigns for animal rights and plant-based diets.",
      "conflict_prompt": "Ariel, a vegan activist, runs a chain of steakhouse restaurants promoting meat consumption.",
      "question": "What advocacy is Ariel involved in according to the statement?",
      "options": [
        "A. Animal rights and plant-based diets",
        "B. Promotion of meat consumption",
        "C. Urban development",
        "D. Cryptocurrency trading"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The suitcase is carry-on sized and meets airline overhead bin dimensions.",
      "conflict_prompt": "The suitcase is carry-on sized but exceeds all airline size limits and must be checked.",
      "question": "What luggage size is claimed for the suitcase in the statement?",
      "options": [
        "A. Oversized checked luggage",
        "B. Carry-on sized meeting overhead bin dimensions",
        "C. Bulky sports equipment",
        "D. Personal item only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Diego, a fluent Mandarin speaker, taught conversational Mandarin classes at the community center.",
      "conflict_prompt": "Diego, a fluent Mandarin speaker, cannot pronounce basic Mandarin words and avoids speaking it.",
      "question": "Which language does Diego teach according to the statement?",
      "options": [
        "A. Mandarin",
        "B. Russian",
        "C. Portuguese",
        "D. Italian"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bicycle helmet is certified to meet safety standard CPSC for head protection.",
      "conflict_prompt": "The bicycle helmet is certified to meet safety standard CPSC but fails all safety tests and offers no protection.",
      "question": "What safety certification is mentioned for the helmet?",
      "options": [
        "A. DOT",
        "B. CPSC",
        "C. NFPA",
        "D. No certification"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lila, a full-time kindergarten teacher, works weekdays at the elementary school.",
      "conflict_prompt": "Lila, a full-time kindergarten teacher, is unemployed and has never taught a class.",
      "question": "What is Lila's occupation in the statement?",
      "options": [
        "A. Kindergarten teacher working full-time",
        "B. College professor",
        "C. Freelance consultant",
        "D. Unemployed artist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The research paper is peer-reviewed and published in an academic journal.",
      "conflict_prompt": "The research paper is peer-reviewed but was never published and remains unvetted.",
      "question": "What publication status is given for the research paper?",
      "options": [
        "A. Published in an academic journal after peer review",
        "B. Self-published blog post",
        "C. Unfinished manuscript",
        "D. Conference poster only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael, a licensed architect, designed the new community center following building codes.",
      "conflict_prompt": "Rafael, a licensed architect, has no architectural credentials and illegally altered the design.",
      "question": "What professional license does Rafael hold according to the statement?",
      "options": [
        "A. Licensed architect",
        "B. Licensed plumber",
        "C. Licensed pilot",
        "D. Licensed pharmacist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The backyard pool is fenced and has safety locks to prevent unsupervised access by children.",
      "conflict_prompt": "The backyard pool is fenced and has no locks or safety measures, allowing unsupervised access.",
      "question": "What safety measures are described for the backyard pool?",
      "options": [
        "A. No fence or safety features",
        "B. Fenced with safety locks to prevent unsupervised access",
        "C. Indoor pool only",
        "D. Used only by staff"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah, a conservatory-trained cellist, performed a solo at the gala.",
      "conflict_prompt": "Hannah, a conservatory-trained cellist, has never taken a music lesson and cannot play the cello.",
      "question": "What instrument does Hannah play professionally according to the statement?",
      "options": [
        "A. Violin",
        "B. Cello",
        "C. Trumpet",
        "D. Saxophone"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The software license prohibits commercial use without purchasing a business license.",
      "conflict_prompt": "The software license prohibits commercial use yet explicitly allows free commercial exploitation without payment.",
      "question": "What restriction does the software license impose according to the statement?",
      "options": [
        "A. Allows unrestricted commercial use",
        "B. Prohibits commercial use without purchasing a business license",
        "C. Only educational use permitted",
        "D. Abandons all license terms"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah, a vegetarian athlete, follows a plant-based diet to fuel his training.",
      "conflict_prompt": "Noah, a vegetarian athlete, consumes meat daily and relies on animal protein exclusively.",
      "question": "What diet does Noah adhere to according to the statement?",
      "options": [
        "A. Carnivorous",
        "B. Plant-based vegetarian",
        "C. Ketogenic meat-heavy",
        "D. Fruitarian only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The orphanage director, a longtime child welfare advocate, has cared for children for decades.",
      "conflict_prompt": "The orphanage director, a longtime child welfare advocate, has no experience with children and opposes child welfare.",
      "question": "What role does the director hold according to the statement?",
      "options": [
        "A. Orphanage director and child welfare advocate",
        "B. Real estate developer",
        "C. Corporate lawyer",
        "D. Investment banker"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felipe, a bilingual emergency dispatcher, answers 911 calls in both English and Spanish.",
      "conflict_prompt": "Felipe, a bilingual emergency dispatcher, does not speak English or Spanish and cannot dispatch emergency services.",
      "question": "What languages does Felipe use on the job according to the statement?",
      "options": [
        "A. English and Spanish",
        "B. Mandarin and Cantonese",
        "C. French and German",
        "D. Russian and Ukrainian"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The cottage is off-grid with composting toilets and rainwater collection systems installed.",
      "conflict_prompt": "The cottage is off-grid with composting toilets but relies entirely on the municipal sewer and water systems.",
      "question": "What off-grid features are described for the cottage?",
      "options": [
        "A. Connected to municipal utilities only",
        "B. Off-grid with composting toilets and rainwater collection",
        "C. Industrial wastewater discharge",
        "D. Solar thermal only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Liam, a sober driver tested for alcohol, recorded a zero blood alcohol level before driving.",
      "conflict_prompt": "Liam, a sober driver tested for alcohol, had a very high blood alcohol concentration and was intoxicated.",
      "question": "What was Liam's blood alcohol status according to the statement?",
      "options": [
        "A. High intoxication",
        "B. Moderate intoxication",
        "C. Zero blood alcohol level (sober)",
        "D. Underage with no test"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The theater seat is reserved for patrons with disabilities and marked as accessible seating.",
      "conflict_prompt": "The theater seat is reserved for patrons with disabilities but the seat is blocked off and not accessible at all.",
      "question": "For whom is the theater seat reserved according to the statement?",
      "options": [
        "A. VIP guests only",
        "B. Patrons with disabilities (accessible seating)",
        "C. Standing room only patrons",
        "D. Crew members"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Adele, a certified lifeguard, enforced all pool safety rules during the public swim hours.",
      "conflict_prompt": "Adele, a certified lifeguard, ignored safety rules and allowed dangerous behavior at the pool.",
      "question": "What role does Adele fulfill at the pool?",
      "options": [
        "A. Certified lifeguard enforcing safety",
        "B. Snack bar attendant",
        "C. Pool equipment technician",
        "D. Unauthorized volunteer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The smartphone camera has optical image stabilization to reduce blur in low light photos.",
      "conflict_prompt": "The smartphone camera has optical image stabilization but produces extremely blurry photos even in bright light.",
      "question": "What camera feature is highlighted for the smartphone?",
      "options": [
        "A. No stabilization",
        "B. Optical image stabilization (OIS)",
        "C. Film-camera only",
        "D. 3D stereoscopic capture"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priest Matteo, a parish priest, administers sacraments and pastoral care to his congregation.",
      "conflict_prompt": "Priest Matteo, a parish priest, publicly denies the sacraments and refuses to minister to congregants.",
      "question": "What is Priest Matteo's role in the community according to the statement?",
      "options": [
        "A. Parish priest providing sacraments and pastoral care",
        "B. School principal",
        "C. City mayor",
        "D. Local pharmacist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The travel guide is up-to-date and includes current maps and attraction hours for the city.",
      "conflict_prompt": "The travel guide is up-to-date but contains maps from a different country and outdated attraction times.",
      "question": "What quality is claimed for the travel guide?",
      "options": [
        "A. Outdated and inaccurate",
        "B. Up-to-date with current maps and hours",
        "C. Only covers rural areas",
        "D. Exclusive to cruise passengers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector, a vegetarian chef at the restaurant, creates innovative plant-based tasting menus.",
      "conflict_prompt": "Hector, a vegetarian chef at the restaurant, cooks only meat-based tasting menus and bans vegetables.",
      "question": "What type of menus does Hector design according to the statement?",
      "options": [
        "A. Meat-based tasting menus",
        "B. Plant-based vegetarian tasting menus",
        "C. Seafood-only tasting menus",
        "D. Fast-food style menus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The research institute is independent and funded by a broad consortium of academic donors.",
      "conflict_prompt": "The research institute is independent but controlled entirely by a single private corporation with conflicting interests.",
      "question": "What is the funding/governance described for the research institute?",
      "options": [
        "A. Controlled by a single corporation",
        "B. Independent and funded by a consortium of academic donors",
        "C. Funded only by the government",
        "D. Crowdfunded by the general public"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zoe, a certified dog trainer, teaches positive reinforcement techniques to new pet owners.",
      "conflict_prompt": "Zoe, a certified dog trainer, uses harmful punishment methods and lacks any certification.",
      "question": "What training philosophy does Zoe use according to the statement?",
      "options": [
        "A. Positive reinforcement",
        "B. Shock-collar punishment",
        "C. No training at all",
        "D. Herding techniques for livestock"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The charity hospital provides free medical care to uninsured patients in the region.",
      "conflict_prompt": "The charity hospital provides free medical care yet charges full market rates and denies free services.",
      "question": "What service does the charity hospital offer according to the statement?",
      "options": [
        "A. High-cost specialized care only",
        "B. Free medical care to uninsured patients",
        "C. Veterinary services only",
        "D. Cosmetic surgery for paying clients"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Isabel, a certified mountain guide, escorted climbers to the summit using established routes.",
      "conflict_prompt": "Isabel, a certified mountain guide, has no climbing experience and leads people onto dangerous uncharted routes.",
      "question": "What is Isabel's professional role according to the statement?",
      "options": [
        "A. Mountain guide with certification",
        "B. Office administrator",
        "C. Airline dispatcher",
        "D. Nightclub promoter"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The stored data is encrypted at rest with industry-standard AES-256 encryption.",
      "conflict_prompt": "The stored data is encrypted at rest but is stored in plain text readable by anyone.",
      "question": "What security measure is used for stored data according to the statement?",
      "options": [
        "A. No encryption",
        "B. AES-256 encryption at rest",
        "C. Base64 encoding only",
        "D. Encrypted in transit only, not at rest"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noel, an ordained minister, officiated the wedding ceremony at the chapel.",
      "conflict_prompt": "Noel, an ordained minister, refuses to perform marriages and is not ordained.",
      "question": "What religious function did Noel perform according to the statement?",
      "options": [
        "A. Officiated weddings as an ordained minister",
        "B. Performed veterinary procedures",
        "C. Sang in the choir but not officiate",
        "D. Managed the chapel's finances only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The aquarium exhibit houses tropical reef fish and maintains stable warm water conditions.",
      "conflict_prompt": "The aquarium exhibit houses tropical reef fish but keeps water temperatures at freezing levels unsuitable for them.",
      "question": "What type of animals are kept in the aquarium exhibit according to the statement?",
      "options": [
        "A. Polar marine species",
        "B. Freshwater carp only",
        "C. Tropical reef fish",
        "D. Terrestrial reptiles"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amira, a licensed midwife, assisted in multiple safe home births and provides prenatal care.",
      "conflict_prompt": "Amira, a licensed midwife, has no midwifery license and refuses to attend births.",
      "question": "What healthcare role does Amira perform according to the statement?",
      "options": [
        "A. Licensed midwife providing prenatal care and birth assistance",
        "B. Cardiologist",
        "C. Radiology technician",
        "D. Medical receptionist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The electric car has a range of 300 miles on a single charge under normal driving conditions.",
      "conflict_prompt": "The electric car has a range of 300 miles but only runs for a few miles before losing power completely.",
      "question": "What is the specified driving range for the electric car?",
      "options": [
        "A. 50 miles",
        "B. 1000 miles",
        "C. 300 miles per charge",
        "D. 10 miles in town only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The preschool teacher, a childcare specialist, implemented a curriculum for early development.",
      "conflict_prompt": "The preschool teacher, a childcare specialist, avoids interacting with children and provides no instruction.",
      "question": "What role does the preschool teacher have according to the statement?",
      "options": [
        "A. Childcare specialist implementing curriculum",
        "B. High school guidance counselor",
        "C. IT support technician",
        "D. School janitor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The hiking guide, an experienced outdoorsman, carried emergency gear and mapped the route in advance.",
      "conflict_prompt": "The hiking guide, an experienced outdoorsman, had no gear and refused to plan or map the route.",
      "question": "What preparation did the hiking guide make according to the statement?",
      "options": [
        "A. No preparation and no gear",
        "B. Carried emergency gear and mapped the route in advance",
        "C. Relied only on GPS on a phone with no backup",
        "D. Hired a helicopter instead"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector, a professional beekeeper, manages several hives and harvests honey each season.",
      "conflict_prompt": "Hector, a professional beekeeper, destroys every hive he encounters and avoids beekeeping tasks.",
      "question": "What occupation does Hector hold according to the statement?",
      "options": [
        "A. Professional beekeeper managing hives",
        "B. Urban traffic planner",
        "C. Software quality assurance tester",
        "D. Professional gambler"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The children's book author, a former teacher, wrote stories aimed at early readers.",
      "conflict_prompt": "The children's book author, a former teacher, writes only technical manuals for engineers and never writes for kids.",
      "question": "What audience does the author primarily write for according to the statement?",
      "options": [
        "A. Early readers and children",
        "B. Professional engineers",
        "C. Academic philosophers",
        "D. Culinary students"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain cabin is off-grid and relies on a rainwater catchment system for water supply.",
      "conflict_prompt": "The mountain cabin is off-grid but depends on municipal water connected by underground pipes.",
      "question": "How does the mountain cabin obtain water according to the statement?",
      "options": [
        "A. Municipal water via pipes",
        "B. Rainwater catchment system off-grid",
        "C. Bottled water deliveries only",
        "D. No water access at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samantha, a certified lifeguard, performed a rescue and administered CPR successfully.",
      "conflict_prompt": "Samantha, a certified lifeguard, panicked during the incident and failed to perform any rescue procedures.",
      "question": "What action did Samantha take during the emergency according to the statement?",
      "options": [
        "A. Performed a rescue and administered CPR successfully",
        "B. Did nothing and left the scene",
        "C. Called for backup but did not intervene",
        "D. Only cleaned the pool after the incident"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The historical document is original and dated to the eighteenth century by paleography experts.",
      "conflict_prompt": "The historical document is original and dated to the eighteenth century, yet everyone agrees it was printed last month.",
      "question": "What is the age attribution for the historical document according to the statement?",
      "options": [
        "A. Contemporary reproduction",
        "B. Dated to the eighteenth century",
        "C. From the medieval period",
        "D. Manufactured in the 21st century"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leo, a military veteran, served two tours overseas before retiring from active duty.",
      "conflict_prompt": "Leo, a military veteran, never served in the armed forces and was never deployed anywhere.",
      "question": "What is Leo's background according to the statement?",
      "options": [
        "A. Two tours overseas as a military veteran",
        "B. Never served in armed forces",
        "C. Active duty currently",
        "D. Civilian contractor only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The coffee shop roasts its beans on-site and uses single-origin beans from a family farm.",
      "conflict_prompt": "The coffee shop roasts its beans on-site but sells only instant coffee made from blends of unknown origin.",
      "question": "What roasting practice is described for the coffee shop?",
      "options": [
        "A. Uses instant coffee blends only",
        "B. Roasts beans on-site using single-origin beans",
        "C. Imports pre-roasted commercial beans only",
        "D. Serves only decaf tea"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The kindergarten classroom teacher, a certified early childhood educator, planned age-appropriate activities.",
      "conflict_prompt": "The kindergarten classroom teacher, a certified early childhood educator, fails to prepare lessons and leaves children unattended.",
      "question": "What qualifications and duties are described for the teacher?",
      "options": [
        "A. Certified early childhood educator planning age-appropriate activities",
        "B. High school substitute teacher",
        "C. Office manager scheduling meetings",
        "D. Unqualified babysitter"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The electric kettle is auto-shutoff and prevents boiling dry to ensure safety.",
      "conflict_prompt": "The electric kettle is auto-shutoff but continues to boil and catches fire when empty.",
      "question": "What safety feature is included with the electric kettle according to the statement?",
      "options": [
        "A. Auto-shutoff to prevent boiling dry",
        "B. Manual watch required at all times",
        "C. No safety features",
        "D. Requires constant water addition"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tariq, a certified diver, explored shipwrecks at significant depths with proper training.",
      "conflict_prompt": "Tariq, a certified diver, has never undergone diving training and refuses to dive at all.",
      "question": "What capability does Tariq possess according to the statement?",
      "options": [
        "A. Certified diver trained for deep dives",
        "B. Only snorkels in shallow pools",
        "C. Pilots submarines professionally",
        "D. Scuba equipment salesperson"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The children's playground is fenced and has soft impact-absorbing surfacing beneath the equipment.",
      "conflict_prompt": "The children's playground is fenced but has hard concrete under the equipment making it unsafe.",
      "question": "What safety feature is mentioned for the playground surface?",
      "options": [
        "A. Concrete surface under equipment",
        "B. Soft impact-absorbing surfacing",
        "C. Rocky gravel only",
        "D. No surface, natural ground only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, a fluent sign-language interpreter, provides interpretation for deaf attendees at public events.",
      "conflict_prompt": "Marta, a fluent sign-language interpreter, cannot sign and refuses to interpret for the deaf.",
      "question": "What service does Marta provide according to the statement?",
      "options": [
        "A. Sign-language interpretation for deaf attendees",
        "B. Audio-only translation services",
        "C. Legal counsel",
        "D. Catering services"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The campground is family-friendly and enforces quiet hours after 10 PM for campers.",
      "conflict_prompt": "The campground is family-friendly but encourages loud parties all night with no quiet hours.",
      "question": "What policy is in place at the campground according to the statement?",
      "options": [
        "A. Encourages loud parties all night",
        "B. Family-friendly with quiet hours after 10 PM",
        "C. Adults-only with late-night DJs",
        "D. No camping allowed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bicycle is equipped with front and rear lights for safe night riding.",
      "conflict_prompt": "The bicycle is equipped with front and rear lights but they never illuminate and are nonfunctional.",
      "question": "What safety equipment is specified for the bicycle?",
      "options": [
        "A. Reflective tape only",
        "B. Front and rear lights for night riding",
        "C. Airbag system",
        "D. No safety features"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Giulia, a licensed pharmacist, dispensed prescriptions and counseled patients at the pharmacy.",
      "conflict_prompt": "Giulia, a licensed pharmacist, has no pharmacy license and dispenses medications without training.",
      "question": "What professional role does Giulia have according to the statement?",
      "options": [
        "A. Licensed pharmacist dispensing prescriptions",
        "B. Grocery store clerk",
        "C. Dental hygienist",
        "D. Veterinarian assistant"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The theater troupe is professional and performs original scripted plays in a dedicated venue.",
      "conflict_prompt": "The theater troupe is professional but only improvises street performances without any scripted plays.",
      "question": "What type of performances does the theater troupe present according to the statement?",
      "options": [
        "A. Street improvisations only",
        "B. Original scripted plays in a dedicated venue",
        "C. Mime acts exclusively",
        "D. Film screenings"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Oliver, a lifelong vegetarian, avoids all meat products for ethical reasons.",
      "conflict_prompt": "Oliver, a lifelong vegetarian, raises cattle for slaughter and eats beef daily.",
      "question": "What dietary lifestyle is Oliver described as following?",
      "options": [
        "A. Lifelong vegetarian avoiding meat",
        "B. Full-time butcher",
        "C. Exclusive seafood eater",
        "D. Intermittent fasting advocate"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The library's rare book collection is climate-controlled to preserve fragile volumes.",
      "conflict_prompt": "The library's rare book collection is climate-controlled but stored in a damp, mold-prone basement.",
      "question": "What preservation method is used for the rare book collection according to the statement?",
      "options": [
        "A. Climate-controlled environment to preserve volumes",
        "B. Stored in a damp basement",
        "C. Kept in direct sunlight for display",
        "D. No preservation measures"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain bike is full-suspension and designed for technical downhill trails.",
      "conflict_prompt": "The mountain bike is full-suspension but has rigid forks and is unsuitable for trails.",
      "question": "What type of bike is described in the statement?",
      "options": [
        "A. Road racing bike",
        "B. Full-suspension mountain bike for downhill trails",
        "C. BMX trick bike",
        "D. Folding commuter bike"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Adele, a certified sommelier, curated the wine list for the restaurant's grand opening.",
      "conflict_prompt": "Adele, a certified sommelier, refuses to taste wine and lacks any wine knowledge.",
      "question": "What culinary expertise does Adele have according to the statement?",
      "options": [
        "A. Certified sommelier curating wine lists",
        "B. Head pastry chef",
        "C. Line cook with no wine training",
        "D. Restaurant accountant"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The newborn was full-term and born at 39 weeks without complications.",
      "conflict_prompt": "The newborn was full-term yet was delivered preterm at 28 weeks with severe complications.",
      "question": "At how many weeks gestation was the baby born according to the statement?",
      "options": [
        "A. 28 weeks (preterm)",
        "B. 39 weeks (full-term)",
        "C. Post-term over 42 weeks",
        "D. Not specified"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The rental car includes unlimited mileage allowing extensive road trips without surcharges.",
      "conflict_prompt": "The rental car includes unlimited mileage but charges extra for every mile driven beyond 10 miles.",
      "question": "What mileage policy is provided with the rental car according to the statement?",
      "options": [
        "A. Limited to 10 miles only",
        "B. Unlimited mileage with no surcharges",
        "C. Charged per mile by default",
        "D. Only local short-trip usage allowed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan, who is 34 years old, celebrated his 34th birthday with friends.",
      "conflict_prompt": "Ethan, who is 34 years old, celebrated his 40th birthday with friends.",
      "question": "How old is Ethan?",
      "options": [
        "A. 40 years old",
        "B. 34 years old",
        "C. 36 years old",
        "D. 30 years old"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, a certified pilot, flew the regional jet last Tuesday.",
      "conflict_prompt": "Maya, a certified pilot and an uncertified pilot, flew the regional jet last Tuesday.",
      "question": "What is Maya's credential status?",
      "options": [
        "A. Uncertified pilot",
        "B. Certified pilot",
        "C. Ground crew",
        "D. Flight attendant"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carlos, a vegetarian, ordered a vegetable stir-fry for lunch.",
      "conflict_prompt": "Carlos, a vegetarian and a meat-eater, ordered a vegetable stir-fry for lunch.",
      "question": "What is Carlos's diet?",
      "options": [
        "A. Meat-eater",
        "B. Pescatarian",
        "C. Vegetarian",
        "D. Vegan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aisha, who is right-handed, wrote the note with her right hand.",
      "conflict_prompt": "Aisha, who is right-handed and left-handed, wrote the note with her right hand.",
      "question": "Which hand does Aisha primarily use?",
      "options": [
        "A. Ambidextrous",
        "B. Left-handed",
        "C. Right-handed",
        "D. Uses prosthetic"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tom, a full-time teacher, returned to school on Monday.",
      "conflict_prompt": "Tom, a full-time teacher and an unemployed person, returned to school on Monday.",
      "question": "What is Tom's employment status?",
      "options": [
        "A. Unemployed",
        "B. Part-time employee",
        "C. Full-time teacher",
        "D. Retired"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lina, a resident of Paris, booked a local museum tour.",
      "conflict_prompt": "Lina, a resident of Paris and a resident of London, booked a local museum tour.",
      "question": "Where does Lina reside according to the statement?",
      "options": [
        "A. London",
        "B. Berlin",
        "C. Paris",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah, who has blue eyes, inherited his mother's blue eyes.",
      "conflict_prompt": "Noah, who has blue eyes and brown eyes, inherited his mother's blue eyes.",
      "question": "What is Noah's eye color?",
      "options": [
        "A. Green",
        "B. Brown",
        "C. Hazel",
        "D. Blue"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya, a married woman, introduced her spouse at the event.",
      "conflict_prompt": "Priya, a married and single woman, introduced her spouse at the event.",
      "question": "What is Priya's marital status in the statement?",
      "options": [
        "A. Single",
        "B. Separated",
        "C. Married",
        "D. Divorced"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dmitri, who is 6 feet tall, reached the top shelf easily.",
      "conflict_prompt": "Dmitri, who is 6 feet tall and 5 feet tall, reached the top shelf easily.",
      "question": "How tall is Dmitri according to the statement?",
      "options": [
        "A. 5 feet tall",
        "B. 6 feet tall",
        "C. 5 feet 5 inches",
        "D. 7 feet tall"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia, a native Spanish speaker, interpreted the meeting for the group.",
      "conflict_prompt": "Sofia, a native Spanish speaker and a native English speaker, interpreted the meeting for the group.",
      "question": "What is Sofia's native language according to the statement?",
      "options": [
        "A. Spanish",
        "B. French",
        "C. German",
        "D. Mandarin"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jordan, an only child, spent the holiday with no siblings.",
      "conflict_prompt": "Jordan, an only child and one of three siblings, spent the holiday with no siblings.",
      "question": "How many siblings does Jordan have according to the statement?",
      "options": [
        "A. Two siblings",
        "B. One sibling",
        "C. None (only child)",
        "D. Three siblings"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hana, who uses a manual wheelchair, navigated the accessible ramp.",
      "conflict_prompt": "Hana, who uses a manual wheelchair and does not use a wheelchair, navigated the accessible ramp.",
      "question": "What mobility aid does Hana use according to the statement?",
      "options": [
        "A. Powered wheelchair",
        "B. No aid",
        "C. Manual wheelchair",
        "D. Walker"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Miguel, a graduate of Oxford University, mentioned his time at Oxford in the interview.",
      "conflict_prompt": "Miguel, a graduate of Oxford University and a graduate of Harvard University, mentioned his time at Oxford in the interview.",
      "question": "Which university is Miguel explicitly said to have graduated from?",
      "options": [
        "A. Yale University",
        "B. Harvard University",
        "C. Oxford University",
        "D. Stanford University"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Grace, who is allergic to peanuts, avoided dishes containing peanuts at the picnic.",
      "conflict_prompt": "Grace, who is allergic to peanuts and not allergic to peanuts, avoided dishes containing peanuts at the picnic.",
      "question": "What allergy does Grace have according to the statement?",
      "options": [
        "A. Shellfish allergy",
        "B. No allergies",
        "C. Peanut allergy",
        "D. Dairy allergy"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar, a vegan baker, prepared egg-free pastries for the café.",
      "conflict_prompt": "Omar, a vegan baker and a pastry chef who uses eggs, prepared egg-free pastries for the café.",
      "question": "What kind of baker is Omar according to the statement?",
      "options": [
        "A. Uses eggs in all recipes",
        "B. Vegan baker",
        "C. Gluten-free baker",
        "D. Meat-cooking chef"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Yuki, who drives a compact car, parked in the small-car space.",
      "conflict_prompt": "Yuki, who drives a compact car and a large SUV, parked in the small-car space.",
      "question": "What type of vehicle does Yuki drive according to the statement?",
      "options": [
        "A. Motorcycle",
        "B. Large SUV",
        "C. Compact car",
        "D. Truck"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ben, a morning person, wakes up at 5:30 a.m. every weekday.",
      "conflict_prompt": "Ben, a morning person and a night owl, wakes up at 5:30 a.m. every weekday.",
      "question": "What is Ben's stated chronotype in the statement?",
      "options": [
        "A. Night owl",
        "B. Morning person",
        "C. Intermediate chronotype",
        "D. Irregular sleeper"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Clara, a non-smoker, requested a smoke-free hotel room.",
      "conflict_prompt": "Clara, a non-smoker and a smoker, requested a smoke-free hotel room.",
      "question": "Does Clara smoke according to the statement?",
      "options": [
        "A. Yes, she is a smoker",
        "B. No, she is a non-smoker",
        "C. Occasionally",
        "D. The statement doesn't say"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leo, who was born in 1992, carries ID showing his 1992 birth year.",
      "conflict_prompt": "Leo, who was born in 1992 and 1985, carries ID showing his 1992 birth year.",
      "question": "What year was Leo born according to the statement?",
      "options": [
        "A. 1985",
        "B. 1992",
        "C. 2000",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora, a certified scuba diver, completed a deep dive last weekend.",
      "conflict_prompt": "Nora, a certified scuba diver and someone who has never dived, completed a deep dive last weekend.",
      "question": "What certification does Nora hold according to the statement?",
      "options": [
        "A. Skipper license",
        "B. Certified scuba diver",
        "C. Parachute instructor",
        "D. No certification"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Victor, a republican, voted in the primary as a Republican.",
      "conflict_prompt": "Victor, a republican and a democrat, voted in the primary as a Republican.",
      "question": "What political affiliation is Victor described as having?",
      "options": [
        "A. Independent",
        "B. Democrat",
        "C. Republican",
        "D. Libertarian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zara, an only reader of mystery novels, finished a mystery book yesterday.",
      "conflict_prompt": "Zara, an only reader of mystery novels and a reader of romance novels, finished a mystery book yesterday.",
      "question": "What genre does Zara primarily read according to the statement?",
      "options": [
        "A. Fantasy",
        "B. Romance",
        "C. Mystery",
        "D. Science fiction"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felix, who is diabetic, checked his blood sugar before dinner.",
      "conflict_prompt": "Felix, who is diabetic and not diabetic, checked his blood sugar before dinner.",
      "question": "What medical condition does Felix have according to the statement?",
      "options": [
        "A. Hypertension",
        "B. Diabetes",
        "C. Asthma",
        "D. No medical conditions"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivy, who speaks fluent French, led the French-language workshop.",
      "conflict_prompt": "Ivy, who speaks fluent French and does not speak French, led the French-language workshop.",
      "question": "Which language does Ivy speak fluently according to the statement?",
      "options": [
        "A. German",
        "B. Spanish",
        "C. French",
        "D. Italian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ravi, the team's goalkeeper, saved the penalty kick.",
      "conflict_prompt": "Ravi, the team's goalkeeper and the team's striker, saved the penalty kick.",
      "question": "What position does Ravi play on the team according to the statement?",
      "options": [
        "A. Midfielder",
        "B. Striker",
        "C. Goalkeeper",
        "D. Coach"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah, a published poet, read from her poetry collection.",
      "conflict_prompt": "Hannah, a published poet and an unpublished writer, read from her poetry collection.",
      "question": "What is Hannah's publication status according to the statement?",
      "options": [
        "A. Unpublished",
        "B. Self-published only",
        "C. Published poet",
        "D. Ghostwriter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Arjun, who owns a bakery, sells fresh bread every morning.",
      "conflict_prompt": "Arjun, who owns a bakery and does not own any business, sells fresh bread every morning.",
      "question": "What business does Arjun own according to the statement?",
      "options": [
        "A. A bookstore",
        "B. A bakery",
        "C. A coffee shop",
        "D. No business"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sven, a fluent German speaker, translated the document from German.",
      "conflict_prompt": "Sven, a fluent German speaker and someone who doesn't speak German, translated the document from German.",
      "question": "What language can Sven speak fluently according to the statement?",
      "options": [
        "A. Spanish",
        "B. German",
        "C. Japanese",
        "D. Russian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lola, who prefers tea, declined the offered coffee.",
      "conflict_prompt": "Lola, who prefers tea and prefers coffee, declined the offered coffee.",
      "question": "Which beverage does Lola prefer according to the statement?",
      "options": [
        "A. Coffee",
        "B. Tea",
        "C. Juice",
        "D. Soda"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marco, a northbound train engineer, operated the train heading north.",
      "conflict_prompt": "Marco, a northbound train engineer and a southbound train engineer, operated the train heading north.",
      "question": "Which direction does Marco's train operate according to the statement?",
      "options": [
        "A. Eastbound",
        "B. Southbound",
        "C. Northbound",
        "D. Westbound"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ada, the head nurse, supervised the ER shift last night.",
      "conflict_prompt": "Ada, the head nurse and a trainee nurse, supervised the ER shift last night.",
      "question": "What role did Ada hold during the ER shift according to the statement?",
      "options": [
        "A. Trainee nurse",
        "B. Head nurse",
        "C. Volunteer",
        "D. Receptionist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pavel, who is left-handed, customized his tools for left-handed use.",
      "conflict_prompt": "Pavel, who is left-handed and right-handed, customized his tools for left-handed use.",
      "question": "What is Pavel's handedness according to the statement?",
      "options": [
        "A. Right-handed",
        "B. Ambidextrous",
        "C. Left-handed",
        "D. Uses both equally"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olga, a city council member, voted in favor of the zoning change.",
      "conflict_prompt": "Olga, a city council member and a private citizen with no council role, voted in favor of the zoning change.",
      "question": "What position does Olga hold according to the statement?",
      "options": [
        "A. Mayor",
        "B. City council member",
        "C. State senator",
        "D. No public office"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus, who rides a bicycle to work, locked his bike at the rack.",
      "conflict_prompt": "Marcus, who rides a bicycle to work and drives a motorcycle to work, locked his bike at the rack.",
      "question": "How does Marcus typically commute according to the statement?",
      "options": [
        "A. By car",
        "B. By bus",
        "C. By bicycle",
        "D. By train"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bianca, a high school principal, attended the education conference.",
      "conflict_prompt": "Bianca, a high school principal and an elementary school teacher, attended the education conference.",
      "question": "What is Bianca's role in education according to the statement?",
      "options": [
        "A. High school principal",
        "B. Elementary teacher",
        "C. College professor",
        "D. Substitute teacher"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Kenji, who is a former Olympic athlete, coaches the school's track team.",
      "conflict_prompt": "Kenji, who is a former Olympic athlete and has never competed in sports, coaches the school's track team.",
      "question": "What is Kenji's athletic background according to the statement?",
      "options": [
        "A. Never competed in sports",
        "B. Former Olympic athlete",
        "C. Casual hobby runner",
        "D. Professional dancer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samantha, a vegetarian by choice, brought a salad to the potluck.",
      "conflict_prompt": "Samantha, a vegetarian by choice and someone who eats meat, brought a salad to the potluck.",
      "question": "What dietary choice does Samantha have according to the statement?",
      "options": [
        "A. Omnivore",
        "B. Vegetarian",
        "C. Pescatarian",
        "D. Carnivore"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gabe, who is sober, declined the offered alcoholic drink.",
      "conflict_prompt": "Gabe, who is sober and an active drinker, declined the offered alcoholic drink.",
      "question": "What is Gabe's drinking status according to the statement?",
      "options": [
        "A. Regular drinker",
        "B. Sober",
        "C. Occasional drinker",
        "D. Medical abstainer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rina, a twin's older sibling, often took care of her younger twin.",
      "conflict_prompt": "Rina, a twin's older sibling and a twin's younger sibling, often took care of her younger twin.",
      "question": "What sibling relation is Rina described as having according to the statement?",
      "options": [
        "A. Only child",
        "B. Younger twin",
        "C. Older twin sibling",
        "D. Middle child"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Evelyn, who holds a driver's license, legally drives a car.",
      "conflict_prompt": "Evelyn, who holds a driver's license and does not hold a driver's license, legally drives a car.",
      "question": "Does Evelyn hold a driver's license according to the statement?",
      "options": [
        "A. Yes, she holds a license",
        "B. No, she does not",
        "C. Her license is suspended",
        "D. She only has a learner permit"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mateo, a registered nurse, administered the vaccine to the patient.",
      "conflict_prompt": "Mateo, a registered nurse and someone without medical credentials, administered the vaccine to the patient.",
      "question": "What medical title does Mateo hold according to the statement?",
      "options": [
        "A. Physician",
        "B. Registered nurse",
        "C. Pharmacist",
        "D. Medical volunteer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Isabel, a fan of classical music, attended the symphony concert.",
      "conflict_prompt": "Isabel, a fan of classical music and a fan of heavy metal exclusively, attended the symphony concert.",
      "question": "What music genre does Isabel like according to the statement?",
      "options": [
        "A. Jazz",
        "B. Heavy metal",
        "C. Classical music",
        "D. Hip hop"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Haris, who owns a bookstore, opened the shop every morning.",
      "conflict_prompt": "Haris, who owns a bookstore and does not own any shop, opened the shop every morning.",
      "question": "What business ownership is attributed to Haris in the statement?",
      "options": [
        "A. Coffee shop",
        "B. Bookstore",
        "C. Clothing boutique",
        "D. No shop"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lena, a morning shift nurse, started work at 7:00 a.m.",
      "conflict_prompt": "Lena, a morning shift nurse and a night shift nurse, started work at 7:00 a.m.",
      "question": "Which shift does Lena work according to the statement?",
      "options": [
        "A. Night shift",
        "B. Weekend shift",
        "C. Morning shift",
        "D. Rotating shift"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noelle, a graduate student, defended her master's thesis last spring.",
      "conflict_prompt": "Noelle, a graduate student and an undergraduate student, defended her master's thesis last spring.",
      "question": "What level of study is Noelle undertaking according to the statement?",
      "options": [
        "A. Undergraduate",
        "B. Graduate student (master's)",
        "C. High school student",
        "D. Postdoctoral researcher"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Darius, a vegetarian chef, specializes in plant-based cuisine.",
      "conflict_prompt": "Darius, a vegetarian chef and a chef who specializes in meat dishes, specializes in plant-based cuisine.",
      "question": "What type of cuisine does Darius specialize in according to the statement?",
      "options": [
        "A. Seafood",
        "B. Plant-based cuisine",
        "C. Barbecue",
        "D. Pastry"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Yara, who has a hearing impairment, uses sign language to communicate.",
      "conflict_prompt": "Yara, who has a hearing impairment and full hearing, uses sign language to communicate.",
      "question": "What communication method does Yara use according to the statement?",
      "options": [
        "A. Lip reading only",
        "B. Sign language",
        "C. Braille",
        "D. Morse code"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Owen, an only driver of the delivery van, completed the route.",
      "conflict_prompt": "Owen, an only driver of the delivery van and one of several drivers, completed the route.",
      "question": "What is Owen's role regarding the delivery van according to the statement?",
      "options": [
        "A. One of several drivers",
        "B. The owner but not a driver",
        "C. The only driver",
        "D. Dispatcher"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Fiona, who prefers cold climates, booked a trip to Iceland.",
      "conflict_prompt": "Fiona, who prefers cold climates and prefers tropical climates, booked a trip to Iceland.",
      "question": "What climate does Fiona prefer according to the statement?",
      "options": [
        "A. Tropical",
        "B. Temperate",
        "C. Cold",
        "D. Desert"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Niko, a former firefighter, volunteers at the community safety fair.",
      "conflict_prompt": "Niko, a former firefighter and someone who has never worked as a firefighter, volunteers at the community safety fair.",
      "question": "What is Niko's past occupation according to the statement?",
      "options": [
        "A. Former firefighter",
        "B. Police officer",
        "C. Teacher",
        "D. Paramedic"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tara, who skis competitively, trained on the slopes daily.",
      "conflict_prompt": "Tara, who skis competitively and does not ski at all, trained on the slopes daily.",
      "question": "What activity does Tara engage in competitively according to the statement?",
      "options": [
        "A. Snowboarding",
        "B. Figure skating",
        "C. Competitive skiing",
        "D. Ice hockey"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Vera, a left-leaning political activist, attended the protest in support of education reform.",
      "conflict_prompt": "Vera, a left-leaning political activist and a right-leaning political activist, attended the protest in support of education reform.",
      "question": "Which side of the political spectrum is Vera described as leaning toward?",
      "options": [
        "A. Centrist",
        "B. Right-leaning",
        "C. Left-leaning",
        "D. Apolitical"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector, who is colorblind, asked for a labeled map instead of relying on colors.",
      "conflict_prompt": "Hector, who is colorblind and has normal color vision, asked for a labeled map instead of relying on colors.",
      "question": "What visual condition does Hector have according to the statement?",
      "options": [
        "A. Nearsightedness",
        "B. Colorblindness",
        "C. Astigmatism",
        "D. Perfect vision"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amira, a practicing Buddhist, visited the temple for a meditation retreat.",
      "conflict_prompt": "Amira, a practicing Buddhist and a practicing Christian, visited the temple for a meditation retreat.",
      "question": "Which religion is Amira described as practicing according to the statement?",
      "options": [
        "A. Islam",
        "B. Christianity",
        "C. Buddhism",
        "D. Hinduism"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Quinn, a commuter from Zone 3, purchased a Zone 3 monthly transit pass.",
      "conflict_prompt": "Quinn, a commuter from Zone 3 and Zone 1, purchased a Zone 3 monthly transit pass.",
      "question": "Which transit zone does Quinn come from according to the statement?",
      "options": [
        "A. Zone 1",
        "B. Zone 2",
        "C. Zone 3",
        "D. Zone 4"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sora, an only public school teacher in the district, taught fifth grade.",
      "conflict_prompt": "Sora, an only public school teacher and a private tutor with no school employment, taught fifth grade.",
      "question": "Where does Sora teach according to the statement?",
      "options": [
        "A. Private tutoring center",
        "B. Public school",
        "C. Homeschool co-op",
        "D. Online only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ibrahim, who is a non-driver, used public transportation to commute.",
      "conflict_prompt": "Ibrahim, who is a non-driver and a licensed driver, used public transportation to commute.",
      "question": "Does Ibrahim drive according to the statement?",
      "options": [
        "A. Yes, he drives",
        "B. No, he is a non-driver",
        "C. Occasionally",
        "D. Only rideshare"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, who is fluent in Mandarin, frequently interprets for visiting delegations.",
      "conflict_prompt": "Marta, who is fluent in Mandarin and does not speak Mandarin, frequently interprets for visiting delegations.",
      "question": "What language is Marta fluent in according to the statement?",
      "options": [
        "A. Cantonese",
        "B. Mandarin",
        "C. Korean",
        "D. Vietnamese"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noel, a full-time university lecturer, teaches four courses this semester.",
      "conflict_prompt": "Noel, a full-time university lecturer and an adjunct with no full-time role, teaches four courses this semester.",
      "question": "What is Noel's employment type according to the statement?",
      "options": [
        "A. Adjunct instructor",
        "B. Graduate assistant",
        "C. Full-time university lecturer",
        "D. Visiting scholar"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tess, who works remotely, joined the meeting from her home office.",
      "conflict_prompt": "Tess, who works remotely and exclusively on-site, joined the meeting from her home office.",
      "question": "Where does Tess work according to the statement?",
      "options": [
        "A. On-site only",
        "B. Remotely",
        "C. Hybrid schedule",
        "D. Freelance"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Oksana, a single mother, raised her child alone after the divorce.",
      "conflict_prompt": "Oksana, a single mother and a married mother, raised her child alone after the divorce.",
      "question": "What is Oksana's parental status according to the statement?",
      "options": [
        "A. Married mother",
        "B. Single mother",
        "C. Foster parent",
        "D. Guardian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael, who is left-footed, prefers taking free kicks with his left foot.",
      "conflict_prompt": "Rafael, who is left-footed and right-footed, prefers taking free kicks with his left foot.",
      "question": "Which foot does Rafael prefer for free kicks according to the statement?",
      "options": [
        "A. Both equally",
        "B. Right foot",
        "C. Left foot",
        "D. Neither"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Keiko, the head chef, designed the new seasonal menu.",
      "conflict_prompt": "Keiko, the head chef and a line cook, designed the new seasonal menu.",
      "question": "What is Keiko's position in the kitchen according to the statement?",
      "options": [
        "A. Dishwasher",
        "B. Line cook",
        "C. Head chef",
        "D. Host"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Liam, a full-time software engineer, works 40 hours per week.",
      "conflict_prompt": "Liam, a full-time software engineer and a freelancer with no full-time hours, works 40 hours per week.",
      "question": "What employment status does Liam have according to the statement?",
      "options": [
        "A. Part-time intern",
        "B. Freelancer only",
        "C. Full-time software engineer",
        "D. Unemployed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ava, who does not consume caffeine, brought herbal tea to the meeting.",
      "conflict_prompt": "Ava, who does not consume caffeine and regularly drinks coffee, brought herbal tea to the meeting.",
      "question": "Does Ava consume caffeine according to the statement?",
      "options": [
        "A. Yes, regularly",
        "B. No, she does not",
        "C. Only energy drinks",
        "D. Only decaf"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hugo, a licensed architect, designed the new community center.",
      "conflict_prompt": "Hugo, a licensed architect and an unlicensed designer, designed the new community center.",
      "question": "What professional credential does Hugo hold according to the statement?",
      "options": [
        "A. Licensed architect",
        "B. Civil engineer",
        "C. Interior decorator",
        "D. Landscape artist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zoe, who prefers numeric puzzles, solved a Sudoku in under ten minutes.",
      "conflict_prompt": "Zoe, who prefers numeric puzzles and prefers word puzzles exclusively, solved a Sudoku in under ten minutes.",
      "question": "What kind of puzzles does Zoe prefer according to the statement?",
      "options": [
        "A. Word puzzles",
        "B. Jigsaw puzzles",
        "C. Numeric puzzles",
        "D. Logic grid puzzles"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pablo, who is a non-drinker, ordered a sparkling water at dinner.",
      "conflict_prompt": "Pablo, who is a non-drinker and an occasional drinker, ordered a sparkling water at dinner.",
      "question": "What is Pablo's drinking habit according to the statement?",
      "options": [
        "A. Heavy drinker",
        "B. Non-drinker",
        "C. Occasional drinker",
        "D. Social drinker"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Salma, who owns only one passport from Country A, traveled with that passport.",
      "conflict_prompt": "Salma, who owns only one passport from Country A and also holds a passport from Country B, traveled with that passport.",
      "question": "Which passport does Salma own according to the statement?",
      "options": [
        "A. Passport from Country B",
        "B. Passport from Country A",
        "C. No passport",
        "D. Refugee travel document"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ilan, a daytime radio host, broadcasts the morning show every weekday.",
      "conflict_prompt": "Ilan, a daytime radio host and a late-night DJ, broadcasts the morning show every weekday.",
      "question": "When does Ilan host the radio show according to the statement?",
      "options": [
        "A. Late night",
        "B. Weekends only",
        "C. Morning (daytime)",
        "D. Evening drive time"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mina, who is left-eye dominant, lines up her camera using her left eye.",
      "conflict_prompt": "Mina, who is left-eye dominant and right-eye dominant, lines up her camera using her left eye.",
      "question": "Which eye is Mina dominant with according to the statement?",
      "options": [
        "A. Right eye",
        "B. Left eye",
        "C. Both equally",
        "D. Neither"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dalia, a primary school principal, approved the new curriculum for elementary grades.",
      "conflict_prompt": "Dalia, a primary school principal and a college dean, approved the new curriculum for elementary grades.",
      "question": "What educational role does Dalia hold according to the statement?",
      "options": [
        "A. College dean",
        "B. Primary school principal",
        "C. High school counselor",
        "D. College student"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felipe, who is retired, enjoys gardening in his free time.",
      "conflict_prompt": "Felipe, who is retired and currently employed full-time, enjoys gardening in his free time.",
      "question": "What is Felipe's employment status according to the statement?",
      "options": [
        "A. Full-time employed",
        "B. Student",
        "C. Retired",
        "D. Self-employed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sienna, a left-lane driver on the highway, maintained a steady left-lane speed.",
      "conflict_prompt": "Sienna, a left-lane driver and a right-lane driver, maintained a steady left-lane speed.",
      "question": "Which highway lane does Sienna primarily use according to the statement?",
      "options": [
        "A. Right lane",
        "B. Middle lane",
        "C. Left lane",
        "D. Emergency lane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Andre, a vegetarian marathon runner, consumes plant-based energy bars during races.",
      "conflict_prompt": "Andre, a vegetarian marathon runner and a meat-eating athlete, consumes plant-based energy bars during races.",
      "question": "What diet does Andre follow according to the statement?",
      "options": [
        "A. Carnivore",
        "B. Vegetarian",
        "C. Keto",
        "D. Paleo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rosa, who celebrates New Year's Eve at midnight, hosted friends for a countdown party.",
      "conflict_prompt": "Rosa, who celebrates New Year's Eve at midnight and at noon, hosted friends for a countdown party.",
      "question": "When does Rosa celebrate New Year's Eve according to the statement?",
      "options": [
        "A. Noon",
        "B. Midnight",
        "C. She does not celebrate",
        "D. At sunrise"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ken, an only bilingual speaker of English, conducted the tour in English.",
      "conflict_prompt": "Ken, an only bilingual speaker of English and also speaks Spanish, conducted the tour in English.",
      "question": "What language did Ken conduct the tour in according to the statement?",
      "options": [
        "A. Spanish",
        "B. French",
        "C. English",
        "D. German"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aria, a non-driver who uses public transit, bought a monthly bus pass.",
      "conflict_prompt": "Aria, a non-driver and a car owner who drives daily, bought a monthly bus pass.",
      "question": "What does Aria use for transportation according to the statement?",
      "options": [
        "A. Personal car",
        "B. Public transit",
        "C. Bicycle exclusively",
        "D. Walks only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Selim, who is an only child, received a single inheritance from his parents.",
      "conflict_prompt": "Selim, who is an only child and one of several siblings, received a single inheritance from his parents.",
      "question": "How many siblings does Selim have according to the statement?",
      "options": [
        "A. Several siblings",
        "B. One sibling",
        "C. None (only child)",
        "D. The statement doesn't say"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jun, a certified yoga instructor, leads the morning yoga class.",
      "conflict_prompt": "Jun, a certified yoga instructor and someone with no yoga certification, leads the morning yoga class.",
      "question": "What certification does Jun hold according to the statement?",
      "options": [
        "A. No certification",
        "B. Certified yoga instructor",
        "C. Fitness bootcamp trainer",
        "D. Massage therapist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, who prefers savory snacks, brought cheese and crackers to the movie night.",
      "conflict_prompt": "Marta, who prefers savory snacks and prefers sweet snacks exclusively, brought cheese and crackers to the movie night.",
      "question": "What snack preference does Marta have according to the statement?",
      "options": [
        "A. Sweet snacks",
        "B. Savory snacks",
        "C. No preference",
        "D. Only fruit"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Akio, a daytime security guard, worked the morning shift at the museum.",
      "conflict_prompt": "Akio, a daytime security guard and a nighttime security guard, worked the morning shift at the museum.",
      "question": "Which shift did Akio work at the museum according to the statement?",
      "options": [
        "A. Night shift",
        "B. Morning (daytime) shift",
        "C. Evening shift",
        "D. He did not work"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bea, who does not own a smartphone, uses a basic mobile phone instead.",
      "conflict_prompt": "Bea, who does not own a smartphone and owns a smartphone, uses a basic mobile phone instead.",
      "question": "Does Bea own a smartphone according to the statement?",
      "options": [
        "A. Yes",
        "B. No",
        "C. Only a tablet",
        "D. The statement doesn't say"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samir, who is a non-smoker, requested a non-smoking apartment lease.",
      "conflict_prompt": "Samir, who is a non-smoker and a habitual smoker, requested a non-smoking apartment lease.",
      "question": "What smoking status does Samir have according to the statement?",
      "options": [
        "A. Smoker",
        "B. Non-smoker",
        "C. Vaper",
        "D. Social smoker"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olive, who has type O blood, donated blood at the drive.",
      "conflict_prompt": "Olive, who has type O blood and type A blood, donated blood at the drive.",
      "question": "What blood type does Olive have according to the statement?",
      "options": [
        "A. Type AB",
        "B. Type A",
        "C. Type O",
        "D. Type B"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jonas, who is allergic to shellfish, avoided the seafood buffet.",
      "conflict_prompt": "Jonas, who is allergic to shellfish and not allergic to shellfish, avoided the seafood buffet.",
      "question": "What allergy does Jonas have according to the statement?",
      "options": [
        "A. Shellfish allergy",
        "B. Nut allergy",
        "C. No allergies",
        "D. Dairy allergy"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Keisha, an experienced mountain climber, summited the peak without incident.",
      "conflict_prompt": "Keisha, an experienced mountain climber and someone who has never climbed, summited the peak without incident.",
      "question": "What is Keisha's climbing experience according to the statement?",
      "options": [
        "A. Never climbed",
        "B. Some experience",
        "C. Experienced mountain climber",
        "D. Expert skier"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar, who prefers winter sports, signed up for the skiing workshop.",
      "conflict_prompt": "Omar, who prefers winter sports and prefers summer sports exclusively, signed up for the skiing workshop.",
      "question": "What season's sports does Omar prefer according to the statement?",
      "options": [
        "A. Summer sports",
        "B. Winter sports",
        "C. Spring sports",
        "D. Autumn sports"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dina, a registered voter in District 5, received her ballot by mail.",
      "conflict_prompt": "Dina, a registered voter in District 5 and in District 2, received her ballot by mail.",
      "question": "In which district is Dina registered according to the statement?",
      "options": [
        "A. District 2",
        "B. District 3",
        "C. District 5",
        "D. District 7"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rico, a certified lifeguard, supervised swimmers at the community pool.",
      "conflict_prompt": "Rico, a certified lifeguard and someone with no lifeguard training, supervised swimmers at the community pool.",
      "question": "What certification does Rico have according to the statement?",
      "options": [
        "A. No certification",
        "B. CPR only",
        "C. Certified lifeguard",
        "D. Fire safety"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ellen, an ardent cyclist, commutes 15 miles by bike each day.",
      "conflict_prompt": "Ellen, an ardent cyclist and someone who never bikes, commutes 15 miles by bike each day.",
      "question": "How does Ellen commute according to the statement?",
      "options": [
        "A. By car",
        "B. By bicycle",
        "C. By bus",
        "D. By train"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gina, who is ambidextrous, can write legibly with either hand.",
      "conflict_prompt": "Gina, who is ambidextrous and strictly right-handed, can write legibly with either hand.",
      "question": "What is Gina's handedness according to the statement?",
      "options": [
        "A. Strictly right-handed",
        "B. Strictly left-handed",
        "C. Ambidextrous",
        "D. Uses only foot"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector, who works as a morning baker, arrives at the bakery at 3:00 a.m.",
      "conflict_prompt": "Hector, who works as a morning baker and an evening shift cook, arrives at the bakery at 3:00 a.m.",
      "question": "When does Hector start work according to the statement?",
      "options": [
        "A. Afternoon",
        "B. Morning (3:00 a.m.)",
        "C. Evening",
        "D. Night (11:00 p.m.)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia, who is registered as a Democrat, donated to the Democratic campaign.",
      "conflict_prompt": "Nadia, who is registered as a Democrat and registered as a Republican, donated to the Democratic campaign.",
      "question": "What party registration does Nadia have according to the statement?",
      "options": [
        "A. Republican",
        "B. Independent",
        "C. Democrat",
        "D. Green Party"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Yves, a daytime pharmacist, dispensed prescriptions during his shift.",
      "conflict_prompt": "Yves, a daytime pharmacist and a night shift nurse, dispensed prescriptions during his shift.",
      "question": "What is Yves's profession according to the statement?",
      "options": [
        "A. Pharmacist",
        "B. Nurse",
        "C. Doctor",
        "D. Lab technician"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Talia, who is a fluent Italian speaker, taught an Italian language class.",
      "conflict_prompt": "Talia, who is a fluent Italian speaker and someone who does not speak Italian, taught an Italian language class.",
      "question": "Which language does Talia speak fluently according to the statement?",
      "options": [
        "A. Spanish",
        "B. Italian",
        "C. Portuguese",
        "D. Russian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Vikram, a full-time paramedic, responded to the emergency call at noon.",
      "conflict_prompt": "Vikram, a full-time paramedic and an office clerk, responded to the emergency call at noon.",
      "question": "What is Vikram's primary job according to the statement?",
      "options": [
        "A. Office clerk",
        "B. Full-time paramedic",
        "C. Volunteer",
        "D. Security guard"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sana, who is a non-smoker, chose a non-smoking table at the cafe.",
      "conflict_prompt": "Sana, who is a non-smoker and a smoker, chose a non-smoking table at the cafe.",
      "question": "Does Sana smoke according to the statement?",
      "options": [
        "A. Yes",
        "B. No",
        "C. Only socially",
        "D. Occasionally"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivor, a lifelong resident of Dublin, knows the city's neighborhoods well.",
      "conflict_prompt": "Ivor, a lifelong resident of Dublin and someone who recently moved in, knows the city's neighborhoods well.",
      "question": "How long has Ivor lived in Dublin according to the statement?",
      "options": [
        "A. Recently moved",
        "B. Lived there for a year",
        "C. Lifelong resident",
        "D. Visits occasionally"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priam, who is blood type AB, was called as a universal recipient during the drive.",
      "conflict_prompt": "Priam, who is blood type AB and blood type O, was called as a universal recipient during the drive.",
      "question": "What blood type is Priam according to the statement?",
      "options": [
        "A. Type A",
        "B. Type O",
        "C. Type B",
        "D. Type AB"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, a pediatric dentist, treated children at the clinic on Tuesday.",
      "conflict_prompt": "Maya, a pediatric dentist and a general dentist with no pediatric training, treated children at the clinic on Tuesday.",
      "question": "What specialty does Maya practice according to the statement?",
      "options": [
        "A. Orthodontics",
        "B. Pediatric dentistry",
        "C. Oral surgery",
        "D. General medicine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lin, who considers herself an introvert, left the crowded party early.",
      "conflict_prompt": "Lin, who considers herself an introvert and an extrovert, left the crowded party early.",
      "question": "How does Lin describe her personality according to the statement?",
      "options": [
        "A. Extrovert",
        "B. Ambivert",
        "C. Introvert",
        "D. Assertive"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ola, who works at the town library, shelved new books on Monday.",
      "conflict_prompt": "Ola, who works at the town library and does not work there, shelved new books on Monday.",
      "question": "Where does Ola work according to the statement?",
      "options": [
        "A. Town library",
        "B. Retail store",
        "C. Hospital",
        "D. She does not work"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hana, a vegetarian chef, refused to include meat options on the tasting menu.",
      "conflict_prompt": "Hana, a vegetarian chef and a chef who specializes in meats, refused to include meat options on the tasting menu.",
      "question": "What type of chef is Hana according to the statement?",
      "options": [
        "A. Vegetarian chef",
        "B. Butcher",
        "C. Barbecue specialist",
        "D. Seafood chef"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jules, who is legally blind, reads Braille for written materials.",
      "conflict_prompt": "Jules, who is legally blind and has normal vision, reads Braille for written materials.",
      "question": "What condition does Jules have according to the statement?",
      "options": [
        "A. Colorblindness",
        "B. Legally blind",
        "C. Nearsightedness",
        "D. 20/20 vision"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rana, who is a retiree, volunteers at the community center during weekdays.",
      "conflict_prompt": "Rana, who is a retiree and currently employed full-time, volunteers at the community center during weekdays.",
      "question": "What is Rana's employment status according to the statement?",
      "options": [
        "A. Employed full-time",
        "B. Student",
        "C. Retiree",
        "D. Unemployed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Piers, a left-lane cyclist, maintained a steady left-lane position on the road.",
      "conflict_prompt": "Piers, a left-lane cyclist and a right-lane cyclist, maintained a steady left-lane position on the road.",
      "question": "Which lane does Piers use according to the statement?",
      "options": [
        "A. Right lane",
        "B. Left lane",
        "C. Center lane",
        "D. Bike path only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amal, who is a vegetarian by religion, abstains from eating meat for spiritual reasons.",
      "conflict_prompt": "Amal, who is a vegetarian by religion and eats meat regularly, abstains from eating meat for spiritual reasons.",
      "question": "Why does Amal avoid eating meat according to the statement?",
      "options": [
        "A. Health reasons",
        "B. Religious reasons (vegetarian)",
        "C. Allergies",
        "D. Taste preference"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tobias, who has a PhD in physics, teaches graduate-level courses in quantum mechanics.",
      "conflict_prompt": "Tobias, who has a PhD in physics and no advanced degree, teaches graduate-level courses in quantum mechanics.",
      "question": "What degree does Tobias hold according to the statement?",
      "options": [
        "A. Bachelor's degree",
        "B. Master's degree",
        "C. PhD in physics",
        "D. No degree"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Imani, who is insured through Company X, filed a claim with Company X last week.",
      "conflict_prompt": "Imani, who is insured through Company X and not insured at all, filed a claim with Company X last week.",
      "question": "Which insurance company covers Imani according to the statement?",
      "options": [
        "A. Company Y",
        "B. Company X",
        "C. None",
        "D. Government only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ruben, who prefers digital books, borrowed an e-book from the library.",
      "conflict_prompt": "Ruben, who prefers digital books and prefers physical books exclusively, borrowed an e-book from the library.",
      "question": "What format does Ruben prefer according to the statement?",
      "options": [
        "A. Physical books only",
        "B. Audiobooks",
        "C. Digital books",
        "D. Newspapers"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Penny, a single occupant in her apartment, pays the rent herself every month.",
      "conflict_prompt": "Penny, a single occupant and someone who shares the apartment with roommates, pays the rent herself every month.",
      "question": "Does Penny have roommates according to the statement?",
      "options": [
        "A. Yes, multiple roommates",
        "B. Yes, one roommate",
        "C. No, single occupant",
        "D. The statement doesn't say"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hiro, who is ambidextrous, uses either hand when drawing sketches.",
      "conflict_prompt": "Hiro, who is ambidextrous and strictly left-handed, uses either hand when drawing sketches.",
      "question": "What is Hiro's handedness according to the statement?",
      "options": [
        "A. Strictly left-handed",
        "B. Strictly right-handed",
        "C. Ambidextrous",
        "D. Prosthetic user"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samantha is a vegetarian who ordered a salad for dinner.",
      "conflict_prompt": "Samantha is a vegetarian who ordered a steak for dinner.",
      "question": "What is Samantha's diet?",
      "options": [
        "A. Omnivore",
        "B. Vegetarian",
        "C. Pescatarian",
        "D. Vegan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carlos, a certified lifeguard, monitored swimmers at the public pool.",
      "conflict_prompt": "Carlos, a certified accountant, monitored swimmers at the public pool.",
      "question": "What is Carlos's certification?",
      "options": [
        "A. Paramedic",
        "B. Swimming coach",
        "C. Lifeguard",
        "D. Firefighter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Emma, who is left-handed, writes her notes with her left hand.",
      "conflict_prompt": "Emma, who is right-handed, writes her notes with her left hand.",
      "question": "Which hand does Emma use to write?",
      "options": [
        "A. Right hand",
        "B. Both hands",
        "C. Left hand",
        "D. Neither hand"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dr. Patel is a pediatrician who treats children at the clinic.",
      "conflict_prompt": "Dr. Patel is a surgeon who treats children at the clinic.",
      "question": "What is Dr. Patel's medical specialty?",
      "options": [
        "A. Dentist",
        "B. Pediatrician",
        "C. Cardiologist",
        "D. Dermatologist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olivia is an only child who has no siblings.",
      "conflict_prompt": "Olivia is an only child who has an older sister.",
      "question": "Does Olivia have siblings?",
      "options": [
        "A. Yes, multiple siblings",
        "B. Yes, one sibling",
        "C. No, she has no siblings",
        "D. Unsure"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus is the team's goalkeeper who defends the net for the soccer club.",
      "conflict_prompt": "Marcus is the team's striker who defends the net for the soccer club.",
      "question": "What position does Marcus play on the soccer team?",
      "options": [
        "A. Midfielder",
        "B. Defender",
        "C. Goalkeeper",
        "D. Striker"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nina speaks fluent French and often translates documents from French to English.",
      "conflict_prompt": "Nina speaks fluent Spanish and often translates documents from French to English.",
      "question": "Which language is Nina fluent in?",
      "options": [
        "A. German",
        "B. Spanish",
        "C. French",
        "D. Japanese"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The family car runs on electricity and is charged overnight at home.",
      "conflict_prompt": "The family car runs on diesel and is charged overnight at home.",
      "question": "What type of fuel does the family car use?",
      "options": [
        "A. Gasoline",
        "B. Diesel",
        "C. Electricity",
        "D. Hydrogen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Professor Lin teaches history courses at the university each semester.",
      "conflict_prompt": "Professor Lin teaches physics courses at the university each semester.",
      "question": "What subject does Professor Lin teach?",
      "options": [
        "A. Biology",
        "B. History",
        "C. Mathematics",
        "D. Chemistry"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Avery is allergic to peanuts and avoids any food containing them.",
      "conflict_prompt": "Avery is allergic to peanuts but eats peanut butter sandwiches daily.",
      "question": "To which food allergen does Avery react?",
      "options": [
        "A. Shellfish",
        "B. Tree nuts",
        "C. Peanuts",
        "D. Lactose"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The apartment building has a no-pets policy and residents are not allowed to keep animals.",
      "conflict_prompt": "The apartment building has a no-pets policy but everyone keeps dogs in their units.",
      "question": "What is the building's policy on pets?",
      "options": [
        "A. Pets allowed with a fee",
        "B. Only fish allowed",
        "C. No pets allowed",
        "D. Pets allowed with landlord approval"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan holds a valid pilot's license and flies small airplanes for recreation.",
      "conflict_prompt": "Ethan holds a valid driver's license and flies small airplanes for recreation.",
      "question": "What license does Ethan have for flying planes?",
      "options": [
        "A. Sailing license",
        "B. Pilot's license",
        "C. Driver's license",
        "D. Fishing license"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya is a vegetarian chef who prepares meat-free meals at her restaurant.",
      "conflict_prompt": "Maya is a barbecue chef who prepares meat-free meals at her restaurant.",
      "question": "What type of cuisine does Maya specialize in at her restaurant?",
      "options": [
        "A. Vegan pastries",
        "B. Barbecue",
        "C. Meat-free dishes",
        "D. Seafood"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The research lab stores all flammable chemicals in a locked flammable cabinet.",
      "conflict_prompt": "The research lab stores all flammable chemicals in the unlocked office filing cabinet.",
      "question": "Where are the flammable chemicals kept?",
      "options": [
        "A. In unlocked desk drawers",
        "B. In the freezer",
        "C. In a locked flammable cabinet",
        "D. On open shelving"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jordan, a marathon runner, trains by running twenty miles on weekends.",
      "conflict_prompt": "Jordan, a casual walker, trains by running twenty miles on weekends.",
      "question": "What type of athlete is Jordan?",
      "options": [
        "A. Sprinter",
        "B. Swimmer",
        "C. Marathon runner",
        "D. Cyclist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum is closed on Mondays and does not admit visitors that day.",
      "conflict_prompt": "The museum is closed on Mondays but allows bus tours every Monday.",
      "question": "On which day is the museum closed to visitors?",
      "options": [
        "A. Sunday",
        "B. Monday",
        "C. Wednesday",
        "D. Friday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Isabel, who is fluent in Mandarin, interprets for Chinese delegations.",
      "conflict_prompt": "Isabel, who is fluent in Portuguese, interprets for Chinese delegations.",
      "question": "Which language does Isabel speak fluently?",
      "options": [
        "A. Spanish",
        "B. Mandarin",
        "C. Arabic",
        "D. Russian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery bakes only gluten-free bread and advertises itself as gluten-free.",
      "conflict_prompt": "The bakery bakes only regular wheat bread and advertises itself as gluten-free.",
      "question": "What type of bread does the bakery specialize in?",
      "options": [
        "A. Sourdough made with wheat",
        "B. Gluten-free bread",
        "C. Rye bread",
        "D. Multigrain"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Liam is a night shift nurse who works overnight at the hospital.",
      "conflict_prompt": "Liam is a day shift receptionist who works overnight at the hospital.",
      "question": "When does Liam work at the hospital?",
      "options": [
        "A. Morning shifts",
        "B. Afternoon shifts",
        "C. Overnight",
        "D. Weekends only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The conference room is reserved for the marketing team from 2 pm to 4 pm.",
      "conflict_prompt": "The conference room is reserved for the janitorial staff from 2 pm to 4 pm.",
      "question": "Which team reserved the conference room from 2 pm to 4 pm?",
      "options": [
        "A. Legal team",
        "B. Marketing team",
        "C. IT team",
        "D. Janitorial staff"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah is a twin and shares a birthday with her twin sister.",
      "conflict_prompt": "Hannah is a twin but does not share a birthday with her twin sister.",
      "question": "What is true about Hannah's birth status?",
      "options": [
        "A. Only child",
        "B. Triplet",
        "C. Twin",
        "D. Adopted"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The canoe is designed for two paddlers and has two seats.",
      "conflict_prompt": "The canoe is designed for solo paddlers and has two seats.",
      "question": "How many paddlers is the canoe designed for?",
      "options": [
        "A. One",
        "B. Two",
        "C. Four",
        "D. Six"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah is a licensed electrician who installs household wiring.",
      "conflict_prompt": "Noah is a licensed plumber who installs household wiring.",
      "question": "What trade is Noah licensed in?",
      "options": [
        "A. Carpentry",
        "B. Plumbing",
        "C. Electrical work",
        "D. Landscaping"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Grace is a practicing Buddhist who attends meditation sessions weekly.",
      "conflict_prompt": "Grace is a practicing Christian who attends meditation sessions weekly.",
      "question": "What religion does Grace practice?",
      "options": [
        "A. Islam",
        "B. Judaism",
        "C. Buddhism",
        "D. Christianity"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The shipping label lists the destination country as Canada.",
      "conflict_prompt": "The shipping label lists the destination country as Australia.",
      "question": "To which country is the package being sent according to the label?",
      "options": [
        "A. United States",
        "B. Canada",
        "C. Mexico",
        "D. France"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zoe drinks only decaffeinated coffee and never has caffeinated coffee.",
      "conflict_prompt": "Zoe drinks only decaffeinated coffee but starts her day with an espresso.",
      "question": "What type of coffee does Zoe drink according to the statement?",
      "options": [
        "A. Only caffeinated coffee",
        "B. Espresso only",
        "C. Only decaffeinated coffee",
        "D. No coffee at all"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The toddler cannot walk yet and uses a baby walker for mobility.",
      "conflict_prompt": "The toddler walks unaided and uses a baby walker for mobility.",
      "question": "What is the toddler's current mobility status?",
      "options": [
        "A. Walks unaided",
        "B. Uses a wheelchair",
        "C. Cannot walk yet",
        "D. Climbs stairs independently"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The green houseplant requires indirect sunlight and is kept away from windows.",
      "conflict_prompt": "The green houseplant requires full direct sunlight and is kept away from windows.",
      "question": "What kind of light does the houseplant need?",
      "options": [
        "A. No light",
        "B. Indirect sunlight",
        "C. Full direct sunlight",
        "D. Artificial light only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "David is a vegetarian firefighter who follows a meat-free diet despite his job.",
      "conflict_prompt": "David is a firefighter who follows a carnivorous diet that excludes plants.",
      "question": "Which dietary choice does David follow?",
      "options": [
        "A. Vegan",
        "B. Vegetarian",
        "C. Pescatarian",
        "D. Carnivorous diet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The laptop's battery is removable and can be swapped by the user.",
      "conflict_prompt": "The laptop's battery is built-in and can be swapped by the user.",
      "question": "What is true about the laptop battery?",
      "options": [
        "A. It is solar-powered",
        "B. It is non-removable",
        "C. It is removable",
        "D. It uses AA batteries"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector is the youngest sibling in his family and has three older sisters.",
      "conflict_prompt": "Hector is the oldest sibling in his family and has three older sisters.",
      "question": "Which sibling position does Hector hold?",
      "options": [
        "A. Middle child",
        "B. Youngest",
        "C. Only child",
        "D. Oldest"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rachel is a licensed scuba diver who dives in open water regularly.",
      "conflict_prompt": "Rachel is a certified knitting instructor who dives in open water regularly.",
      "question": "What certification does Rachel hold related to diving?",
      "options": [
        "A. Lifeguard",
        "B. Scuba diver",
        "C. Sailing captain",
        "D. Ski instructor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The train bound for Boston departs at 9:30 am from Platform 4.",
      "conflict_prompt": "The train bound for Boston departs at 9:30 am from Platform 1.",
      "question": "From which platform does the Boston train depart at 9:30 am?",
      "options": [
        "A. Platform 1",
        "B. Platform 2",
        "C. Platform 3",
        "D. Platform 4"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amira is a fluent sign language user who communicates using American Sign Language.",
      "conflict_prompt": "Amira is a fluent speaker of Portuguese who communicates using American Sign Language.",
      "question": "Which visual language does Amira use to communicate?",
      "options": [
        "A. British Sign Language",
        "B. American Sign Language",
        "C. Italian",
        "D. Portuguese"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The newborn kitten is a male and has been named Max.",
      "conflict_prompt": "The newborn kitten is a female and has been named Max.",
      "question": "What is the kitten's sex according to the statement?",
      "options": [
        "A. Male",
        "B. Female",
        "C. Unknown",
        "D. Neutered"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The painter uses oil-based paints exclusively for his landscape works.",
      "conflict_prompt": "The painter uses water-based paints exclusively for his landscape works.",
      "question": "What type of paint does the painter use for landscapes?",
      "options": [
        "A. Oil-based paints",
        "B. Watercolor",
        "C. Acrylic",
        "D. Tempera"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Toby is a vegetarian dog owner who feeds his dog a meat-free diet.",
      "conflict_prompt": "Toby is a vegetarian dog owner who feeds his dog raw meat every day.",
      "question": "What kind of diet does Toby feed his dog according to the clean statement?",
      "options": [
        "A. Raw meat",
        "B. Meat-free diet",
        "C. High-protein meat diet",
        "D. Fish-only diet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The volunteer ambulance crew are trained paramedics who provide emergency care.",
      "conflict_prompt": "The volunteer ambulance crew are trained librarians who provide emergency care.",
      "question": "What training do the volunteer ambulance crew have?",
      "options": [
        "A. Librarianship",
        "B. Paramedic training",
        "C. Teaching certification",
        "D. Security training"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olga owns a sailboat that is intended for coastal sailing only.",
      "conflict_prompt": "Olga owns a motor yacht that is intended for coastal sailing only.",
      "question": "What type of vessel does Olga own?",
      "options": [
        "A. Submarine",
        "B. Kayak",
        "C. Sailboat",
        "D. Cargo ship"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The newborn baby was delivered at 7:15 am on a Tuesday morning.",
      "conflict_prompt": "The newborn baby was delivered at 7:15 pm on a Tuesday morning.",
      "question": "At what time was the baby born?",
      "options": [
        "A. 7:15 am",
        "B. Noon",
        "C. 7:15 pm",
        "D. Midnight"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The historical document is written in Latin and requires translation for modern readers.",
      "conflict_prompt": "The historical document is written in modern English and requires translation for modern readers.",
      "question": "In what language is the historical document written?",
      "options": [
        "A. Old Norse",
        "B. Latin",
        "C. Modern English",
        "D. Arabic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah commutes to work by bicycle and parks it at the company bike rack.",
      "conflict_prompt": "Noah commutes to work by car and parks it at the company bike rack.",
      "question": "How does Noah commute to work according to the statement?",
      "options": [
        "A. By subway",
        "B. By bicycle",
        "C. By train",
        "D. By car"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya is a vegetarian chef who refuses to prepare meat dishes in the kitchen.",
      "conflict_prompt": "Priya is a butcher who refuses to prepare meat dishes in the kitchen.",
      "question": "What type of cook is Priya?",
      "options": [
        "A. Butcher",
        "B. Vegetarian chef",
        "C. Pastry chef",
        "D. Sushi chef"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The emergency exit is clearly marked and kept unobstructed at all times.",
      "conflict_prompt": "The emergency exit is clearly marked but permanently blocked by boxes.",
      "question": "What is the status of the emergency exit according to the clean statement?",
      "options": [
        "A. Obstructed regularly",
        "B. Not marked",
        "C. Clearly marked and unobstructed",
        "D. Locked at all times"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Holly is a vegetarian who keeps a strict plant-based diet year-round.",
      "conflict_prompt": "Holly is a vegetarian who celebrates holidays with roast turkey dinners every year.",
      "question": "What is Holly's dietary habit?",
      "options": [
        "A. Omnivore",
        "B. Vegetarian",
        "C. Pescatarian",
        "D. Carnivore"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The art gallery only displays original paintings and does not sell reproductions.",
      "conflict_prompt": "The art gallery only displays original paintings but sells mass-produced prints at the front desk.",
      "question": "What does the gallery display according to the statement?",
      "options": [
        "A. Only photographs",
        "B. Only sculptures",
        "C. Original paintings",
        "D. Mass-produced posters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Luca is a vegan athlete who consumes no animal products to support training.",
      "conflict_prompt": "Luca is a vegan athlete who eats grilled chicken after every practice.",
      "question": "Which diet does Luca follow for athletic training?",
      "options": [
        "A. Keto",
        "B. Vegan",
        "C. Paleo",
        "D. High-meat diet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's signature cake contains eggs and cannot be served to vegan customers.",
      "conflict_prompt": "The bakery's signature cake contains no eggs but cannot be served to vegan customers.",
      "question": "What ingredient does the bakery's signature cake contain according to the clean statement?",
      "options": [
        "A. Nuts",
        "B. Eggs",
        "C. Soy",
        "D. Coconut"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aiden is a production manager who oversees the factory assembly line every day.",
      "conflict_prompt": "Aiden is a quality inspector who oversees the factory assembly line every day.",
      "question": "What is Aiden's job role?",
      "options": [
        "A. Shipping clerk",
        "B. Production manager",
        "C. Quality inspector",
        "D. Sales representative"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain trail is rated as beginner-friendly and has gentle slopes throughout.",
      "conflict_prompt": "The mountain trail is rated as expert-only and has gentle slopes throughout.",
      "question": "How difficult is the mountain trail according to the clean statement?",
      "options": [
        "A. Beginner-friendly",
        "B. Intermediate",
        "C. Expert-only",
        "D. Closed"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta is a practicing vegetarian who runs a butchery shop that sells only plant-based meats.",
      "conflict_prompt": "Marta is a butcher who runs a butchery shop that sells only plant-based meats.",
      "question": "What does Marta's shop sell according to the statement?",
      "options": [
        "A. Fresh seafood",
        "B. Plant-based meats",
        "C. Live poultry",
        "D. Dairy products"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The new smartphone model uses a USB-C charging port for power and data.",
      "conflict_prompt": "The new smartphone model uses a proprietary connector for power and data and a USB-C charging port.",
      "question": "What type of charging port does the new smartphone use according to the clean statement?",
      "options": [
        "A. Micro-USB",
        "B. Lightning",
        "C. USB-C",
        "D. Wireless only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Kim is a morning person who wakes up at 5:30 am every weekday.",
      "conflict_prompt": "Kim is a night owl who wakes up at 5:30 am every weekday.",
      "question": "What is Kim's preferred daily schedule according to the clean statement?",
      "options": [
        "A. Night owl",
        "B. Morning person",
        "C. Shift worker",
        "D. Irregular sleeper"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The soccer club requires all players to be over 18 to join the adult team.",
      "conflict_prompt": "The soccer club requires all players to be under 18 to join the adult team.",
      "question": "What is the age requirement to join the adult team?",
      "options": [
        "A. Under 16",
        "B. Under 18",
        "C. Over 18",
        "D. Any age"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felix owns a hybrid car that runs on both electricity and gasoline.",
      "conflict_prompt": "Felix owns an electric motorcycle that runs on both electricity and gasoline.",
      "question": "What type of vehicle does Felix own according to the statement?",
      "options": [
        "A. Hybrid car",
        "B. Electric motorcycle",
        "C. Diesel truck",
        "D. Gas bicycle"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The classroom door is locked during exams and students cannot enter late.",
      "conflict_prompt": "The classroom door is unlocked during exams but students cannot enter late.",
      "question": "What is the protocol for the classroom door during exams according to the clean statement?",
      "options": [
        "A. Unlocked and open to all",
        "B. Locked and late entry not allowed",
        "C. Locked but late entry allowed",
        "D. Open for study groups"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hiroshi is a vegetarian who works at a fish market but refuses to taste the products.",
      "conflict_prompt": "Hiroshi is a fishmonger who works at a fish market but refuses to taste the products.",
      "question": "What is Hiroshi's dietary preference or profession according to the clean statement?",
      "options": [
        "A. Vegetarian",
        "B. Fishmonger",
        "C. Chef",
        "D. Butcher"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The office recycling policy requires paper and cardboard to be placed in the blue bin.",
      "conflict_prompt": "The office recycling policy requires electronics to be placed in the blue bin.",
      "question": "Which materials should go in the blue bin according to the office policy?",
      "options": [
        "A. Food waste",
        "B. Paper and cardboard",
        "C. Electronics",
        "D. Confidential documents"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amir is an identical twin who shares the same birthday and birthdate with his twin brother.",
      "conflict_prompt": "Amir is an identical twin who has a different birthday from his twin brother.",
      "question": "What is true about Amir's birth situation?",
      "options": [
        "A. He is triplet",
        "B. He is an identical twin",
        "C. He is an only child",
        "D. He is adopted"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vegetarian restaurant serves only plant-based dishes and no meat products.",
      "conflict_prompt": "The vegetarian restaurant serves only meat dishes and no plant-based options.",
      "question": "What type of food does the restaurant serve according to the clean statement?",
      "options": [
        "A. Meat dishes only",
        "B. Seafood only",
        "C. Plant-based dishes",
        "D. Deli sandwiches"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Alicia's passport lists her nationality as Canadian and she holds Canadian citizenship.",
      "conflict_prompt": "Alicia's passport lists her nationality as Canadian but she holds only British citizenship.",
      "question": "What nationality does Alicia's passport indicate?",
      "options": [
        "A. American",
        "B. British",
        "C. Canadian",
        "D. Australian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Diego is a vegan baker who never uses eggs or dairy in his pastries.",
      "conflict_prompt": "Diego is a pastry chef who never uses eggs or dairy in his pastries.",
      "question": "What diet does Diego follow in his baking according to the clean statement?",
      "options": [
        "A. Uses eggs and dairy",
        "B. Vegetarian",
        "C. Vegan",
        "D. Pescatarian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The hiking trail is closed in winter due to heavy snow and dangerous conditions.",
      "conflict_prompt": "The hiking trail is open year-round despite heavy snow and dangerous conditions.",
      "question": "When is the hiking trail closed according to the clean statement?",
      "options": [
        "A. Summer",
        "B. Spring",
        "C. Winter",
        "D. Autumn"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The baby monitor picks up only sounds in the nursery and is not connected to other rooms.",
      "conflict_prompt": "The baby monitor picks up sounds from all rooms but only in the nursery.",
      "question": "From where does the baby monitor pick up sound according to the clean statement?",
      "options": [
        "A. From the entire house",
        "B. From the garage only",
        "C. From the nursery only",
        "D. From outdoor areas"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Connor is a certified lifeguard who has completed water rescue training.",
      "conflict_prompt": "Connor is a certified scuba instructor who has completed water rescue training.",
      "question": "What certification does Connor hold related to water safety?",
      "options": [
        "A. Lifeguard",
        "B. Scuba instructor",
        "C. Sailing captain",
        "D. Surf instructor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The rental apartment is pet-friendly and allows cats and small dogs.",
      "conflict_prompt": "The rental apartment is non-pet-friendly but allows cats and small dogs.",
      "question": "What is the apartment's pet policy according to the clean statement?",
      "options": [
        "A. No pets allowed",
        "B. Only birds allowed",
        "C. Pet-friendly for cats and small dogs",
        "D. Only service animals allowed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah is a professional pianist who performs classical music at concerts.",
      "conflict_prompt": "Hannah is a professional drummer who performs classical music at concerts.",
      "question": "What instrument does Hannah professionally play according to the clean statement?",
      "options": [
        "A. Violin",
        "B. Guitar",
        "C. Piano",
        "D. Drums"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school requires students to wear uniforms consisting of a white shirt and navy pants.",
      "conflict_prompt": "The school requires students to wear casual clothes consisting of a white shirt and navy pants.",
      "question": "What does the school's dress requirement include according to the clean statement?",
      "options": [
        "A. Casual clothes",
        "B. Uniform with white shirt and navy pants",
        "C. Formal suits",
        "D. Sports attire"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marco is allergic to pollen and experiences sneezing throughout springtime.",
      "conflict_prompt": "Marco is allergic to pollen but never shows allergic symptoms in springtime.",
      "question": "What allergy does Marco have according to the clean statement?",
      "options": [
        "A. Dust mites",
        "B. Pollen",
        "C. Latex",
        "D. Pet dander"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The ferry operates between Island A and the mainland daily during daylight hours.",
      "conflict_prompt": "The ferry operates only at night between Island A and the mainland daily during daylight hours.",
      "question": "When does the ferry operate between Island A and the mainland according to the clean statement?",
      "options": [
        "A. Nighttime only",
        "B. Weekends only",
        "C. Daily during daylight hours",
        "D. Once a month"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lucia is a certified yoga instructor who teaches Hatha yoga classes weekly.",
      "conflict_prompt": "Lucia is a certified CrossFit coach who teaches Hatha yoga classes weekly.",
      "question": "What type of classes does Lucia teach according to the clean statement?",
      "options": [
        "A. CrossFit",
        "B. Hatha yoga",
        "C. Ballet",
        "D. Pilates"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The office printer uses only black toner cartridges and prints in monochrome.",
      "conflict_prompt": "The office printer uses color cartridges but prints only in monochrome.",
      "question": "What type of ink/toner does the office printer use according to the clean statement?",
      "options": [
        "A. Color inks",
        "B. Black toner only",
        "C. Dye-sublimation",
        "D. Thermal paper"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia is the committee chair who schedules and leads monthly meetings.",
      "conflict_prompt": "Sofia is a committee member who schedules and leads monthly meetings.",
      "question": "What role does Sofia have on the committee according to the clean statement?",
      "options": [
        "A. Treasurer",
        "B. Secretary",
        "C. Chair",
        "D. Member"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The concert ticket includes assigned seating and lists seat 12B for Alejandro.",
      "conflict_prompt": "The concert ticket is general admission and lists assigned seat 12B for Alejandro.",
      "question": "What type of seating does Alejandro's concert ticket have according to the clean statement?",
      "options": [
        "A. Standing room only",
        "B. General admission",
        "C. Assigned seating (12B)",
        "D. VIP lounge"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mason is a professional barista who prepares espresso drinks every day.",
      "conflict_prompt": "Mason is a professional software engineer who prepares espresso drinks every day.",
      "question": "What is Mason's profession according to the clean statement?",
      "options": [
        "A. Software engineer",
        "B. Barista",
        "C. Carpenter",
        "D. Electrician"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The medication must be stored in a refrigerator between 2°C and 8°C.",
      "conflict_prompt": "The medication must be stored at room temperature between 2°C and 8°C.",
      "question": "At what temperature should the medication be stored according to the clean statement?",
      "options": [
        "A. Below -20°C",
        "B. Between 2°C and 8°C in a refrigerator",
        "C. Above 30°C",
        "D. In direct sunlight"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noelle is a licensed marriage counselor who conducts couples therapy sessions weekly.",
      "conflict_prompt": "Noelle is a wedding planner who conducts couples therapy sessions weekly.",
      "question": "What is Noelle's professional license according to the clean statement?",
      "options": [
        "A. Marriage counselor",
        "B. Wedding planner",
        "C. Travel agent",
        "D. Event DJ"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery advertises gluten-free pastries and bakes no wheat products in the kitchen.",
      "conflict_prompt": "The bakery advertises gluten-free pastries but bakes wheat bread on the same counters.",
      "question": "What does the bakery advertise according to the clean statement?",
      "options": [
        "A. Wheat-based pastries",
        "B. Gluten-free pastries",
        "C. Dairy-free pastries",
        "D. Sugar-free pastries"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael is a professional lifeguard who patrols the beach during summer weekends.",
      "conflict_prompt": "Rafael is a professional lifeguard who patrols the ski resort during summer weekends.",
      "question": "Where does Rafael work during summer weekends according to the clean statement?",
      "options": [
        "A. Ski resort",
        "B. Beach",
        "C. Museum",
        "D. Hospital"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The theater bans photography during performances and enforces a no-flash policy.",
      "conflict_prompt": "The theater allows photography during performances but enforces a no-flash policy.",
      "question": "What is the theater's policy on photography during performances according to the clean statement?",
      "options": [
        "A. Photography allowed with flash",
        "B. Photography banned",
        "C. Photography allowed with permission",
        "D. Only professional photographers allowed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Arielle is a certified beekeeper who tends several hives behind her property.",
      "conflict_prompt": "Arielle is a certified entomologist who tends several hives behind her property.",
      "question": "What certification does Arielle hold according to the clean statement?",
      "options": [
        "A. Certified chef",
        "B. Certified beekeeper",
        "C. Certified mechanic",
        "D. Certified plumber"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The conference badge identifies attendees as speakers, exhibitors, or guests; Jorge's badge says 'Speaker.'",
      "conflict_prompt": "The conference badge identifies attendees as speakers, exhibitors, or guests; Jorge's badge says 'Guest.'",
      "question": "What role is printed on Jorge's conference badge according to the clean statement?",
      "options": [
        "A. Exhibitor",
        "B. Guest",
        "C. Speaker",
        "D. Volunteer"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora is an experienced paramedic who administers emergency care in the ambulance.",
      "conflict_prompt": "Nora is a hospital receptionist who administers emergency care in the ambulance.",
      "question": "What role does Nora serve in emergency medical situations according to the clean statement?",
      "options": [
        "A. Receptionist",
        "B. Paramedic",
        "C. Janitor",
        "D. Pharmacist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The orchard grows only apple trees and produces apple cider every autumn.",
      "conflict_prompt": "The orchard grows only peach trees and produces apple cider every autumn.",
      "question": "What type of fruit trees does the orchard grow according to the clean statement?",
      "options": [
        "A. Peach trees",
        "B. Apple trees",
        "C. Citrus trees",
        "D. Olive trees"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Katarina is a fluent Spanish speaker who translates Spanish literature into English.",
      "conflict_prompt": "Katarina is a fluent German speaker who translates Spanish literature into English.",
      "question": "Which language does Katarina speak fluently according to the clean statement?",
      "options": [
        "A. French",
        "B. German",
        "C. Spanish",
        "D. Italian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The company's policy states employees must be full-time to receive benefits; Jason is a full-time employee.",
      "conflict_prompt": "The company's policy states employees must be part-time to receive benefits; Jason is a full-time employee.",
      "question": "What is Jason's employment status according to the clean statement?",
      "options": [
        "A. Part-time",
        "B. Full-time",
        "C. Contractor",
        "D. Intern"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The campsite permits tent camping only and forbids motorhomes on the grassy loop.",
      "conflict_prompt": "The campsite permits motorhomes only and forbids tent camping on the grassy loop.",
      "question": "What type of camping is allowed on the grassy loop according to the clean statement?",
      "options": [
        "A. Motorhomes only",
        "B. Tent camping only",
        "C. Both tents and motorhomes",
        "D. No camping allowed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leah is a retired teacher who now volunteers at the school library every afternoon.",
      "conflict_prompt": "Leah is a current elementary teacher who now volunteers at the school library every afternoon.",
      "question": "What is Leah's professional status according to the clean statement?",
      "options": [
        "A. Retired teacher",
        "B. Current teacher",
        "C. Student",
        "D. Principal"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The company's emergency protocol requires employees to evacuate the building immediately during a fire alarm.",
      "conflict_prompt": "The company's emergency protocol requires employees to remain in the building during a fire alarm.",
      "question": "What does the emergency protocol require during a fire alarm according to the clean statement?",
      "options": [
        "A. Remain at desks",
        "B. Evacuate immediately",
        "C. Hide in closets",
        "D. Continue working"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar is a left-handed violinist who plays his instrument with the bow in his left hand.",
      "conflict_prompt": "Omar is a right-handed violinist who plays his instrument with the bow in his left hand.",
      "question": "Which hand does Omar primarily use to play the violin according to the clean statement?",
      "options": [
        "A. Right hand",
        "B. Left hand",
        "C. Both hands equally",
        "D. Neither hand"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The local theater troupe performs comedies and stages a new comedy production every month.",
      "conflict_prompt": "The local theater troupe performs tragedies and stages a new comedy production every month.",
      "question": "What genre of plays does the troupe perform according to the clean statement?",
      "options": [
        "A. Tragedies",
        "B. Musicals",
        "C. Comedies",
        "D. Operas"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Enzo is a pescatarian who eats fish and seafood but avoids other meats.",
      "conflict_prompt": "Enzo is a carnivore who eats only red meat and avoids fish and seafood.",
      "question": "What dietary category does Enzo follow according to the clean statement?",
      "options": [
        "A. Vegan",
        "B. Pescatarian",
        "C. Carnivore",
        "D. Vegetarian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum exhibit is temporary and will be on display for three months only.",
      "conflict_prompt": "The museum exhibit is permanent but will be on display for three months only.",
      "question": "How long will the museum exhibit be on display according to the clean statement?",
      "options": [
        "A. Indefinitely",
        "B. One week",
        "C. Three months",
        "D. One year"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The science lab requires all personnel to wear safety goggles at all times during experiments.",
      "conflict_prompt": "The science lab requires all personnel to remove safety goggles during experiments.",
      "question": "What protective equipment is required in the lab according to the clean statement?",
      "options": [
        "A. No protective equipment",
        "B. Gloves only",
        "C. Safety goggles",
        "D. Hard hats"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pablo is a vegetarian gardener who never uses bone meal or animal-based fertilizers.",
      "conflict_prompt": "Pablo is a gardener who never uses any fertilizers including bone meal or animal-based ones.",
      "question": "What fertilization practice does Pablo avoid according to the clean statement?",
      "options": [
        "A. Chemical fertilizers",
        "B. Bone meal and animal-based fertilizers",
        "C. Compost",
        "D. Organic slow-release"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The daycare center cares for children up to age five and does not accept school-aged kids.",
      "conflict_prompt": "The daycare center cares for teenagers and does not accept preschool-aged children.",
      "question": "What age range does the daycare center serve according to the clean statement?",
      "options": [
        "A. Teenagers only",
        "B. Up to age five",
        "C. School-aged children",
        "D. Adults"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Isla is a certified scuba diver who always dives with a buddy for safety.",
      "conflict_prompt": "Isla is a certified solo freediver who always dives with a buddy for safety.",
      "question": "What safety practice does Isla follow during dives according to the statement?",
      "options": [
        "A. Always dives alone",
        "B. Uses no equipment",
        "C. Always dives with a buddy",
        "D. Never dives"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The emergency kit contains only non-perishable food and bottled water for 72 hours.",
      "conflict_prompt": "The emergency kit contains only perishable food and bottled water for 72 hours.",
      "question": "What type of food is included in the emergency kit according to the clean statement?",
      "options": [
        "A. Perishable food",
        "B. Fresh produce only",
        "C. Non-perishable food",
        "D. Frozen meals"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gina is a licensed child psychologist who specializes in early childhood development.",
      "conflict_prompt": "Gina is a neonatal nurse who specializes in early childhood development.",
      "question": "What is Gina's professional license according to the clean statement?",
      "options": [
        "A. Pediatrician",
        "B. Child psychologist",
        "C. Neonatal nurse",
        "D. Occupational therapist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The workshop is intended for beginner knitters and covers basic stitches and techniques.",
      "conflict_prompt": "The workshop is intended for advanced knitters and covers basic stitches and techniques.",
      "question": "Who is the knitting workshop designed for according to the clean statement?",
      "options": [
        "A. Advanced knitters",
        "B. Intermediate knitters",
        "C. Beginner knitters",
        "D. Non-knitters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tess is a vegetarian who volunteers at an animal shelter and avoids animal products.",
      "conflict_prompt": "Tess is a veterinarian who volunteers at an animal shelter and avoids animal products.",
      "question": "Which statement describes Tess's dietary preference according to the clean statement?",
      "options": [
        "A. Eats meat daily",
        "B. Vegetarian",
        "C. Omnivore",
        "D. Carnivore"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bike lane is reserved for cyclists only and motor vehicles are prohibited.",
      "conflict_prompt": "The bike lane is reserved for pedestrians only and motor vehicles are prohibited.",
      "question": "Who is the bike lane intended for according to the clean statement?",
      "options": [
        "A. Pedestrians",
        "B. Cyclists",
        "C. Motor vehicles",
        "D. Emergency vehicles"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rita is a professional violinist who performs with the city orchestra.",
      "conflict_prompt": "Rita is a professional saxophonist who performs with the city orchestra.",
      "question": "What instrument does Rita play professionally according to the clean statement?",
      "options": [
        "A. Saxophone",
        "B. Drums",
        "C. Violin",
        "D. Trumpet"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The tour guide is fluent in three languages and leads tours in English daily.",
      "conflict_prompt": "The tour guide is fluent in only one language and leads tours in English daily.",
      "question": "How many languages is the tour guide fluent in according to the clean statement?",
      "options": [
        "A. One",
        "B. Two",
        "C. Three",
        "D. Four"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vegetarian cafe offers tofu and tempeh as protein options but no meat.",
      "conflict_prompt": "The vegetarian cafe offers bacon and sausage as protein options but no meat.",
      "question": "What protein options does the vegetarian cafe offer according to the clean statement?",
      "options": [
        "A. Bacon and sausage",
        "B. Tofu and tempeh",
        "C. Grilled chicken",
        "D. Pork chops"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nolan is an identical twin born two minutes after his brother and shares many features.",
      "conflict_prompt": "Nolan is an identical twin born two minutes before his brother and shares many features.",
      "question": "When was Nolan born relative to his twin according to the clean statement?",
      "options": [
        "A. Two minutes before",
        "B. Two minutes after",
        "C. One hour earlier",
        "D. Same minute"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The city ordinance bans fireworks within city limits and enforces fines for violations.",
      "conflict_prompt": "The city ordinance requires fireworks to be set off daily and enforces fines for violations.",
      "question": "What does the city ordinance do regarding fireworks according to the clean statement?",
      "options": [
        "A. Encourages fireworks displays",
        "B. Bans fireworks within city limits",
        "C. Requires daily fireworks",
        "D. Has no rules about fireworks"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Miriam is a registered nurse working the pediatric ward who administers vaccinations to children.",
      "conflict_prompt": "Miriam is a veterinary technician working the pediatric ward who administers vaccinations to children.",
      "question": "What is Miriam's profession according to the clean statement?",
      "options": [
        "A. Veterinarian",
        "B. Registered nurse",
        "C. Pharmacist",
        "D. Dental hygienist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The theater performance is restricted to ages 12 and up due to mature themes.",
      "conflict_prompt": "The theater performance is suitable for all ages but restricted to ages 12 and up due to mature themes.",
      "question": "What age restriction applies to the theater performance according to the clean statement?",
      "options": [
        "A. All ages",
        "B. 12 and up",
        "C. 18 and up",
        "D. 5 and up"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hiro is a sushi chef who uses only fresh raw fish in his sashimi platters.",
      "conflict_prompt": "Hiro is a vegan chef who uses only fresh raw fish in his sashimi platters.",
      "question": "What type of chef is Hiro according to the clean statement?",
      "options": [
        "A. Vegan chef",
        "B. Sushi chef",
        "C. Pastry chef",
        "D. BBQ chef"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school bus picks up students at 7:15 am and drops them off by 3:30 pm.",
      "conflict_prompt": "The school bus picks up students at 7:15 pm and drops them off by 3:30 pm.",
      "question": "What time does the school bus pick up students according to the clean statement?",
      "options": [
        "A. 6:00 am",
        "B. 7:15 am",
        "C. 7:15 pm",
        "D. Noon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The community garden allows only organic fertilizers and prohibits chemical pesticides.",
      "conflict_prompt": "The community garden allows chemical pesticides and prohibits organic fertilizers.",
      "question": "What does the community garden prohibit according to the clean statement?",
      "options": [
        "A. Organic fertilizers",
        "B. Chemical pesticides",
        "C. Hand tools",
        "D. Composting"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The pet store sells only freshwater fish and does not stock any saltwater species.",
      "conflict_prompt": "The pet store sells only saltwater fish and does not stock any freshwater species.",
      "question": "What type of fish does the pet store sell according to the clean statement?",
      "options": [
        "A. Saltwater fish only",
        "B. Freshwater fish only",
        "C. Both saltwater and freshwater",
        "D. No fish at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The parking garage is for compact cars only and larger vehicles are not permitted.",
      "conflict_prompt": "The parking garage is for oversized vehicles only and larger vehicles are not permitted.",
      "question": "Which vehicles are allowed in the parking garage according to the clean statement?",
      "options": [
        "A. Motorcycles only",
        "B. Compact cars only",
        "C. Oversized vehicles only",
        "D. Buses only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Owen is an organ donor registered to give his kidneys and lungs after death.",
      "conflict_prompt": "Owen is an organ donor registered to donate only his corneas after death.",
      "question": "Which organs is Owen registered to donate according to the clean statement?",
      "options": [
        "A. Corneas only",
        "B. Kidneys and lungs",
        "C. Heart only",
        "D. Liver only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The child's cereal is sugar-free and marketed to toddlers with no added sugar.",
      "conflict_prompt": "The child's cereal is high-sugar and marketed to toddlers with no added sugar.",
      "question": "What is true about the cereal according to the clean statement?",
      "options": [
        "A. High in sugar",
        "B. Sugar-free",
        "C. Contains chocolate chips",
        "D. Contains caffeine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia is a public school teacher employed by the district who teaches fifth grade.",
      "conflict_prompt": "Nadia is a private tutor employed by the district who teaches fifth grade.",
      "question": "Where does Nadia work according to the clean statement?",
      "options": [
        "A. Private tutoring company",
        "B. District public school",
        "C. University",
        "D. Head Start program"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The courtroom is reserved for jury trials only and no cameras are permitted inside.",
      "conflict_prompt": "The courtroom is reserved for televised broadcasts only and no cameras are permitted inside.",
      "question": "What is the courtroom reserved for according to the clean statement?",
      "options": [
        "A. Televised broadcasts",
        "B. Jury trials",
        "C. Weddings",
        "D. Town hall meetings"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The eco-lodge runs entirely on solar power and has no connection to the local grid.",
      "conflict_prompt": "The eco-lodge is fully connected to the local grid and has no solar panels.",
      "question": "How does the eco-lodge generate its power according to the clean statement?",
      "options": [
        "A. Solar power only",
        "B. Grid connection only",
        "C. Diesel generators",
        "D. Wind turbines"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amir is the head librarian who manages acquisitions and library staff.",
      "conflict_prompt": "Amir is the night janitor who manages acquisitions and library staff.",
      "question": "What position does Amir hold at the library according to the clean statement?",
      "options": [
        "A. Night janitor",
        "B. Head librarian",
        "C. Security guard",
        "D. Volunteer coordinator"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The pediatric vaccine is recommended for children aged two months and older.",
      "conflict_prompt": "The pediatric vaccine is recommended only for adults and not for children aged two months and older.",
      "question": "For which age group is the pediatric vaccine recommended according to the clean statement?",
      "options": [
        "A. Adults only",
        "B. Teenagers only",
        "C. Children aged two months and older",
        "D. Newborns only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nate is a certified scuba diver who always uses a tank and regulator when diving.",
      "conflict_prompt": "Nate is a free diver who always uses a tank and regulator when diving.",
      "question": "What equipment does Nate use when diving according to the clean statement?",
      "options": [
        "A. No equipment",
        "B. Snorkel only",
        "C. Tank and regulator",
        "D. Kayak"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The farmers' market is open Saturdays from 8 am to 1 pm and closes on weekdays.",
      "conflict_prompt": "The farmers' market is open weekdays from 8 am to 1 pm and closes on Saturdays.",
      "question": "When is the farmers' market open according to the clean statement?",
      "options": [
        "A. Weekdays 8 am to 1 pm",
        "B. Saturdays 8 am to 1 pm",
        "C. Sundays 8 am to 1 pm",
        "D. Open 24/7"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lena is a vegetarian who works as a head chef at a plant-based restaurant.",
      "conflict_prompt": "Lena is a head butcher who works as a head chef at a plant-based restaurant.",
      "question": "What type of restaurant does Lena work at according to the clean statement?",
      "options": [
        "A. Steakhouse",
        "B. Seafood restaurant",
        "C. Plant-based restaurant",
        "D. Fast food chain"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The emergency siren sounds for five minutes to warn residents of severe weather.",
      "conflict_prompt": "The emergency siren sounds for five minutes to signal the end of severe weather.",
      "question": "What does the emergency siren warn residents about according to the clean statement?",
      "options": [
        "A. Severe weather warning",
        "B. End of weather event",
        "C. Daily curfew",
        "D. Park opening"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Diego is a practicing vegetarian who owns a butcher shop specializing in plant-based meats.",
      "conflict_prompt": "Diego is a butcher who owns a butcher shop specializing in plant-based meats.",
      "question": "What is unique about the butcher shop Diego owns according to the clean statement?",
      "options": [
        "A. It sells live poultry",
        "B. It specializes in plant-based meats",
        "C. It only sells pork products",
        "D. It imports exotic meats"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The mountain rescue team is trained for winter avalanches and responds year-round.",
      "conflict_prompt": "The mountain rescue team is trained only for desert rescues and responds to winter avalanches.",
      "question": "What type of rescue are the mountain rescue team trained for according to the clean statement?",
      "options": [
        "A. Desert rescues",
        "B. Urban rescues",
        "C. Winter avalanches",
        "D. Marine rescues"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    }
  ]
}