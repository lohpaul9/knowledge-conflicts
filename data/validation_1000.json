{
  "metadata": {
    "generated_at": "2025-08-29T22:25:24.854110",
    "total_examples": 985,
    "categories": [
      "factual_contradictions",
      "role_attribute_contradictions",
      "temporal_contradictions",
      "negation_contradictions"
    ],
    "description": "Intra-context knowledge conflict dataset for mechanistic interpretability research"
  },
  "examples": [
    {
      "clean_prompt": "The Nile River is the longest river in Africa.",
      "conflict_prompt": "The Amazon River is the longest river in Africa.",
      "question": "Which river is the longest in Africa?",
      "options": [
        "A. Amazon",
        "B. Congo",
        "C. Nile",
        "D. Niger"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Everest's official height is 8,848 meters.",
      "conflict_prompt": "Mount Everest's official height is 9,010 meters.",
      "question": "What is the commonly cited official height of Mount Everest?",
      "options": [
        "A. 8,000 meters",
        "B. 9,010 meters",
        "C. 8,848 meters",
        "D. 10,000 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of France is Paris.",
      "conflict_prompt": "The capital city of France is Lyon.",
      "question": "What is the capital city of France?",
      "options": [
        "A. Marseille",
        "B. Paris",
        "C. Lyon",
        "D. Nice"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Water's chemical formula is H2O, meaning two hydrogen atoms and one oxygen atom.",
      "conflict_prompt": "Water's chemical formula is H3O, meaning three hydrogen atoms and one oxygen atom.",
      "question": "What is the chemical formula for water?",
      "options": [
        "A. H3O",
        "B. CO2",
        "C. H2O",
        "D. O2"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Wall of China is located primarily in northern China.",
      "conflict_prompt": "The Great Wall of China is located primarily in southern China.",
      "question": "In which part of China is the Great Wall primarily located?",
      "options": [
        "A. Southern China",
        "B. Eastern China",
        "C. Northern China",
        "D. Western China"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human heart has four chambers: two atria and two ventricles.",
      "conflict_prompt": "The human heart has three chambers: two atria and one ventricle.",
      "question": "How many chambers does the human heart have?",
      "options": [
        "A. Three",
        "B. Four",
        "C. Two",
        "D. Five"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for gold is Au.",
      "conflict_prompt": "The chemical symbol for gold is Ag.",
      "question": "What is the chemical symbol for gold?",
      "options": [
        "A. Ag",
        "B. Gd",
        "C. Au",
        "D. Go"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet in our solar system is Jupiter.",
      "conflict_prompt": "The largest planet in our solar system is Saturn.",
      "question": "Which is the largest planet in our solar system?",
      "options": [
        "A. Earth",
        "B. Saturn",
        "C. Jupiter",
        "D. Neptune"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty is located on Liberty Island in New York Harbor.",
      "conflict_prompt": "The Statue of Liberty is located on Ellis Island in New York Harbor.",
      "question": "On which island is the Statue of Liberty located?",
      "options": [
        "A. Ellis Island",
        "B. Manhattan",
        "C. Liberty Island",
        "D. Governors Island"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Shakespeare is widely credited with writing the play 'Romeo and Juliet.'",
      "conflict_prompt": "Christopher Marlowe is widely credited with writing the play 'Romeo and Juliet.'",
      "question": "Who is traditionally credited as the author of 'Romeo and Juliet'?",
      "options": [
        "A. Christopher Marlowe",
        "B. William Shakespeare",
        "C. Ben Jonson",
        "D. John Donne"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language spoken in Brazil is Portuguese.",
      "conflict_prompt": "The primary language spoken in Brazil is Spanish.",
      "question": "What is the primary language spoken in Brazil?",
      "options": [
        "A. Spanish",
        "B. English",
        "C. Portuguese",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human body normally has 206 bones in adulthood.",
      "conflict_prompt": "The human body normally has 250 bones in adulthood.",
      "question": "Approximately how many bones are in a typical adult human skeleton?",
      "options": [
        "A. 250",
        "B. 206",
        "C. 180",
        "D. 300"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Japan is the yen.",
      "conflict_prompt": "The currency of Japan is the won.",
      "question": "What is the currency of Japan?",
      "options": [
        "A. Yuan",
        "B. Yen",
        "C. Won",
        "D. Dollar"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light travels in a vacuum at approximately 299,792 kilometers per second.",
      "conflict_prompt": "Light travels in a vacuum at approximately 150,000 kilometers per second.",
      "question": "Approximately how fast does light travel in a vacuum?",
      "options": [
        "A. 150,000 km/s",
        "B. 300,000 km/s",
        "C. 1,500 km/s",
        "D. 30,000 km/s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest ocean on Earth is the Pacific Ocean.",
      "conflict_prompt": "The largest ocean on Earth is the Atlantic Ocean.",
      "question": "Which is the largest ocean on Earth?",
      "options": [
        "A. Indian Ocean",
        "B. Arctic Ocean",
        "C. Atlantic Ocean",
        "D. Pacific Ocean"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Neil Armstrong was the first person to walk on the Moon in 1969.",
      "conflict_prompt": "Buzz Aldrin was the first person to walk on the Moon in 1969.",
      "question": "Who was the first person to walk on the Moon?",
      "options": [
        "A. Michael Collins",
        "B. Yuri Gagarin",
        "C. Neil Armstrong",
        "D. Buzz Aldrin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Italy is Rome.",
      "conflict_prompt": "The capital of Italy is Milan.",
      "question": "What is the capital city of Italy?",
      "options": [
        "A. Venice",
        "B. Florence",
        "C. Milan",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A triangle's interior angles sum to 180 degrees.",
      "conflict_prompt": "A triangle's interior angles sum to 360 degrees.",
      "question": "What is the sum of the interior angles of a triangle in Euclidean geometry?",
      "options": [
        "A. 90 degrees",
        "B. 180 degrees",
        "C. 270 degrees",
        "D. 360 degrees"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest mammal on Earth is the blue whale.",
      "conflict_prompt": "The largest mammal on Earth is the African elephant.",
      "question": "Which animal is the largest mammal on Earth?",
      "options": [
        "A. African elephant",
        "B. White rhinoceros",
        "C. Blue whale",
        "D. Giraffe"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 1 is hydrogen.",
      "conflict_prompt": "The chemical element with atomic number 1 is helium.",
      "question": "Which element has atomic number 1?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Lithium",
        "D. Oxygen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas in Earth's atmosphere is nitrogen, making up about 78%.",
      "conflict_prompt": "The primary gas in Earth's atmosphere is oxygen, making up about 78%.",
      "question": "Which gas makes up about 78% of Earth's atmosphere?",
      "options": [
        "A. Oxygen",
        "B. Carbon dioxide",
        "C. Nitrogen",
        "D. Argon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Albert Einstein developed the theory of general relativity in 1915.",
      "conflict_prompt": "Isaac Newton developed the theory of general relativity in 1915.",
      "question": "Who developed the theory of general relativity?",
      "options": [
        "A. Isaac Newton",
        "B. Albert Einstein",
        "C. Niels Bohr",
        "D. Galileo Galilei"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest desert in the world is the Sahara Desert (non-polar desert).",
      "conflict_prompt": "The largest desert in the world is the Gobi Desert.",
      "question": "Which desert is the largest hot desert on Earth?",
      "options": [
        "A. Gobi Desert",
        "B. Arabian Desert",
        "C. Sahara Desert",
        "D. Kalahari Desert"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest land animal is the cheetah, capable of short bursts up to around 100 km/h.",
      "conflict_prompt": "The fastest land animal is the pronghorn, capable of short bursts up to around 100 km/h.",
      "question": "Which animal is considered the fastest land animal in short bursts?",
      "options": [
        "A. Pronghorn",
        "B. Lion",
        "C. Cheetah",
        "D. Ostrich"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful powered airplane flight was by the Wright brothers in 1903 at Kitty Hawk.",
      "conflict_prompt": "The first successful powered airplane flight was by Alberto Santos-Dumont in 1903 at Kitty Hawk.",
      "question": "Who is credited with the first successful powered airplane flight in 1903 at Kitty Hawk?",
      "options": [
        "A. Alberto Santos-Dumont",
        "B. Orville and Wilbur Wright",
        "C. Glenn Curtiss",
        "D. Samuel Langley"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element oxygen has the atomic number 8.",
      "conflict_prompt": "The element oxygen has the atomic number 7.",
      "question": "What is the atomic number of oxygen?",
      "options": [
        "A. 6",
        "B. 7",
        "C. 8",
        "D. 9"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning British monarch before Queen Elizabeth II was Queen Victoria.",
      "conflict_prompt": "The longest reigning British monarch before Queen Elizabeth II was King George III.",
      "question": "Who was the longest reigning British monarch prior to Queen Elizabeth II?",
      "options": [
        "A. King George III",
        "B. Queen Victoria",
        "C. King Henry VIII",
        "D. Queen Elizabeth I"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Ring of Fire is known for frequent earthquakes and volcanic eruptions around the Pacific Ocean.",
      "conflict_prompt": "The Atlantic Ring of Fire is known for frequent earthquakes and volcanic eruptions around the Atlantic Ocean.",
      "question": "Which region is known for a high concentration of earthquakes and volcanic activity around the Pacific Ocean?",
      "options": [
        "A. Mediterranean Belt",
        "B. Mid-Atlantic Ridge",
        "C. Pacific Ring of Fire",
        "D. Himalayan Belt"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city by population in the United States is New York City.",
      "conflict_prompt": "The largest city by population in the United States is Los Angeles.",
      "question": "Which city has the largest population in the United States?",
      "options": [
        "A. Chicago",
        "B. Los Angeles",
        "C. New York City",
        "D. Houston"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in traditional Japanese miso soup is fermented soybean paste called miso.",
      "conflict_prompt": "The primary ingredient in traditional Japanese miso soup is tomato paste.",
      "question": "What is the primary fermented paste ingredient used in traditional Japanese miso soup?",
      "options": [
        "A. Tomato paste",
        "B. Miso (fermented soybean paste)",
        "C. Fish sauce",
        "D. Tahini"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main organ for human respiration is the lungs.",
      "conflict_prompt": "The main organ for human respiration is the liver.",
      "question": "Which organ is the primary organ for respiration in humans?",
      "options": [
        "A. Liver",
        "B. Heart",
        "C. Skin",
        "D. Lungs"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first man-made satellite launched into space was Sputnik 1 by the Soviet Union in 1957.",
      "conflict_prompt": "The first man-made satellite launched into space was Explorer 1 by the United States in 1957.",
      "question": "Which satellite was the first artificial satellite launched into space in 1957?",
      "options": [
        "A. Explorer 1",
        "B. Sputnik 1",
        "C. Vanguard 1",
        "D. Explorer 2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for common table salt (sodium chloride) is NaCl.",
      "conflict_prompt": "The chemical formula for common table salt (sodium chloride) is KCl.",
      "question": "What is the chemical formula for common table salt (sodium chloride)?",
      "options": [
        "A. NaCl",
        "B. KCl",
        "C. Na2O",
        "D. Cl2"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The author of 'Pride and Prejudice' is Jane Austen.",
      "conflict_prompt": "The author of 'Pride and Prejudice' is Charlotte Brontë.",
      "question": "Who wrote 'Pride and Prejudice'?",
      "options": [
        "A. Emily Brontë",
        "B. Jane Austen",
        "C. Charlotte Brontë",
        "D. Mary Shelley"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The shortest day of the year in the Northern Hemisphere is the winter solstice in December.",
      "conflict_prompt": "The shortest day of the year in the Northern Hemisphere is the summer solstice in June.",
      "question": "Which event marks the shortest day of the year in the Northern Hemisphere?",
      "options": [
        "A. Vernal equinox",
        "B. Summer solstice",
        "C. Winter solstice",
        "D. Autumnal equinox"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary organ responsible for filtering blood and producing urine is the kidney.",
      "conflict_prompt": "The primary organ responsible for filtering blood and producing urine is the pancreas.",
      "question": "Which organ primarily filters blood and produces urine?",
      "options": [
        "A. Liver",
        "B. Pancreas",
        "C. Kidney",
        "D. Spleen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Canada is Ottawa.",
      "conflict_prompt": "The capital of Canada is Toronto.",
      "question": "What is the capital city of Canada?",
      "options": [
        "A. Toronto",
        "B. Vancouver",
        "C. Ottawa",
        "D. Montreal"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Honeybees produce honey from the nectar they collect from flowers.",
      "conflict_prompt": "Honeybees produce honey from the pollen they collect from flowers.",
      "question": "From which floral substance do honeybees primarily produce honey?",
      "options": [
        "A. Nectar",
        "B. Pollen",
        "C. Sap",
        "D. Leaves"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mariana Trench is the deepest known part of the Earth's oceans.",
      "conflict_prompt": "The Puerto Rico Trench is the deepest known part of the Earth's oceans.",
      "question": "Which trench is known as the deepest part of Earth's oceans?",
      "options": [
        "A. Puerto Rico Trench",
        "B. Mariana Trench",
        "C. Tonga Trench",
        "D. Java Trench"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in guacamole is avocado.",
      "conflict_prompt": "The primary ingredient in guacamole is cucumber.",
      "question": "What is the main ingredient in guacamole?",
      "options": [
        "A. Tomato",
        "B. Avocado",
        "C. Cucumber",
        "D. Pepper"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of the United Kingdom is the pound sterling.",
      "conflict_prompt": "The currency of the United Kingdom is the euro.",
      "question": "What is the official currency of the United Kingdom?",
      "options": [
        "A. Euro",
        "B. Pound sterling",
        "C. Dollar",
        "D. Franc"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Thomas Edison is commonly credited with inventing the commercially practical incandescent light bulb.",
      "conflict_prompt": "Nikola Tesla is commonly credited with inventing the commercially practical incandescent light bulb.",
      "question": "Who is commonly credited with inventing the commercially practical incandescent light bulb?",
      "options": [
        "A. Nikola Tesla",
        "B. Thomas Edison",
        "C. Alexander Graham Bell",
        "D. Guglielmo Marconi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon rainforest is primarily located in Brazil.",
      "conflict_prompt": "The Amazon rainforest is primarily located in Australia.",
      "question": "In which country is the majority of the Amazon rainforest located?",
      "options": [
        "A. Colombia",
        "B. Peru",
        "C. Brazil",
        "D. Australia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for silver is Ag.",
      "conflict_prompt": "The chemical symbol for silver is Al.",
      "question": "What is the chemical symbol for silver?",
      "options": [
        "A. Ag",
        "B. Al",
        "C. Si",
        "D. Au"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest continent by land area is Asia.",
      "conflict_prompt": "The largest continent by land area is Africa.",
      "question": "Which continent is the largest by land area?",
      "options": [
        "A. Africa",
        "B. North America",
        "C. Asia",
        "D. Europe"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The planet Mars is known as the Red Planet due to iron oxide on its surface.",
      "conflict_prompt": "The planet Venus is known as the Red Planet due to iron oxide on its surface.",
      "question": "Which planet is commonly called the Red Planet?",
      "options": [
        "A. Venus",
        "B. Mars",
        "C. Mercury",
        "D. Jupiter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The classical music composer Ludwig van Beethoven was born in Bonn, Germany.",
      "conflict_prompt": "The classical music composer Ludwig van Beethoven was born in Vienna, Austria.",
      "question": "Where was Ludwig van Beethoven born?",
      "options": [
        "A. Vienna, Austria",
        "B. Salzburg, Austria",
        "C. Bonn, Germany",
        "D. Leipzig, Germany"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pacific Northwest region of the United States receives significant rainfall compared with many other U.S. regions.",
      "conflict_prompt": "The Pacific Northwest region of the United States is known for being arid and receiving very little rainfall.",
      "question": "Which description best fits the Pacific Northwest's general climate?",
      "options": [
        "A. Arid and very dry",
        "B. Tropical and humid",
        "C. Mediterranean with dry summers",
        "D. Generally wet with significant rainfall"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Spanish city of Barcelona is located in the region of Catalonia.",
      "conflict_prompt": "The Spanish city of Barcelona is located in the region of Andalusia.",
      "question": "In which Spanish autonomous community is Barcelona located?",
      "options": [
        "A. Andalusia",
        "B. Catalonia",
        "C. Madrid",
        "D. Valencia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Wright brothers' first powered flight occurred in 1903.",
      "conflict_prompt": "The Wright brothers' first powered flight occurred in 1913.",
      "question": "In what year did the Wright brothers achieve their first powered flight?",
      "options": [
        "A. 1893",
        "B. 1903",
        "C. 1913",
        "D. 1923"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The country with the largest population in the world is China.",
      "conflict_prompt": "The country with the largest population in the world is India.",
      "question": "Which country currently has the largest population in the world?",
      "options": [
        "A. United States",
        "B. India",
        "C. China",
        "D. Indonesia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common abbreviation for kilogram is kg.",
      "conflict_prompt": "The common abbreviation for kilogram is lb.",
      "question": "What is the standard abbreviation for kilogram?",
      "options": [
        "A. lb",
        "B. kg",
        "C. g",
        "D. oz"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island in the world is Greenland.",
      "conflict_prompt": "The largest island in the world is Australia.",
      "question": "Which is considered the largest island in the world by area?",
      "options": [
        "A. Australia",
        "B. Greenland",
        "C. New Guinea",
        "D. Borneo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest building in the world is the Burj Khalifa in Dubai.",
      "conflict_prompt": "The tallest building in the world is the Shanghai Tower in Shanghai.",
      "question": "Which building is currently recognized as the tallest in the world?",
      "options": [
        "A. Shanghai Tower",
        "B. One World Trade Center",
        "C. Burj Khalifa",
        "D. Taipei 101"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol Fe is iron.",
      "conflict_prompt": "The chemical element with symbol Fe is fluorine.",
      "question": "Which element is represented by the chemical symbol Fe?",
      "options": [
        "A. Fluorine",
        "B. Iron",
        "C. Francium",
        "D. Fermium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary component of natural gas used for heating is methane.",
      "conflict_prompt": "The primary component of natural gas used for heating is propane.",
      "question": "What is the primary component of natural gas used in residential heating?",
      "options": [
        "A. Propane",
        "B. Methane",
        "C. Butane",
        "D. Ethane"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest mountain in Africa is Mount Kilimanjaro.",
      "conflict_prompt": "The tallest mountain in Africa is Mount Kenya.",
      "question": "Which mountain is the highest on the African continent?",
      "options": [
        "A. Mount Kenya",
        "B. Mount Meru",
        "C. Mount Kilimanjaro",
        "D. Mount Elgon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official language of Egypt is Arabic.",
      "conflict_prompt": "The official language of Egypt is Turkish.",
      "question": "What is the official language of Egypt?",
      "options": [
        "A. Turkish",
        "B. Arabic",
        "C. Farsi",
        "D. Greek"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the telephone is commonly credited as Alexander Graham Bell.",
      "conflict_prompt": "The inventor of the telephone is commonly credited as Thomas Edison.",
      "question": "Who is commonly credited with inventing the telephone?",
      "options": [
        "A. Nikola Tesla",
        "B. Thomas Edison",
        "C. Alexander Graham Bell",
        "D. Guglielmo Marconi"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize in Physics is awarded annually by the Royal Swedish Academy of Sciences.",
      "conflict_prompt": "The Nobel Prize in Physics is awarded annually by the Norwegian Nobel Committee.",
      "question": "Which organization awards the Nobel Prize in Physics?",
      "options": [
        "A. Norwegian Nobel Committee",
        "B. Royal Swedish Academy of Sciences",
        "C. The Nobel Foundation",
        "D. Swedish Academy"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Australia is Canberra.",
      "conflict_prompt": "The capital of Australia is Sydney.",
      "question": "What is the capital city of Australia?",
      "options": [
        "A. Sydney",
        "B. Melbourne",
        "C. Perth",
        "D. Canberra"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The commonly used abbreviation for United Nations is UN.",
      "conflict_prompt": "The commonly used abbreviation for United Nations is EU.",
      "question": "What is the usual abbreviation for the United Nations?",
      "options": [
        "A. EU",
        "B. NATO",
        "C. UN",
        "D. WHO"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The planet closest to the Sun is Mercury.",
      "conflict_prompt": "The planet closest to the Sun is Venus.",
      "question": "Which planet is closest to the Sun?",
      "options": [
        "A. Venus",
        "B. Mercury",
        "C. Earth",
        "D. Mars"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first successful vaccine for polio was developed by Jonas Salk in the 1950s.",
      "conflict_prompt": "The first successful vaccine for polio was developed by Albert Sabin in the 1950s.",
      "question": "Who developed the first successful polio vaccine in the 1950s?",
      "options": [
        "A. Albert Sabin",
        "B. Jonas Salk",
        "C. Edward Jenner",
        "D. Louis Pasteur"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Russia is Moscow.",
      "conflict_prompt": "The capital of Russia is Saint Petersburg.",
      "question": "What is the capital city of Russia?",
      "options": [
        "A. Saint Petersburg",
        "B. Novosibirsk",
        "C. Moscow",
        "D. Vladivostok"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mediterranean Sea lies between Europe, Africa, and Asia.",
      "conflict_prompt": "The Mediterranean Sea lies between North America and Europe.",
      "question": "Between which general regions is the Mediterranean Sea located?",
      "options": [
        "A. North America and Europe",
        "B. Europe, Africa, and Asia",
        "C. South America and Africa",
        "D. Australia and Asia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The sport of soccer is known as football in most countries outside the United States.",
      "conflict_prompt": "The sport of soccer is known as rugby in most countries outside the United States.",
      "question": "What is the common international name for the sport called 'soccer' in the United States?",
      "options": [
        "A. Rugby",
        "B. Gaelic football",
        "C. Football",
        "D. American football"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human brain is protected by the skull and three layers called meninges.",
      "conflict_prompt": "The human brain is protected by the skull and four layers called meninges.",
      "question": "How many meningeal layers protect the human brain?",
      "options": [
        "A. Two",
        "B. Three",
        "C. Four",
        "D. Five"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary greenhouse gas produced by human activities is carbon dioxide (CO2).",
      "conflict_prompt": "The primary greenhouse gas produced by human activities is methane (CH4).",
      "question": "Which gas is the primary greenhouse gas emitted by human activities in terms of overall volume?",
      "options": [
        "A. Methane (CH4)",
        "B. Carbon dioxide (CO2)",
        "C. Nitrous oxide (N2O)",
        "D. Ozone (O3)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Beatles were a British rock band formed in Liverpool.",
      "conflict_prompt": "The Beatles were a British rock band formed in London.",
      "question": "In which city did The Beatles form?",
      "options": [
        "A. London",
        "B. Manchester",
        "C. Liverpool",
        "D. Birmingham"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest species of penguin is the emperor penguin.",
      "conflict_prompt": "The largest species of penguin is the king penguin.",
      "question": "Which species is the largest penguin?",
      "options": [
        "A. King penguin",
        "B. Gentoo penguin",
        "C. Emperor penguin",
        "D. Adelie penguin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest tree species is the coast redwood (Sequoia sempervirens).",
      "conflict_prompt": "The tallest tree species is the giant sequoia (Sequoiadendron giganteum).",
      "question": "Which tree species is known for being the tallest on Earth?",
      "options": [
        "A. Douglas fir",
        "B. Giant sequoia (Sequoiadendron giganteum)",
        "C. Coast redwood (Sequoia sempervirens)",
        "D. Eucalyptus regnans"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of chlorophyll in plants is to capture light energy for photosynthesis.",
      "conflict_prompt": "The primary function of chlorophyll in plants is to store water in leaves.",
      "question": "What is the primary role of chlorophyll in plants?",
      "options": [
        "A. Store water",
        "B. Capture light energy for photosynthesis",
        "C. Transport sugars",
        "D. Protect against pests"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest U.S. state by area is Alaska.",
      "conflict_prompt": "The largest U.S. state by area is Texas.",
      "question": "Which U.S. state is largest by land area?",
      "options": [
        "A. California",
        "B. Texas",
        "C. Colorado",
        "D. Alaska"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element helium is lighter than air and is used to fill balloons.",
      "conflict_prompt": "The chemical element argon is lighter than air and is used to fill balloons.",
      "question": "Which gas is lighter than air and commonly used to fill balloons?",
      "options": [
        "A. Argon",
        "B. Helium",
        "C. Nitrogen",
        "D. Carbon dioxide"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first president of the United States was George Washington.",
      "conflict_prompt": "The first president of the United States was John Adams.",
      "question": "Who served as the first President of the United States?",
      "options": [
        "A. Thomas Jefferson",
        "B. John Adams",
        "C. George Washington",
        "D. James Madison"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main ingredient in traditional Italian pesto is basil.",
      "conflict_prompt": "The main ingredient in traditional Italian pesto is cilantro.",
      "question": "What herb is the main ingredient in traditional Italian pesto?",
      "options": [
        "A. Cilantro",
        "B. Parsley",
        "C. Basil",
        "D. Oregano"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency used in India is the Indian rupee.",
      "conflict_prompt": "The currency used in India is the Indian dollar.",
      "question": "What is the official currency of India?",
      "options": [
        "A. Indian dollar",
        "B. Indian rupee",
        "C. Taka",
        "D. Lira"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 6 is carbon.",
      "conflict_prompt": "The chemical element with atomic number 6 is nitrogen.",
      "question": "Which element has atomic number 6?",
      "options": [
        "A. Nitrogen",
        "B. Carbon",
        "C. Oxygen",
        "D. Boron"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largets national park in the United States is Wrangell–St. Elias National Park and Preserve in Alaska.",
      "conflict_prompt": "The largest national park in the United States is Yellowstone National Park.",
      "question": "Which national park is the largest by area in the United States?",
      "options": [
        "A. Yellowstone National Park",
        "B. Yosemite National Park",
        "C. Grand Canyon National Park",
        "D. Wrangell–St. Elias National Park and Preserve"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main constituent of natural sea salt is sodium chloride.",
      "conflict_prompt": "The main constituent of natural sea salt is potassium chloride.",
      "question": "What is the principal constituent of common sea salt?",
      "options": [
        "A. Potassium chloride",
        "B. Sodium chloride",
        "C. Magnesium sulfate",
        "D. Calcium carbonate"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Germany is Berlin.",
      "conflict_prompt": "The capital of Germany is Munich.",
      "question": "What is the capital city of Germany?",
      "options": [
        "A. Munich",
        "B. Frankfurt",
        "C. Hamburg",
        "D. Berlin"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The highest federal court in the United States is the Supreme Court.",
      "conflict_prompt": "The highest federal court in the United States is the Federal District Court.",
      "question": "Which court is the highest federal court in the United States?",
      "options": [
        "A. Federal District Court",
        "B. Court of Appeals",
        "C. Supreme Court",
        "D. State Supreme Courts"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary component of Earth's core is iron and nickel.",
      "conflict_prompt": "The primary component of Earth's core is silicon and oxygen.",
      "question": "Which elements primarily make up Earth's core?",
      "options": [
        "A. Silicon and oxygen",
        "B. Iron and nickel",
        "C. Carbon and hydrogen",
        "D. Aluminum and magnesium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for carbon dioxide is CO2.",
      "conflict_prompt": "The chemical formula for carbon dioxide is CO3.",
      "question": "What is the chemical formula for carbon dioxide?",
      "options": [
        "A. CO",
        "B. CO2",
        "C. CO3",
        "D. O2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island nation by area is Indonesia.",
      "conflict_prompt": "The largest island nation by area is Madagascar.",
      "question": "Which country is the largest island nation by land area?",
      "options": [
        "A. Madagascar",
        "B. Indonesia",
        "C. Philippines",
        "D. Japan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A leap year normally includes 29 days in February.",
      "conflict_prompt": "A leap year normally includes 30 days in February.",
      "question": "How many days does February have in a leap year?",
      "options": [
        "A. 28",
        "B. 29",
        "C. 30",
        "D. 31"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The composer of the opera 'The Magic Flute' is Wolfgang Amadeus Mozart.",
      "conflict_prompt": "The composer of the opera 'The Magic Flute' is Ludwig van Beethoven.",
      "question": "Who composed the opera 'The Magic Flute'?",
      "options": [
        "A. Ludwig van Beethoven",
        "B. Giuseppe Verdi",
        "C. Wolfgang Amadeus Mozart",
        "D. Richard Wagner"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The official residence of the U.S. President is the White House in Washington, D.C.",
      "conflict_prompt": "The official residence of the U.S. President is the Capitol Building in Washington, D.C.",
      "question": "What is the official residence of the President of the United States?",
      "options": [
        "A. The Capitol Building",
        "B. The Pentagon",
        "C. The White House",
        "D. Camp David"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest organ in the human body by surface area is the skin.",
      "conflict_prompt": "The largest organ in the human body by surface area is the liver.",
      "question": "Which organ is the largest in the human body by surface area?",
      "options": [
        "A. Liver",
        "B. Skin",
        "C. Lungs",
        "D. Heart"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The modern Olympics were revived in 1896 in Athens, Greece.",
      "conflict_prompt": "The modern Olympics were revived in 1900 in Paris, France.",
      "question": "In which year and city were the modern Olympic Games first held after revival?",
      "options": [
        "A. 1896, Athens",
        "B. 1900, Paris",
        "C. 1912, Stockholm",
        "D. 1888, London"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest bird in level flight is the common swift, while the peregrine falcon is fastest in a dive.",
      "conflict_prompt": "The fastest bird in level flight is the peregrine falcon, while the common swift is fastest in a dive.",
      "question": "Which bird is renowned as the fastest in a diving stoop?",
      "options": [
        "A. Common swift",
        "B. Peregrine falcon",
        "C. Golden eagle",
        "D. Albatross"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Rosetta Stone helped scholars decode ancient Egyptian hieroglyphs because it contained the same text in three scripts.",
      "conflict_prompt": "The Rosetta Stone hindered scholars decoding ancient Egyptian hieroglyphs because it contained only unknown symbols.",
      "question": "Why was the Rosetta Stone important for deciphering ancient Egyptian writing?",
      "options": [
        "A. It contained only unknown symbols",
        "B. It contained the same text in three scripts, enabling comparison",
        "C. It was written entirely in Greek",
        "D. It had detailed illustrations of hieroglyphs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest waterfall in the world is Angel Falls in Venezuela.",
      "conflict_prompt": "The tallest waterfall in the world is Niagara Falls on the US-Canada border.",
      "question": "Which waterfall is regarded as the tallest waterfall in the world?",
      "options": [
        "A. Niagara Falls",
        "B. Victoria Falls",
        "C. Angel Falls",
        "D. Iguazu Falls"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with the symbol Na is sodium.",
      "conflict_prompt": "The chemical element with the symbol Na is neon.",
      "question": "Which element has the chemical symbol Na?",
      "options": [
        "A. Neon",
        "B. Sodium",
        "C. Nitrogen",
        "D. Nickel"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest lake in Africa by area is Lake Victoria.",
      "conflict_prompt": "The largest lake in Africa by area is Lake Tanganyika.",
      "question": "Which is the largest lake in Africa by surface area?",
      "options": [
        "A. Lake Tanganyika",
        "B. Lake Malawi",
        "C. Lake Victoria",
        "D. Lake Chad"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most widely spoken native language in the world is Mandarin Chinese.",
      "conflict_prompt": "The most widely spoken native language in the world is English.",
      "question": "Which language has the largest number of native speakers worldwide?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. Mandarin Chinese",
        "D. Hindi"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary colors of light are red, green, and blue (RGB).",
      "conflict_prompt": "The primary colors of light are red, yellow, and blue.",
      "question": "What are the primary colors of light in additive color mixing?",
      "options": [
        "A. Red, yellow, blue",
        "B. Red, green, blue",
        "C. Cyan, magenta, yellow",
        "D. Red, black, white"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest volcano on Earth by volume is Mauna Loa in Hawaii.",
      "conflict_prompt": "The largest volcano on Earth by volume is Mount Fuji in Japan.",
      "question": "Which volcano is considered the largest by volume on Earth?",
      "options": [
        "A. Mount Fuji",
        "B. Mauna Loa",
        "C. Mount Kilimanjaro",
        "D. Mount St. Helens"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial Earth satellite was launched by the Soviet Union in 1957 and named Sputnik 1.",
      "conflict_prompt": "The first artificial Earth satellite was launched by the United States in 1957 and named Vanguard 1.",
      "question": "Which satellite was the first artificial satellite placed into Earth orbit?",
      "options": [
        "A. Vanguard 1",
        "B. Explorer 1",
        "C. Sputnik 1",
        "D. Telstar 1"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary method of heat transfer in a vacuum is radiation.",
      "conflict_prompt": "The primary method of heat transfer in a vacuum is convection.",
      "question": "Which mode of heat transfer can effectively occur in a vacuum?",
      "options": [
        "A. Conduction",
        "B. Convection",
        "C. Radiation",
        "D. Evaporation"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Spain is Madrid.",
      "conflict_prompt": "The capital of Spain is Barcelona.",
      "question": "What is the capital city of Spain?",
      "options": [
        "A. Barcelona",
        "B. Seville",
        "C. Madrid",
        "D. Valencia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary nutrient responsible for muscle repair and growth is protein.",
      "conflict_prompt": "The primary nutrient responsible for muscle repair and growth is carbohydrate.",
      "question": "Which macronutrient is most directly responsible for muscle repair and growth?",
      "options": [
        "A. Carbohydrate",
        "B. Fat",
        "C. Protein",
        "D. Fiber"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous painting 'Mona Lisa' is displayed in the Louvre Museum in Paris.",
      "conflict_prompt": "The famous painting 'Mona Lisa' is displayed in the Uffizi Gallery in Florence.",
      "question": "In which museum is the 'Mona Lisa' exhibited?",
      "options": [
        "A. Uffizi Gallery",
        "B. Prado Museum",
        "C. Louvre Museum",
        "D. Metropolitan Museum of Art"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The speed of sound in air at sea level is approximately 343 meters per second.",
      "conflict_prompt": "The speed of sound in air at sea level is approximately 150 meters per second.",
      "question": "Approximately what is the speed of sound in air at sea level?",
      "options": [
        "A. 150 m/s",
        "B. 343 m/s",
        "C. 1,000 m/s",
        "D. 30 m/s"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The author of '1984' is George Orwell.",
      "conflict_prompt": "The author of '1984' is Aldous Huxley.",
      "question": "Who wrote the novel '1984'?",
      "options": [
        "A. Aldous Huxley",
        "B. George Orwell",
        "C. Ray Bradbury",
        "D. J.R.R. Tolkien"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city in South America by population is São Paulo, Brazil.",
      "conflict_prompt": "The largest city in South America by population is Buenos Aires, Argentina.",
      "question": "Which city is the largest by population in South America?",
      "options": [
        "A. Buenos Aires",
        "B. Lima",
        "C. Bogotá",
        "D. São Paulo"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The major tectonic plates include the Pacific Plate, North American Plate, Eurasian Plate, African Plate, and others.",
      "conflict_prompt": "The major tectonic plates include the Caribbean Plate, Antarctic Plate, and the single Eurasian-African Plate combined.",
      "question": "Which of the following is recognized as a major tectonic plate?",
      "options": [
        "A. Eurasian-African combined plate",
        "B. Pacific Plate",
        "C. Single continental plate for both Africa and Europe",
        "D. No tectonic plates exist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the World Wide Web is Tim Berners-Lee.",
      "conflict_prompt": "The inventor of the World Wide Web is Vint Cerf.",
      "question": "Who is credited with inventing the World Wide Web?",
      "options": [
        "A. Vint Cerf",
        "B. Tim Berners-Lee",
        "C. Steve Jobs",
        "D. Bill Gates"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest bay in the world by surface area is the Bay of Bengal.",
      "conflict_prompt": "The largest bay in the world by surface area is the Hudson Bay.",
      "question": "Which bay is the largest in the world by surface area?",
      "options": [
        "A. Hudson Bay",
        "B. Bay of Bengal",
        "C. Gulf of Mexico",
        "D. Persian Gulf"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common cooking technique 'sauté' involves cooking food quickly in a small amount of oil over relatively high heat.",
      "conflict_prompt": "The common cooking technique 'sauté' involves cooking food slowly in a large amount of water over low heat.",
      "question": "What best describes the cooking technique 'sauté'?",
      "options": [
        "A. Cook slowly in water over low heat",
        "B. Cook quickly in a small amount of oil over high heat",
        "C. Bake food at moderate temperatures",
        "D. Deep fry food in large amounts of oil"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest building in the United States is One World Trade Center in New York City (by architectural height).",
      "conflict_prompt": "The tallest building in the United States is Willis Tower in Chicago.",
      "question": "Which building is the tallest in the United States by architectural height?",
      "options": [
        "A. Willis Tower",
        "B. Empire State Building",
        "C. One World Trade Center",
        "D. Chrysler Building"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Switzerland is the Swiss franc.",
      "conflict_prompt": "The currency of Switzerland is the euro.",
      "question": "What is the official currency of Switzerland?",
      "options": [
        "A. Euro",
        "B. Swiss franc",
        "C. Krona",
        "D. Lira"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary function of red blood cells is to transport oxygen using hemoglobin.",
      "conflict_prompt": "The primary function of red blood cells is to produce antibodies.",
      "question": "What is the primary role of red blood cells in the circulatory system?",
      "options": [
        "A. Produce antibodies",
        "B. Transport oxygen via hemoglobin",
        "C. Fight infections",
        "D. Clot blood"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Argentina is Buenos Aires.",
      "conflict_prompt": "The capital of Argentina is Córdoba.",
      "question": "What is the capital city of Argentina?",
      "options": [
        "A. Córdoba",
        "B. Rosario",
        "C. Buenos Aires",
        "D. Mendoza"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest bay by volume in the world is the Bay of Bengal.",
      "conflict_prompt": "The largest bay by volume in the world is the Gulf of Oman.",
      "question": "Which bay is known as one of the largest by surface area and volume in the world, located between India and Southeast Asia?",
      "options": [
        "A. Gulf of Oman",
        "B. Bay of Bengal",
        "C. Gulf of Mexico",
        "D. Arabian Sea"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary purpose of the ozone layer is to absorb most of the Sun's harmful ultraviolet radiation.",
      "conflict_prompt": "The primary purpose of the ozone layer is to reflect visible light to cool the Earth.",
      "question": "What is the primary protective function of Earth's ozone layer?",
      "options": [
        "A. Reflect visible light",
        "B. Absorb most of the Sun's harmful ultraviolet radiation",
        "C. Increase greenhouse warming",
        "D. Produce oxygen for the atmosphere"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous scientist Marie Curie won Nobel Prizes in both Physics and Chemistry.",
      "conflict_prompt": "The famous scientist Marie Curie won Nobel Prizes only in Literature.",
      "question": "In which fields did Marie Curie receive Nobel Prizes?",
      "options": [
        "A. Literature only",
        "B. Physics and Chemistry",
        "C. Peace and Economics",
        "D. Medicine and Peace"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language of Egypt is Arabic, and ancient Egyptian hieroglyphs were used in earlier periods.",
      "conflict_prompt": "The primary language of Egypt is Coptic, and ancient Egyptian hieroglyphs were never used.",
      "question": "What is the modern official language of Egypt?",
      "options": [
        "A. Ancient Egyptian (hieroglyphs)",
        "B. Coptic",
        "C. Arabic",
        "D. Greek"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main export product historically associated with Saudi Arabia is crude oil.",
      "conflict_prompt": "The main export product historically associated with Saudi Arabia is coffee.",
      "question": "Which product has historically been the primary export of Saudi Arabia?",
      "options": [
        "A. Coffee",
        "B. Crude oil",
        "C. Textiles",
        "D. Electronics"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial heart transplant as a long-term bridge device was performed using the Jarvik-7 in the early 1980s.",
      "conflict_prompt": "The first artificial heart transplant as a long-term bridge device was performed using the Jarvik-7 in the 1960s.",
      "question": "In which decade was the Jarvik-7 artificial heart notably used as a long-term bridge device in humans?",
      "options": [
        "A. 1960s",
        "B. 1970s",
        "C. 1980s",
        "D. 1990s"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous painting 'Starry Night' was created by Vincent van Gogh.",
      "conflict_prompt": "The famous painting 'Starry Night' was created by Claude Monet.",
      "question": "Who painted 'Starry Night'?",
      "options": [
        "A. Claude Monet",
        "B. Vincent van Gogh",
        "C. Pablo Picasso",
        "D. Paul Cézanne"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest country in Africa by area is Algeria.",
      "conflict_prompt": "The largest country in Africa by area is Sudan.",
      "question": "Which country is the largest by land area in Africa?",
      "options": [
        "A. Sudan",
        "B. Algeria",
        "C. Democratic Republic of the Congo",
        "D. Libya"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The oldest known continuous civilization is often considered to be in China with origins over 4,000 years ago.",
      "conflict_prompt": "The oldest known continuous civilization is often considered to be in Australia with origins over 4,000 years ago.",
      "question": "Which region is commonly cited as one of the oldest continuous civilizations with recorded history over 4,000 years old?",
      "options": [
        "A. Australia",
        "B. Sub-Saharan Africa",
        "C. China",
        "D. South America"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common name for sodium bicarbonate used in baking is baking soda.",
      "conflict_prompt": "The common name for sodium bicarbonate used in baking is baking powder.",
      "question": "What is the common household name for sodium bicarbonate used in baking?",
      "options": [
        "A. Baking powder",
        "B. Baking soda",
        "C. Yeast",
        "D. Cornstarch"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of France is Paris.",
      "conflict_prompt": "The capital of France is Berlin.",
      "question": "What is the capital of France?",
      "options": [
        "A. Paris",
        "B. Berlin",
        "C. Madrid",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Everest's elevation is 8,848 meters above sea level.",
      "conflict_prompt": "Mount Everest's elevation is 9,000 meters above sea level.",
      "question": "What is Mount Everest's commonly cited elevation?",
      "options": [
        "A. 9,000 meters",
        "B. 8,000 meters",
        "C. 8,848 meters",
        "D. 10,000 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nile is widely considered the longest river in the world.",
      "conflict_prompt": "The Amazon is widely considered the longest river in the world.",
      "question": "Which river is traditionally considered the longest in the world?",
      "options": [
        "A. Yangtze",
        "B. Mississippi",
        "C. Amazon",
        "D. Nile"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Water freezes at 0 degrees Celsius at standard atmospheric pressure.",
      "conflict_prompt": "Water freezes at 100 degrees Celsius at standard atmospheric pressure.",
      "question": "At what temperature does water freeze at standard atmospheric pressure?",
      "options": [
        "A. 100°C",
        "B. 0°C",
        "C. -10°C",
        "D. 50°C"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Great Wall of China is located primarily in northern China.",
      "conflict_prompt": "The Great Wall of China is located primarily in southern China.",
      "question": "In which part of China is the Great Wall primarily located?",
      "options": [
        "A. Southern China",
        "B. Eastern China",
        "C. Western China",
        "D. Northern China"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical symbol for gold is Au.",
      "conflict_prompt": "The chemical symbol for gold is Ag.",
      "question": "What is the chemical symbol for gold?",
      "options": [
        "A. Ag",
        "B. Go",
        "C. Au",
        "D. Gd"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Venus is the second planet from the Sun.",
      "conflict_prompt": "Mars is the second planet from the Sun.",
      "question": "Which planet is the second from the Sun?",
      "options": [
        "A. Mars",
        "B. Venus",
        "C. Mercury",
        "D. Earth"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Prime Meridian passes through Greenwich, London.",
      "conflict_prompt": "The Prime Meridian passes through Paris.",
      "question": "Through which city does the Prime Meridian pass?",
      "options": [
        "A. Paris",
        "B. Rome",
        "C. Greenwich, London",
        "D. Madrid"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The boiling point of water at sea level is 100 degrees Celsius.",
      "conflict_prompt": "The boiling point of water at sea level is 0 degrees Celsius.",
      "question": "What is the boiling point of water at sea level in degrees Celsius?",
      "options": [
        "A. 0°C",
        "B. 50°C",
        "C. 100°C",
        "D. 150°C"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty is located on Liberty Island in New York Harbor.",
      "conflict_prompt": "The Statue of Liberty is located on Ellis Island in New York Harbor.",
      "question": "On which island is the Statue of Liberty located?",
      "options": [
        "A. Ellis Island",
        "B. Manhattan Island",
        "C. Liberty Island",
        "D. Staten Island"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of Japan is the yen.",
      "conflict_prompt": "The currency of Japan is the won.",
      "question": "What is the currency of Japan?",
      "options": [
        "A. Yuan",
        "B. Won",
        "C. Yen",
        "D. Dollar"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Amazon rainforest is primarily located in Brazil.",
      "conflict_prompt": "The Amazon rainforest is primarily located in Argentina.",
      "question": "In which country is the majority of the Amazon rainforest located?",
      "options": [
        "A. Argentina",
        "B. Peru",
        "C. Brazil",
        "D. Colombia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Albert Einstein developed the theory of general relativity.",
      "conflict_prompt": "Isaac Newton developed the theory of general relativity.",
      "question": "Who developed the theory of general relativity?",
      "options": [
        "A. Isaac Newton",
        "B. Niels Bohr",
        "C. Albert Einstein",
        "D. Galileo Galilei"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human heart has four chambers: two atria and two ventricles.",
      "conflict_prompt": "The human heart has three chambers: two atria and one ventricle.",
      "question": "How many chambers does the human heart have?",
      "options": [
        "A. Three",
        "B. Four",
        "C. Five",
        "D. Two"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Shakespeare wrote the play Romeo and Juliet.",
      "conflict_prompt": "Charles Dickens wrote the play Romeo and Juliet.",
      "question": "Who wrote Romeo and Juliet?",
      "options": [
        "A. Charles Dickens",
        "B. William Shakespeare",
        "C. Jane Austen",
        "D. Mark Twain"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Light travels faster in a vacuum than in air.",
      "conflict_prompt": "Light travels slower in a vacuum than in air.",
      "question": "How does the speed of light in a vacuum compare to its speed in air?",
      "options": [
        "A. Slower in vacuum",
        "B. Faster in vacuum",
        "C. The same in both",
        "D. Faster in air"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet in the Solar System is Jupiter.",
      "conflict_prompt": "The largest planet in the Solar System is Saturn.",
      "question": "Which is the largest planet in the Solar System?",
      "options": [
        "A. Saturn",
        "B. Jupiter",
        "C. Neptune",
        "D. Earth"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human adult skeleton typically has 206 bones.",
      "conflict_prompt": "The human adult skeleton typically has 250 bones.",
      "question": "Approximately how many bones are in a typical adult human skeleton?",
      "options": [
        "A. 250",
        "B. 206",
        "C. 198",
        "D. 300"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest ocean on Earth is the Pacific Ocean.",
      "conflict_prompt": "The largest ocean on Earth is the Atlantic Ocean.",
      "question": "Which ocean is the largest on Earth?",
      "options": [
        "A. Indian Ocean",
        "B. Atlantic Ocean",
        "C. Pacific Ocean",
        "D. Arctic Ocean"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Statue of Liberty was a gift from France to the United States.",
      "conflict_prompt": "The Statue of Liberty was a gift from the United Kingdom to the United States.",
      "question": "Which country gifted the Statue of Liberty to the United States?",
      "options": [
        "A. United Kingdom",
        "B. France",
        "C. Spain",
        "D. Netherlands"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Christopher Columbus first reached the Americas in 1492.",
      "conflict_prompt": "Christopher Columbus first reached the Americas in 1592.",
      "question": "In what year did Christopher Columbus first reach the Americas?",
      "options": [
        "A. 1492",
        "B. 1592",
        "C. 1392",
        "D. 1692"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Eiffel Tower is located in Paris, France.",
      "conflict_prompt": "The Eiffel Tower is located in Madrid, Spain.",
      "question": "In which city is the Eiffel Tower located?",
      "options": [
        "A. Madrid",
        "B. Paris",
        "C. Rome",
        "D. Berlin"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Mona Lisa was painted by Leonardo da Vinci.",
      "conflict_prompt": "The Mona Lisa was painted by Vincent van Gogh.",
      "question": "Who painted the Mona Lisa?",
      "options": [
        "A. Vincent van Gogh",
        "B. Pablo Picasso",
        "C. Leonardo da Vinci",
        "D. Claude Monet"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The United States declared independence in 1776.",
      "conflict_prompt": "The United States declared independence in 1876.",
      "question": "In what year did the United States declare independence?",
      "options": [
        "A. 1876",
        "B. 1776",
        "C. 1676",
        "D. 1976"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common chemical formula for table salt is NaCl.",
      "conflict_prompt": "The common chemical formula for table salt is KCl.",
      "question": "What is the chemical formula for common table salt?",
      "options": [
        "A. KCl",
        "B. NaCl",
        "C. Na2SO4",
        "D. CaCl2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Pyramids of Giza are located in Egypt.",
      "conflict_prompt": "The Pyramids of Giza are located in Mexico.",
      "question": "In which country are the Pyramids of Giza located?",
      "options": [
        "A. Mexico",
        "B. Peru",
        "C. Egypt",
        "D. Sudan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The organ primarily responsible for filtering blood in humans is the kidney.",
      "conflict_prompt": "The organ primarily responsible for filtering blood in humans is the liver.",
      "question": "Which organ is primarily responsible for filtering blood in humans?",
      "options": [
        "A. Liver",
        "B. Heart",
        "C. Lungs",
        "D. Kidneys"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest land animal is the cheetah.",
      "conflict_prompt": "The fastest land animal is the pronghorn.",
      "question": "Which animal is the fastest land animal?",
      "options": [
        "A. Pronghorn",
        "B. Cheetah",
        "C. Lion",
        "D. Gazelle"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary language spoken in Brazil is Portuguese.",
      "conflict_prompt": "The primary language spoken in Brazil is Spanish.",
      "question": "What is the primary language spoken in Brazil?",
      "options": [
        "A. Spanish",
        "B. Portuguese",
        "C. French",
        "D. English"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The moon orbits the Earth approximately once every 27.3 days (sidereal month).",
      "conflict_prompt": "The moon orbits the Earth approximately once every 365 days.",
      "question": "Approximately how many days does it take the Moon to orbit the Earth (sidereal month)?",
      "options": [
        "A. 365 days",
        "B. 27.3 days",
        "C. 7 days",
        "D. 100 days"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest desert in the world by area is the Sahara Desert.",
      "conflict_prompt": "The largest desert in the world by area is the Gobi Desert.",
      "question": "Which is the largest hot desert in the world by area?",
      "options": [
        "A. Gobi Desert",
        "B. Arabian Desert",
        "C. Sahara Desert",
        "D. Kalahari Desert"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary ingredient in guacamole is avocado.",
      "conflict_prompt": "The primary ingredient in guacamole is tomato.",
      "question": "What is the primary ingredient in guacamole?",
      "options": [
        "A. Tomato",
        "B. Avocado",
        "C. Onion",
        "D. Pepper"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Mount Kilimanjaro is located in Tanzania.",
      "conflict_prompt": "Mount Kilimanjaro is located in Kenya.",
      "question": "In which country is Mount Kilimanjaro located?",
      "options": [
        "A. Kenya",
        "B. Tanzania",
        "C. Uganda",
        "D. Ethiopia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element with atomic number 1 is hydrogen.",
      "conflict_prompt": "The element with atomic number 1 is helium.",
      "question": "Which element has atomic number 1?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Lithium",
        "D. Oxygen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the telephone is Alexander Graham Bell.",
      "conflict_prompt": "The inventor of the telephone is Thomas Edison.",
      "question": "Who is credited with inventing the telephone?",
      "options": [
        "A. Thomas Edison",
        "B. Alexander Graham Bell",
        "C. Nikola Tesla",
        "D. Guglielmo Marconi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary gas making up Earth's atmosphere is nitrogen (about 78%).",
      "conflict_prompt": "The primary gas making up Earth's atmosphere is oxygen (about 78%).",
      "question": "Which gas makes up about 78% of Earth's atmosphere?",
      "options": [
        "A. Oxygen",
        "B. Nitrogen",
        "C. Carbon dioxide",
        "D. Argon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Australia is Canberra.",
      "conflict_prompt": "The capital of Australia is Sydney.",
      "question": "What is the capital city of Australia?",
      "options": [
        "A. Sydney",
        "B. Melbourne",
        "C. Canberra",
        "D. Brisbane"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Renaissance began in Italy during the 14th century.",
      "conflict_prompt": "The Renaissance began in France during the 14th century.",
      "question": "In which country did the Renaissance originate?",
      "options": [
        "A. France",
        "B. England",
        "C. Italy",
        "D. Germany"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest land mammal is the African elephant.",
      "conflict_prompt": "The largest land mammal is the blue whale.",
      "question": "Which is the largest land mammal?",
      "options": [
        "A. Blue whale",
        "B. African elephant",
        "C. Giraffe",
        "D. Hippopotamus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The human eye's color is determined by the amount and type of melanin in the iris.",
      "conflict_prompt": "The human eye's color is determined by the length of the eyeball.",
      "question": "What primarily determines the color of the human eye?",
      "options": [
        "A. Length of the eyeball",
        "B. Amount and type of melanin in the iris",
        "C. Size of the pupil",
        "D. Blood flow to the retina"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first manned Moon landing occurred in 1969 during NASA's Apollo 11 mission.",
      "conflict_prompt": "The first manned Moon landing occurred in 1959.",
      "question": "In what year did the first manned Moon landing occur?",
      "options": [
        "A. 1959",
        "B. 1969",
        "C. 1979",
        "D. 1949"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The smallest prime number is 2.",
      "conflict_prompt": "The smallest prime number is 1.",
      "question": "Which is the smallest prime number?",
      "options": [
        "A. 1",
        "B. 2",
        "C. 3",
        "D. 0"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The novel '1984' was written by George Orwell.",
      "conflict_prompt": "The novel '1984' was written by Aldous Huxley.",
      "question": "Who wrote the novel '1984'?",
      "options": [
        "A. Aldous Huxley",
        "B. George Orwell",
        "C. Ray Bradbury",
        "D. J.R.R. Tolkien"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main ingredient in bread that gives it structure is gluten (when made from wheat).",
      "conflict_prompt": "The main ingredient in bread that gives it structure is sugar.",
      "question": "Which component in wheat-based dough primarily provides structure to bread?",
      "options": [
        "A. Sugar",
        "B. Gluten",
        "C. Salt",
        "D. Yeast"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Panama Canal connects the Atlantic and Pacific Oceans.",
      "conflict_prompt": "The Panama Canal connects the Indian and Pacific Oceans.",
      "question": "Which two oceans does the Panama Canal connect?",
      "options": [
        "A. Atlantic and Indian",
        "B. Indian and Pacific",
        "C. Atlantic and Pacific",
        "D. Arctic and Pacific"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Nobel Prize in Physics was awarded to Marie Curie in 1903 (shared).",
      "conflict_prompt": "The Nobel Prize in Physics was awarded to Marie Curie in 1953.",
      "question": "In which year did Marie Curie receive the Nobel Prize in Physics (shared)?",
      "options": [
        "A. 1903",
        "B. 1953",
        "C. 1913",
        "D. 1893"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest building in the world as of 2020 is the Burj Khalifa in Dubai.",
      "conflict_prompt": "The tallest building in the world as of 2020 is the Shanghai Tower in Shanghai.",
      "question": "Which building was the tallest in the world as of 2020?",
      "options": [
        "A. Shanghai Tower",
        "B. One World Trade Center",
        "C. Empire State Building",
        "D. Burj Khalifa"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Canada is Ottawa.",
      "conflict_prompt": "The capital of Canada is Toronto.",
      "question": "What is the capital of Canada?",
      "options": [
        "A. Toronto",
        "B. Vancouver",
        "C. Montreal",
        "D. Ottawa"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first element in the periodic table is hydrogen.",
      "conflict_prompt": "The first element in the periodic table is helium.",
      "question": "Which element is listed first in the periodic table?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Lithium",
        "D. Beryllium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The World War II ended in 1945.",
      "conflict_prompt": "World War II ended in 1939.",
      "question": "In which year did World War II end?",
      "options": [
        "A. 1939",
        "B. 1945",
        "C. 1918",
        "D. 1950"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The common cold is most often caused by rhinoviruses.",
      "conflict_prompt": "The common cold is most often caused by Streptococcus bacteria.",
      "question": "Which type of pathogen most often causes the common cold?",
      "options": [
        "A. Bacteria like Streptococcus",
        "B. Fungi",
        "C. Viruses like rhinoviruses",
        "D. Parasites"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest continent by land area is Asia.",
      "conflict_prompt": "The largest continent by land area is Africa.",
      "question": "Which continent has the largest land area?",
      "options": [
        "A. Africa",
        "B. Asia",
        "C. North America",
        "D. Europe"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous scientist Isaac Newton formulated the law of universal gravitation.",
      "conflict_prompt": "The famous scientist Albert Einstein formulated the law of universal gravitation.",
      "question": "Who formulated the law of universal gravitation?",
      "options": [
        "A. Albert Einstein",
        "B. Galileo Galilei",
        "C. Isaac Newton",
        "D. Johannes Kepler"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The fastest bird in level flight is the common swift (or frigate bird depending on measure), but the peregrine falcon is fastest in a dive.",
      "conflict_prompt": "The fastest bird in level flight is the ostrich.",
      "question": "Which bird is known for being the fastest in a dive?",
      "options": [
        "A. Ostrich",
        "B. Penguin",
        "C. Peregrine falcon",
        "D. Turkey"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest island in the world is Greenland.",
      "conflict_prompt": "The largest island in the world is Australia.",
      "question": "Which is the largest island in the world (excluding continents)?",
      "options": [
        "A. Australia",
        "B. Greenland",
        "C. New Guinea",
        "D. Borneo"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The tallest mountain in Africa is Mount Kilimanjaro.",
      "conflict_prompt": "The tallest mountain in Africa is Mount Kenya.",
      "question": "Which mountain is the tallest in Africa?",
      "options": [
        "A. Mount Kenya",
        "B. Mount Kilimanjaro",
        "C. Mount Elgon",
        "D. Ras Dashen"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "H2O is the chemical formula for water.",
      "conflict_prompt": "CO2 is the chemical formula for water.",
      "question": "What is the chemical formula for water?",
      "options": [
        "A. CO2",
        "B. H2O",
        "C. O2",
        "D. H2SO4"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Italy is Rome.",
      "conflict_prompt": "The capital of Italy is Venice.",
      "question": "What is the capital of Italy?",
      "options": [
        "A. Venice",
        "B. Florence",
        "C. Rome",
        "D. Milan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The commonly used SI unit for mass is the kilogram.",
      "conflict_prompt": "The commonly used SI unit for mass is the meter.",
      "question": "What is the SI base unit for mass?",
      "options": [
        "A. Meter",
        "B. Kilogram",
        "C. Second",
        "D. Ampere"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chef Julia Child popularized French cuisine in America and wrote 'Mastering the Art of French Cooking'.",
      "conflict_prompt": "The chef Julia Child popularized Japanese cuisine in America and wrote 'Mastering the Art of Sushi'.",
      "question": "Which cuisine did Julia Child help popularize in America?",
      "options": [
        "A. Japanese",
        "B. Chinese",
        "C. French",
        "D. Italian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest living land carnivore is the polar bear (by mass).",
      "conflict_prompt": "The largest living land carnivore is the African lion.",
      "question": "Which is considered the largest living land carnivore by mass?",
      "options": [
        "A. African lion",
        "B. Siberian tiger",
        "C. Polar bear",
        "D. Grizzly bear"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The medicinal compound penicillin was discovered by Alexander Fleming.",
      "conflict_prompt": "The medicinal compound penicillin was discovered by Louis Pasteur.",
      "question": "Who discovered penicillin?",
      "options": [
        "A. Louis Pasteur",
        "B. Alexander Fleming",
        "C. Robert Koch",
        "D. Joseph Lister"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest U.S. state by area is Alaska.",
      "conflict_prompt": "The largest U.S. state by area is Texas.",
      "question": "Which U.S. state is largest by area?",
      "options": [
        "A. Texas",
        "B. California",
        "C. Alaska",
        "D. Montana"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning British monarch before Queen Elizabeth II was Queen Victoria.",
      "conflict_prompt": "The longest reigning British monarch before Queen Elizabeth II was King George V.",
      "question": "Who was the long-reigning British monarch immediately preceding Queen Elizabeth II's predecessor as the longest-reigning before Elizabeth II?",
      "options": [
        "A. King George V",
        "B. King George III",
        "C. Queen Victoria",
        "D. King William IV"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The Wright brothers carried out the first powered, controlled airplane flight in 1903.",
      "conflict_prompt": "The Wright brothers carried out the first powered, controlled airplane flight in 1803.",
      "question": "In what year did the Wright brothers achieve the first powered, controlled airplane flight?",
      "options": [
        "A. 1803",
        "B. 1863",
        "C. 1903",
        "D. 1923"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The instrument commonly used to measure atmospheric pressure is the barometer.",
      "conflict_prompt": "The instrument commonly used to measure atmospheric pressure is the thermometer.",
      "question": "Which instrument measures atmospheric pressure?",
      "options": [
        "A. Thermometer",
        "B. Anemometer",
        "C. Barometer",
        "D. Hygrometer"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Germany is Berlin.",
      "conflict_prompt": "The capital of Germany is Munich.",
      "question": "What is the capital of Germany?",
      "options": [
        "A. Munich",
        "B. Frankfurt",
        "C. Berlin",
        "D. Hamburg"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The shortest day of the year in the Northern Hemisphere occurs at the December solstice.",
      "conflict_prompt": "The shortest day of the year in the Northern Hemisphere occurs at the June solstice.",
      "question": "During which solstice does the Northern Hemisphere experience its shortest day?",
      "options": [
        "A. June solstice",
        "B. September equinox",
        "C. December solstice",
        "D. March equinox"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with atomic number 6 is carbon.",
      "conflict_prompt": "The chemical element with atomic number 6 is nitrogen.",
      "question": "Which element has atomic number 6?",
      "options": [
        "A. Oxygen",
        "B. Nitrogen",
        "C. Carbon",
        "D. Hydrogen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary author of the U.S. Declaration of Independence was Thomas Jefferson.",
      "conflict_prompt": "The primary author of the U.S. Declaration of Independence was Benjamin Franklin.",
      "question": "Who was the principal author of the U.S. Declaration of Independence?",
      "options": [
        "A. Benjamin Franklin",
        "B. John Adams",
        "C. Thomas Jefferson",
        "D. George Washington"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The shortest complete war in history is often cited as the Anglo-Zanzibar War of 1896, lasting under an hour.",
      "conflict_prompt": "The shortest complete war in history is often cited as the Hundred Years' War.",
      "question": "Which conflict is famously cited as lasting under an hour and sometimes called the shortest war?",
      "options": [
        "A. Hundred Years' War",
        "B. Anglo-Zanzibar War",
        "C. War of the Roses",
        "D. Crimean War"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "A light year is a unit of distance equal to the distance light travels in one year.",
      "conflict_prompt": "A light year is a unit of time equal to one year.",
      "question": "What does a light year measure?",
      "options": [
        "A. Time",
        "B. Distance",
        "C. Speed",
        "D. Mass"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "DNA stands for deoxyribonucleic acid.",
      "conflict_prompt": "DNA stands for deoxyribonucleic enzyme.",
      "question": "What does DNA stand for?",
      "options": [
        "A. Deoxyribonucleic enzyme",
        "B. Deoxyribonucleic acid",
        "C. Deoxyribose nucleic acid",
        "D. Dyed ribonucleic acid"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Russia is Moscow.",
      "conflict_prompt": "The capital of Russia is Saint Petersburg.",
      "question": "What is the capital of Russia?",
      "options": [
        "A. Saint Petersburg",
        "B. Moscow",
        "C. Novosibirsk",
        "D. Kazan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The bicycle was popularized in the 19th century and earlier designs included the 'draisine' or hobby horse.",
      "conflict_prompt": "The bicycle was popularized in the 17th century and earlier designs included the 'draisine' or hobby horse.",
      "question": "In which century did early bicycle-like designs such as the draisine become known?",
      "options": [
        "A. 17th century",
        "B. 18th century",
        "C. 19th century",
        "D. 20th century"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency of the United Kingdom is the pound sterling.",
      "conflict_prompt": "The currency of the United Kingdom is the euro.",
      "question": "What is the official currency of the United Kingdom?",
      "options": [
        "A. Euro",
        "B. Pound sterling",
        "C. Dollar",
        "D. Franc"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element oxygen has the atomic number 8.",
      "conflict_prompt": "The element oxygen has the atomic number 10.",
      "question": "What is the atomic number of oxygen?",
      "options": [
        "A. 6",
        "B. 8",
        "C. 10",
        "D. 12"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city in the United States by population is New York City.",
      "conflict_prompt": "The largest city in the United States by population is Los Angeles.",
      "question": "Which city has the largest population in the United States?",
      "options": [
        "A. Los Angeles",
        "B. Chicago",
        "C. New York City",
        "D. Houston"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Thomas Edison is credited with inventing the practical incandescent light bulb.",
      "conflict_prompt": "Alexander Graham Bell is credited with inventing the practical incandescent light bulb.",
      "question": "Who is commonly credited with inventing a practical incandescent light bulb?",
      "options": [
        "A. Alexander Graham Bell",
        "B. Thomas Edison",
        "C. Nikola Tesla",
        "D. James Watt"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most abundant element in the Earth's crust is oxygen.",
      "conflict_prompt": "The most abundant element in the Earth's crust is iron.",
      "question": "Which element is most abundant in the Earth's crust by mass?",
      "options": [
        "A. Iron",
        "B. Oxygen",
        "C. Silicon",
        "D. Aluminum"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Venice is a famous Italian city built on a network of canals.",
      "conflict_prompt": "Venice is a famous Swiss city built on a network of canals.",
      "question": "In which country is the canal city of Venice located?",
      "options": [
        "A. Switzerland",
        "B. Italy",
        "C. France",
        "D. Austria"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest species of shark is the whale shark.",
      "conflict_prompt": "The largest species of shark is the great white shark.",
      "question": "Which species is the largest shark?",
      "options": [
        "A. Great white shark",
        "B. Tiger shark",
        "C. Whale shark",
        "D. Hammerhead shark"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol Fe is iron.",
      "conflict_prompt": "The chemical element with symbol Fe is fluorine.",
      "question": "Which element has the chemical symbol Fe?",
      "options": [
        "A. Fluorine",
        "B. Iron",
        "C. Francium",
        "D. Fermium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most spoken native language in the world is Mandarin Chinese.",
      "conflict_prompt": "The most spoken native language in the world is English.",
      "question": "Which language has the largest number of native speakers worldwide?",
      "options": [
        "A. English",
        "B. Spanish",
        "C. Mandarin Chinese",
        "D. Hindi"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest reigning monarch of the United Kingdom was Queen Elizabeth II (as of 2022).",
      "conflict_prompt": "The longest reigning monarch of the United Kingdom was King George III.",
      "question": "Who was the longest reigning British monarch as of 2022?",
      "options": [
        "A. King George III",
        "B. Queen Victoria",
        "C. Queen Elizabeth II",
        "D. King Henry VIII"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The deepest known point in Earth's oceans is the Mariana Trench's Challenger Deep.",
      "conflict_prompt": "The deepest known point in Earth's oceans is the Mariana Trench's Great Barrier.",
      "question": "What is the name of the deepest known point in Earth's oceans located in the Mariana Trench?",
      "options": [
        "A. Great Barrier",
        "B. Challenger Deep",
        "C. Tonga Trench",
        "D. Puerto Rico Trench"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest planet's largest moon, Ganymede, orbits Jupiter.",
      "conflict_prompt": "The largest planet's largest moon, Ganymede, orbits Saturn.",
      "question": "Which planet does the moon Ganymede orbit?",
      "options": [
        "A. Saturn",
        "B. Jupiter",
        "C. Uranus",
        "D. Neptune"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The animal known for changing color using chromatophores is the octopus (and other cephalopods).",
      "conflict_prompt": "The animal known for changing color using chromatophores is the elephant.",
      "question": "Which animal is known for changing color using chromatophores?",
      "options": [
        "A. Elephant",
        "B. Octopus",
        "C. Cow",
        "D. Rabbit"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Spain is Madrid.",
      "conflict_prompt": "The capital of Spain is Barcelona.",
      "question": "What is the capital of Spain?",
      "options": [
        "A. Barcelona",
        "B. Seville",
        "C. Madrid",
        "D. Valencia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The theory that species evolve through natural selection was proposed by Charles Darwin.",
      "conflict_prompt": "The theory that species evolve through natural selection was proposed by Gregor Mendel.",
      "question": "Who proposed the theory of evolution by natural selection?",
      "options": [
        "A. Gregor Mendel",
        "B. Jean-Baptiste Lamarck",
        "C. Charles Darwin",
        "D. Alfred Russel Wallace"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest freshwater lake by area is Lake Superior (one of the Great Lakes).",
      "conflict_prompt": "The largest freshwater lake by area is Lake Baikal.",
      "question": "Which lake is the largest freshwater lake by surface area?",
      "options": [
        "A. Lake Baikal",
        "B. Lake Victoria",
        "C. Lake Superior",
        "D. Caspian Sea"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The inventor of the World Wide Web is Tim Berners-Lee.",
      "conflict_prompt": "The inventor of the World Wide Web is Bill Gates.",
      "question": "Who invented the World Wide Web?",
      "options": [
        "A. Bill Gates",
        "B. Steve Jobs",
        "C. Tim Berners-Lee",
        "D. Vinton Cerf"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first artificial satellite to orbit Earth was Sputnik 1, launched by the Soviet Union in 1957.",
      "conflict_prompt": "The first artificial satellite to orbit Earth was Explorer 1, launched by the United States in 1957.",
      "question": "Which was the first artificial satellite to orbit Earth?",
      "options": [
        "A. Explorer 1",
        "B. Sputnik 1",
        "C. Vanguard 1",
        "D. Apollo 1"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The organ primarily responsible for pumping blood through the human body is the heart.",
      "conflict_prompt": "The organ primarily responsible for pumping blood through the human body is the liver.",
      "question": "Which organ pumps blood through the human body?",
      "options": [
        "A. Liver",
        "B. Heart",
        "C. Kidney",
        "D. Lung"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital city of India is New Delhi.",
      "conflict_prompt": "The capital city of India is Mumbai.",
      "question": "What is the capital of India?",
      "options": [
        "A. Mumbai",
        "B. New Delhi",
        "C. Kolkata",
        "D. Chennai"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary colors of additive color mixing are red, green, and blue (RGB).",
      "conflict_prompt": "The primary colors of additive color mixing are cyan, magenta, and yellow.",
      "question": "Which set represents the primary colors used in additive color mixing?",
      "options": [
        "A. Cyan, magenta, yellow",
        "B. Red, green, blue",
        "C. Red, yellow, blue",
        "D. Black, white, gray"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Egypt is Cairo.",
      "conflict_prompt": "The capital of Egypt is Alexandria.",
      "question": "What is the capital of Egypt?",
      "options": [
        "A. Alexandria",
        "B. Luxor",
        "C. Cairo",
        "D. Giza"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "Sound travels faster in water than in air.",
      "conflict_prompt": "Sound travels slower in water than in air.",
      "question": "How does the speed of sound in water compare to its speed in air?",
      "options": [
        "A. Slower in water",
        "B. Faster in water",
        "C. The same in both",
        "D. Faster in air"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The baseball player widely known as 'The Sultan of Swat' was Babe Ruth.",
      "conflict_prompt": "The baseball player widely known as 'The Sultan of Swat' was Jackie Robinson.",
      "question": "Which baseball player was nicknamed 'The Sultan of Swat'?",
      "options": [
        "A. Jackie Robinson",
        "B. Joe DiMaggio",
        "C. Mickey Mantle",
        "D. Babe Ruth"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Turkey is Ankara.",
      "conflict_prompt": "The capital of Turkey is Istanbul.",
      "question": "What is the capital of Turkey?",
      "options": [
        "A. Istanbul",
        "B. Izmir",
        "C. Ankara",
        "D. Bursa"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The currency used in India is the Indian rupee (INR).",
      "conflict_prompt": "The currency used in India is the Indian dollar.",
      "question": "What is the official currency of India?",
      "options": [
        "A. Indian dollar",
        "B. Indian rupee",
        "C. Yen",
        "D. Rupee (Pakistan)"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous physicist who proposed the theory of special relativity was Albert Einstein in 1905.",
      "conflict_prompt": "The famous physicist who proposed the theory of special relativity was Max Planck in 1905.",
      "question": "Who proposed the theory of special relativity in 1905?",
      "options": [
        "A. Max Planck",
        "B. Albert Einstein",
        "C. Niels Bohr",
        "D. Werner Heisenberg"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of China is Beijing.",
      "conflict_prompt": "The capital of China is Shanghai.",
      "question": "What is the capital of China?",
      "options": [
        "A. Shanghai",
        "B. Guangzhou",
        "C. Beijing",
        "D. Shenzhen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary anatomical structure used for human speech production is the larynx (voice box).",
      "conflict_prompt": "The primary anatomical structure used for human speech production is the stomach.",
      "question": "Which anatomical structure is primarily used for human voice production?",
      "options": [
        "A. Stomach",
        "B. Lungs",
        "C. Larynx (voice box)",
        "D. Esophagus"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Mexico is Mexico City.",
      "conflict_prompt": "The capital of Mexico is Guadalajara.",
      "question": "What is the capital of Mexico?",
      "options": [
        "A. Guadalajara",
        "B. Monterrey",
        "C. Mexico City",
        "D. Puebla"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The most abundant gas in the Sun is hydrogen.",
      "conflict_prompt": "The most abundant gas in the Sun is helium.",
      "question": "Which element is most abundant in the Sun?",
      "options": [
        "A. Helium",
        "B. Hydrogen",
        "C. Oxygen",
        "D. Carbon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous composer Ludwig van Beethoven was German.",
      "conflict_prompt": "The famous composer Ludwig van Beethoven was Austrian.",
      "question": "Of which nationality was Ludwig van Beethoven?",
      "options": [
        "A. Austrian",
        "B. German",
        "C. Italian",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for carbon dioxide is CO2.",
      "conflict_prompt": "The chemical formula for carbon dioxide is CO.",
      "question": "What is the chemical formula for carbon dioxide?",
      "options": [
        "A. CO",
        "B. CO2",
        "C. O2",
        "D. CH4"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest city in South America by population is São Paulo.",
      "conflict_prompt": "The largest city in South America by population is Buenos Aires.",
      "question": "Which city is the most populous in South America?",
      "options": [
        "A. Buenos Aires",
        "B. Rio de Janeiro",
        "C. Bogotá",
        "D. São Paulo"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary cause of tides on Earth is the gravitational pull of the Moon (and Sun).",
      "conflict_prompt": "The primary cause of tides on Earth is the rotation of the Earth alone.",
      "question": "What is the primary cause of Earth's ocean tides?",
      "options": [
        "A. Rotation of the Earth alone",
        "B. Wind patterns",
        "C. Gravitational pull of the Moon (and Sun)",
        "D. Ocean currents"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The main organ of photosynthesis in most plants is the leaf.",
      "conflict_prompt": "The main organ of photosynthesis in most plants is the root.",
      "question": "Which plant organ performs most photosynthesis?",
      "options": [
        "A. Root",
        "B. Stem",
        "C. Leaf",
        "D. Flower"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Argentina is Buenos Aires.",
      "conflict_prompt": "The capital of Argentina is Córdoba.",
      "question": "What is the capital of Argentina?",
      "options": [
        "A. Córdoba",
        "B. Mendoza",
        "C. Rosario",
        "D. Buenos Aires"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The primary material used to make glass is silica (silicon dioxide).",
      "conflict_prompt": "The primary material used to make glass is cellulose.",
      "question": "What is the primary raw material for most commercial glass?",
      "options": [
        "A. Cellulose",
        "B. Silicon dioxide (silica)",
        "C. Calcium carbonate",
        "D. Iron oxide"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The composer of the Brandenburg Concertos was Johann Sebastian Bach.",
      "conflict_prompt": "The composer of the Brandenburg Concertos was George Frideric Handel.",
      "question": "Who composed the Brandenburg Concertos?",
      "options": [
        "A. George Frideric Handel",
        "B. Ludwig van Beethoven",
        "C. Johann Sebastian Bach",
        "D. Wolfgang Amadeus Mozart"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The element with the chemical symbol Na is sodium.",
      "conflict_prompt": "The element with the chemical symbol Na is nitrogen.",
      "question": "Which element has the symbol Na?",
      "options": [
        "A. Nitrogen",
        "B. Sodium",
        "C. Neon",
        "D. Niobium"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of Brazil is Brasília.",
      "conflict_prompt": "The capital of Brazil is Rio de Janeiro.",
      "question": "What is the capital of Brazil?",
      "options": [
        "A. Rio de Janeiro",
        "B. São Paulo",
        "C. Brasília",
        "D. Salvador"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The famous painting 'The Starry Night' was painted by Vincent van Gogh.",
      "conflict_prompt": "The famous painting 'The Starry Night' was painted by Pablo Picasso.",
      "question": "Who painted 'The Starry Night'?",
      "options": [
        "A. Pablo Picasso",
        "B. Claude Monet",
        "C. Vincent van Gogh",
        "D. Henri Matisse"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest muscle in the human body by mass is the gluteus maximus.",
      "conflict_prompt": "The largest muscle in the human body by mass is the biceps brachii.",
      "question": "Which is the largest muscle in the human body by mass?",
      "options": [
        "A. Biceps brachii",
        "B. Deltoid",
        "C. Gluteus maximus",
        "D. Quadriceps"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of the United States is Washington, D.C.",
      "conflict_prompt": "The capital of the United States is New York City.",
      "question": "What is the capital of the United States?",
      "options": [
        "A. New York City",
        "B. Los Angeles",
        "C. Washington, D.C.",
        "D. Chicago"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The process by which plants convert sunlight into chemical energy is called photosynthesis.",
      "conflict_prompt": "The process by which plants convert sunlight into chemical energy is called respiration.",
      "question": "What is the process by which plants convert sunlight into chemical energy?",
      "options": [
        "A. Respiration",
        "B. Fermentation",
        "C. Photosynthesis",
        "D. Transpiration"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The longest river in Europe is the Volga River.",
      "conflict_prompt": "The longest river in Europe is the Danube River.",
      "question": "Which is the longest river in Europe?",
      "options": [
        "A. Danube",
        "B. Rhine",
        "C. Volga",
        "D. Dnieper"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first female prime minister of the United Kingdom was Margaret Thatcher.",
      "conflict_prompt": "The first female prime minister of the United Kingdom was Theresa May.",
      "question": "Who was the first female Prime Minister of the United Kingdom?",
      "options": [
        "A. Theresa May",
        "B. Margaret Thatcher",
        "C. Angela Merkel",
        "D. Indira Gandhi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical formula for ozone is O3.",
      "conflict_prompt": "The chemical formula for ozone is O2.",
      "question": "What is the chemical formula for ozone?",
      "options": [
        "A. O2",
        "B. O3",
        "C. O4",
        "D. O"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The largest coral reef system in the world is the Great Barrier Reef.",
      "conflict_prompt": "The largest coral reef system in the world is the Red Sea Reef.",
      "question": "Which is the largest coral reef system in the world?",
      "options": [
        "A. Red Sea Reef",
        "B. Great Barrier Reef",
        "C. Belize Barrier Reef",
        "D. New Caledonian Barrier Reef"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The first Olympic Games of the modern era were held in Athens in 1896.",
      "conflict_prompt": "The first Olympic Games of the modern era were held in Paris in 1896.",
      "question": "Where were the first modern Olympic Games held in 1896?",
      "options": [
        "A. Paris",
        "B. London",
        "C. Athens",
        "D. Rome"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The organelle responsible for producing ATP in eukaryotic cells is the mitochondrion.",
      "conflict_prompt": "The organelle responsible for producing ATP in eukaryotic cells is the chloroplast.",
      "question": "Which organelle is primarily responsible for ATP production in most eukaryotic cells?",
      "options": [
        "A. Chloroplast",
        "B. Nucleus",
        "C. Ribosome",
        "D. Mitochondrion"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The capital of South Africa (one of the three capitals) with the administrative seat is Pretoria.",
      "conflict_prompt": "The capital of South Africa (one of the three capitals) with the administrative seat is Cape Town.",
      "question": "Which city serves as the administrative (executive) capital of South Africa?",
      "options": [
        "A. Cape Town",
        "B. Johannesburg",
        "C. Durban",
        "D. Pretoria"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The chemical element with symbol K is potassium.",
      "conflict_prompt": "The chemical element with symbol K is krypton.",
      "question": "Which element has the chemical symbol K?",
      "options": [
        "A. Krypton",
        "B. Potassium",
        "C. Calcium",
        "D. Kryptonite"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "factual_contradictions",
      "conflict_type": "factual_inconsistency"
    },
    {
      "clean_prompt": "The company was founded in 2005 and went public in 2015.",
      "conflict_prompt": "The company was founded in 2005 and went public in 2000.",
      "question": "In what year did the company go public?",
      "options": [
        "A. 2000",
        "B. 2012",
        "C. 2015",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She completed medical school in 2010 and started residency in 2011.",
      "conflict_prompt": "She completed medical school in 2010 and started residency in 2005.",
      "question": "When did she begin her residency?",
      "options": [
        "A. 2005",
        "B. 2011",
        "C. 2013",
        "D. 2009"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge construction began in 2016 and finished in 2019.",
      "conflict_prompt": "The bridge construction began in 2016 and finished in 2014.",
      "question": "In which year was the bridge completed?",
      "options": [
        "A. 2014",
        "B. 2018",
        "C. 2019",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He was born in 1988 and published his first novel in 2014.",
      "conflict_prompt": "He was born in 1988 and published his first novel in 1975.",
      "question": "When was his first novel published?",
      "options": [
        "A. 1980",
        "B. 1975",
        "C. 2014",
        "D. 1999"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film premiered in 2017 and filming took place in 2016.",
      "conflict_prompt": "The film premiered in 2017 and filming took place in 2020.",
      "question": "In what year did principal filming occur?",
      "options": [
        "A. 2015",
        "B. 2016",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The scientist discovered the compound in 1998 and published the results in 1999.",
      "conflict_prompt": "The scientist discovered the compound in 1998 and published the results in 1985.",
      "question": "In which year were the results published?",
      "options": [
        "A. 1985",
        "B. 1997",
        "C. 1999",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival started in 2003 and celebrated its tenth anniversary in 2013.",
      "conflict_prompt": "The festival started in 2003 and celebrated its tenth anniversary in 2008.",
      "question": "When did the festival mark its tenth anniversary?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He took office as mayor in 2012 and was reelected in 2016.",
      "conflict_prompt": "He took office as mayor in 2012 and was reelected in 2008.",
      "question": "In what year was he reelected?",
      "options": [
        "A. 2008",
        "B. 2016",
        "C. 2014",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Her retirement party was held in 2021 after she retired that same year.",
      "conflict_prompt": "Her retirement party was held in 2021 before she retired in 2015.",
      "question": "When was her retirement party?",
      "options": [
        "A. 2015",
        "B. 2018",
        "C. 2021",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new stadium opened in 2018 after construction began in 2015.",
      "conflict_prompt": "The new stadium opened in 2018 after construction began in 2020.",
      "question": "In what year did construction of the stadium begin?",
      "options": [
        "A. 2014",
        "B. 2020",
        "C. 2015",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They planted the orchard in spring 2010 and harvested the apples in autumn 2013.",
      "conflict_prompt": "They planted the orchard in spring 2010 and harvested the apples in autumn 2008.",
      "question": "When was the orchard harvested?",
      "options": [
        "A. 2008",
        "B. 2011",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The concert tour began in June 2019 and ended in September 2019.",
      "conflict_prompt": "The concert tour began in June 2019 and ended in March 2018.",
      "question": "When did the tour end?",
      "options": [
        "A. March 2018",
        "B. July 2019",
        "C. September 2019",
        "D. November 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He graduated from law school in 2007 and passed the bar exam later that year.",
      "conflict_prompt": "He graduated from law school in 2007 and passed the bar exam in 2000.",
      "question": "When did he pass the bar exam?",
      "options": [
        "A. 2000",
        "B. 2007",
        "C. 2010",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeological dig started in 1996 and uncovered artifacts in 1997.",
      "conflict_prompt": "The archaeological dig started in 1996 and uncovered artifacts in 1989.",
      "question": "In what year were artifacts first uncovered at the dig?",
      "options": [
        "A. 1989",
        "B. 1996",
        "C. 1997",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery opened its first location in 2010 and expanded to a second location in 2014.",
      "conflict_prompt": "The bakery opened its first location in 2010 and expanded to a second location in 2006.",
      "question": "When did the bakery open its second location?",
      "options": [
        "A. 2006",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He earned his PhD in 2015 after completing his dissertation that year.",
      "conflict_prompt": "He earned his PhD in 2015 but completed his dissertation in 2010.",
      "question": "When did he earn his PhD?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2015",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software version 2.0 was released in March 2020 following a beta period in late 2019.",
      "conflict_prompt": "The software version 2.0 was released in March 2020 following a beta period in 2022.",
      "question": "When was version 2.0 released?",
      "options": [
        "A. March 2019",
        "B. March 2020",
        "C. March 2021",
        "D. March 2022"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author signed the book contract in 2011 and the book was published in 2013.",
      "conflict_prompt": "The author signed the book contract in 2011 and the book was published in 2009.",
      "question": "When was the book published?",
      "options": [
        "A. 2009",
        "B. 2011",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They launched the beta site in January 2018 and launched the full site in April 2018.",
      "conflict_prompt": "They launched the beta site in January 2018 and launched the full site in April 2016.",
      "question": "When was the full site launched?",
      "options": [
        "A. April 2016",
        "B. February 2018",
        "C. April 2018",
        "D. May 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon took place on October 10, 2015 and registration opened in June 2015.",
      "conflict_prompt": "The marathon took place on October 10, 2015 and registration opened in June 2014.",
      "question": "When did registration open for the marathon?",
      "options": [
        "A. June 2014",
        "B. June 2015",
        "C. August 2015",
        "D. September 2015"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He was hired as a researcher in 2002 and promoted to lead in 2008.",
      "conflict_prompt": "He was hired as a researcher in 2002 and promoted to lead in 1997.",
      "question": "When was he promoted to lead?",
      "options": [
        "A. 1997",
        "B. 2005",
        "C. 2008",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city council approved the plan in 2014 and construction started in 2015.",
      "conflict_prompt": "The city council approved the plan in 2014 and construction started in 2010.",
      "question": "When did construction begin?",
      "options": [
        "A. 2010",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She recorded the album in 2012 and released it in 2013.",
      "conflict_prompt": "She recorded the album in 2012 and released it in 2008.",
      "question": "In what year was the album released?",
      "options": [
        "A. 2008",
        "B. 2012",
        "C. 2013",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school opened in September 1995 and added a new wing in 2002.",
      "conflict_prompt": "The school opened in September 1995 and added a new wing in 1988.",
      "question": "When was the new wing added?",
      "options": [
        "A. 1988",
        "B. 1999",
        "C. 2002",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He set up his practice in 1999 and performed his last surgery before retiring in 2020.",
      "conflict_prompt": "He set up his practice in 1999 and performed his last surgery before retiring in 1990.",
      "question": "When did he retire?",
      "options": [
        "A. 1990",
        "B. 2005",
        "C. 2017",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research grant was awarded in 2017 and the project concluded in 2020.",
      "conflict_prompt": "The research grant was awarded in 2017 and the project concluded in 2015.",
      "question": "When did the project conclude?",
      "options": [
        "A. 2015",
        "B. 2017",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The astronaut launched into orbit in 2009 and returned to Earth in 2010.",
      "conflict_prompt": "The astronaut launched into orbit in 2009 and returned to Earth in 2005.",
      "question": "When did the astronaut return to Earth?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2010",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The restaurant opened in 2001 and was remodeled in 2010.",
      "conflict_prompt": "The restaurant opened in 2001 and was remodeled in 1995.",
      "question": "When was the restaurant remodeled?",
      "options": [
        "A. 1995",
        "B. 2001",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They acquired the startup in 2016 and integrated its team in 2017.",
      "conflict_prompt": "They acquired the startup in 2016 and integrated its team in 2014.",
      "question": "When was the startup team integrated?",
      "options": [
        "A. 2014",
        "B. 2016",
        "C. 2017",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete won the national title in 2011 and competed internationally in 2012.",
      "conflict_prompt": "The athlete won the national title in 2011 and competed internationally in 2009.",
      "question": "In what year did the athlete compete internationally?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play premiered in February 2000 and closed in May 2000.",
      "conflict_prompt": "The play premiered in February 2000 and closed in December 1999.",
      "question": "When did the play close?",
      "options": [
        "A. December 1999",
        "B. April 2000",
        "C. May 2000",
        "D. June 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient was diagnosed with the condition in 2013 and began treatment in 2014.",
      "conflict_prompt": "The patient was diagnosed with the condition in 2013 and began treatment in 2010.",
      "question": "When did the patient begin treatment?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The textbook was first published in 1992 and revised edition appeared in 2001.",
      "conflict_prompt": "The textbook was first published in 1992 and revised edition appeared in 1987.",
      "question": "When was the revised edition published?",
      "options": [
        "A. 1987",
        "B. 1995",
        "C. 2001",
        "D. 2003"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She launched her podcast in 2019 and released episode 50 in 2020.",
      "conflict_prompt": "She launched her podcast in 2019 and released episode 50 in 2016.",
      "question": "When did she release episode 50?",
      "options": [
        "A. 2016",
        "B. 2018",
        "C. 2020",
        "D. 2021"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The medication received approval in 2003 and hit the market in 2004.",
      "conflict_prompt": "The medication received approval in 2003 and hit the market in 1999.",
      "question": "When did the medication become available on the market?",
      "options": [
        "A. 1999",
        "B. 2001",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite was launched in 2011 and decommissioned in 2019.",
      "conflict_prompt": "The satellite was launched in 2011 and decommissioned in 2007.",
      "question": "In what year was the satellite decommissioned?",
      "options": [
        "A. 2007",
        "B. 2011",
        "C. 2015",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer wrote the symphony in 1888 and conducted its premiere in 1889.",
      "conflict_prompt": "The composer wrote the symphony in 1888 and conducted its premiere in 1875.",
      "question": "When was the symphony premiered under his baton?",
      "options": [
        "A. 1875",
        "B. 1888",
        "C. 1889",
        "D. 1892"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship was launched in 1912 and completed fitting out in 1913.",
      "conflict_prompt": "The ship was launched in 1912 and completed fitting out in 1905.",
      "question": "When was the ship's fitting out completed?",
      "options": [
        "A. 1905",
        "B. 1910",
        "C. 1913",
        "D. 1915"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He moved to the city in 2004 and bought his house in 2007.",
      "conflict_prompt": "He moved to the city in 2004 and bought his house in 1999.",
      "question": "When did he purchase his house?",
      "options": [
        "A. 1999",
        "B. 2004",
        "C. 2007",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software patch was applied on April 10, 2021 and the system restarted that evening.",
      "conflict_prompt": "The software patch was applied on April 10, 2021 and the system restarted on March 5, 2020.",
      "question": "On which date was the patch applied?",
      "options": [
        "A. March 5, 2020",
        "B. April 10, 2021",
        "C. May 1, 2021",
        "D. April 1, 2020"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The course began in January 2016 and final exams were in May 2016.",
      "conflict_prompt": "The course began in January 2016 and final exams were in December 2014.",
      "question": "When were the final exams held?",
      "options": [
        "A. December 2014",
        "B. March 2016",
        "C. May 2016",
        "D. June 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They adopted the dog in 2018 and celebrated its third birthday in 2021.",
      "conflict_prompt": "They adopted the dog in 2018 and celebrated its third birthday in 2016.",
      "question": "When did they celebrate the dog's third birthday?",
      "options": [
        "A. 2016",
        "B. 2018",
        "C. 2020",
        "D. 2021"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The monument was commissioned in 1970 and unveiled in 1975.",
      "conflict_prompt": "The monument was commissioned in 1970 and unveiled in 1965.",
      "question": "In what year was the monument unveiled?",
      "options": [
        "A. 1965",
        "B. 1973",
        "C. 1975",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new law was proposed in 2018 and went into effect in 2019.",
      "conflict_prompt": "The new law was proposed in 2018 and went into effect in 2016.",
      "question": "When did the law go into effect?",
      "options": [
        "A. 2016",
        "B. 2017",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The fashion line debuted at Fashion Week in 2014 and hit stores in 2015.",
      "conflict_prompt": "The fashion line debuted at Fashion Week in 2014 and hit stores in 2012.",
      "question": "When did the line become available in stores?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The hospital wing opened in 2000 and the first patient was admitted that year.",
      "conflict_prompt": "The hospital wing opened in 2000 and the first patient was admitted in 1995.",
      "question": "When was the first patient admitted to the new wing?",
      "options": [
        "A. 1995",
        "B. 1998",
        "C. 2000",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The game was released worldwide in 2013 and a sequel was announced in 2016.",
      "conflict_prompt": "The game was released worldwide in 2013 and a sequel was announced in 2010.",
      "question": "When was the sequel announced?",
      "options": [
        "A. 2010",
        "B. 2013",
        "C. 2015",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They completed the renovation in August 2012 and reopened the gallery that month.",
      "conflict_prompt": "They completed the renovation in August 2012 and reopened the gallery in January 2009.",
      "question": "When did the gallery reopen after renovation?",
      "options": [
        "A. January 2009",
        "B. August 2010",
        "C. August 2012",
        "D. September 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The candidate announced his campaign in March 2020 and won the primary in July 2020.",
      "conflict_prompt": "The candidate announced his campaign in March 2020 and won the primary in 2018.",
      "question": "When did he win the primary?",
      "options": [
        "A. 2018",
        "B. March 2020",
        "C. July 2020",
        "D. November 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The book club began meeting monthly in 2016 and read ten books that year.",
      "conflict_prompt": "The book club began meeting monthly in 2016 and read ten books in 2012.",
      "question": "When did the book club read ten books?",
      "options": [
        "A. 2012",
        "B. 2014",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patent application was filed in 2007 and granted in 2010.",
      "conflict_prompt": "The patent application was filed in 2007 and granted in 2003.",
      "question": "When was the patent granted?",
      "options": [
        "A. 2003",
        "B. 2007",
        "C. 2010",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir was formed in 1990 and performed its 100th concert in 2005.",
      "conflict_prompt": "The choir was formed in 1990 and performed its 100th concert in 1980.",
      "question": "When did the choir give its 100th performance?",
      "options": [
        "A. 1980",
        "B. 1995",
        "C. 2005",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She launched a charity in 2013 and organized its first fundraiser in 2014.",
      "conflict_prompt": "She launched a charity in 2013 and organized its first fundraiser in 2010.",
      "question": "When was the charity's first fundraiser organized?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The academy admitted its first class in fall 1982 and graduated the first class in 1986.",
      "conflict_prompt": "The academy admitted its first class in fall 1982 and graduated the first class in 1978.",
      "question": "When did the academy graduate its first class?",
      "options": [
        "A. 1978",
        "B. 1982",
        "C. 1984",
        "D. 1986"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The plane made its maiden flight in June 2001 and entered service in 2003.",
      "conflict_prompt": "The plane made its maiden flight in June 2001 and entered service in 1999.",
      "question": "When did the aircraft enter commercial service?",
      "options": [
        "A. 1999",
        "B. 2001",
        "C. 2003",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He married in 1994 and celebrated his 25th anniversary in 2019.",
      "conflict_prompt": "He married in 1994 and celebrated his 25th anniversary in 2010.",
      "question": "When did he celebrate his 25th wedding anniversary?",
      "options": [
        "A. 2010",
        "B. 2015",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The biotech startup began trials in 2012 and received results in 2016.",
      "conflict_prompt": "The biotech startup began trials in 2012 and received results in 2008.",
      "question": "When were the trial results obtained?",
      "options": [
        "A. 2008",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The sculpture was carved in 1890 and installed in the park in 1891.",
      "conflict_prompt": "The sculpture was carved in 1890 and installed in the park in 1880.",
      "question": "When was the sculpture installed in the park?",
      "options": [
        "A. 1880",
        "B. 1888",
        "C. 1890",
        "D. 1891"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup was incorporated in 2010 and closed its seed round in 2011.",
      "conflict_prompt": "The startup was incorporated in 2010 and closed its seed round in 2005.",
      "question": "When did the startup close its seed round?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2011",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new highway was approved in 2009 and opened to traffic in 2014.",
      "conflict_prompt": "The new highway was approved in 2009 and opened to traffic in 2006.",
      "question": "When did the highway open to traffic?",
      "options": [
        "A. 2006",
        "B. 2009",
        "C. 2012",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She enrolled in the master's program in 2017 and completed it in 2019.",
      "conflict_prompt": "She enrolled in the master's program in 2017 and completed it in 2015.",
      "question": "When did she complete her master's degree?",
      "options": [
        "A. 2015",
        "B. 2017",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The clock tower was repaired in 1999 and resumed chiming in 2000.",
      "conflict_prompt": "The clock tower was repaired in 1999 and resumed chiming in 1992.",
      "question": "When did the clock tower resume chiming?",
      "options": [
        "A. 1992",
        "B. 1995",
        "C. 1999",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra recorded the album in 1986 and released it in 1987.",
      "conflict_prompt": "The orchestra recorded the album in 1986 and released it in 1980.",
      "question": "When was the album released?",
      "options": [
        "A. 1980",
        "B. 1984",
        "C. 1986",
        "D. 1987"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference was scheduled for May 2015 and the keynote speaker was confirmed in March 2015.",
      "conflict_prompt": "The conference was scheduled for May 2015 and the keynote speaker was confirmed in March 2010.",
      "question": "When was the keynote speaker confirmed?",
      "options": [
        "A. March 2010",
        "B. January 2015",
        "C. March 2015",
        "D. April 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum acquired the painting in 1978 and displayed it publicly in 1979.",
      "conflict_prompt": "The museum acquired the painting in 1978 and displayed it publicly in 1970.",
      "question": "When was the painting first displayed to the public by the museum?",
      "options": [
        "A. 1970",
        "B. 1976",
        "C. 1978",
        "D. 1979"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new tram line was planned in 2006 and service began in 2011.",
      "conflict_prompt": "The new tram line was planned in 2006 and service began in 2004.",
      "question": "When did tram service start on the new line?",
      "options": [
        "A. 2004",
        "B. 2006",
        "C. 2009",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The television series premiered in 2002 and aired its final episode in 2008.",
      "conflict_prompt": "The television series premiered in 2002 and aired its final episode in 1999.",
      "question": "When did the series air its last episode?",
      "options": [
        "A. 1999",
        "B. 2004",
        "C. 2006",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The colony was founded in 1620 and celebrated its bicentennial in 1820.",
      "conflict_prompt": "The colony was founded in 1620 and celebrated its bicentennial in 1720.",
      "question": "When was the colony's bicentennial celebrated?",
      "options": [
        "A. 1720",
        "B. 1776",
        "C. 1800",
        "D. 1820"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer completed the opera in 1910 and its first performance was in 1911.",
      "conflict_prompt": "The composer completed the opera in 1910 and its first performance was in 1905.",
      "question": "When was the opera first performed?",
      "options": [
        "A. 1905",
        "B. 1908",
        "C. 1910",
        "D. 1911"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The highway toll was introduced in 2002 and removed in 2012.",
      "conflict_prompt": "The highway toll was introduced in 2002 and removed in 1998.",
      "question": "When was the toll removed?",
      "options": [
        "A. 1998",
        "B. 2002",
        "C. 2008",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The annual awards ceremony began in 1999 and honored its 20th recipient in 2018.",
      "conflict_prompt": "The annual awards ceremony began in 1999 and honored its 20th recipient in 2008.",
      "question": "When did the ceremony honor its 20th recipient?",
      "options": [
        "A. 2008",
        "B. 2010",
        "C. 2018",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The excavated tomb was dated to 1200 BCE and its artifacts were cataloged in 2010.",
      "conflict_prompt": "The excavated tomb was dated to 1200 BCE and its artifacts were cataloged in 1995.",
      "question": "When were the artifacts cataloged?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2010",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The community theater was built in 1955 and renovated in 1995.",
      "conflict_prompt": "The community theater was built in 1955 and renovated in 1945.",
      "question": "When did the renovation occur?",
      "options": [
        "A. 1945",
        "B. 1960",
        "C. 1990",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer premiered the quartet in 1977 and recorded it in 1979.",
      "conflict_prompt": "The composer premiered the quartet in 1977 and recorded it in 1970.",
      "question": "When was the quartet recorded?",
      "options": [
        "A. 1970",
        "B. 1975",
        "C. 1977",
        "D. 1979"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity held its first gala in 2003 and doubled its donations by 2006.",
      "conflict_prompt": "The charity held its first gala in 2003 and doubled its donations by 2000.",
      "question": "By what year had donations doubled?",
      "options": [
        "A. 2000",
        "B. 2003",
        "C. 2006",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The science center opened in 1988 and added a planetarium in 1994.",
      "conflict_prompt": "The science center opened in 1988 and added a planetarium in 1980.",
      "question": "When was the planetarium added to the science center?",
      "options": [
        "A. 1980",
        "B. 1988",
        "C. 1992",
        "D. 1994"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete broke the record in 2004 and retired the following year.",
      "conflict_prompt": "The athlete broke the record in 2004 and retired the previous year.",
      "question": "When did the athlete retire?",
      "options": [
        "A. 2003",
        "B. 2004",
        "C. 2005",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mobile app launched in 2012 and reached one million downloads in 2014.",
      "conflict_prompt": "The mobile app launched in 2012 and reached one million downloads in 2010.",
      "question": "When did the app reach one million downloads?",
      "options": [
        "A. 2010",
        "B. 2012",
        "C. 2014",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ensemble formed in 2000 and released its debut album in 2003.",
      "conflict_prompt": "The ensemble formed in 2000 and released its debut album in 1995.",
      "question": "When was the ensemble's debut album released?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2003",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The dam's construction began in 1972 and the reservoir filled by 1976.",
      "conflict_prompt": "The dam's construction began in 1972 and the reservoir filled by 1968.",
      "question": "By what year was the reservoir filled?",
      "options": [
        "A. 1968",
        "B. 1972",
        "C. 1974",
        "D. 1976"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The biographer completed research in 2008 and published the biography in 2011.",
      "conflict_prompt": "The biographer completed research in 2008 and published the biography in 2002.",
      "question": "When was the biography published?",
      "options": [
        "A. 2002",
        "B. 2006",
        "C. 2008",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The railroad line was surveyed in 1850 and construction started in 1852.",
      "conflict_prompt": "The railroad line was surveyed in 1850 and construction started in 1840.",
      "question": "When did construction of the railroad start?",
      "options": [
        "A. 1840",
        "B. 1848",
        "C. 1850",
        "D. 1852"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The community garden was planted in spring 2011 and produced vegetables that summer.",
      "conflict_prompt": "The community garden was planted in spring 2011 and produced vegetables that summer in 2009.",
      "question": "When did the garden produce vegetables that summer?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The exhibition opened on April 1, 1997 and closed on July 31, 1997.",
      "conflict_prompt": "The exhibition opened on April 1, 1997 and closed on December 31, 1996.",
      "question": "When did the exhibition close?",
      "options": [
        "A. December 31, 1996",
        "B. May 1, 1997",
        "C. July 31, 1997",
        "D. August 1, 1997"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity was registered in 1993 and began operating in 1994.",
      "conflict_prompt": "The charity was registered in 1993 and began operating in 1985.",
      "question": "When did the charity begin operations?",
      "options": [
        "A. 1985",
        "B. 1990",
        "C. 1993",
        "D. 1994"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research paper was submitted in June 2009 and accepted in November 2009.",
      "conflict_prompt": "The research paper was submitted in June 2009 and accepted in May 2008.",
      "question": "When was the paper accepted?",
      "options": [
        "A. May 2008",
        "B. June 2009",
        "C. September 2009",
        "D. November 2009"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The statue was commissioned in 1855 and completed by 1860.",
      "conflict_prompt": "The statue was commissioned in 1855 and completed by 1848.",
      "question": "By what year was the statue completed?",
      "options": [
        "A. 1848",
        "B. 1855",
        "C. 1858",
        "D. 1860"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The zoo opened its reptile house in 1999 and added an aviary in 2004.",
      "conflict_prompt": "The zoo opened its reptile house in 1999 and added an aviary in 1992.",
      "question": "When was the aviary added to the zoo?",
      "options": [
        "A. 1992",
        "B. 1999",
        "C. 2001",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The researcher joined the lab in 2000 and led the experiment in 2005.",
      "conflict_prompt": "The researcher joined the lab in 2000 and led the experiment in 1995.",
      "question": "When did the researcher lead the experiment?",
      "options": [
        "A. 1995",
        "B. 2000",
        "C. 2003",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The winter festival was held in December 2010 and featured fireworks on December 31.",
      "conflict_prompt": "The winter festival was held in December 2010 and featured fireworks on December 31, 2009.",
      "question": "When were the fireworks held?",
      "options": [
        "A. December 31, 2009",
        "B. December 24, 2010",
        "C. December 31, 2010",
        "D. January 1, 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship set sail in July 1840 and returned to port in November 1841.",
      "conflict_prompt": "The ship set sail in July 1840 and returned to port in May 1839.",
      "question": "When did the ship return to port?",
      "options": [
        "A. May 1839",
        "B. July 1840",
        "C. September 1840",
        "D. November 1841"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The artist apprenticed from 1798 to 1804 and opened his studio in 1805.",
      "conflict_prompt": "The artist apprenticed from 1798 to 1804 and opened his studio in 1790.",
      "question": "When did he open his studio?",
      "options": [
        "A. 1790",
        "B. 1798",
        "C. 1804",
        "D. 1805"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot logged his first solo flight in 2003 and earned his commercial license in 2006.",
      "conflict_prompt": "The pilot logged his first solo flight in 2003 and earned his commercial license in 1998.",
      "question": "When did he earn his commercial pilot license?",
      "options": [
        "A. 1998",
        "B. 2000",
        "C. 2003",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival parade was canceled in 2020 and resumed in 2022.",
      "conflict_prompt": "The festival parade was canceled in 2020 and resumed in 2018.",
      "question": "When did the parade resume?",
      "options": [
        "A. 2018",
        "B. 2019",
        "C. 2021",
        "D. 2022"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historian published the monograph in 1991 and won an award in 1992.",
      "conflict_prompt": "The historian published the monograph in 1991 and won an award in 1985.",
      "question": "When did the historian win the award for the monograph?",
      "options": [
        "A. 1985",
        "B. 1989",
        "C. 1991",
        "D. 1992"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The factory introduced automation in 2007 and reduced staff the following year.",
      "conflict_prompt": "The factory introduced automation in 2007 and reduced staff in 2000.",
      "question": "When did the staff reductions occur?",
      "options": [
        "A. 2000",
        "B. 2005",
        "C. 2007",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research team was formed in 2014 and published its first paper in 2016.",
      "conflict_prompt": "The research team was formed in 2014 and published its first paper in 2010.",
      "question": "When was the team's first paper published?",
      "options": [
        "A. 2010",
        "B. 2014",
        "C. 2016",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city hosted the games in 1991 and renovated the stadium in 1989 in preparation.",
      "conflict_prompt": "The city hosted the games in 1991 and renovated the stadium in 1995 in preparation.",
      "question": "When did the stadium renovation occur in preparation for the games?",
      "options": [
        "A. 1985",
        "B. 1989",
        "C. 1995",
        "D. 1991"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum closed for refurbishment in 2008 and reopened in 2010.",
      "conflict_prompt": "The museum closed for refurbishment in 2008 and reopened in 2005.",
      "question": "When did the museum reopen after refurbishment?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2009",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer began sketching the theme in 1964 and finished the symphony in 1967.",
      "conflict_prompt": "The composer began sketching the theme in 1964 and finished the symphony in 1959.",
      "question": "When was the symphony completed?",
      "options": [
        "A. 1959",
        "B. 1961",
        "C. 1964",
        "D. 1967"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The entrepreneur founded the business in 1998 and sold it in 2006.",
      "conflict_prompt": "The entrepreneur founded the business in 1998 and sold it in 1990.",
      "question": "When did the entrepreneur sell the business?",
      "options": [
        "A. 1990",
        "B. 1998",
        "C. 2002",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The TV station began broadcasting in 1956 and moved to color broadcasts in 1968.",
      "conflict_prompt": "The TV station began broadcasting in 1956 and moved to color broadcasts in 1950.",
      "question": "When did the station switch to color broadcasts?",
      "options": [
        "A. 1950",
        "B. 1956",
        "C. 1960",
        "D. 1968"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The farmer sowed the seeds in April 2013 and harvest began in September 2013.",
      "conflict_prompt": "The farmer sowed the seeds in April 2013 and harvest began in September 2010.",
      "question": "When did the harvest begin?",
      "options": [
        "A. September 2010",
        "B. April 2013",
        "C. July 2013",
        "D. September 2013"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ballet company toured Europe in 2007 and returned home in December 2007.",
      "conflict_prompt": "The ballet company toured Europe in 2007 and returned home in December 2005.",
      "question": "When did the company return home from the tour?",
      "options": [
        "A. December 2005",
        "B. October 2007",
        "C. December 2007",
        "D. January 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The magazine published its inaugural issue in January 1990 and celebrated 25 years in 2015.",
      "conflict_prompt": "The magazine published its inaugural issue in January 1990 and celebrated 25 years in 2000.",
      "question": "When did the magazine celebrate its 25th anniversary?",
      "options": [
        "A. 2000",
        "B. 2005",
        "C. 2015",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The farmer installed irrigation in 1997 and reported higher yields by 2000.",
      "conflict_prompt": "The farmer installed irrigation in 1997 and reported higher yields by 1990.",
      "question": "By what year did yields increase after installing irrigation?",
      "options": [
        "A. 1990",
        "B. 1995",
        "C. 1997",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir recorded a live album in 2010 and released it in 2011.",
      "conflict_prompt": "The choir recorded a live album in 2010 and released it in 2008.",
      "question": "When was the live album released?",
      "options": [
        "A. 2008",
        "B. 2009",
        "C. 2010",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ice rink opened for the season in November 2018 and closed in March 2019.",
      "conflict_prompt": "The ice rink opened for the season in November 2018 and closed in March 2015.",
      "question": "When did the rink close for the season?",
      "options": [
        "A. March 2015",
        "B. December 2018",
        "C. March 2019",
        "D. April 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The band formed in 1996 and released a greatest hits album in 2006.",
      "conflict_prompt": "The band formed in 1996 and released a greatest hits album in 1990.",
      "question": "When did the band release its greatest hits album?",
      "options": [
        "A. 1990",
        "B. 1996",
        "C. 2000",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The library acquired the rare manuscript in 1972 and digitized it in 2005.",
      "conflict_prompt": "The library acquired the rare manuscript in 1972 and digitized it in 1965.",
      "question": "When was the manuscript digitized?",
      "options": [
        "A. 1965",
        "B. 1972",
        "C. 1995",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company launched its flagship product in 1991 and introduced an upgraded model in 1994.",
      "conflict_prompt": "The company launched its flagship product in 1991 and introduced an upgraded model in 1985.",
      "question": "When was the upgraded model introduced?",
      "options": [
        "A. 1985",
        "B. 1991",
        "C. 1993",
        "D. 1994"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new museum wing was proposed in 2003 and construction completed in 2008.",
      "conflict_prompt": "The new museum wing was proposed in 2003 and construction completed in 2000.",
      "question": "When was construction of the new wing completed?",
      "options": [
        "A. 2000",
        "B. 2003",
        "C. 2006",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The singer released a single in May 2010 and an album in November 2010.",
      "conflict_prompt": "The singer released a single in May 2010 and an album in November 2008.",
      "question": "When was the album released?",
      "options": [
        "A. November 2008",
        "B. May 2010",
        "C. September 2010",
        "D. November 2010"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The tidal barrier was approved in 1980 and completed in 1986.",
      "conflict_prompt": "The tidal barrier was approved in 1980 and completed in 1975.",
      "question": "When was the tidal barrier completed?",
      "options": [
        "A. 1975",
        "B. 1979",
        "C. 1980",
        "D. 1986"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The university opened a satellite campus in 2004 and enrolled its first students in 2005.",
      "conflict_prompt": "The university opened a satellite campus in 2004 and enrolled its first students in 2000.",
      "question": "When did the satellite campus accept its first students?",
      "options": [
        "A. 2000",
        "B. 2002",
        "C. 2004",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The parade route was redesigned in 2011 and the new route debuted in 2012.",
      "conflict_prompt": "The parade route was redesigned in 2011 and the new route debuted in 2009.",
      "question": "When did the parade use the redesigned route for the first time?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The electronics firm started R&D on the prototype in 2013 and built a working model in 2015.",
      "conflict_prompt": "The electronics firm started R&D on the prototype in 2013 and built a working model in 2012.",
      "question": "When did they build a working model?",
      "options": [
        "A. 2012",
        "B. 2013",
        "C. 2014",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright wrote the script in 2001 and rehearsals began in 2002.",
      "conflict_prompt": "The playwright wrote the script in 2001 and rehearsals began in 1998.",
      "question": "When did rehearsals for the play start?",
      "options": [
        "A. 1998",
        "B. 2000",
        "C. 2001",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author's first novel was published in 2003 and the sequel came out in 2007.",
      "conflict_prompt": "The author's first novel was published in 2003 and the sequel came out in 1999.",
      "question": "In what year was the sequel published?",
      "options": [
        "A. 1999",
        "B. 2005",
        "C. 2007",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She started university in 2011 and completed her degree in 2015.",
      "conflict_prompt": "She started university in 2011 and completed her degree in 2009.",
      "question": "When did she complete her degree?",
      "options": [
        "A. 2012",
        "B. 2010",
        "C. 2015",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bridge construction began in March 2018 and was finished in November 2020.",
      "conflict_prompt": "The bridge construction began in March 2018 and was finished in January 2016.",
      "question": "When was the bridge finished?",
      "options": [
        "A. 2016",
        "B. 2019",
        "C. 2020",
        "D. 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He was hired as a junior analyst in 2010 and promoted to manager in 2016.",
      "conflict_prompt": "He was hired as a junior analyst in 2010 and promoted to manager in 2008.",
      "question": "In what year did he become manager?",
      "options": [
        "A. 2008",
        "B. 2012",
        "C. 2016",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The festival took place every summer from 2012 through 2016 and ended in 2017.",
      "conflict_prompt": "The festival took place every summer from 2012 through 2016 and ended in 2009.",
      "question": "When did the festival end?",
      "options": [
        "A. 2014",
        "B. 2009",
        "C. 2017",
        "D. 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patient was diagnosed with the condition in June 2014 and underwent surgery in August 2014.",
      "conflict_prompt": "The patient was diagnosed with the condition in June 2014 and underwent surgery in April 2013.",
      "question": "When did the patient have surgery?",
      "options": [
        "A. April 2013",
        "B. July 2014",
        "C. August 2014",
        "D. May 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city installed new traffic lights in 2019 and collected data on congestion in 2020.",
      "conflict_prompt": "The city installed new traffic lights in 2019 and collected data on congestion in 2017.",
      "question": "When was congestion data collected?",
      "options": [
        "A. 2018",
        "B. 2017",
        "C. 2020",
        "D. 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They signed the lease in January 2021 and moved into the office in March 2021.",
      "conflict_prompt": "They signed the lease in January 2021 and moved into the office in November 2020.",
      "question": "When did they move into the office?",
      "options": [
        "A. December 2020",
        "B. November 2020",
        "C. March 2021",
        "D. February 2021"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The art exhibition opened in June 2016 and closed in September 2016.",
      "conflict_prompt": "The art exhibition opened in June 2016 and closed in March 2015.",
      "question": "In which month did the exhibition close?",
      "options": [
        "A. March 2015",
        "B. August 2016",
        "C. September 2016",
        "D. July 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup incorporated in 2014 and launched its product in 2017.",
      "conflict_prompt": "The startup incorporated in 2014 and launched its product in 2012.",
      "question": "When did the company launch its product?",
      "options": [
        "A. 2013",
        "B. 2012",
        "C. 2017",
        "D. 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer wrote the symphony in 1888 and premiered it in 1890.",
      "conflict_prompt": "The composer wrote the symphony in 1888 and premiered it in 1885.",
      "question": "When was the symphony premiered?",
      "options": [
        "A. 1885",
        "B. 1889",
        "C. 1890",
        "D. 1892"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She obtained her pilot's license in 2013 and logged her first solo flight in 2014.",
      "conflict_prompt": "She obtained her pilot's license in 2013 and logged her first solo flight in 2011.",
      "question": "When was her first solo flight logged?",
      "options": [
        "A. 2010",
        "B. 2011",
        "C. 2014",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum acquired the painting in 1995 and exhibited it publicly in 1998.",
      "conflict_prompt": "The museum acquired the painting in 1995 and exhibited it publicly in 1990.",
      "question": "When was the painting first exhibited publicly by the museum?",
      "options": [
        "A. 1993",
        "B. 1990",
        "C. 1998",
        "D. 1996"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference was scheduled for May 2022 and the keynote was announced in March 2022.",
      "conflict_prompt": "The conference was scheduled for May 2022 and the keynote was announced in July 2023.",
      "question": "When was the keynote announced?",
      "options": [
        "A. January 2022",
        "B. July 2023",
        "C. March 2022",
        "D. May 2022"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The college football team won the championship in 1999 after finishing the regular season in 1999 undefeated.",
      "conflict_prompt": "The college football team won the championship in 1999 after finishing the regular season in 2001 undefeated.",
      "question": "In what year did they finish the regular season undefeated?",
      "options": [
        "A. 2000",
        "B. 1998",
        "C. 1999",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renovation project started in October 2015 and was completed in April 2016.",
      "conflict_prompt": "The renovation project started in October 2015 and was completed in June 2014.",
      "question": "When did the renovation finish?",
      "options": [
        "A. June 2014",
        "B. December 2015",
        "C. April 2016",
        "D. February 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The spacecraft launched on July 14, 2015 and entered orbit on July 16, 2015.",
      "conflict_prompt": "The spacecraft launched on July 14, 2015 and entered orbit on July 12, 2015.",
      "question": "When did the spacecraft enter orbit?",
      "options": [
        "A. July 12, 2015",
        "B. July 15, 2015",
        "C. July 16, 2015",
        "D. July 14, 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He completed the marathon in 2018 after training for eight months starting in January 2018.",
      "conflict_prompt": "He completed the marathon in 2018 after training for eight months starting in May 2019.",
      "question": "When did his training start?",
      "options": [
        "A. May 2019",
        "B. January 2018",
        "C. February 2018",
        "D. March 2017"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software beta was released in September 2020 and the stable version shipped in February 2021.",
      "conflict_prompt": "The software beta was released in September 2020 and the stable version shipped in June 2020.",
      "question": "When did the stable version ship?",
      "options": [
        "A. July 2021",
        "B. June 2020",
        "C. February 2021",
        "D. October 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete set the record during the 2012 finals after qualifying in the 2012 semifinals.",
      "conflict_prompt": "The athlete set the record during the 2012 finals after qualifying in the 2014 semifinals.",
      "question": "When did the athlete qualify for the semifinals?",
      "options": [
        "A. 2011",
        "B. 2014",
        "C. 2012",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company was incorporated in 1990 and celebrated its 25th anniversary in 2015.",
      "conflict_prompt": "The company was incorporated in 1990 and celebrated its 25th anniversary in 2005.",
      "question": "When did the company celebrate its 25th anniversary?",
      "options": [
        "A. 2005",
        "B. 2010",
        "C. 2015",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She adopted the puppy in March 2017 and took it to its first vet visit in April 2017.",
      "conflict_prompt": "She adopted the puppy in March 2017 and took it to its first vet visit in January 2016.",
      "question": "When was the puppy's first vet visit?",
      "options": [
        "A. January 2016",
        "B. April 2017",
        "C. March 2017",
        "D. February 2018"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The newspaper reported the scandal in May 2001 and the official inquiry began in June 2001.",
      "conflict_prompt": "The newspaper reported the scandal in May 2001 and the official inquiry began in April 2000.",
      "question": "When did the official inquiry begin?",
      "options": [
        "A. April 2000",
        "B. June 2001",
        "C. May 2001",
        "D. July 2001"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The harvest started in late September 2010 and ended in mid October 2010.",
      "conflict_prompt": "The harvest started in late September 2010 and ended in August 2010.",
      "question": "When did the harvest end?",
      "options": [
        "A. August 2010",
        "B. September 2010",
        "C. October 2010",
        "D. November 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play premiered on March 3, 2002 and closed on April 20, 2002.",
      "conflict_prompt": "The play premiered on March 3, 2002 and closed on January 15, 2001.",
      "question": "When did the play close?",
      "options": [
        "A. January 15, 2001",
        "B. March 3, 2002",
        "C. April 20, 2002",
        "D. February 28, 2002"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The biologist discovered the species in 1997 and published the paper describing it in 1999.",
      "conflict_prompt": "The biologist discovered the species in 1997 and published the paper describing it in 1995.",
      "question": "When was the paper published?",
      "options": [
        "A. 1996",
        "B. 1995",
        "C. 1999",
        "D. 1998"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The album was recorded in 2010 and released in 2011.",
      "conflict_prompt": "The album was recorded in 2010 and released in 2008.",
      "question": "When was the album released?",
      "options": [
        "A. 2009",
        "B. 2008",
        "C. 2011",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The committee formed in 2006 and issued its final report in 2009.",
      "conflict_prompt": "The committee formed in 2006 and issued its final report in 2004.",
      "question": "When did the committee issue its final report?",
      "options": [
        "A. 2007",
        "B. 2004",
        "C. 2009",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The twins were born in 2004 and celebrated their 10th birthday in 2014.",
      "conflict_prompt": "The twins were born in 2004 and celebrated their 10th birthday in 2009.",
      "question": "When did they celebrate their 10th birthday?",
      "options": [
        "A. 2009",
        "B. 2012",
        "C. 2014",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The retailer opened its first store in 1987 and expanded to 50 locations by 1997.",
      "conflict_prompt": "The retailer opened its first store in 1987 and expanded to 50 locations by 1980.",
      "question": "By what year did the retailer have 50 locations?",
      "options": [
        "A. 1986",
        "B. 1980",
        "C. 1997",
        "D. 1990"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright wrote the script in 2001 and directed the production in 2003.",
      "conflict_prompt": "The playwright wrote the script in 2001 and directed the production in 1999.",
      "question": "When did the playwright direct the production?",
      "options": [
        "A. 2000",
        "B. 1999",
        "C. 2003",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity was founded in 1993 and ran its first fundraiser in 1994.",
      "conflict_prompt": "The charity was founded in 1993 and ran its first fundraiser in 1990.",
      "question": "When was the first fundraiser held?",
      "options": [
        "A. 1991",
        "B. 1990",
        "C. 1994",
        "D. 1995"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ocean liner embarked on its maiden voyage in June 1969 and was decommissioned in 1990.",
      "conflict_prompt": "The ocean liner embarked on its maiden voyage in June 1969 and was decommissioned in 1965.",
      "question": "When was the ship decommissioned?",
      "options": [
        "A. 1965",
        "B. 1978",
        "C. 1990",
        "D. 1985"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The actor filmed the sequel in 2004 after finishing the original movie in 2003.",
      "conflict_prompt": "The actor filmed the sequel in 2004 after finishing the original movie in 2006.",
      "question": "When was the original movie finished?",
      "options": [
        "A. 2002",
        "B. 2006",
        "C. 2003",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The satellite was built in 2017 and launched in 2019.",
      "conflict_prompt": "The satellite was built in 2017 and launched in 2016.",
      "question": "When was the satellite launched?",
      "options": [
        "A. 2015",
        "B. 2016",
        "C. 2019",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He passed his bar exam in July 2008 and was admitted to the bar in September 2008.",
      "conflict_prompt": "He passed his bar exam in July 2008 and was admitted to the bar in May 2007.",
      "question": "When was he admitted to the bar?",
      "options": [
        "A. May 2007",
        "B. July 2008",
        "C. September 2008",
        "D. August 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The film's principal photography took place in 2013 and post-production lasted through 2014.",
      "conflict_prompt": "The film's principal photography took place in 2013 and post-production lasted through 2011.",
      "question": "Until what year did post-production last?",
      "options": [
        "A. 2012",
        "B. 2011",
        "C. 2014",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research began in January 2000 and the longitudinal study concluded in December 2010.",
      "conflict_prompt": "The research began in January 2000 and the longitudinal study concluded in December 1998.",
      "question": "When did the longitudinal study conclude?",
      "options": [
        "A. December 1998",
        "B. December 2005",
        "C. December 2010",
        "D. December 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vineyard planted its first vines in 1999 and produced its first vintage in 2003.",
      "conflict_prompt": "The vineyard planted its first vines in 1999 and produced its first vintage in 1996.",
      "question": "When was the first vintage produced?",
      "options": [
        "A. 1996",
        "B. 1999",
        "C. 2003",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city installed the new water mains in 2002 and connected homes in 2003.",
      "conflict_prompt": "The city installed the new water mains in 2002 and connected homes in 2000.",
      "question": "When were homes connected to the new mains?",
      "options": [
        "A. 1999",
        "B. 2000",
        "C. 2003",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historic house was purchased by the trust in 1980 and restored between 1981 and 1983.",
      "conflict_prompt": "The historic house was purchased by the trust in 1980 and restored between 1978 and 1979.",
      "question": "During which years was the house restored according to the statement?",
      "options": [
        "A. 1978–1979",
        "B. 1981–1983",
        "C. 1985–1987",
        "D. 1975–1976"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The meteorologist recorded the storm on August 2, 2016 and issued a warning on August 1, 2016.",
      "conflict_prompt": "The meteorologist recorded the storm on August 2, 2016 and issued a warning on August 3, 2017.",
      "question": "On what date was the warning issued?",
      "options": [
        "A. August 3, 2017",
        "B. August 1, 2016",
        "C. August 2, 2016",
        "D. August 4, 2016"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "They broke ground on the stadium in 2007 and the first game was held there in 2009.",
      "conflict_prompt": "They broke ground on the stadium in 2007 and the first game was held there in 2005.",
      "question": "When was the first game held in the stadium?",
      "options": [
        "A. 2006",
        "B. 2005",
        "C. 2009",
        "D. 2008"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The heir was born in 1992 and inherited the estate upon the owner's death in 2018.",
      "conflict_prompt": "The heir was born in 1992 and inherited the estate upon the owner's death in 1985.",
      "question": "When did the owner die according to the statement?",
      "options": [
        "A. 1985",
        "B. 1992",
        "C. 2018",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The highway extension was approved in 2010 and construction started in 2011.",
      "conflict_prompt": "The highway extension was approved in 2010 and construction started in 2008.",
      "question": "When did construction start?",
      "options": [
        "A. 2009",
        "B. 2008",
        "C. 2011",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "She gave her first public lecture in 1996 and published the book based on it in 1998.",
      "conflict_prompt": "She gave her first public lecture in 1996 and published the book based on it in 1994.",
      "question": "When was the book published?",
      "options": [
        "A. 1994",
        "B. 1995",
        "C. 1998",
        "D. 1997"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity concert was organized in April 2005 and raised funds through May 2005.",
      "conflict_prompt": "The charity concert was organized in April 2005 and raised funds through March 2004.",
      "question": "Until when did the fundraiser run?",
      "options": [
        "A. March 2004",
        "B. April 2005",
        "C. May 2005",
        "D. June 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The athlete retired after the 2010 season following an injury sustained in 2010.",
      "conflict_prompt": "The athlete retired after the 2010 season following an injury sustained in 2008.",
      "question": "When was the injury sustained according to the statement?",
      "options": [
        "A. 2009",
        "B. 2008",
        "C. 2010",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The software's alpha test ran in February 2019 and the public beta started in April 2019.",
      "conflict_prompt": "The software's alpha test ran in February 2019 and the public beta started in January 2018.",
      "question": "When did the public beta start?",
      "options": [
        "A. March 2019",
        "B. January 2018",
        "C. April 2019",
        "D. February 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer completed the concerto in 1910 and it premiered in 1912.",
      "conflict_prompt": "The composer completed the concerto in 1910 and it premiered in 1908.",
      "question": "When did the concerto premiere?",
      "options": [
        "A. 1909",
        "B. 1908",
        "C. 1912",
        "D. 1910"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The company announced layoffs in June 2020 and the severance payments were distributed in July 2020.",
      "conflict_prompt": "The company announced layoffs in June 2020 and the severance payments were distributed in March 2019.",
      "question": "When were severance payments distributed?",
      "options": [
        "A. March 2019",
        "B. June 2020",
        "C. July 2020",
        "D. August 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The botanist collected samples in May 1983 and published the identification key in 1986.",
      "conflict_prompt": "The botanist collected samples in May 1983 and published the identification key in 1980.",
      "question": "When was the identification key published?",
      "options": [
        "A. 1981",
        "B. 1980",
        "C. 1986",
        "D. 1984"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot completed training in December 2002 and flew commercial routes starting January 2003.",
      "conflict_prompt": "The pilot completed training in December 2002 and flew commercial routes starting November 2001.",
      "question": "When did the pilot begin flying commercial routes according to the statement?",
      "options": [
        "A. October 2002",
        "B. November 2001",
        "C. January 2003",
        "D. December 2002"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The bakery opened in 2012 and introduced gluten-free options in 2015.",
      "conflict_prompt": "The bakery opened in 2012 and introduced gluten-free options in 2010.",
      "question": "When were gluten-free options introduced?",
      "options": [
        "A. 2011",
        "B. 2010",
        "C. 2015",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The documentary was filmed in 2007 and broadcast on television in 2008.",
      "conflict_prompt": "The documentary was filmed in 2007 and broadcast on television in 2005.",
      "question": "When was the documentary broadcast on television?",
      "options": [
        "A. 2006",
        "B. 2005",
        "C. 2008",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The historian completed archival research in 1994 and presented findings at a conference in 1995.",
      "conflict_prompt": "The historian completed archival research in 1994 and presented findings at a conference in 1993.",
      "question": "When were the findings presented at a conference?",
      "options": [
        "A. 1993",
        "B. 1994",
        "C. 1995",
        "D. 1996"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The road was closed for repairs from January through March 2011 and reopened in April 2011.",
      "conflict_prompt": "The road was closed for repairs from January through March 2011 and reopened in December 2010.",
      "question": "When did the road reopen?",
      "options": [
        "A. December 2010",
        "B. March 2011",
        "C. April 2011",
        "D. January 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play was rehearsed in September 1990 and first performed in October 1990.",
      "conflict_prompt": "The play was rehearsed in September 1990 and first performed in August 1989.",
      "question": "When was the first performance?",
      "options": [
        "A. August 1989",
        "B. September 1990",
        "C. October 1990",
        "D. July 1990"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The graduate defended her thesis in May 2013 and graduated in June 2013.",
      "conflict_prompt": "The graduate defended her thesis in May 2013 and graduated in April 2012.",
      "question": "When did she graduate?",
      "options": [
        "A. April 2012",
        "B. May 2013",
        "C. June 2013",
        "D. July 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The city hosted the parade on July 4, 2010 and cancelled it the following year in 2011.",
      "conflict_prompt": "The city hosted the parade on July 4, 2010 and cancelled it the following year in 2009.",
      "question": "When was the parade cancelled according to the statement?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new curriculum was approved in 2018 and implemented at the start of the 2019 school year.",
      "conflict_prompt": "The new curriculum was approved in 2018 and implemented at the start of the 2017 school year.",
      "question": "When was the curriculum implemented?",
      "options": [
        "A. 2017",
        "B. 2018",
        "C. 2019",
        "D. 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The painter completed the mural in August 2000 and vaccinated his assistant afterward in September 2000.",
      "conflict_prompt": "The painter completed the mural in August 2000 and vaccinated his assistant afterward in July 1999.",
      "question": "When was the assistant vaccinated according to the statement?",
      "options": [
        "A. July 1999",
        "B. August 2000",
        "C. September 2000",
        "D. June 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The school opened its new science wing in 2004 and the dedication ceremony was held in October 2004.",
      "conflict_prompt": "The school opened its new science wing in 2004 and the dedication ceremony was held in March 2003.",
      "question": "When was the dedication ceremony held?",
      "options": [
        "A. March 2003",
        "B. September 2004",
        "C. October 2004",
        "D. November 2004"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "He signed the contract in February 1997 and started work under the contract on April 1, 1997.",
      "conflict_prompt": "He signed the contract in February 1997 and started work under the contract on January 1, 1996.",
      "question": "When did he start work under the contract?",
      "options": [
        "A. January 1, 1996",
        "B. March 1, 1997",
        "C. April 1, 1997",
        "D. February 1, 1997"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The pilot scheme ran from May 2016 through November 2016 and evaluation reports were released in December 2016.",
      "conflict_prompt": "The pilot scheme ran from May 2016 through November 2016 and evaluation reports were released in January 2015.",
      "question": "When were the evaluation reports released?",
      "options": [
        "A. January 2015",
        "B. November 2016",
        "C. December 2016",
        "D. October 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The singer recorded the single in spring 2018 and performed it live for the first time in June 2018.",
      "conflict_prompt": "The singer recorded the single in spring 2018 and performed it live for the first time in May 2017.",
      "question": "When was the first live performance?",
      "options": [
        "A. May 2017",
        "B. March 2018",
        "C. June 2018",
        "D. April 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The patent application was filed in 2000 and the patent was granted in 2004.",
      "conflict_prompt": "The patent application was filed in 2000 and the patent was granted in 1998.",
      "question": "When was the patent granted?",
      "options": [
        "A. 1999",
        "B. 1998",
        "C. 2004",
        "D. 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon registration opened in January 2017 and closed in March 2017.",
      "conflict_prompt": "The marathon registration opened in January 2017 and closed in December 2016.",
      "question": "When did registration close?",
      "options": [
        "A. December 2016",
        "B. January 2017",
        "C. March 2017",
        "D. February 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The theater troupe rehearsed the show for six weeks in summer 1995 and premiered it on August 1, 1995.",
      "conflict_prompt": "The theater troupe rehearsed the show for six weeks in summer 1995 and premiered it on July 1, 1994.",
      "question": "When was the premiere?",
      "options": [
        "A. July 1, 1994",
        "B. August 1, 1995",
        "C. June 1, 1995",
        "D. September 1, 1995"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The startup received seed funding in 2012 and closed Series A in 2014.",
      "conflict_prompt": "The startup received seed funding in 2012 and closed Series A in 2010.",
      "question": "When was Series A closed?",
      "options": [
        "A. 2011",
        "B. 2010",
        "C. 2014",
        "D. 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum exhibit traveled to three cities between 2006 and 2008 and returned to the original gallery in 2009.",
      "conflict_prompt": "The museum exhibit traveled to three cities between 2006 and 2008 and returned to the original gallery in 2005.",
      "question": "When did the exhibit return to the original gallery?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2009",
        "D. 2007"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference call occurred on Monday, October 12, 2015 and minutes were distributed on Wednesday, October 14, 2015.",
      "conflict_prompt": "The conference call occurred on Monday, October 12, 2015 and minutes were distributed on Friday, October 10, 2014.",
      "question": "When were the minutes distributed?",
      "options": [
        "A. Friday, October 10, 2014",
        "B. Monday, October 12, 2015",
        "C. Wednesday, October 14, 2015",
        "D. Tuesday, October 13, 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marathon was postponed from April 2020 and eventually run in October 2020.",
      "conflict_prompt": "The marathon was postponed from April 2020 and eventually run in March 2019.",
      "question": "When was the marathon eventually run?",
      "options": [
        "A. March 2019",
        "B. April 2020",
        "C. October 2020",
        "D. May 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The genealogist traced the family line back to 1720 and published the results in 2005.",
      "conflict_prompt": "The genealogist traced the family line back to 1720 and published the results in 1995.",
      "question": "When were the results published?",
      "options": [
        "A. 1998",
        "B. 1995",
        "C. 2005",
        "D. 2000"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new tax law was passed in December 2017 and took effect on January 1, 2018.",
      "conflict_prompt": "The new tax law was passed in December 2017 and took effect on January 1, 2016.",
      "question": "When did the law take effect?",
      "options": [
        "A. January 1, 2016",
        "B. December 2017",
        "C. January 1, 2018",
        "D. February 1, 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ship set sail from port on May 5, 1942 and arrived at its destination on June 10, 1942.",
      "conflict_prompt": "The ship set sail from port on May 5, 1942 and arrived at its destination on April 30, 1941.",
      "question": "When did the ship arrive at its destination?",
      "options": [
        "A. April 30, 1941",
        "B. May 5, 1942",
        "C. June 10, 1942",
        "D. July 1, 1942"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The novelist finished the manuscript in November 2010 and began revisions in December 2010.",
      "conflict_prompt": "The novelist finished the manuscript in November 2010 and began revisions in October 2009.",
      "question": "When did revisions begin?",
      "options": [
        "A. November 2010",
        "B. October 2009",
        "C. December 2010",
        "D. January 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The orchestra toured Europe in the summer of 1992 and returned home in September 1992.",
      "conflict_prompt": "The orchestra toured Europe in the summer of 1992 and returned home in June 1991.",
      "question": "When did they return home according to the statement?",
      "options": [
        "A. June 1991",
        "B. July 1992",
        "C. September 1992",
        "D. August 1992"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The vineyard expansion was planned in 2005 and construction occurred in 2006.",
      "conflict_prompt": "The vineyard expansion was planned in 2005 and construction occurred in 2003.",
      "question": "When did construction occur?",
      "options": [
        "A. 2003",
        "B. 2004",
        "C. 2006",
        "D. 2005"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The scientist received funding in 2011 and published results from the funded study in 2014.",
      "conflict_prompt": "The scientist received funding in 2011 and published results from the funded study in 2009.",
      "question": "When were the study results published?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2014",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The ferry schedule listed departures every hour starting at 6:00 AM, with the first trip on July 1, 2010.",
      "conflict_prompt": "The ferry schedule listed departures every hour starting at 6:00 AM, with the first trip on June 1, 2009.",
      "question": "When was the first trip according to the statement?",
      "options": [
        "A. June 1, 2009",
        "B. July 1, 2010",
        "C. July 1, 2009",
        "D. June 1, 2010"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The volunteer program began in 2001 and celebrated its 10th anniversary in 2011.",
      "conflict_prompt": "The volunteer program began in 2001 and celebrated its 10th anniversary in 2005.",
      "question": "When was the 10th anniversary celebrated?",
      "options": [
        "A. 2005",
        "B. 2008",
        "C. 2011",
        "D. 2010"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer was commissioned in 1950 and completed the piece in 1953.",
      "conflict_prompt": "The composer was commissioned in 1950 and completed the piece in 1948.",
      "question": "When was the piece completed?",
      "options": [
        "A. 1948",
        "B. 1950",
        "C. 1953",
        "D. 1951"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The conference registration opened on August 1, 2014 and early bird rates ended on September 1, 2014.",
      "conflict_prompt": "The conference registration opened on August 1, 2014 and early bird rates ended on July 1, 2013.",
      "question": "When did early bird rates end?",
      "options": [
        "A. July 1, 2013",
        "B. August 1, 2014",
        "C. September 1, 2014",
        "D. October 1, 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The archaeologists excavated the site in 1987 and carbon-dated samples in 1988.",
      "conflict_prompt": "The archaeologists excavated the site in 1987 and carbon-dated samples in 1985.",
      "question": "When were samples carbon-dated?",
      "options": [
        "A. 1984",
        "B. 1985",
        "C. 1988",
        "D. 1987"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The election campaign began in late 2019 and the candidate withdrew in January 2020.",
      "conflict_prompt": "The election campaign began in late 2019 and the candidate withdrew in December 2018.",
      "question": "When did the candidate withdraw according to the statement?",
      "options": [
        "A. December 2018",
        "B. January 2020",
        "C. Late 2019",
        "D. November 2019"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The play's author died in 1975 and the centennial production was staged in 1990.",
      "conflict_prompt": "The play's author died in 1975 and the centennial production was staged in 1965.",
      "question": "When was the centennial production staged?",
      "options": [
        "A. 1965",
        "B. 1975",
        "C. 1990",
        "D. 1980"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The marketing campaign started in January 2015 and the product was launched in March 2015.",
      "conflict_prompt": "The marketing campaign started in January 2015 and the product was launched in December 2014.",
      "question": "When was the product launched?",
      "options": [
        "A. December 2014",
        "B. January 2015",
        "C. March 2015",
        "D. February 2015"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town installed holiday lights in November 2018 and held a lighting ceremony on December 1, 2018.",
      "conflict_prompt": "The town installed holiday lights in November 2018 and held a lighting ceremony on December 1, 2016.",
      "question": "When was the lighting ceremony held according to the statement?",
      "options": [
        "A. December 1, 2016",
        "B. November 2018",
        "C. December 1, 2018",
        "D. January 2019"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The singer won a regional contest in 2002 and released an EP in 2003.",
      "conflict_prompt": "The singer won a regional contest in 2002 and released an EP in 2001.",
      "question": "When was the EP released?",
      "options": [
        "A. 2001",
        "B. 2002",
        "C. 2003",
        "D. 2004"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The trial began on March 4, 1998 and the verdict was delivered on March 20, 1998.",
      "conflict_prompt": "The trial began on March 4, 1998 and the verdict was delivered on March 1, 1997.",
      "question": "When was the verdict delivered?",
      "options": [
        "A. March 1, 1997",
        "B. March 4, 1998",
        "C. March 20, 1998",
        "D. March 15, 1998"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum restored the sculpture in 2010 and put it back on display in 2011.",
      "conflict_prompt": "The museum restored the sculpture in 2010 and put it back on display in 2009.",
      "question": "When was the sculpture returned to display?",
      "options": [
        "A. 2009",
        "B. 2010",
        "C. 2011",
        "D. 2012"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research grant was awarded in August 2016 and fieldwork began in September 2016.",
      "conflict_prompt": "The research grant was awarded in August 2016 and fieldwork began in July 2015.",
      "question": "When did fieldwork begin according to the statement?",
      "options": [
        "A. July 2015",
        "B. August 2016",
        "C. September 2016",
        "D. October 2016"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The series finale was filmed in May 2014 and broadcast in September 2014.",
      "conflict_prompt": "The series finale was filmed in May 2014 and broadcast in April 2013.",
      "question": "When was the finale broadcast?",
      "options": [
        "A. April 2013",
        "B. May 2014",
        "C. September 2014",
        "D. June 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The composer revised the score in 1921 and the revised version premiered in 1923.",
      "conflict_prompt": "The composer revised the score in 1921 and the revised version premiered in 1918.",
      "question": "When did the revised version premiere?",
      "options": [
        "A. 1918",
        "B. 1921",
        "C. 1923",
        "D. 1920"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity was established in 1978 and held its first gala in 1979.",
      "conflict_prompt": "The charity was established in 1978 and held its first gala in 1976.",
      "question": "When was the first gala held?",
      "options": [
        "A. 1976",
        "B. 1977",
        "C. 1979",
        "D. 1978"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The irrigation system was installed in May 2013 and crops were irrigated for the first time in June 2013.",
      "conflict_prompt": "The irrigation system was installed in May 2013 and crops were irrigated for the first time in April 2012.",
      "question": "When were crops irrigated for the first time according to the statement?",
      "options": [
        "A. April 2012",
        "B. May 2013",
        "C. June 2013",
        "D. July 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The actor was cast in 2006 and started shooting scenes in January 2007.",
      "conflict_prompt": "The actor was cast in 2006 and started shooting scenes in December 2005.",
      "question": "When did shooting start according to the statement?",
      "options": [
        "A. December 2005",
        "B. January 2007",
        "C. February 2007",
        "D. November 2006"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The choir was founded in 1988 and gave its first public concert in 1989.",
      "conflict_prompt": "The choir was founded in 1988 and gave its first public concert in 1986.",
      "question": "When did the choir give its first public concert?",
      "options": [
        "A. 1986",
        "B. 1987",
        "C. 1989",
        "D. 1988"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The library opened its new wing in 2000 and installed the computers in 2001.",
      "conflict_prompt": "The library opened its new wing in 2000 and installed the computers in 1999.",
      "question": "When were the computers installed according to the statement?",
      "options": [
        "A. 1999",
        "B. 2000",
        "C. 2001",
        "D. 2002"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The town celebrated its bicentennial in 2010 and held commemorative events throughout that year.",
      "conflict_prompt": "The town celebrated its bicentennial in 2010 and held commemorative events throughout 2008.",
      "question": "During which year were commemorative events held according to the statement?",
      "options": [
        "A. 2008",
        "B. 2009",
        "C. 2010",
        "D. 2011"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The trial transcript was published in May 1991 after hearings that took place in April 1991.",
      "conflict_prompt": "The trial transcript was published in May 1991 after hearings that took place in March 1990.",
      "question": "When did the hearings take place according to the statement?",
      "options": [
        "A. March 1990",
        "B. April 1991",
        "C. May 1991",
        "D. February 1991"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The charity's annual report covered fiscal year 2017, which ran from July 1, 2016 to June 30, 2017.",
      "conflict_prompt": "The charity's annual report covered fiscal year 2017, which ran from July 1, 2016 to June 30, 2015.",
      "question": "When did the fiscal year end according to the statement?",
      "options": [
        "A. June 30, 2015",
        "B. June 30, 2016",
        "C. June 30, 2017",
        "D. July 1, 2017"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The mountain lodge opened for the season on November 15, 2019 and closed for the season on April 1, 2020.",
      "conflict_prompt": "The mountain lodge opened for the season on November 15, 2019 and closed for the season on October 1, 2018.",
      "question": "When did the lodge close for the season?",
      "options": [
        "A. October 1, 2018",
        "B. November 15, 2019",
        "C. April 1, 2020",
        "D. May 1, 2020"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The renewable energy project received permits in 2014 and began construction in 2015.",
      "conflict_prompt": "The renewable energy project received permits in 2014 and began construction in 2012.",
      "question": "When did construction begin according to the statement?",
      "options": [
        "A. 2012",
        "B. 2013",
        "C. 2015",
        "D. 2014"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The author gave a lecture on November 10, 2007 and released the related essay in January 2008.",
      "conflict_prompt": "The author gave a lecture on November 10, 2007 and released the related essay in October 2006.",
      "question": "When was the essay released according to the statement?",
      "options": [
        "A. October 2006",
        "B. November 2007",
        "C. January 2008",
        "D. December 2007"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The clinic opened in March 1999 and began offering free screenings in June 1999.",
      "conflict_prompt": "The clinic opened in March 1999 and began offering free screenings in May 1998.",
      "question": "When did free screenings begin according to the statement?",
      "options": [
        "A. May 1998",
        "B. March 1999",
        "C. June 1999",
        "D. July 1999"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The newspaper changed ownership in 2004 and redesigned its layout in early 2005.",
      "conflict_prompt": "The newspaper changed ownership in 2004 and redesigned its layout in late 2003.",
      "question": "When was the layout redesigned according to the statement?",
      "options": [
        "A. Late 2003",
        "B. Early 2005",
        "C. 2004",
        "D. 2006"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The research institute was established in 1972 and built its main laboratory in 1974.",
      "conflict_prompt": "The research institute was established in 1972 and built its main laboratory in 1969.",
      "question": "When was the main laboratory built according to the statement?",
      "options": [
        "A. 1969",
        "B. 1972",
        "C. 1974",
        "D. 1973"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The museum loaned the exhibit in 1999 and it was displayed abroad from February through June 2000.",
      "conflict_prompt": "The museum loaned the exhibit in 1999 and it was displayed abroad from February through June 1998.",
      "question": "When was the exhibit displayed abroad according to the statement?",
      "options": [
        "A. February–June 1998",
        "B. February–June 1999",
        "C. February–June 2000",
        "D. February–June 2001"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The new tram line was inaugurated on May 5, 2013 and passenger service began the next day, May 6, 2013.",
      "conflict_prompt": "The new tram line was inaugurated on May 5, 2013 and passenger service began the previous year, May 6, 2012.",
      "question": "When did passenger service begin according to the statement?",
      "options": [
        "A. May 6, 2012",
        "B. May 5, 2013",
        "C. May 6, 2013",
        "D. May 7, 2013"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "The playwright completed Act I in 1984 and finished the entire play in 1986.",
      "conflict_prompt": "The playwright completed Act I in 1984 and finished the entire play in 1980.",
      "question": "When was the entire play finished according to the statement?",
      "options": [
        "A. 1980",
        "B. 1984",
        "C. 1986",
        "D. 1985"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "temporal_contradictions",
      "conflict_type": "temporal_inconsistency"
    },
    {
      "clean_prompt": "Dr. Morales is a cardiologist. She performs heart surgeries at the regional hospital.",
      "conflict_prompt": "Dr. Morales is not a cardiologist. She performs heart surgeries at the regional hospital.",
      "question": "What is Dr. Morales's medical specialty?",
      "options": [
        "A. Neurologist",
        "B. General practitioner",
        "C. Cardiologist",
        "D. Dermatologist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public library is open on Saturdays. It hosts children's story hour every Saturday morning.",
      "conflict_prompt": "The public library is not open on Saturdays. It hosts children's story hour every Saturday morning.",
      "question": "Is the library open on Saturdays according to the statement?",
      "options": [
        "A. Only for special events",
        "B. No",
        "C. Yes",
        "D. By appointment only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery sells sourdough bread daily. It bakes fresh sourdough loaves each morning.",
      "conflict_prompt": "The bakery does not sell sourdough bread daily. It bakes fresh sourdough loaves each morning.",
      "question": "How often does the bakery bake fresh sourdough loaves according to the statement?",
      "options": [
        "A. Weekly",
        "B. Daily",
        "C. Only on weekends",
        "D. Monthly"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city operates a bike-sharing program. There are stations available throughout downtown.",
      "conflict_prompt": "The city does not operate a bike-sharing program. There are stations available throughout downtown.",
      "question": "What program does the city operate according to the statement?",
      "options": [
        "A. Carpool matching",
        "B. Bike-sharing",
        "C. Scooter repair",
        "D. Public parking management"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university offers an on-campus daycare. Faculty members can enroll their children in the daycare.",
      "conflict_prompt": "The university does not offer an on-campus daycare. Faculty members can enroll their children in the daycare.",
      "question": "Who can enroll their children in the facility mentioned?",
      "options": [
        "A. Local residents only",
        "B. Adjunct faculty only",
        "C. Faculty members",
        "D. Alumni"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The airline allows two checked bags on international flights. Passengers may check two suitcases without extra fee on those routes.",
      "conflict_prompt": "The airline does not allow two checked bags on international flights. Passengers may check two suitcases without extra fee on those routes.",
      "question": "How many checked bags does the airline allow on international flights according to the statement?",
      "options": [
        "A. None",
        "B. One",
        "C. Two",
        "D. Unlimited"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum offers free admission to children under 12. Families with kids under 12 enter free.",
      "conflict_prompt": "The museum does not offer free admission to children under 12. Families with kids under 12 enter free.",
      "question": "Who gets free admission according to the statement?",
      "options": [
        "A. Seniors only",
        "B. Students with ID",
        "C. Children under 12",
        "D. Military personnel"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software is open-source. Developers can view and modify the source code on the project's repository.",
      "conflict_prompt": "The software is not open-source. Developers can view and modify the source code on the project's repository.",
      "question": "What is the licensing nature of the software according to the statement?",
      "options": [
        "A. Proprietary",
        "B. Shareware",
        "C. Open-source",
        "D. Trialware"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The gardener plants tomatoes in raised beds each spring. He starts transplanting seedlings into the raised beds in April.",
      "conflict_prompt": "The gardener does not plant tomatoes in raised beds each spring. He starts transplanting seedlings into the raised beds in April.",
      "question": "When does the gardener start transplanting seedlings into the raised beds?",
      "options": [
        "A. April",
        "B. July",
        "C. December",
        "D. February"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The orchestra performs a holiday concert every December. They rehearse the seasonal program throughout November.",
      "conflict_prompt": "The orchestra does not perform a holiday concert every December. They rehearse the seasonal program throughout November.",
      "question": "During which month do they rehearse the seasonal program according to the statement?",
      "options": [
        "A. January",
        "B. November",
        "C. March",
        "D. June"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The clinic offers flu shots during the fall. Patients can get vaccinated beginning in September.",
      "conflict_prompt": "The clinic does not offer flu shots during the fall. Patients can get vaccinated beginning in September.",
      "question": "When can patients start getting vaccinated according to the statement?",
      "options": [
        "A. September",
        "B. May",
        "C. February",
        "D. December"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone model includes a water-resistant rating. It is rated IP68 for dust and water protection.",
      "conflict_prompt": "The smartphone model does not include a water-resistant rating. It is rated IP68 for dust and water protection.",
      "question": "What IP rating does the smartphone have according to the statement?",
      "options": [
        "A. IP20",
        "B. IP44",
        "C. IP68",
        "D. IP12"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood watch meets monthly. Volunteers gather at the community center every first Tuesday.",
      "conflict_prompt": "The neighborhood watch does not meet monthly. Volunteers gather at the community center every first Tuesday.",
      "question": "When do volunteers gather at the community center according to the statement?",
      "options": [
        "A. Every first Tuesday",
        "B. Every second Friday",
        "C. Every last Sunday",
        "D. Every Wednesday"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant offers vegetarian options on the main menu. Several plant-based entrees are available daily.",
      "conflict_prompt": "The restaurant does not offer vegetarian options on the main menu. Several plant-based entrees are available daily.",
      "question": "What type of entrees are available daily according to the statement?",
      "options": [
        "A. Meat-only entrees",
        "B. Plant-based entrees",
        "C. Seafood-only entrees",
        "D. No entrees available"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Sara is a licensed electrician. She installs residential wiring and inspects home electrical systems.",
      "conflict_prompt": "Sara is not a licensed electrician. She installs residential wiring and inspects home electrical systems.",
      "question": "What trade is Sara licensed in according to the statement?",
      "options": [
        "A. Plumbing",
        "B. Carpentry",
        "C. Electrical",
        "D. HVAC"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conference center has Wi-Fi throughout its meeting rooms. Attendees can connect to the venue network in every room.",
      "conflict_prompt": "The conference center does not have Wi-Fi throughout its meeting rooms. Attendees can connect to the venue network in every room.",
      "question": "Where can attendees connect to the venue network according to the statement?",
      "options": [
        "A. Only in the lobby",
        "B. In the parking lot",
        "C. In every meeting room",
        "D. Only at registration"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter train runs express between downtown and the airport. It does not stop at intermediate suburban stations on that service.",
      "conflict_prompt": "The commuter train does not run express between downtown and the airport. It does not stop at intermediate suburban stations on that service.",
      "question": "What type of service does the train provide between downtown and the airport according to the statement?",
      "options": [
        "A. Local with many stops",
        "B. Freight-only",
        "C. Express",
        "D. Tourist sightseeing"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The textbook includes problem sets at the end of each chapter. Students are asked to solve practice problems after every chapter.",
      "conflict_prompt": "The textbook does not include problem sets at the end of each chapter. Students are asked to solve practice problems after every chapter.",
      "question": "Where are students asked to solve practice problems according to the statement?",
      "options": [
        "A. At the start of each chapter",
        "B. After every chapter",
        "C. Only in the appendix",
        "D. During lectures only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel provides complimentary breakfast for guests. A buffet breakfast is served each morning for registered guests.",
      "conflict_prompt": "The hotel does not provide complimentary breakfast for guests. A buffet breakfast is served each morning for registered guests.",
      "question": "What type of breakfast is served each morning according to the statement?",
      "options": [
        "A. Continental only upon request",
        "B. Packed breakfasts from outside",
        "C. Buffet breakfast for registered guests",
        "D. No breakfast service"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The regional park allows dogs on-leash. Pet owners must keep dogs on a leash when using the trails.",
      "conflict_prompt": "The regional park does not allow dogs on-leash. Pet owners must keep dogs on a leash when using the trails.",
      "question": "What must pet owners do with their dogs on the trails according to the statement?",
      "options": [
        "A. Let them run free",
        "B. Keep them on a leash",
        "C. Leave them at home",
        "D. Use a muzzle only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The charity distributes winter coats to low-income families. Volunteers collect donated coats and hand them out in November.",
      "conflict_prompt": "The charity does not distribute winter coats to low-income families. Volunteers collect donated coats and hand them out in November.",
      "question": "When are donated coats handed out according to the statement?",
      "options": [
        "A. In March",
        "B. In November",
        "C. In August",
        "D. In January"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The film festival screens independent documentaries. Several non-fiction films are shown during the weekend lineup.",
      "conflict_prompt": "The film festival does not screen independent documentaries. Several non-fiction films are shown during the weekend lineup.",
      "question": "What type of films are shown during the weekend lineup according to the statement?",
      "options": [
        "A. Animated shorts only",
        "B. Sitcom episodes",
        "C. Non-fiction films",
        "D. Live theater broadcasts"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The dentist practices pediatric dentistry. She treats dental issues in children and adolescents.",
      "conflict_prompt": "The dentist does not practice pediatric dentistry. She treats dental issues in children and adolescents.",
      "question": "What patient group does the dentist treat according to the statement?",
      "options": [
        "A. Geriatric patients",
        "B. Children and adolescents",
        "C. Veterinary patients",
        "D. Cardiovascular patients"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The startup secured seed funding last year. Investors provided capital to help the company develop its prototype.",
      "conflict_prompt": "The startup did not secure seed funding last year. Investors provided capital to help the company develop its prototype.",
      "question": "What did investors provide according to the statement?",
      "options": [
        "A. Legal advice only",
        "B. Office space only",
        "C. Capital",
        "D. Manufacturing equipment only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farm practices organic agriculture. No synthetic pesticides or fertilizers are used on the crops.",
      "conflict_prompt": "The farm does not practice organic agriculture. No synthetic pesticides or fertilizers are used on the crops.",
      "question": "What does the farm avoid using according to the statement?",
      "options": [
        "A. Crop rotation",
        "B. Synthetic pesticides or fertilizers",
        "C. Natural compost",
        "D. Irrigation systems"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The courier guarantees next-day delivery for domestic packages. Parcels shipped before the cutoff arrive the next business day.",
      "conflict_prompt": "The courier does not guarantee next-day delivery for domestic packages. Parcels shipped before the cutoff arrive the next business day.",
      "question": "When do parcels shipped before the cutoff arrive according to the statement?",
      "options": [
        "A. After two weeks",
        "B. The next business day",
        "C. On the same hour",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The apartment building has a secure entry system. Residents use key fobs to enter through the main door.",
      "conflict_prompt": "The apartment building does not have a secure entry system. Residents use key fobs to enter through the main door.",
      "question": "How do residents enter through the main door according to the statement?",
      "options": [
        "A. By returning a physical key only",
        "B. By using key fobs",
        "C. Through an unlocked gate",
        "D. By dialing a phone number"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater screens live opera broadcasts. The cinema hosts satellite transmissions of Metropolitan Opera performances.",
      "conflict_prompt": "The theater does not screen live opera broadcasts. The cinema hosts satellite transmissions of Metropolitan Opera performances.",
      "question": "What special transmissions does the cinema host according to the statement?",
      "options": [
        "A. Sports replays",
        "B. Satellite transmissions of Metropolitan Opera performances",
        "C. Animated series marathons",
        "D. Weather updates"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local clinic provides mental health counseling. Licensed therapists offer sessions by appointment.",
      "conflict_prompt": "The local clinic does not provide mental health counseling. Licensed therapists offer sessions by appointment.",
      "question": "Who offers sessions by appointment according to the statement?",
      "options": [
        "A. Licensed therapists",
        "B. Construction workers",
        "C. Bank tellers",
        "D. Real estate agents"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers' market operates every Sunday morning. Vendors set up stalls at the town square starting at 8 AM.",
      "conflict_prompt": "The farmers' market does not operate every Sunday morning. Vendors set up stalls at the town square starting at 8 AM.",
      "question": "At what time do vendors start setting up stalls according to the statement?",
      "options": [
        "A. 8 AM",
        "B. Midnight",
        "C. 3 PM",
        "D. 10 PM"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The research lab follows biosafety level 2 protocols. Personnel wear protective gloves and lab coats when handling samples.",
      "conflict_prompt": "The research lab does not follow biosafety level 2 protocols. Personnel wear protective gloves and lab coats when handling samples.",
      "question": "What protective equipment do personnel wear when handling samples according to the statement?",
      "options": [
        "A. Swimwear",
        "B. Protective gloves and lab coats",
        "C. Football helmets",
        "D. No protective gear"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The deli uses locally sourced cheese in its sandwiches. All of the sandwich menus list the regional cheesemaker.",
      "conflict_prompt": "The deli does not use locally sourced cheese in its sandwiches. All of the sandwich menus list the regional cheesemaker.",
      "question": "What information is listed on all of the sandwich menus according to the statement?",
      "options": [
        "A. Calorie counts only",
        "B. The regional cheesemaker",
        "C. International shipping rates",
        "D. Airline schedules"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conservation group monitors water quality in the river. Volunteers collect samples weekly to track pollutant levels.",
      "conflict_prompt": "The conservation group does not monitor water quality in the river. Volunteers collect samples weekly to track pollutant levels.",
      "question": "How often do volunteers collect water samples according to the statement?",
      "options": [
        "A. Annually",
        "B. Weekly",
        "C. Never",
        "D. Only during storms"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The stationery shop stocks archival-quality paper. Artists can buy acid-free paper for long-term projects.",
      "conflict_prompt": "The stationery shop does not stock archival-quality paper. Artists can buy acid-free paper for long-term projects.",
      "question": "What type of paper can artists buy according to the statement?",
      "options": [
        "A. Acid-free archival-quality paper",
        "B. Sandpaper",
        "C. Wallpaper rolls",
        "D. Disposable napkins"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The marathon is certified by the national athletics federation. Runners who qualify will have official qualifying times recorded.",
      "conflict_prompt": "The marathon is not certified by the national athletics federation. Runners who qualify will have official qualifying times recorded.",
      "question": "What will be recorded for runners who qualify according to the statement?",
      "options": [
        "A. Their shopping receipts",
        "B. Official qualifying times",
        "C. Their blood type",
        "D. Their passport numbers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare requires at least one year of experience for lead teachers. Applicants must document teaching experience when applying.",
      "conflict_prompt": "The daycare does not require at least one year of experience for lead teachers. Applicants must document teaching experience when applying.",
      "question": "What must applicants document when applying according to the statement?",
      "options": [
        "A. Their favorite color",
        "B. Teaching experience",
        "C. Their grocery list",
        "D. A handwritten poem"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The aquarium displays a coral reef exhibit. Marine biologists maintain live coral and associated fish species in the exhibit.",
      "conflict_prompt": "The aquarium does not display a coral reef exhibit. Marine biologists maintain live coral and associated fish species in the exhibit.",
      "question": "Who maintains the live coral and fish species according to the statement?",
      "options": [
        "A. Astronauts",
        "B. Marine biologists",
        "C. Botanists",
        "D. Accountants"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The law firm specializes in intellectual property. Attorneys handle trademark and patent filings for clients.",
      "conflict_prompt": "The law firm does not specialize in intellectual property. Attorneys handle trademark and patent filings for clients.",
      "question": "What services do the attorneys provide according to the statement?",
      "options": [
        "A. Restaurant catering",
        "B. Trademark and patent filings",
        "C. Hair styling",
        "D. Plumbing repairs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's recycling program accepts glass bottles. Residents may place clean glass containers in the blue bin.",
      "conflict_prompt": "The city's recycling program does not accept glass bottles. Residents may place clean glass containers in the blue bin.",
      "question": "What can residents place in the blue bin according to the statement?",
      "options": [
        "A. Hazardous waste",
        "B. Glass containers",
        "C. Live animals",
        "D. Hot coals"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain lodge is open year-round. Guests can book rooms during both summer hiking season and winter ski season.",
      "conflict_prompt": "The mountain lodge is not open year-round. Guests can book rooms during both summer hiking season and winter ski season.",
      "question": "During which seasons can guests book rooms according to the statement?",
      "options": [
        "A. Only spring",
        "B. Summer and winter",
        "C. Only autumn",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery uses cage-free eggs for all pastries. The ingredient list specifies that eggs are cage-free.",
      "conflict_prompt": "The bakery does not use cage-free eggs for all pastries. The ingredient list specifies that eggs are cage-free.",
      "question": "What does the ingredient list specify about the eggs according to the statement?",
      "options": [
        "A. They are synthetic",
        "B. They are cage-free",
        "C. They are from ducks",
        "D. There are no eggs used"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The tech conference features keynote speeches from industry leaders. Attendees can watch keynote sessions in the main hall.",
      "conflict_prompt": "The tech conference does not feature keynote speeches from industry leaders. Attendees can watch keynote sessions in the main hall.",
      "question": "Where can attendees watch keynote sessions according to the statement?",
      "options": [
        "A. In an online-only forum",
        "B. In the main hall",
        "C. In private hotel rooms only",
        "D. They cannot watch them"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal pool offers lap swimming hours each morning. Swimmers can use lane lines reserved for lap swimming from 6 to 9 AM.",
      "conflict_prompt": "The municipal pool does not offer lap swimming hours each morning. Swimmers can use lane lines reserved for lap swimming from 6 to 9 AM.",
      "question": "Between what times are lane lines reserved for lap swimming according to the statement?",
      "options": [
        "A. 6 to 9 AM",
        "B. 2 to 5 PM",
        "C. 10 PM to midnight",
        "D. 11 AM to 1 PM"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The online course includes weekly quizzes. Students must complete a quiz at the end of each module.",
      "conflict_prompt": "The online course does not include weekly quizzes. Students must complete a quiz at the end of each module.",
      "question": "When must students complete a quiz according to the statement?",
      "options": [
        "A. At the beginning of the course",
        "B. At the end of each module",
        "C. Only at the final exam",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The violinist plays with a professional symphony. He is a member of the city's symphony orchestra.",
      "conflict_prompt": "The violinist does not play with a professional symphony. He is a member of the city's symphony orchestra.",
      "question": "Of which musical ensemble is the violinist a member according to the statement?",
      "options": [
        "A. A children's choir",
        "B. City symphony orchestra",
        "C. A rock band",
        "D. A marching band"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community pool offers swim lessons for toddlers. Instructors run parent-and-tot classes on weekday mornings.",
      "conflict_prompt": "The community pool does not offer swim lessons for toddlers. Instructors run parent-and-tot classes on weekday mornings.",
      "question": "What type of classes do instructors run on weekday mornings according to the statement?",
      "options": [
        "A. Parent-and-tot swim classes",
        "B. Adult water polo only",
        "C. Weightlifting sessions",
        "D. Cooking lessons"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The plumbing service responds to emergency calls 24/7. A technician is dispatched any time there is a burst pipe.",
      "conflict_prompt": "The plumbing service does not respond to emergency calls 24/7. A technician is dispatched any time there is a burst pipe.",
      "question": "When is a technician dispatched for burst pipes according to the statement?",
      "options": [
        "A. Only during business hours",
        "B. Any time there is a burst pipe",
        "C. Only on weekends",
        "D. Only after approval from a manager"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden labels all plant species in the conservatory. Visitors can read identification tags next to each specimen.",
      "conflict_prompt": "The botanical garden does not label all plant species in the conservatory. Visitors can read identification tags next to each specimen.",
      "question": "What can visitors read next to each specimen according to the statement?",
      "options": [
        "A. Movie reviews",
        "B. Identification tags",
        "C. Weather reports",
        "D. Restaurant menus"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The fishing charter targets tuna during summer months. Anglers are taken to deep-sea hotspots known for tuna runs.",
      "conflict_prompt": "The fishing charter does not target tuna during summer months. Anglers are taken to deep-sea hotspots known for tuna runs.",
      "question": "What fish does the charter target according to the statement?",
      "options": [
        "A. Tuna",
        "B. Trout",
        "C. Carp",
        "D. Catfish"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The café brews single-origin coffee. Baristas prepare espresso shots using beans from a specific farm.",
      "conflict_prompt": "The café does not brew single-origin coffee. Baristas prepare espresso shots using beans from a specific farm.",
      "question": "What type of coffee beans are used for espresso according to the statement?",
      "options": [
        "A. A generic blend only",
        "B. Beans from a specific farm (single-origin)",
        "C. Instant coffee granules",
        "D. Tea leaves"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The volunteer fire department conducts monthly training drills. Members practice hose deployments and rescue techniques every month.",
      "conflict_prompt": "The volunteer fire department does not conduct monthly training drills. Members practice hose deployments and rescue techniques every month.",
      "question": "How often do members practice hose deployments and rescue techniques according to the statement?",
      "options": [
        "A. Monthly",
        "B. Once in five years",
        "C. Only during disasters",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ice cream shop makes small-batch flavors in-house. The proprietor churns limited-quantity flavors daily.",
      "conflict_prompt": "The ice cream shop does not make small-batch flavors in-house. The proprietor churns limited-quantity flavors daily.",
      "question": "Who churns the limited-quantity flavors according to the statement?",
      "options": [
        "A. The proprietor",
        "B. A neighboring restaurant",
        "C. An offsite manufacturer only",
        "D. No one churns them"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city park has an observatory open to the public. Visitors can attend stargazing nights hosted by volunteers.",
      "conflict_prompt": "The city park does not have an observatory open to the public. Visitors can attend stargazing nights hosted by volunteers.",
      "question": "What event can visitors attend according to the statement?",
      "options": [
        "A. Stonemasonry workshops",
        "B. Stargazing nights",
        "C. Indoor bowling tournaments",
        "D. Ice sculpting classes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The publisher offers hardcover editions of the novel. A limited-run hardcover print was released alongside the paperback.",
      "conflict_prompt": "The publisher does not offer hardcover editions of the novel. A limited-run hardcover print was released alongside the paperback.",
      "question": "What format of the novel was released alongside the paperback according to the statement?",
      "options": [
        "A. An audiobook only",
        "B. A limited-run hardcover",
        "C. A handwritten copy",
        "D. A serialized comic strip"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pediatric clinic accepts walk-ins for immunizations. Parents can bring children without appointments for routine shots.",
      "conflict_prompt": "The pediatric clinic does not accept walk-ins for immunizations. Parents can bring children without appointments for routine shots.",
      "question": "Who can bring children without appointments according to the statement?",
      "options": [
        "A. Parents",
        "B. Strangers from another city",
        "C. Only school nurses",
        "D. No one"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university's museum loans artifacts to other institutions. Curators arrange outgoing loans for exhibitions at partner museums.",
      "conflict_prompt": "The university's museum does not loan artifacts to other institutions. Curators arrange outgoing loans for exhibitions at partner museums.",
      "question": "What do curators arrange according to the statement?",
      "options": [
        "A. Local sports matches",
        "B. Outgoing loans for exhibitions",
        "C. Road repairs",
        "D. Concert tours"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The chocolate factory uses fair-trade cocoa beans. The packaging notes that beans are sourced from certified cooperatives.",
      "conflict_prompt": "The chocolate factory does not use fair-trade cocoa beans. The packaging notes that beans are sourced from certified cooperatives.",
      "question": "What does the packaging note about the cocoa beans according to the statement?",
      "options": [
        "A. They are synthetic",
        "B. They are sourced from certified cooperatives",
        "C. They are made of plastic",
        "D. They are imported directly from a grocery store"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ferry allows bicycles on board during daytime runs. Cyclists can bring their bikes on the ferry between 7 AM and 7 PM.",
      "conflict_prompt": "The ferry does not allow bicycles on board during daytime runs. Cyclists can bring their bikes on the ferry between 7 AM and 7 PM.",
      "question": "Between what hours can cyclists bring their bikes on the ferry according to the statement?",
      "options": [
        "A. 7 AM and 7 PM",
        "B. 11 PM and 5 AM",
        "C. Only at noon",
        "D. They cannot bring bikes at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The high school choir performs at the regional competition. Members travel with the director to the contest each spring.",
      "conflict_prompt": "The high school choir does not perform at the regional competition. Members travel with the director to the contest each spring.",
      "question": "Who travels with the choir members to the contest according to the statement?",
      "options": [
        "A. The director",
        "B. The school librarian",
        "C. No one travels with them",
        "D. The cafeteria staff only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The tailor offers same-day hemming services. Customers drop off garments in the morning and pick them up the same afternoon.",
      "conflict_prompt": "The tailor does not offer same-day hemming services. Customers drop off garments in the morning and pick them up the same afternoon.",
      "question": "When can customers pick up garments dropped off in the morning according to the statement?",
      "options": [
        "A. The following month",
        "B. The same afternoon",
        "C. Only on weekends",
        "D. Next year"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter bus offers free Wi-Fi to passengers. Riders can connect to an onboard network for internet access.",
      "conflict_prompt": "The commuter bus does not offer free Wi-Fi to passengers. Riders can connect to an onboard network for internet access.",
      "question": "What amenity can riders connect to according to the statement?",
      "options": [
        "A. A coffee machine",
        "B. An onboard Wi-Fi network",
        "C. A rooftop garden",
        "D. A printed encyclopedia"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campground sells firewood at the ranger station. Campers can purchase bundles for use at designated fire rings.",
      "conflict_prompt": "The campground does not sell firewood at the ranger station. Campers can purchase bundles for use at designated fire rings.",
      "question": "Where can campers purchase firewood according to the statement?",
      "options": [
        "A. At the ranger station",
        "B. At the local airport",
        "C. On the mountain peak",
        "D. They cannot purchase firewood"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The electrician installs energy-efficient lighting. Clients can upgrade to LED fixtures during scheduled retrofits.",
      "conflict_prompt": "The electrician does not install energy-efficient lighting. Clients can upgrade to LED fixtures during scheduled retrofits.",
      "question": "What lighting upgrade can clients get during retrofits according to the statement?",
      "options": [
        "A. LED fixtures",
        "B. Oil lanterns",
        "C. Candle sconces",
        "D. Incandescent-only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal zoo participates in captive-breeding programs for endangered species. Keepers provide care and breeding management plans.",
      "conflict_prompt": "The municipal zoo does not participate in captive-breeding programs for endangered species. Keepers provide care and breeding management plans.",
      "question": "What do keepers provide according to the statement?",
      "options": [
        "A. Care and breeding management plans",
        "B. Taxi services",
        "C. Banking operations",
        "D. Grocery deliveries"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The grocery store labels gluten-free products clearly. Shoppers can find a gluten-free symbol on qualifying items.",
      "conflict_prompt": "The grocery store does not label gluten-free products clearly. Shoppers can find a gluten-free symbol on qualifying items.",
      "question": "What symbol can shoppers find on qualifying items according to the statement?",
      "options": [
        "A. A gluten-free symbol",
        "B. A movie rating",
        "C. A weather icon",
        "D. A traffic sign"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The historical society preserves oral histories from local residents. Volunteers record interviews and archive the audio files.",
      "conflict_prompt": "The historical society does not preserve oral histories from local residents. Volunteers record interviews and archive the audio files.",
      "question": "What do volunteers record and archive according to the statement?",
      "options": [
        "A. Weather data only",
        "B. Interviews and audio files",
        "C. Sports scores only",
        "D. Medical records only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The optometrist prescribes corrective lenses. Patients receive eyeglass prescriptions after eye exams.",
      "conflict_prompt": "The optometrist does not prescribe corrective lenses. Patients receive eyeglass prescriptions after eye exams.",
      "question": "What do patients receive after eye exams according to the statement?",
      "options": [
        "A. Eyeglass prescriptions",
        "B. Driving licenses",
        "C. Home appliances",
        "D. Pet adoptions"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The postal office offers certified mail services. Customers can send documents with tracking and proof of delivery.",
      "conflict_prompt": "The postal office does not offer certified mail services. Customers can send documents with tracking and proof of delivery.",
      "question": "What service do customers receive when using the option described according to the statement?",
      "options": [
        "A. Free coffee",
        "B. Tracking and proof of delivery",
        "C. A haircut",
        "D. Complimentary tickets"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The youth soccer league fields U-12 teams. Children under 12 play in the league's age-group divisions.",
      "conflict_prompt": "The youth soccer league does not field U-12 teams. Children under 12 play in the league's age-group divisions.",
      "question": "Which age group plays in the league's divisions according to the statement?",
      "options": [
        "A. Seniors over 65",
        "B. Children under 12",
        "C. Professional adults only",
        "D. Toddlers under 2"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vineyard produces a reserve Cabernet Sauvignon. The winemaker ages the reserve in French oak barrels for 18 months.",
      "conflict_prompt": "The vineyard does not produce a reserve Cabernet Sauvignon. The winemaker ages the reserve in French oak barrels for 18 months.",
      "question": "How long is the reserve aged according to the statement?",
      "options": [
        "A. 18 months",
        "B. One week",
        "C. Ten years",
        "D. Overnight"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city offers tax credits for solar panel installations. Homeowners may apply for a local rebate when they install panels.",
      "conflict_prompt": "The city does not offer tax credits for solar panel installations. Homeowners may apply for a local rebate when they install panels.",
      "question": "What can homeowners apply for when they install solar panels according to the statement?",
      "options": [
        "A. A local rebate",
        "B. A free car",
        "C. A vacation package",
        "D. Military service"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The juice bar uses cold-pressed methods for smoothies. Customers receive drinks made from minimally processed fruit juices.",
      "conflict_prompt": "The juice bar does not use cold-pressed methods for smoothies. Customers receive drinks made from minimally processed fruit juices.",
      "question": "What type of drinks do customers receive according to the statement?",
      "options": [
        "A. Drinks made from minimally processed fruit juices",
        "B. Motor oil",
        "C. Raw eggs only",
        "D. Canned soda only"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pottery studio provides wheel-throwing classes. Beginners learn to center clay and create simple vessels on the wheel.",
      "conflict_prompt": "The pottery studio does not provide wheel-throwing classes. Beginners learn to center clay and create simple vessels on the wheel.",
      "question": "What skill do beginners learn according to the statement?",
      "options": [
        "A. Centering clay on the wheel",
        "B. Tax accounting",
        "C. Welding car frames",
        "D. Flying a plane"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The animal shelter vaccinates all dogs before adoption. Adopters receive vaccination records at pickup.",
      "conflict_prompt": "The animal shelter does not vaccinate all dogs before adoption. Adopters receive vaccination records at pickup.",
      "question": "What do adopters receive at pickup according to the statement?",
      "options": [
        "A. Vaccination records",
        "B. A car title",
        "C. A mortgage",
        "D. A passport"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers rotate crops each season to maintain soil health. Fields alternate between legumes and cereals annually.",
      "conflict_prompt": "The farmers do not rotate crops each season to maintain soil health. Fields alternate between legumes and cereals annually.",
      "question": "What crop rotation pattern is described according to the statement?",
      "options": [
        "A. Alternating legumes and cereals annually",
        "B. Planting the same crop every year",
        "C. Only leaving fields fallow permanently",
        "D. Using concrete instead of soil"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The language school offers beginner French classes. New students attend an introductory course designed for absolute beginners.",
      "conflict_prompt": "The language school does not offer beginner French classes. New students attend an introductory course designed for absolute beginners.",
      "question": "What type of course do new students attend according to the statement?",
      "options": [
        "A. An introductory course for absolute beginners",
        "B. A doctoral seminar",
        "C. A piloting course",
        "D. An advanced surgical rotation"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike shop tunes gears and brakes as part of a full service. Mechanics perform gear adjustments during full tune-ups.",
      "conflict_prompt": "The bike shop does not tune gears and brakes as part of a full service. Mechanics perform gear adjustments during full tune-ups.",
      "question": "What do mechanics perform during full tune-ups according to the statement?",
      "options": [
        "A. Gear adjustments",
        "B. Brain surgery",
        "C. Hair coloring",
        "D. Landscape design"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conservatory accepts sheet music donations. Musicians can donate scores for inclusion in the library catalog.",
      "conflict_prompt": "The conservatory does not accept sheet music donations. Musicians can donate scores for inclusion in the library catalog.",
      "question": "What can musicians donate according to the statement?",
      "options": [
        "A. Scores for inclusion in the library catalog",
        "B. Automobiles",
        "C. Only perishable food",
        "D. Live llamas"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The festival provides translation headsets for attendees. Non-English speakers can borrow headsets for simultaneous translation.",
      "conflict_prompt": "The festival does not provide translation headsets for attendees. Non-English speakers can borrow headsets for simultaneous translation.",
      "question": "What can non-English speakers borrow according to the statement?",
      "options": [
        "A. Translation headsets",
        "B. Lawn chairs only",
        "C. Snow shovels",
        "D. Fishing rods"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software company issues security patches monthly. Administrators download updates from the vendor's portal on a regular schedule.",
      "conflict_prompt": "The software company does not issue security patches monthly. Administrators download updates from the vendor's portal on a regular schedule.",
      "question": "Where do administrators download updates from according to the statement?",
      "options": [
        "A. A vendor's portal",
        "B. A printed newspaper",
        "C. A random street vendor",
        "D. A garden hose"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pottery gallery hosts an annual juried show. Artists submit work and a panel selects pieces for exhibition each year.",
      "conflict_prompt": "The pottery gallery does not host an annual juried show. Artists submit work and a panel selects pieces for exhibition each year.",
      "question": "Who selects pieces for exhibition according to the statement?",
      "options": [
        "A. A panel",
        "B. A single anonymous critic only",
        "C. Random passersby",
        "D. The mail carrier"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local radio station airs a morning news program. Listeners tune in weekdays for the 7 AM news segment.",
      "conflict_prompt": "The local radio station does not air a morning news program. Listeners tune in weekdays for the 7 AM news segment.",
      "question": "When do listeners tune in for the news segment according to the statement?",
      "options": [
        "A. 7 AM on weekdays",
        "B. Midnight on Sundays",
        "C. 3 AM daily",
        "D. Only every leap year"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The thrift store accepts furniture donations. Donors can schedule pickups for large items like sofas and tables.",
      "conflict_prompt": "The thrift store does not accept furniture donations. Donors can schedule pickups for large items like sofas and tables.",
      "question": "What can donors schedule pickups for according to the statement?",
      "options": [
        "A. Large items like sofas and tables",
        "B. Only small jewelry items",
        "C. Live pets",
        "D. Freshly baked bread"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike lane is separated from traffic by a concrete barrier. Cyclists ride in a protected lane along the avenue.",
      "conflict_prompt": "The bike lane is not separated from traffic by a concrete barrier. Cyclists ride in a protected lane along the avenue.",
      "question": "What physical feature separates the bike lane from traffic according to the statement?",
      "options": [
        "A. A concrete barrier",
        "B. A row of fountains",
        "C. A chain-link fence with barbed wire",
        "D. There is no separation"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The observatory conducts public planetarium shows every Friday. Families attend evening presentations under the dome.",
      "conflict_prompt": "The observatory does not conduct public planetarium shows every Friday. Families attend evening presentations under the dome.",
      "question": "When do families attend presentations under the dome according to the statement?",
      "options": [
        "A. Evening on Fridays",
        "B. Morning of Tuesdays only",
        "C. Noon on national holidays only",
        "D. They do not attend any presentations"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community theater stages five productions each season. Volunteers help with sets and costumes for every show.",
      "conflict_prompt": "The community theater does not stage five productions each season. Volunteers help with sets and costumes for every show.",
      "question": "How many productions does the theater stage each season according to the statement?",
      "options": [
        "A. One hundred",
        "B. Five",
        "C. Zero",
        "D. Only musical concerts"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The veterinarian clinic performs spay and neuter surgeries. Pet owners schedule procedures with the clinic's surgical team.",
      "conflict_prompt": "The veterinarian clinic does not perform spay and neuter surgeries. Pet owners schedule procedures with the clinic's surgical team.",
      "question": "What type of procedures do pet owners schedule according to the statement?",
      "options": [
        "A. Spay and neuter surgeries",
        "B. Space shuttle launches",
        "C. Car engine overhauls",
        "D. Mountain climbing expeditions"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipality enforces a noise ordinance after 10 PM. Residents must limit amplified sound in residential neighborhoods at night.",
      "conflict_prompt": "The municipality does not enforce a noise ordinance after 10 PM. Residents must limit amplified sound in residential neighborhoods at night.",
      "question": "What must residents limit according to the statement?",
      "options": [
        "A. Amplified sound",
        "B. Their water usage only",
        "C. Gasoline purchases",
        "D. The number of pets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The co-working space provides private meeting rooms. Members can reserve rooms for client meetings and presentations.",
      "conflict_prompt": "The co-working space does not provide private meeting rooms. Members can reserve rooms for client meetings and presentations.",
      "question": "What can members reserve according to the statement?",
      "options": [
        "A. Private meeting rooms",
        "B. An entire movie theater",
        "C. A submarine",
        "D. No facilities at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The florist delivers same-day orders within the city limits. Customers placing orders before noon receive same-day delivery.",
      "conflict_prompt": "The florist does not deliver same-day orders within the city limits. Customers placing orders before noon receive same-day delivery.",
      "question": "By when must customers place orders to receive same-day delivery according to the statement?",
      "options": [
        "A. Before noon",
        "B. After midnight only",
        "C. A week in advance",
        "D. Only during leap years"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The antiques dealer authenticates items before purchase. Experts examine provenance and materials for authenticity verification.",
      "conflict_prompt": "The antiques dealer does not authenticate items before purchase. Experts examine provenance and materials for authenticity verification.",
      "question": "What do experts examine for authenticity verification according to the statement?",
      "options": [
        "A. Provenance and materials",
        "B. Only the owner's favorite color",
        "C. The weather forecast",
        "D. The time of day"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The ski resort grooms slopes every morning. Snowcats groom the runs before lifts open each day.",
      "conflict_prompt": "The ski resort does not groom slopes every morning. Snowcats groom the runs before lifts open each day.",
      "question": "When do snowcats groom the runs according to the statement?",
      "options": [
        "A. Before lifts open each day",
        "B. Only during summer",
        "C. Once every decade",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery accepts online preorders for wedding cakes. Couples place orders through the bakery's website at least three months in advance.",
      "conflict_prompt": "The bakery does not accept online preorders for wedding cakes. Couples place orders through the bakery's website at least three months in advance.",
      "question": "How far in advance must couples place wedding cake orders according to the statement?",
      "options": [
        "A. At least three months",
        "B. One day before",
        "C. Ten years in advance",
        "D. Only on the wedding day"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hospice provides palliative care services at home. Nurses visit patients to administer pain management and support.",
      "conflict_prompt": "The hospice does not provide palliative care services at home. Nurses visit patients to administer pain management and support.",
      "question": "What services do nurses provide during home visits according to the statement?",
      "options": [
        "A. Pain management and support",
        "B. Car painting",
        "C. Tree pruning",
        "D. Space launches"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pastry chef teaches a weekend éclair workshop. Participants learn choux pastry and filling techniques during the class.",
      "conflict_prompt": "The pastry chef does not teach a weekend éclair workshop. Participants learn choux pastry and filling techniques during the class.",
      "question": "What techniques do participants learn according to the statement?",
      "options": [
        "A. Choux pastry and filling techniques",
        "B. Rocket propulsion",
        "C. Urban planning",
        "D. Software debugging"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The health club includes a sauna in the locker rooms. Members may use the sauna after workouts during staffed hours.",
      "conflict_prompt": "The health club does not include a sauna in the locker rooms. Members may use the sauna after workouts during staffed hours.",
      "question": "When may members use the sauna according to the statement?",
      "options": [
        "A. After workouts during staffed hours",
        "B. Only during the full moon",
        "C. Only if they bring their own wood",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The painter uses lead-free pigments in all commissioned portraits. Clients receive assurances that paints are non-toxic.",
      "conflict_prompt": "The painter does not use lead-free pigments in all commissioned portraits. Clients receive assurances that paints are non-toxic.",
      "question": "What assurance do clients receive according to the statement?",
      "options": [
        "A. That paints are non-toxic",
        "B. That paintings will sing",
        "C. That portraits are made of marble",
        "D. That the gallery will provide free lodging"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The research institute publishes an annual report. Stakeholders receive a yearly summary of the institute's findings and finances.",
      "conflict_prompt": "The research institute does not publish an annual report. Stakeholders receive a yearly summary of the institute's findings and finances.",
      "question": "What do stakeholders receive according to the statement?",
      "options": [
        "A. A yearly summary of findings and finances",
        "B. Only a phone call with no details",
        "C. A handwritten recipe",
        "D. A set of random postcards"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mobile app supports offline reading of saved articles. Users can download articles to read without an internet connection.",
      "conflict_prompt": "The mobile app does not support offline reading of saved articles. Users can download articles to read without an internet connection.",
      "question": "What feature allows users to read without internet according to the statement?",
      "options": [
        "A. Downloading articles for offline reading",
        "B. Printing articles on demand",
        "C. Watching live broadcasts only",
        "D. Listening to elevator music"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The carpentry shop builds custom cabinets to order. Clients provide measurements and designs, and the shop fabricates tailored cabinetry.",
      "conflict_prompt": "The carpentry shop does not build custom cabinets to order. Clients provide measurements and designs, and the shop fabricates tailored cabinetry.",
      "question": "What does the shop fabricate according to the statement?",
      "options": [
        "A. Tailored cabinetry",
        "B. Nuclear reactors",
        "C. Space suits",
        "D. Frozen meals"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college offers evening welding classes. Students attend practical sessions in the metal shop after 5 PM.",
      "conflict_prompt": "The community college does not offer evening welding classes. Students attend practical sessions in the metal shop after 5 PM.",
      "question": "When do students attend practical sessions in the metal shop according to the statement?",
      "options": [
        "A. After 5 PM",
        "B. Only before dawn",
        "C. At noon on holidays",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The courier service offers insurance for high-value packages. Senders can purchase declared-value coverage at the time of shipment.",
      "conflict_prompt": "The courier service does not offer insurance for high-value packages. Senders can purchase declared-value coverage at the time of shipment.",
      "question": "What can senders purchase at the time of shipment according to the statement?",
      "options": [
        "A. Declared-value coverage (insurance)",
        "B. A lifetime warranty for shoes",
        "C. Free airline miles",
        "D. A pet grooming appointment"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery labels products that contain nuts. Customers with allergies are advised to check nut content before purchase.",
      "conflict_prompt": "The bakery does not label products that contain nuts. Customers with allergies are advised to check nut content before purchase.",
      "question": "What are customers with allergies advised to check according to the statement?",
      "options": [
        "A. Nut content",
        "B. The soccer scores",
        "C. The stock market",
        "D. Lottery numbers"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The tutoring center offers SAT prep workshops on Saturdays. Students attend strategy and practice sessions each weekend.",
      "conflict_prompt": "The tutoring center does not offer SAT prep workshops on Saturdays. Students attend strategy and practice sessions each weekend.",
      "question": "When do students attend strategy and practice sessions according to the statement?",
      "options": [
        "A. Each weekend",
        "B. Only during summer camp",
        "C. Once in a decade",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The marina rents kayaks and paddleboards by the hour. Visitors sign waivers and rent watercraft at the dock office.",
      "conflict_prompt": "The marina does not rent kayaks and paddleboards by the hour. Visitors sign waivers and rent watercraft at the dock office.",
      "question": "Where do visitors rent watercraft according to the statement?",
      "options": [
        "A. At the dock office",
        "B. From a grocery store",
        "C. Through a telephone operator in another city",
        "D. They cannot rent any watercraft"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The school cafeteria posts daily menus online. Parents can check the menu and nutritional information on the district website.",
      "conflict_prompt": "The school cafeteria does not post daily menus online. Parents can check the menu and nutritional information on the district website.",
      "question": "Where can parents check menu and nutritional information according to the statement?",
      "options": [
        "A. On the district website",
        "B. Only through carrier pigeons",
        "C. In a secret underground bunker",
        "D. They cannot access it"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art museum maintains a climate-controlled storage facility. Conservators store delicate works in temperature- and humidity-controlled rooms.",
      "conflict_prompt": "The art museum does not maintain a climate-controlled storage facility. Conservators store delicate works in temperature- and humidity-controlled rooms.",
      "question": "In what conditions are delicate works stored according to the statement?",
      "options": [
        "A. Temperature- and humidity-controlled rooms",
        "B. Outside in the open air",
        "C. Submerged in saltwater",
        "D. In a freezer labeled 'cookies'"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike courier covers downtown routes during rush hour. He delivers small packages across the central business district quickly.",
      "conflict_prompt": "The bike courier does not cover downtown routes during rush hour. He delivers small packages across the central business district quickly.",
      "question": "What area does the courier deliver packages across according to the statement?",
      "options": [
        "A. The central business district",
        "B. Outer space",
        "C. A distant continent only",
        "D. Deep underground caves"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The music school requires auditions for advanced ensembles. Prospective members perform prepared pieces for the director.",
      "conflict_prompt": "The music school does not require auditions for advanced ensembles. Prospective members perform prepared pieces for the director.",
      "question": "What must prospective members perform for the director according to the statement?",
      "options": [
        "A. Prepared musical pieces",
        "B. A mathematical proof",
        "C. A dozen pancakes",
        "D. A plumbing inspection"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The beekeeper harvests honey in late summer. Jars of raw honey are bottled after the August extraction.",
      "conflict_prompt": "The beekeeper does not harvest honey in late summer. Jars of raw honey are bottled after the August extraction.",
      "question": "When are jars of raw honey bottled according to the statement?",
      "options": [
        "A. After the August extraction (late summer)",
        "B. In January only",
        "C. During a solar eclipse",
        "D. They are never bottled"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal pool closes for maintenance each September. Lifeguards post closure notices and schedule repairs annually.",
      "conflict_prompt": "The municipal pool does not close for maintenance each September. Lifeguards post closure notices and schedule repairs annually.",
      "question": "When are closure notices and repairs scheduled according to the statement?",
      "options": [
        "A. Each September",
        "B. Every July 4th only",
        "C. Once every hundred years",
        "D. They are never scheduled"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Dr. Morales is a pediatrician. She treats children in her clinic every weekday.",
      "conflict_prompt": "Dr. Morales is not a pediatrician. She treats children in her clinic every weekday.",
      "question": "What is Dr. Morales's medical specialty?",
      "options": [
        "A. Cardiologist",
        "B. Dermatologist",
        "C. Pediatrician",
        "D. Neurologist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood bakery bakes fresh sourdough loaves every morning.",
      "conflict_prompt": "The neighborhood bakery does not bake fresh sourdough loaves every morning.",
      "question": "What type of bread does the bakery bake every morning?",
      "options": [
        "A. Rye",
        "B. Baguette",
        "C. Sourdough",
        "D. Focaccia"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city library lends DVDs to patrons. It has a large film collection.",
      "conflict_prompt": "The city library does not lend DVDs to patrons. It has a large film collection.",
      "question": "What item does the library lend to patrons?",
      "options": [
        "A. Musical instruments",
        "B. Paint brushes",
        "C. DVDs",
        "D. Camping tents"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Olivia speaks fluent Spanish and works as a translator.",
      "conflict_prompt": "Olivia does not speak fluent Spanish and works as a translator.",
      "question": "Which language is Olivia fluent in?",
      "options": [
        "A. French",
        "B. German",
        "C. Spanish",
        "D. Mandarin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The new smartphone has a waterproof design and can be submerged to one meter.",
      "conflict_prompt": "The new smartphone does not have a waterproof design and can be submerged to one meter.",
      "question": "What design feature does the smartphone have?",
      "options": [
        "A. Foldable screen",
        "B. Solar charging",
        "C. Waterproof design",
        "D. Physical keyboard"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Central High School requires uniforms for all students during classes.",
      "conflict_prompt": "Central High School does not require uniforms for all students during classes.",
      "question": "What does Central High School require from students during classes?",
      "options": [
        "A. ID badges",
        "B. Uniforms",
        "C. Laptops",
        "D. Gym shoes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter train stops at Maple Street Station every weekday morning.",
      "conflict_prompt": "The commuter train does not stop at Maple Street Station every weekday morning.",
      "question": "Which station does the commuter train stop at every weekday morning?",
      "options": [
        "A. Oak Avenue Station",
        "B. Pine Hill Station",
        "C. Maple Street Station",
        "D. Riverbend Station"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Professor Chen publishes a peer-reviewed article each year in the chemistry journal.",
      "conflict_prompt": "Professor Chen does not publish a peer-reviewed article each year in the chemistry journal.",
      "question": "How often does Professor Chen publish a peer-reviewed article in the chemistry journal?",
      "options": [
        "A. Monthly",
        "B. Every five years",
        "C. Each year",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Marco owns an electric bicycle and charges it every night.",
      "conflict_prompt": "Marco does not own an electric bicycle and charges it every night.",
      "question": "What type of bicycle does Marco own?",
      "options": [
        "A. Mountain bike",
        "B. Tandem bike",
        "C. Electric bicycle",
        "D. Folding bike"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The conference room has a built-in projector and HDMI ports for presentations.",
      "conflict_prompt": "The conference room does not have a built-in projector and HDMI ports for presentations.",
      "question": "What equipment does the conference room have for presentations?",
      "options": [
        "A. Whiteboard markers",
        "B. Overhead transparency projector",
        "C. Built-in projector and HDMI ports",
        "D. Turntable"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The campground offers freshwater access for campers to refill bottles.",
      "conflict_prompt": "The campground does not offer freshwater access for campers to refill bottles.",
      "question": "What amenity does the campground offer to campers?",
      "options": [
        "A. Electricity hookups",
        "B. Freshwater access",
        "C. Cable TV",
        "D. Wi-Fi in every tent"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "My neighbor keeps a beehive and harvests honey each summer.",
      "conflict_prompt": "My neighbor does not keep a beehive and harvests honey each summer.",
      "question": "What does the neighbor keep and harvest each summer?",
      "options": [
        "A. Apples",
        "B. Bees and honey",
        "C. Pumpkins",
        "D. Fish"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater screens independent films every Thursday night.",
      "conflict_prompt": "The theater does not screen independent films every Thursday night.",
      "question": "What kind of films does the theater screen every Thursday night?",
      "options": [
        "A. Blockbusters",
        "B. Cartoons",
        "C. Independent films",
        "D. Concert films"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Sofia is left-handed and writes with her left hand exclusively.",
      "conflict_prompt": "Sofia is not left-handed and writes with her left hand exclusively.",
      "question": "Which hand does Sofia use to write?",
      "options": [
        "A. Right hand",
        "B. Both hands equally",
        "C. Left hand",
        "D. She types only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum displays the dinosaur skeleton in the main hall.",
      "conflict_prompt": "The museum does not display the dinosaur skeleton in the main hall.",
      "question": "Where is the dinosaur skeleton displayed?",
      "options": [
        "A. In the basement",
        "B. In the courtyard",
        "C. In the main hall",
        "D. In the gift shop"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The electric oven reaches temperatures up to 500°F for baking.",
      "conflict_prompt": "The electric oven does not reach temperatures up to 500°F for baking.",
      "question": "What maximum temperature can the electric oven reach for baking?",
      "options": [
        "A. 350°F",
        "B. 400°F",
        "C. 500°F",
        "D. 250°F"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coastal town enforces a curfew that starts at 11 PM for minors.",
      "conflict_prompt": "The coastal town does not enforce a curfew that starts at 11 PM for minors.",
      "question": "When does the curfew for minors start in the coastal town?",
      "options": [
        "A. 9 PM",
        "B. Midnight",
        "C. 11 PM",
        "D. No curfew"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Daniel trains as a marathon runner and runs at least 40 miles per week.",
      "conflict_prompt": "Daniel does not train as a marathon runner and runs at least 40 miles per week.",
      "question": "How many miles per week does Daniel run at minimum?",
      "options": [
        "A. 20 miles",
        "B. 10 miles",
        "C. 40 miles",
        "D. 100 miles"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The office refrigerator is restocked with milk every Monday morning.",
      "conflict_prompt": "The office refrigerator is not restocked with milk every Monday morning.",
      "question": "When is the office refrigerator restocked with milk?",
      "options": [
        "A. Sunday night",
        "B. Friday afternoon",
        "C. Monday morning",
        "D. It is never restocked"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art class meets on Wednesdays at 6 PM in Studio B.",
      "conflict_prompt": "The art class does not meet on Wednesdays at 6 PM in Studio B.",
      "question": "When does the art class meet?",
      "options": [
        "A. Tuesdays at 4 PM",
        "B. Wednesdays at 6 PM",
        "C. Thursdays at 8 AM",
        "D. Saturdays at noon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The smartphone app requires a fingerprint to unlock for security.",
      "conflict_prompt": "The smartphone app does not require a fingerprint to unlock for security.",
      "question": "What does the app require to unlock for security?",
      "options": [
        "A. Voice recognition",
        "B. PIN only",
        "C. Fingerprint",
        "D. Pattern drawn on screen"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bakery's cinnamon rolls contain real butter and are made daily.",
      "conflict_prompt": "The bakery's cinnamon rolls do not contain real butter and are made daily.",
      "question": "What ingredient do the bakery's cinnamon rolls contain?",
      "options": [
        "A. Margarine",
        "B. Lard",
        "C. Real butter",
        "D. Vegetable oil"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The software license allows each user to install the program on three devices.",
      "conflict_prompt": "The software license does not allow each user to install the program on three devices.",
      "question": "How many devices can each user install the program on according to the license?",
      "options": [
        "A. One device",
        "B. Two devices",
        "C. Three devices",
        "D. Unlimited devices"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Hannah's cat is indoor-only and never goes outside.",
      "conflict_prompt": "Hannah's cat is not indoor-only and never goes outside.",
      "question": "Does Hannah's cat go outside?",
      "options": [
        "A. Yes, often",
        "B. Only supervised",
        "C. No, it never goes outside",
        "D. Only at night"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local hospital runs a 24-hour emergency room that treats all patients regardless of insurance.",
      "conflict_prompt": "The local hospital does not run a 24-hour emergency room that treats all patients regardless of insurance.",
      "question": "What service does the local hospital run 24 hours?",
      "options": [
        "A. Oncology clinic",
        "B. Pediatric outpatient",
        "C. Emergency room",
        "D. Dental clinic"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The condominium complex prohibits pets in common areas.",
      "conflict_prompt": "The condominium complex does not prohibit pets in common areas.",
      "question": "What does the condominium complex prohibit in common areas?",
      "options": [
        "A. Smoking",
        "B. Loud music",
        "C. Pets",
        "D. Bicycles"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical garden cultivates rare alpine plants in its greenhouse.",
      "conflict_prompt": "The botanical garden does not cultivate rare alpine plants in its greenhouse.",
      "question": "What type of plants does the botanical garden cultivate in its greenhouse?",
      "options": [
        "A. Tropical palms",
        "B. Desert cacti",
        "C. Rare alpine plants",
        "D. Mangroves"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Ariana's car has an automatic transmission and she never shifts gears manually.",
      "conflict_prompt": "Ariana's car does not have an automatic transmission and she never shifts gears manually.",
      "question": "What type of transmission does Ariana's car have?",
      "options": [
        "A. Manual transmission",
        "B. Continuously variable transmission",
        "C. Automatic transmission",
        "D. Dual-clutch transmission"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The podcast releases a new episode every Monday morning.",
      "conflict_prompt": "The podcast does not release a new episode every Monday morning.",
      "question": "When does the podcast release a new episode each week?",
      "options": [
        "A. Friday evening",
        "B. Wednesday noon",
        "C. Monday morning",
        "D. Sunday night"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Riverside Park allows bicycles on the paved trail from dawn to dusk.",
      "conflict_prompt": "Riverside Park does not allow bicycles on the paved trail from dawn to dusk.",
      "question": "When are bicycles allowed on the paved trail at Riverside Park?",
      "options": [
        "A. Midnight to dawn",
        "B. Afternoon only",
        "C. From dawn to dusk",
        "D. Bicycles are never allowed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The university bookstore sells official university hoodies in sizes up to XXL.",
      "conflict_prompt": "The university bookstore does not sell official university hoodies in sizes up to XXL.",
      "question": "Up to what size does the university bookstore sell hoodies?",
      "options": [
        "A. Small",
        "B. Large",
        "C. XXL",
        "D. XXXL"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city operates a recycling pickup service every other week.",
      "conflict_prompt": "The city does not operate a recycling pickup service every other week.",
      "question": "How often does the city operate recycling pickup?",
      "options": [
        "A. Daily",
        "B. Weekly",
        "C. Every other week",
        "D. Monthly"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The antique shop appraises Victorian furniture for a fee every Saturday.",
      "conflict_prompt": "The antique shop does not appraise Victorian furniture for a fee every Saturday.",
      "question": "When does the antique shop appraise Victorian furniture for a fee?",
      "options": [
        "A. Mondays",
        "B. Saturdays",
        "C. Fridays",
        "D. Sundays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter ferry accommodates bicycles on board during daytime crossings.",
      "conflict_prompt": "The commuter ferry does not accommodate bicycles on board during daytime crossings.",
      "question": "What does the commuter ferry accommodate during daytime crossings?",
      "options": [
        "A. Cars",
        "B. Livestock",
        "C. Bicycles",
        "D. Large trucks"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The orchard harvests apples in September each year.",
      "conflict_prompt": "The orchard does not harvest apples in September each year.",
      "question": "When does the orchard harvest apples each year?",
      "options": [
        "A. June",
        "B. September",
        "C. December",
        "D. April"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The swimming pool closes for maintenance every winter month.",
      "conflict_prompt": "The swimming pool does not close for maintenance every winter month.",
      "question": "When does the swimming pool close for maintenance?",
      "options": [
        "A. Every summer month",
        "B. Every winter month",
        "C. Every spring month",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Chef Ramos uses olive oil in the restaurant's signature pasta dish.",
      "conflict_prompt": "Chef Ramos does not use olive oil in the restaurant's signature pasta dish.",
      "question": "What ingredient does Chef Ramos use in the signature pasta dish?",
      "options": [
        "A. Butter",
        "B. Vegetable oil",
        "C. Olive oil",
        "D. Mayonnaise"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hiking trail permits dogs on leashes but prohibits off-leash animals.",
      "conflict_prompt": "The hiking trail does not permit dogs on leashes but prohibits off-leash animals.",
      "question": "What is allowed on the hiking trail?",
      "options": [
        "A. Dogs off-leash",
        "B. Dogs on leashes",
        "C. Horses only",
        "D. Motorbikes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The piano in the concert hall is a Steinway grand that is tuned weekly.",
      "conflict_prompt": "The piano in the concert hall is not a Steinway grand and is tuned weekly.",
      "question": "What brand is the concert hall piano?",
      "options": [
        "A. Yamaha",
        "B. Kawai",
        "C. Steinway",
        "D. Baldwin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The charity distributes warm blankets to homeless individuals every December.",
      "conflict_prompt": "The charity does not distribute warm blankets to homeless individuals every December.",
      "question": "What does the charity distribute every December?",
      "options": [
        "A. Hot meals",
        "B. Warm blankets",
        "C. Gift cards",
        "D. Winter boots"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter pass allows unlimited subway rides within city limits for 30 days.",
      "conflict_prompt": "The commuter pass does not allow unlimited subway rides within city limits for 30 days.",
      "question": "What does the commuter pass allow?",
      "options": [
        "A. A single ride",
        "B. Unlimited rides for 7 days",
        "C. Unlimited subway rides within city limits for 30 days",
        "D. Discounted taxi fares"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The restaurant offers a gluten-free menu with multiple entree options.",
      "conflict_prompt": "The restaurant does not offer a gluten-free menu with multiple entree options.",
      "question": "What type of menu does the restaurant offer?",
      "options": [
        "A. Vegan only",
        "B. Gluten-free menu",
        "C. Raw food menu",
        "D. Kids-only menu"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The park's pond contains koi fish that the staff feeds each afternoon.",
      "conflict_prompt": "The park's pond does not contain koi fish that the staff feeds each afternoon.",
      "question": "What kind of fish does the pond contain that staff feed each afternoon?",
      "options": [
        "A. Trout",
        "B. Catfish",
        "C. Koi",
        "D. Salmon"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The courier service delivers packages before 9 PM on weekdays.",
      "conflict_prompt": "The courier service does not deliver packages before 9 PM on weekdays.",
      "question": "By what time does the courier service deliver packages on weekdays?",
      "options": [
        "A. Before 5 PM",
        "B. Before 9 PM",
        "C. After midnight",
        "D. Before 12 PM"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vacation rental includes a washer and dryer for guest use.",
      "conflict_prompt": "The vacation rental does not include a washer and dryer for guest use.",
      "question": "What appliance is available for guests at the vacation rental?",
      "options": [
        "A. Dishwasher",
        "B. Gas grill",
        "C. Washer and dryer",
        "D. Treadmill"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The museum café serves fair-trade coffee sourced from local roasters.",
      "conflict_prompt": "The museum café does not serve fair-trade coffee sourced from local roasters.",
      "question": "What type of coffee does the museum café serve?",
      "options": [
        "A. Instant coffee",
        "B. Fair-trade coffee from local roasters",
        "C. Decaf only",
        "D. Bulk supermarket brand"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coastal highway is closed to heavy trucks over five tons during peak hours.",
      "conflict_prompt": "The coastal highway is not closed to heavy trucks over five tons during peak hours.",
      "question": "Which vehicles are restricted on the coastal highway during peak hours?",
      "options": [
        "A. Motorbikes",
        "B. Bicycles",
        "C. Heavy trucks over five tons",
        "D. Electric cars"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farm raises free-range chickens that lay eggs available at the farm stand.",
      "conflict_prompt": "The farm does not raise free-range chickens that lay eggs available at the farm stand.",
      "question": "What type of chickens does the farm raise that provide eggs at the farm stand?",
      "options": [
        "A. Cage-raised chickens",
        "B. Free-range chickens",
        "C. Broiler chickens",
        "D. Roosters"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pharmacy dispenses prescription refills without a drive-through window.",
      "conflict_prompt": "The pharmacy does not dispense prescription refills without a drive-through window.",
      "question": "How does the pharmacy dispense prescription refills?",
      "options": [
        "A. Only via mail-order",
        "B. With a drive-through window available",
        "C. Walk-in dispensing without a drive-through",
        "D. Through vending machines"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The boutique stocks vintage denim jackets in the back corner of the store.",
      "conflict_prompt": "The boutique does not stock vintage denim jackets in the back corner of the store.",
      "question": "Where in the boutique are vintage denim jackets stocked?",
      "options": [
        "A. Front window display",
        "B. Back corner of the store",
        "C. The basement",
        "D. They are not stocked at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public tennis courts are illuminated for night play until 10 PM.",
      "conflict_prompt": "The public tennis courts are not illuminated for night play until 10 PM.",
      "question": "Until what time are the public tennis courts illuminated for night play?",
      "options": [
        "A. 8 PM",
        "B. 9 PM",
        "C. 10 PM",
        "D. Midnight"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The deli makes its own pastrami in-house using a traditional recipe.",
      "conflict_prompt": "The deli does not make its own pastrami in-house using a traditional recipe.",
      "question": "How does the deli prepare its pastrami?",
      "options": [
        "A. Purchased pre-sliced from a supplier",
        "B. Made in-house using a traditional recipe",
        "C. Made from turkey",
        "D. It does not serve pastrami"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The science fair requires participants to present experiments with clearly labeled hypotheses.",
      "conflict_prompt": "The science fair does not require participants to present experiments with clearly labeled hypotheses.",
      "question": "What must participants include in their experiments for the science fair?",
      "options": [
        "A. Live animals",
        "B. Clearly labeled hypotheses",
        "C. Commercial products only",
        "D. Artistic displays"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The rooftop garden irrigates plants using a drip irrigation system to conserve water.",
      "conflict_prompt": "The rooftop garden does not irrigate plants using a drip irrigation system to conserve water.",
      "question": "What irrigation method does the rooftop garden use?",
      "options": [
        "A. Overhead sprinklers",
        "B. Flood irrigation",
        "C. Drip irrigation",
        "D. Manual watering only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The antique clock chimes on the hour and is maintained by a professional.",
      "conflict_prompt": "The antique clock does not chime on the hour and is maintained by a professional.",
      "question": "When does the antique clock chime?",
      "options": [
        "A. On the half-hour",
        "B. On the hour",
        "C. It doesn't chime",
        "D. Only at midnight"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The vegetarian café serves a nut-free menu for customers with allergies.",
      "conflict_prompt": "The vegetarian café does not serve a nut-free menu for customers with allergies.",
      "question": "What special menu does the vegetarian café serve for customers with allergies?",
      "options": [
        "A. Dairy-free menu",
        "B. Nut-free menu",
        "C. Sugar-free menu",
        "D. Raw-only menu"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal pool requires swim caps for all lap swimmers during peak hours.",
      "conflict_prompt": "The municipal pool does not require swim caps for all lap swimmers during peak hours.",
      "question": "What does the municipal pool require for lap swimmers during peak hours?",
      "options": [
        "A. Goggles only",
        "B. Swim caps",
        "C. Nose clips",
        "D. Life jackets"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The theater box office opens two hours before each performance to sell tickets.",
      "conflict_prompt": "The theater box office does not open two hours before each performance to sell tickets.",
      "question": "How early does the box office open before each performance?",
      "options": [
        "A. One hour",
        "B. Two hours",
        "C. Four hours",
        "D. It opens only online"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The winery produces a Cabernet Sauvignon vintage released every autumn.",
      "conflict_prompt": "The winery does not produce a Cabernet Sauvignon vintage released every autumn.",
      "question": "Which wine vintage does the winery produce and release every autumn?",
      "options": [
        "A. Pinot Grigio",
        "B. Chardonnay",
        "C. Cabernet Sauvignon",
        "D. Moscato"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city museum offers guided tours in English and French on weekends.",
      "conflict_prompt": "The city museum does not offer guided tours in English and French on weekends.",
      "question": "In which languages does the city museum offer guided tours on weekends?",
      "options": [
        "A. Spanish and German",
        "B. English and French",
        "C. Italian and Japanese",
        "D. Only English"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The electronics store includes a two-year warranty with every laptop purchase.",
      "conflict_prompt": "The electronics store does not include a two-year warranty with every laptop purchase.",
      "question": "How long is the warranty included with every laptop purchase?",
      "options": [
        "A. Six months",
        "B. One year",
        "C. Two years",
        "D. Five years"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers' market accepts SNAP benefits for eligible food purchases.",
      "conflict_prompt": "The farmers' market does not accept SNAP benefits for eligible food purchases.",
      "question": "What payment assistance program does the farmers' market accept?",
      "options": [
        "A. SNAP benefits",
        "B. Medicare",
        "C. Student grants",
        "D. Travel vouchers"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The research lab stores samples at -80°C to preserve biological material.",
      "conflict_prompt": "The research lab does not store samples at -80°C to preserve biological material.",
      "question": "At what temperature does the research lab store samples to preserve biological material?",
      "options": [
        "A. 4°C",
        "B. -20°C",
        "C. -80°C",
        "D. Room temperature"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The passenger ferry enforces life jacket use for children under 12 during crossings.",
      "conflict_prompt": "The passenger ferry does not enforce life jacket use for children under 12 during crossings.",
      "question": "Who must wear life jackets on the passenger ferry during crossings?",
      "options": [
        "A. Everyone over 18",
        "B. Children under 12",
        "C. Pets only",
        "D. Crew members only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood watch meets the first Tuesday of each month at the community center.",
      "conflict_prompt": "The neighborhood watch does not meet the first Tuesday of each month at the community center.",
      "question": "When does the neighborhood watch meet each month?",
      "options": [
        "A. First Tuesday",
        "B. Second Thursday",
        "C. Last Saturday",
        "D. Every day"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The eyewear store fits customers for prescription lenses during walk-in hours.",
      "conflict_prompt": "The eyewear store does not fit customers for prescription lenses during walk-in hours.",
      "question": "What service does the eyewear store provide during walk-in hours?",
      "options": [
        "A. Shoe repairs",
        "B. Prescription lens fittings",
        "C. Haircuts",
        "D. Pet vaccinations"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The nature reserve prohibits fishing in the protected wetlands year-round.",
      "conflict_prompt": "The nature reserve does not prohibit fishing in the protected wetlands year-round.",
      "question": "What activity is prohibited in the protected wetlands of the nature reserve year-round?",
      "options": [
        "A. Bird watching",
        "B. Fishing",
        "C. Hiking",
        "D. Picnicking"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hospice provides palliative care and accepts volunteer visitors.",
      "conflict_prompt": "The hospice does not provide palliative care and accepts volunteer visitors.",
      "question": "What type of care does the hospice provide?",
      "options": [
        "A. Cosmetic surgery",
        "B. Palliative care",
        "C. Pediatric vaccinations",
        "D. Dental hygienist services"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The craft brewery bottles its seasonal ale in 500 ml glass bottles.",
      "conflict_prompt": "The craft brewery does not bottle its seasonal ale in 500 ml glass bottles.",
      "question": "In what size bottles does the craft brewery bottle its seasonal ale?",
      "options": [
        "A. 330 ml",
        "B. 750 ml",
        "C. 500 ml",
        "D. 1 liter"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pediatric clinic gives infants their first vaccinations at two months old.",
      "conflict_prompt": "The pediatric clinic does not give infants their first vaccinations at two months old.",
      "question": "At what age does the pediatric clinic give infants their first vaccinations?",
      "options": [
        "A. One week",
        "B. Two months",
        "C. Six months",
        "D. One year"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The commuter bicycle lane is painted green to indicate a protected route.",
      "conflict_prompt": "The commuter bicycle lane is not painted green to indicate a protected route.",
      "question": "What color indicates the commuter bicycle lane as a protected route?",
      "options": [
        "A. Red",
        "B. Blue",
        "C. Green",
        "D. Yellow"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's emergency alert system sends notifications via text message to registered residents.",
      "conflict_prompt": "The city's emergency alert system does not send notifications via text message to registered residents.",
      "question": "How does the city's emergency alert system notify registered residents?",
      "options": [
        "A. By postal mail",
        "B. By in-person visits",
        "C. Via text message",
        "D. Through radio only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The music conservatory offers scholarships to promising young pianists each year.",
      "conflict_prompt": "The music conservatory does not offer scholarships to promising young pianists each year.",
      "question": "Who receives scholarships from the music conservatory each year?",
      "options": [
        "A. Young violinists",
        "B. Promising young pianists",
        "C. Mature singers only",
        "D. Guitarists exclusively"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The online course provides a certificate of completion after passing the final exam.",
      "conflict_prompt": "The online course does not provide a certificate of completion after passing the final exam.",
      "question": "What does the online course provide upon passing the final exam?",
      "options": [
        "A. Free textbooks",
        "B. A refund",
        "C. A certificate of completion",
        "D. A lecture recording only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The daycare center requires that all children be immunized according to state guidelines.",
      "conflict_prompt": "The daycare center does not require that all children be immunized according to state guidelines.",
      "question": "What does the daycare center require for all children?",
      "options": [
        "A. Daily naps",
        "B. Immunizations per state guidelines",
        "C. Private tutors",
        "D. Formal uniforms"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain lodge heats cabins with a wood-burning stove during winter months.",
      "conflict_prompt": "The mountain lodge does not heat cabins with a wood-burning stove during winter months.",
      "question": "How does the mountain lodge heat cabins during winter months?",
      "options": [
        "A. Electric heaters only",
        "B. Solar panels",
        "C. Wood-burning stoves",
        "D. No heating"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The subscription box ships new products to subscribers on the first of every month.",
      "conflict_prompt": "The subscription box does not ship new products to subscribers on the first of every month.",
      "question": "When does the subscription box ship new products to subscribers?",
      "options": [
        "A. The fifteenth of each month",
        "B. The last day of the month",
        "C. The first of every month",
        "D. Only on demand"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The dentist schedules routine cleanings every six months for patients.",
      "conflict_prompt": "The dentist does not schedule routine cleanings every six months for patients.",
      "question": "How often does the dentist schedule routine cleanings for patients?",
      "options": [
        "A. Every month",
        "B. Every six months",
        "C. Every two years",
        "D. Once only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public transit card requires tapping at both entry and exit stations for fare calculation.",
      "conflict_prompt": "The public transit card does not require tapping at both entry and exit stations for fare calculation.",
      "question": "What does the public transit card require for fare calculation?",
      "options": [
        "A. Only entry tap",
        "B. Only exit tap",
        "C. Tapping at both entry and exit stations",
        "D. No tapping at all"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The wildlife rehabilitation center rehabilitates injured raptors and releases them when healthy.",
      "conflict_prompt": "The wildlife rehabilitation center does not rehabilitate injured raptors and releases them when healthy.",
      "question": "What type of animals does the center rehabilitate and release when healthy?",
      "options": [
        "A. Reptiles",
        "B. Raptors (birds of prey)",
        "C. Fish",
        "D. Domestic cats"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The regional airport provides free shuttle service to the downtown hotels.",
      "conflict_prompt": "The regional airport does not provide free shuttle service to the downtown hotels.",
      "question": "What service does the regional airport provide to downtown hotels?",
      "options": [
        "A. Free shuttle service",
        "B. Free breakfast",
        "C. Complimentary spa access",
        "D. Complimentary dry cleaning"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local orchestra rehearses in the concert hall every Thursday evening.",
      "conflict_prompt": "The local orchestra does not rehearse in the concert hall every Thursday evening.",
      "question": "When does the local orchestra rehearse in the concert hall?",
      "options": [
        "A. Monday morning",
        "B. Thursday evening",
        "C. Sunday afternoon",
        "D. Every day at noon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The emergency exit door alarms if opened during regular business hours.",
      "conflict_prompt": "The emergency exit door does not alarm if opened during regular business hours.",
      "question": "What happens if the emergency exit door is opened during regular business hours?",
      "options": [
        "A. It unlocks a bonus room",
        "B. An alarm sounds",
        "C. It dispenses coupons",
        "D. Nothing happens"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local bakery sells vegan pastries made without dairy or eggs.",
      "conflict_prompt": "The local bakery does not sell vegan pastries made without dairy or eggs.",
      "question": "What special type of pastries does the bakery sell?",
      "options": [
        "A. Keto pastries",
        "B. Vegan pastries without dairy or eggs",
        "C. Meat-filled pastries",
        "D. Gluten-heavy pastries"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal garage offers bicycle repair workshops every Sunday morning.",
      "conflict_prompt": "The municipal garage does not offer bicycle repair workshops every Sunday morning.",
      "question": "When does the municipal garage offer bicycle repair workshops?",
      "options": [
        "A. Saturday night",
        "B. Sunday morning",
        "C. Wednesday afternoon",
        "D. They do not offer workshops"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The train conductor inspects tickets before departure at the platform.",
      "conflict_prompt": "The train conductor does not inspect tickets before departure at the platform.",
      "question": "When does the train conductor inspect tickets?",
      "options": [
        "A. After arrival only",
        "B. During the middle of the journey",
        "C. Before departure at the platform",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood coffee shop roasts beans on-site every Friday.",
      "conflict_prompt": "The neighborhood coffee shop does not roast beans on-site every Friday.",
      "question": "When does the coffee shop roast beans on-site?",
      "options": [
        "A. Every Monday",
        "B. Every Friday",
        "C. Every day",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The book club meets to discuss the monthly novel on the last Wednesday evening.",
      "conflict_prompt": "The book club does not meet to discuss the monthly novel on the last Wednesday evening.",
      "question": "When does the book club meet to discuss the monthly novel?",
      "options": [
        "A. First Monday",
        "B. Last Wednesday evening",
        "C. Third Friday morning",
        "D. Every Sunday"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The painting studio provides easels and supplies for weekend classes.",
      "conflict_prompt": "The painting studio does not provide easels and supplies for weekend classes.",
      "question": "What does the painting studio provide for weekend classes?",
      "options": [
        "A. Musical instruments",
        "B. Easels and supplies",
        "C. Sports equipment",
        "D. Cooking utensils"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The hotel includes complimentary breakfast for guests staying in standard rooms.",
      "conflict_prompt": "The hotel does not include complimentary breakfast for guests staying in standard rooms.",
      "question": "What is included for guests staying in standard rooms at the hotel?",
      "options": [
        "A. Complimentary breakfast",
        "B. Free car rental",
        "C. Complimentary spa treatment",
        "D. No amenities included"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coastal lighthouse operates an automated light that rotates every 30 seconds.",
      "conflict_prompt": "The coastal lighthouse does not operate an automated light that rotates every 30 seconds.",
      "question": "How often does the lighthouse light complete a rotation?",
      "options": [
        "A. Every 5 seconds",
        "B. Every 30 seconds",
        "C. Every 10 minutes",
        "D. It does not rotate"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The primary school prohibits cell phone use during instructional hours.",
      "conflict_prompt": "The primary school does not prohibit cell phone use during instructional hours.",
      "question": "What policy does the primary school have regarding cell phones during instructional hours?",
      "options": [
        "A. Encourages constant use",
        "B. Prohibits use",
        "C. Requires use for all students",
        "D. Allows use for teachers only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The surf shop rents wetsuits in various sizes for ocean surfers.",
      "conflict_prompt": "The surf shop does not rent wetsuits in various sizes for ocean surfers.",
      "question": "What does the surf shop rent for ocean surfers?",
      "options": [
        "A. Snowboards",
        "B. Wetsuits in various sizes",
        "C. Mountain bikes",
        "D. Camping tents"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical society holds plant identification walks each month during spring.",
      "conflict_prompt": "The botanical society does not hold plant identification walks each month during spring.",
      "question": "When does the botanical society hold plant identification walks?",
      "options": [
        "A. Each month during spring",
        "B. Only in winter",
        "C. Not at all",
        "D. Every other year"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The child safety seat must be rear-facing until the child reaches two years old.",
      "conflict_prompt": "The child safety seat must not be rear-facing until the child reaches two years old.",
      "question": "Until what age must the child safety seat be rear-facing?",
      "options": [
        "A. Six months",
        "B. One year",
        "C. Two years",
        "D. Four years"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local bakery donates leftover unsold bread to the food bank every evening.",
      "conflict_prompt": "The local bakery does not donate leftover unsold bread to the food bank every evening.",
      "question": "What does the local bakery do with leftover unsold bread every evening?",
      "options": [
        "A. Discards it",
        "B. Donates to the food bank",
        "C. Uses it for compost only",
        "D. Sells it at a higher price"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The veterinary clinic performs spay and neuter surgeries on shelter animals weekly.",
      "conflict_prompt": "The veterinary clinic does not perform spay and neuter surgeries on shelter animals weekly.",
      "question": "How often does the clinic perform spay and neuter surgeries on shelter animals?",
      "options": [
        "A. Daily",
        "B. Weekly",
        "C. Annually",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The electrical outlet in the workshop includes ground-fault protection for safety.",
      "conflict_prompt": "The electrical outlet in the workshop does not include ground-fault protection for safety.",
      "question": "What safety feature does the workshop's electrical outlet include?",
      "options": [
        "A. Surge protection only",
        "B. Ground-fault protection",
        "C. No safety features",
        "D. Built-in Wi-Fi"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The municipal recycling program accepts glass, paper, and aluminum items curbside.",
      "conflict_prompt": "The municipal recycling program does not accept glass, paper, and aluminum items curbside.",
      "question": "Which items does the municipal recycling program accept curbside?",
      "options": [
        "A. Electronics",
        "B. Hazardous chemicals",
        "C. Glass, paper, and aluminum",
        "D. Food waste only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Dr. Patel performs cataract surgeries at the eye clinic every Tuesday morning.",
      "conflict_prompt": "Dr. Patel does not perform cataract surgeries at the eye clinic every Tuesday morning.",
      "question": "When does Dr. Patel perform cataract surgeries at the eye clinic?",
      "options": [
        "A. Monday night",
        "B. Tuesday morning",
        "C. Thursday afternoon",
        "D. Only on weekends"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city pool prohibits glass containers on the pool deck at all times.",
      "conflict_prompt": "The city pool does not prohibit glass containers on the pool deck at all times.",
      "question": "What is prohibited on the pool deck at the city pool?",
      "options": [
        "A. Sunscreen",
        "B. Glass containers",
        "C. Towels",
        "D. Swimwear"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The teaching hospital operates a neonatal intensive care unit for premature infants.",
      "conflict_prompt": "The teaching hospital does not operate a neonatal intensive care unit for premature infants.",
      "question": "What specialized unit does the teaching hospital operate for premature infants?",
      "options": [
        "A. Pediatric ICU for adolescents",
        "B. Neonatal intensive care unit (NICU)",
        "C. Geriatric ward",
        "D. Psychiatric outpatient clinic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmers' cooperative stores produce in a refrigerated warehouse to preserve freshness.",
      "conflict_prompt": "The farmers' cooperative does not store produce in a refrigerated warehouse to preserve freshness.",
      "question": "Where does the cooperative store produce to preserve freshness?",
      "options": [
        "A. In open-air crates",
        "B. In a refrigerated warehouse",
        "C. In a dry shed",
        "D. In off-site markets only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community center offers free Wi-Fi in the lobby area for visitors.",
      "conflict_prompt": "The community center does not offer free Wi-Fi in the lobby area for visitors.",
      "question": "What amenity does the community center offer in the lobby area?",
      "options": [
        "A. Free Wi-Fi",
        "B. Free parking",
        "C. Free breakfast",
        "D. Free concert tickets"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local solar farm generates enough electricity to power approximately 2,000 homes.",
      "conflict_prompt": "The local solar farm does not generate enough electricity to power approximately 2,000 homes.",
      "question": "Approximately how many homes can the local solar farm power?",
      "options": [
        "A. 200 homes",
        "B. 500 homes",
        "C. 2,000 homes",
        "D. 10,000 homes"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The private detective specializes in corporate fraud investigations for businesses.",
      "conflict_prompt": "The private detective does not specialize in corporate fraud investigations for businesses.",
      "question": "What type of investigations does the private detective specialize in?",
      "options": [
        "A. Missing pets",
        "B. Corporate fraud",
        "C. Wedding planning",
        "D. Landscaping"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community theater charges a modest fee for ticketed performances to cover costs.",
      "conflict_prompt": "The community theater does not charge a modest fee for ticketed performances to cover costs.",
      "question": "Why does the community theater charge a modest fee for performances?",
      "options": [
        "A. To cover costs",
        "B. To deter attendance",
        "C. To pay actors excessively",
        "D. To provide free snacks"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The highway rest stop includes an electric vehicle charging station near the restroom.",
      "conflict_prompt": "The highway rest stop does not include an electric vehicle charging station near the restroom.",
      "question": "What amenity does the highway rest stop include near the restroom?",
      "options": [
        "A. A playground",
        "B. An electric vehicle charging station",
        "C. A helicopter pad",
        "D. A laundromat"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The photography club requires members to submit at least one print for the annual exhibition.",
      "conflict_prompt": "The photography club does not require members to submit at least one print for the annual exhibition.",
      "question": "What must photography club members submit for the annual exhibition?",
      "options": [
        "A. A short film",
        "B. A printed photograph",
        "C. A sculpture",
        "D. A poem"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood association plants street trees along Elm Avenue each spring.",
      "conflict_prompt": "The neighborhood association does not plant street trees along Elm Avenue each spring.",
      "question": "Where does the neighborhood association plant street trees each spring?",
      "options": [
        "A. Maple Street",
        "B. Elm Avenue",
        "C. Pine Lane",
        "D. Oak Boulevard"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recycling education program teaches students to separate paper, plastic, and glass.",
      "conflict_prompt": "The recycling education program does not teach students to separate paper, plastic, and glass.",
      "question": "What does the recycling education program teach students to separate?",
      "options": [
        "A. Metals, electronics, and batteries",
        "B. Paper, plastic, and glass",
        "C. Food groups",
        "D. Historical periods"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city ordinance bans fireworks within the municipal boundaries during summer months.",
      "conflict_prompt": "The city ordinance does not ban fireworks within the municipal boundaries during summer months.",
      "question": "What does the city ordinance ban during summer months?",
      "options": [
        "A. Lawn mowing",
        "B. Fireworks within municipal boundaries",
        "C. Playing music outdoors",
        "D. Selling ice cream"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public health clinic offers flu vaccinations free of charge to seniors.",
      "conflict_prompt": "The public health clinic does not offer flu vaccinations free of charge to seniors.",
      "question": "Who receives free flu vaccinations at the public health clinic?",
      "options": [
        "A. Tourists",
        "B. College students",
        "C. Seniors",
        "D. Infants only"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The bike rental shop requires a security deposit for each bicycle rented.",
      "conflict_prompt": "The bike rental shop does not require a security deposit for each bicycle rented.",
      "question": "What does the bike rental shop require when renting a bicycle?",
      "options": [
        "A. Proof of residency only",
        "B. Security deposit",
        "C. A signed book review",
        "D. No identification at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer's cooperative offers bulk grain sales to local bakers at discounted rates.",
      "conflict_prompt": "The farmer's cooperative does not offer bulk grain sales to local bakers at discounted rates.",
      "question": "To whom does the cooperative offer bulk grain sales at discounted rates?",
      "options": [
        "A. International exporters",
        "B. Local bakers",
        "C. Residential gardeners",
        "D. Tourists"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city parking garage accepts monthly permits for residents at a reduced rate.",
      "conflict_prompt": "The city parking garage does not accept monthly permits for residents at a reduced rate.",
      "question": "What does the city parking garage accept for residents at a reduced rate?",
      "options": [
        "A. Annual passes only",
        "B. Hourly cash payments only",
        "C. Monthly permits",
        "D. Biweekly coupons"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The emergency generator automatically turns on within 10 seconds of power loss.",
      "conflict_prompt": "The emergency generator does not automatically turn on within 10 seconds of power loss.",
      "question": "How quickly does the emergency generator activate after power loss?",
      "options": [
        "A. Within 10 seconds",
        "B. After 30 minutes",
        "C. After one hour",
        "D. It never activates"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The neighborhood pharmacy compounds custom creams for dermatology patients by prescription.",
      "conflict_prompt": "The neighborhood pharmacy does not compound custom creams for dermatology patients by prescription.",
      "question": "What service does the neighborhood pharmacy provide for dermatology patients by prescription?",
      "options": [
        "A. Routine blood tests",
        "B. Compounded custom creams",
        "C. Surgical procedures",
        "D. MRI scans"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The botanical lab stores seed samples in a desiccator to keep them dry.",
      "conflict_prompt": "The botanical lab does not store seed samples in a desiccator to keep them dry.",
      "question": "Where does the lab store seed samples to keep them dry?",
      "options": [
        "A. In open trays",
        "B. In a desiccator",
        "C. In a freezer with water",
        "D. In the greenhouse soil"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The urban garden prohibits pesticide use to support pollinator health.",
      "conflict_prompt": "The urban garden does not prohibit pesticide use to support pollinator health.",
      "question": "Why does the urban garden prohibit pesticide use?",
      "options": [
        "A. To save money",
        "B. To support pollinator health",
        "C. To grow only indoor plants",
        "D. Because pesticides are ineffective"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The district library's children's section contains books suitable for ages 3 to 8.",
      "conflict_prompt": "The district library's children's section does not contain books suitable for ages 3 to 8.",
      "question": "For what ages does the children's section contain suitable books?",
      "options": [
        "A. 0 to 2",
        "B. 3 to 8",
        "C. 9 to 12",
        "D. Adults only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The waterfront promenade features benches every 100 meters for visitors to rest.",
      "conflict_prompt": "The waterfront promenade does not feature benches every 100 meters for visitors to rest.",
      "question": "How frequently are benches placed along the waterfront promenade?",
      "options": [
        "A. Every 10 meters",
        "B. Every 100 meters",
        "C. Every kilometer",
        "D. No benches"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The recycling center requires glass be separated by color before drop-off.",
      "conflict_prompt": "The recycling center does not require glass be separated by color before drop-off.",
      "question": "How must glass be prepared before drop-off at the recycling center?",
      "options": [
        "A. Crushed only",
        "B. Separated by color",
        "C. Mixed with organic waste",
        "D. Wrapped in plastic"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The city's bike-share program provides helmets at kiosks for rider safety.",
      "conflict_prompt": "The city's bike-share program does not provide helmets at kiosks for rider safety.",
      "question": "What safety equipment does the bike-share program provide at kiosks?",
      "options": [
        "A. Life vests",
        "B. Helmets",
        "C. Knee pads",
        "D. Safety cones"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The historic bridge underwent structural reinforcement to support modern traffic loads.",
      "conflict_prompt": "The historic bridge did not undergo structural reinforcement to support modern traffic loads.",
      "question": "Why was the historic bridge reinforced structurally?",
      "options": [
        "A. To become a pedestrian-only bridge",
        "B. To support modern traffic loads",
        "C. To add decorative lighting",
        "D. To reduce its length"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The swimming instructor teaches stroke technique and requires goggles for lessons.",
      "conflict_prompt": "The swimming instructor does not teach stroke technique and requires goggles for lessons.",
      "question": "What does the swimming instructor teach and require for lessons?",
      "options": [
        "A. Diving only and no equipment",
        "B. Stroke technique and goggles",
        "C. Lifeguarding and helmets",
        "D. Water polo only and flippers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The public art installation includes an interactive light display powered by solar panels.",
      "conflict_prompt": "The public art installation does not include an interactive light display powered by solar panels.",
      "question": "What powers the interactive light display in the public art installation?",
      "options": [
        "A. Wind turbines",
        "B. Grid electricity only",
        "C. Solar panels",
        "D. Gas generators"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The farmer sells organic heirloom tomatoes at the Saturday market booth.",
      "conflict_prompt": "The farmer does not sell organic heirloom tomatoes at the Saturday market booth.",
      "question": "What does the farmer sell at the Saturday market booth?",
      "options": [
        "A. Industrial canned tomatoes",
        "B. Organic heirloom tomatoes",
        "C. Processed tomato paste only",
        "D. Potatoes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local transit authority offers discounted fares for seniors and students.",
      "conflict_prompt": "The local transit authority does not offer discounted fares for seniors and students.",
      "question": "Who receives discounted fares from the local transit authority?",
      "options": [
        "A. Tourists only",
        "B. Seniors and students",
        "C. Only children under 2",
        "D. No one"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art museum rotates its modern art collection every three months.",
      "conflict_prompt": "The art museum does not rotate its modern art collection every three months.",
      "question": "How often does the art museum rotate its modern art collection?",
      "options": [
        "A. Every month",
        "B. Every three months",
        "C. Every five years",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The local grocer stocks fair-trade chocolate bars on a prominent shelf near the front.",
      "conflict_prompt": "The local grocer does not stock fair-trade chocolate bars on a prominent shelf near the front.",
      "question": "Where are the fair-trade chocolate bars stocked in the local grocer?",
      "options": [
        "A. In the freezer section",
        "B. On a prominent shelf near the front",
        "C. In the back storage only",
        "D. They are unavailable"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The veterinary hospital requires appointments for non-emergency examinations.",
      "conflict_prompt": "The veterinary hospital does not require appointments for non-emergency examinations.",
      "question": "What does the veterinary hospital require for non-emergency examinations?",
      "options": [
        "A. Walk-ins only",
        "B. Appointments",
        "C. Emergency-only visits",
        "D. Pet insurance proof"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The coastal reserve monitors turtle nests each nesting season to protect hatchlings.",
      "conflict_prompt": "The coastal reserve does not monitor turtle nests each nesting season to protect hatchlings.",
      "question": "What does the coastal reserve monitor each nesting season?",
      "options": [
        "A. Bird migrations",
        "B. Turtle nests",
        "C. Coral bleaching",
        "D. Visitor parking"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The regional library network enables interlibrary loan requests for patrons.",
      "conflict_prompt": "The regional library network does not enable interlibrary loan requests for patrons.",
      "question": "What service does the regional library network enable for patrons?",
      "options": [
        "A. Online gaming",
        "B. Interlibrary loan requests",
        "C. Private office rentals",
        "D. House cleaning"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community garden assigns individual plots to members on a first-come, first-served basis.",
      "conflict_prompt": "The community garden does not assign individual plots to members on a first-come, first-served basis.",
      "question": "How are individual plots assigned at the community garden?",
      "options": [
        "A. Lottery only",
        "B. First-come, first-served",
        "C. By donation size",
        "D. Random daily drawing"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The mountain rescue team carries specialized cold-weather gear for high-altitude missions.",
      "conflict_prompt": "The mountain rescue team does not carry specialized cold-weather gear for high-altitude missions.",
      "question": "What type of gear does the mountain rescue team carry for high-altitude missions?",
      "options": [
        "A. Tropical clothing",
        "B. Specialized cold-weather gear",
        "C. Desert survival kits",
        "D. Scuba gear"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The pediatric dentist recommends fluoride varnish applications every six months for children.",
      "conflict_prompt": "The pediatric dentist does not recommend fluoride varnish applications every six months for children.",
      "question": "How often does the pediatric dentist recommend fluoride varnish applications?",
      "options": [
        "A. Every week",
        "B. Every six months",
        "C. Every five years",
        "D. Never"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The community college offers evening classes for adults pursuing continuing education.",
      "conflict_prompt": "The community college does not offer evening classes for adults pursuing continuing education.",
      "question": "What type of classes does the community college offer for adults pursuing continuing education?",
      "options": [
        "A. Daytime only",
        "B. Evening classes",
        "C. Weekend-only seminars",
        "D. None at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "The art supply store accepts returns within 30 days with a receipt.",
      "conflict_prompt": "The art supply store does not accept returns within 30 days with a receipt.",
      "question": "Within how many days does the art supply store accept returns with a receipt?",
      "options": [
        "A. 7 days",
        "B. 14 days",
        "C. 30 days",
        "D. 90 days"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "negation_contradictions",
      "conflict_type": "logical_negation_conflict"
    },
    {
      "clean_prompt": "Daniel, an architect, designed the new downtown library.",
      "conflict_prompt": "Daniel, an engineer, designed the new downtown library.",
      "question": "What is Daniel's profession?",
      "options": [
        "A. Engineer",
        "B. Architect",
        "C. Photographer",
        "D. Musician"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya, a cardiologist, performed the heart transplant.",
      "conflict_prompt": "Priya, an oncologist, performed the heart transplant.",
      "question": "What is Priya's medical specialty?",
      "options": [
        "A. Neurologist",
        "B. Oncologist",
        "C. Cardiologist",
        "D. Pediatrician"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The Falcon is an electric car that charges overnight in the garage.",
      "conflict_prompt": "The Falcon is a gasoline car that charges overnight in the garage.",
      "question": "What type of car is the Falcon described as?",
      "options": [
        "A. Diesel car",
        "B. Electric car",
        "C. Hybrid car",
        "D. Truck"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Liam, a left-handed pianist, practices scales with his left hand.",
      "conflict_prompt": "Liam, a right-handed pianist, practices scales with his left hand.",
      "question": "Which hand is Liam described as favoring?",
      "options": [
        "A. Right hand",
        "B. Both hands equally",
        "C. Left hand",
        "D. Ambidextrous"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia, a vegan, avoids all animal products in her meals.",
      "conflict_prompt": "Sofia, a vegetarian, avoids all animal products in her meals.",
      "question": "What dietary label is Sofia described with?",
      "options": [
        "A. Omnivore",
        "B. Vegetarian",
        "C. Pescatarian",
        "D. Vegan"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carlos, the mayor of Riverton, announced the new recycling program.",
      "conflict_prompt": "Carlos, the city councilor of Riverton, announced the new recycling program.",
      "question": "What official position does Carlos hold?",
      "options": [
        "A. City councilor",
        "B. Senator",
        "C. Mayor",
        "D. Governor"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora, a pediatric nurse, administered the vaccine to the child.",
      "conflict_prompt": "Nora, an emergency room nurse, administered the vaccine to the child.",
      "question": "Which nursing specialty is Nora identified with?",
      "options": [
        "A. Pediatric nurse",
        "B. Oncology nurse",
        "C. Geriatric nurse",
        "D. Psychiatric nurse"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The museum's exhibit features an original 19th-century oil painting.",
      "conflict_prompt": "The museum's exhibit features an original 19th-century watercolor painting.",
      "question": "What painting medium is in the museum's exhibit?",
      "options": [
        "A. Acrylic",
        "B. Watercolor",
        "C. Oil",
        "D. Digital print"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector, a native Spanish speaker, teaches Spanish literature.",
      "conflict_prompt": "Hector, a native English speaker, teaches Spanish literature.",
      "question": "What is Hector's native language according to the statement?",
      "options": [
        "A. German",
        "B. Italian",
        "C. English",
        "D. Spanish"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, a licensed pilot, flew the small private plane to the island.",
      "conflict_prompt": "Maya, an air traffic controller, flew the small private plane to the island.",
      "question": "What license or role is Maya described as having?",
      "options": [
        "A. Air traffic controller",
        "B. Flight attendant",
        "C. Licensed pilot",
        "D. Mechanic"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The university awarded Tom a PhD in physics after his dissertation defense.",
      "conflict_prompt": "The university awarded Tom a master's degree in physics after his dissertation defense.",
      "question": "What degree did Tom receive?",
      "options": [
        "A. Bachelor's degree",
        "B. Master's degree",
        "C. Associate degree",
        "D. PhD"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olga, a fluent Russian speaker, translated the legal contract into Russian.",
      "conflict_prompt": "Olga, a fluent Japanese speaker, translated the legal contract into Russian.",
      "question": "Which language is Olga fluent in according to the statement?",
      "options": [
        "A. Spanish",
        "B. Japanese",
        "C. Russian",
        "D. French"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan, the hockey team's goalkeeper, saved the final penalty.",
      "conflict_prompt": "Ethan, the hockey team's striker, saved the final penalty.",
      "question": "What position does Ethan play on his hockey team?",
      "options": [
        "A. Midfielder",
        "B. Striker",
        "C. Goalkeeper",
        "D. Defender"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ivy, a certified sommelier, recommended the 2015 Bordeaux.",
      "conflict_prompt": "Ivy, a certified brewer, recommended the 2015 Bordeaux.",
      "question": "What certification does Ivy have?",
      "options": [
        "A. Certified chef",
        "B. Certified brewer",
        "C. Certified sommelier",
        "D. Certified barista"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus, an experienced gardener, pruned the rose bushes this morning.",
      "conflict_prompt": "Marcus, a professional landscaper, pruned the rose bushes this morning.",
      "question": "How is Marcus described in relation to gardening?",
      "options": [
        "A. Novice gardener",
        "B. Experienced gardener",
        "C. Botanist",
        "D. Arborist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The concert program listed Julia as the violin soloist for the finale.",
      "conflict_prompt": "The concert program listed Julia as the cello soloist for the finale.",
      "question": "What instrument is Julia listed as soloing on?",
      "options": [
        "A. Flute",
        "B. Violin",
        "C. Cello",
        "D. Trumpet"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Kai, a volunteer firefighter, rushed into the burning building.",
      "conflict_prompt": "Kai, a paramedic, rushed into the burning building.",
      "question": "What volunteer role is Kai described as holding?",
      "options": [
        "A. Police officer",
        "B. Volunteer firefighter",
        "C. Paramedic",
        "D. Lifeguard"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ava, who holds a master's in environmental science, published a paper on wetlands.",
      "conflict_prompt": "Ava, who holds a bachelor's in environmental science, published a paper on wetlands.",
      "question": "What level of degree does Ava hold according to the statement?",
      "options": [
        "A. Associate degree",
        "B. Doctorate",
        "C. Master's degree",
        "D. High school diploma"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah, a certified scuba diver, explored the coral reef on the trip.",
      "conflict_prompt": "Noah, a certified snorkeler, explored the coral reef on the trip.",
      "question": "What certification is Noah described as having?",
      "options": [
        "A. Lifeguard",
        "B. Certified snorkeler",
        "C. Certified scuba diver",
        "D. Sailing license"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zara, the head chef at The Hearth, created the tasting menu.",
      "conflict_prompt": "Zara, the pastry chef at The Hearth, created the tasting menu.",
      "question": "What role does Zara hold at The Hearth?",
      "options": [
        "A. Hostess",
        "B. Pastry chef",
        "C. Head chef",
        "D. Dishwasher"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar, a British diplomat, attended the international summit in London.",
      "conflict_prompt": "Omar, a Canadian diplomat, attended the international summit in London.",
      "question": "What nationality is Omar described as having?",
      "options": [
        "A. American",
        "B. Canadian",
        "C. British",
        "D. Australian"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Greta, a climate scientist, published a study on polar ice melt.",
      "conflict_prompt": "Greta, an astrophysicist, published a study on polar ice melt.",
      "question": "Which field does Greta work in according to the statement?",
      "options": [
        "A. Anthropology",
        "B. Climate science",
        "C. Astrophysics",
        "D. Economics"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sam, a left back on the soccer team, intercepted the cross in the final minute.",
      "conflict_prompt": "Sam, a center forward on the soccer team, intercepted the cross in the final minute.",
      "question": "What position does Sam play on his soccer team?",
      "options": [
        "A. Center forward",
        "B. Goalkeeper",
        "C. Midfielder",
        "D. Left back"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah, a licensed therapist, counseled the couple for two months.",
      "conflict_prompt": "Hannah, a life coach, counseled the couple for two months.",
      "question": "What professional title is Hannah given in the statement?",
      "options": [
        "A. Licensed therapist",
        "B. Social worker",
        "C. Life coach",
        "D. Financial advisor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The library's rare books section includes a first-edition folio from 1623.",
      "conflict_prompt": "The library's rare books section includes a first-edition paperback from 1623.",
      "question": "What type of book from 1623 is in the rare books section?",
      "options": [
        "A. Paperback",
        "B. Hardcover modern reprint",
        "C. First-edition folio",
        "D. Audio CD"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Diego, a veteran marathon runner, completed the race in under three hours.",
      "conflict_prompt": "Diego, a novice runner, completed the race in under three hours.",
      "question": "How is Diego's running experience described?",
      "options": [
        "A. Novice runner",
        "B. Veteran marathon runner",
        "C. Sprinter",
        "D. Cyclist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, the orchestra conductor, raised her baton to begin the symphony.",
      "conflict_prompt": "Marta, the orchestra violinist, raised her baton to begin the symphony.",
      "question": "What role does Marta have in the orchestra?",
      "options": [
        "A. Conductor",
        "B. Violinist",
        "C. Pianist",
        "D. Horn player"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's signature loaf is sourdough made with wild yeast.",
      "conflict_prompt": "The bakery's signature loaf is a brioche made with wild yeast.",
      "question": "What type of bread is the bakery's signature loaf described as?",
      "options": [
        "A. Ciabatta",
        "B. Brioche",
        "C. Sourdough",
        "D. Pita"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ravi, the primary school principal, addressed the parents at the meeting.",
      "conflict_prompt": "Ravi, the secondary school principal, addressed the parents at the meeting.",
      "question": "What level of school is Ravi principal of according to the statement?",
      "options": [
        "A. Elementary school",
        "B. Primary school",
        "C. High school",
        "D. College"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Elena, a professional photographer, shot the wedding photos on film.",
      "conflict_prompt": "Elena, an amateur photographer, shot the wedding photos on film.",
      "question": "How is Elena's photography experience described?",
      "options": [
        "A. Professional photographer",
        "B. Amateur photographer",
        "C. Graphic designer",
        "D. Videographer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The cathedral's bell tower houses a 500-kilogram bronze bell installed in 1880.",
      "conflict_prompt": "The cathedral's bell tower houses a 50-kilogram bronze bell installed in 1880.",
      "question": "Approximately how heavy is the cathedral's bronze bell?",
      "options": [
        "A. 50 kilograms",
        "B. 500 kilograms",
        "C. 5 kilograms",
        "D. 5,000 kilograms"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leon, an experienced beekeeper, harvested honey from the hives last week.",
      "conflict_prompt": "Leon, a novice beekeeper, harvested honey from the hives last week.",
      "question": "What adjective describes Leon's beekeeping experience?",
      "options": [
        "A. Novice",
        "B. Experienced",
        "C. Inactive",
        "D. Retired"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Camila, an Italian chef, taught the class how to make fresh pasta.",
      "conflict_prompt": "Camila, a French chef, taught the class how to make fresh pasta.",
      "question": "What nationality is Camila described as?",
      "options": [
        "A. Spanish",
        "B. French",
        "C. Italian",
        "D. Mexican"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noelle, a licensed electrician, rewired the kitchen sockets.",
      "conflict_prompt": "Noelle, a licensed plumber, rewired the kitchen sockets.",
      "question": "What professional license does Noelle hold according to the statement?",
      "options": [
        "A. Plumber",
        "B. Licensed electrician",
        "C. Carpenter",
        "D. Roofer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Victor, a classical pianist, performed Chopin's nocturnes at the recital.",
      "conflict_prompt": "Victor, a jazz pianist, performed Chopin's nocturnes at the recital.",
      "question": "What style of pianist is Victor described as?",
      "options": [
        "A. Rock pianist",
        "B. Classical pianist",
        "C. Jazz pianist",
        "D. Pop keyboardist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rosa, an experienced mountaineer, led the ascent of Mount Elora.",
      "conflict_prompt": "Rosa, a recreational hiker, led the ascent of Mount Elora.",
      "question": "How is Rosa's climbing experience described?",
      "options": [
        "A. Professional skier",
        "B. Recreational hiker",
        "C. Experienced mountaineer",
        "D. Novice swimmer"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The startup hired Jonah as chief technology officer to oversee software development.",
      "conflict_prompt": "The startup hired Jonah as chief marketing officer to oversee software development.",
      "question": "What position was Jonah hired for according to the statement?",
      "options": [
        "A. Chief financial officer",
        "B. Chief marketing officer",
        "C. Chief technology officer",
        "D. Human resources manager"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Adele, a trained blacksmith, forged the ceremonial sword by hand.",
      "conflict_prompt": "Adele, a trained jeweler, forged the ceremonial sword by hand.",
      "question": "What craft is Adele described as being trained in?",
      "options": [
        "A. Woodworking",
        "B. Blacksmithing",
        "C. Glassblowing",
        "D. Pottery"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The research team identified the specimen as a juvenile loggerhead sea turtle.",
      "conflict_prompt": "The research team identified the specimen as an adult loggerhead sea turtle.",
      "question": "How did the team classify the turtle's age?",
      "options": [
        "A. Hatchling",
        "B. Juvenile",
        "C. Adult",
        "D. Elderly"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia, a violin teacher, gives private lessons twice a week.",
      "conflict_prompt": "Nadia, a piano teacher, gives private lessons twice a week.",
      "question": "What instrument does Nadia teach according to the statement?",
      "options": [
        "A. Guitar",
        "B. Violin",
        "C. Drums",
        "D. Saxophone"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The rescue center reported that Buddy is a two-year-old Labrador retriever.",
      "conflict_prompt": "The rescue center reported that Buddy is a two-year-old Siamese cat.",
      "question": "What species is Buddy described as?",
      "options": [
        "A. Labrador retriever",
        "B. Siamese cat",
        "C. Parrot",
        "D. Rabbit"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Iris, a licensed clinical social worker, provided therapy to the family.",
      "conflict_prompt": "Iris, a licensed clinical psychologist, provided therapy to the family.",
      "question": "What license is Iris said to hold in the statement?",
      "options": [
        "A. Licensed clinical psychologist",
        "B. Licensed clinical social worker",
        "C. Licensed marriage counselor",
        "D. Nurse practitioner"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The antique watch is a Swiss-made timepiece from the 1940s.",
      "conflict_prompt": "The antique watch is a Japanese-made timepiece from the 1940s.",
      "question": "Where was the antique watch manufactured according to the statement?",
      "options": [
        "A. Germany",
        "B. United States",
        "C. Switzerland",
        "D. Japan"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Asha, the choir soprano, hit the high C cleanly during the rehearsal.",
      "conflict_prompt": "Asha, the choir alto, hit the high C cleanly during the rehearsal.",
      "question": "What vocal part does Asha sing in the choir?",
      "options": [
        "A. Bass",
        "B. Tenor",
        "C. Alto",
        "D. Soprano"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felix, a master carpenter, built the custom oak dining table.",
      "conflict_prompt": "Felix, an apprentice carpenter, built the custom oak dining table.",
      "question": "What level of carpentry skill is Felix described as having?",
      "options": [
        "A. Apprentice carpenter",
        "B. Master carpenter",
        "C. Amateur woodworker",
        "D. Cabinet painter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The conference keynote was delivered by Dr. Patel, an expert in infectious diseases.",
      "conflict_prompt": "The conference keynote was delivered by Dr. Patel, an expert in nutritional science.",
      "question": "What field is Dr. Patel described as an expert in?",
      "options": [
        "A. Astronomy",
        "B. Nutrition",
        "C. Infectious diseases",
        "D. Art history"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hugo, the museum's curator, organized the Renaissance exhibit.",
      "conflict_prompt": "Hugo, the museum's docent, organized the Renaissance exhibit.",
      "question": "What role does Hugo have at the museum according to the statement?",
      "options": [
        "A. Security guard",
        "B. Curator",
        "C. Janitor",
        "D. Docent"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mina, a champion chess player, won the regional tournament.",
      "conflict_prompt": "Mina, a recreational chess player, won the regional tournament.",
      "question": "How is Mina's chess ability described?",
      "options": [
        "A. Beginner",
        "B. Recreational",
        "C. Champion",
        "D. Coach"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The cottage's roof is made of thatch, a traditional material in the area.",
      "conflict_prompt": "The cottage's roof is made of slate, a traditional material in the area.",
      "question": "What material is the cottage's roof described as using?",
      "options": [
        "A. Slate",
        "B. Asphalt shingles",
        "C. Thatch",
        "D. Metal"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Avery, a fluent Mandarin speaker, led the bilingual tour group.",
      "conflict_prompt": "Avery, a fluent Cantonese speaker, led the bilingual tour group.",
      "question": "Which Chinese language is Avery said to be fluent in?",
      "options": [
        "A. Shanghainese",
        "B. Cantonese",
        "C. Mandarin",
        "D. Hokkien"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jonas, a marine biologist, studied the kelp forest ecosystem off the coast.",
      "conflict_prompt": "Jonas, a geologist, studied the kelp forest ecosystem off the coast.",
      "question": "What is Jonas's scientific specialty according to the statement?",
      "options": [
        "A. Marine biology",
        "B. Geology",
        "C. Meteorology",
        "D. Sociology"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Keisha, the bakery owner, bakes sourdough loaves every morning before dawn.",
      "conflict_prompt": "Keisha, the café owner, bakes sourdough loaves every morning before dawn.",
      "question": "What type of business does Keisha own according to the statement?",
      "options": [
        "A. Restaurant",
        "B. Bakery",
        "C. Grocery store",
        "D. Florist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael, a certified yoga instructor, teaches Vinyasa classes at the studio.",
      "conflict_prompt": "Rafael, a certified Pilates instructor, teaches Vinyasa classes at the studio.",
      "question": "What certification is Rafael described as holding?",
      "options": [
        "A. Certified Pilates instructor",
        "B. Certified yoga instructor",
        "C. CrossFit trainer",
        "D. Personal trainer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The ferry's captain, Lucas, navigated through the fog using radar.",
      "conflict_prompt": "The ferry's captain, Lucas, navigated through the fog using only visual landmarks.",
      "question": "What navigation aid did Lucas use according to the statement?",
      "options": [
        "A. GPS only",
        "B. Visual landmarks only",
        "C. Radar",
        "D. Celestial navigation"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ida, a professional beekeeper, treats hives for mites each spring.",
      "conflict_prompt": "Ida, a hobby apiarist, treats hives for mites each spring.",
      "question": "How is Ida's beekeeping described?",
      "options": [
        "A. Professional beekeeper",
        "B. Hobby apiarist",
        "C. Retired farmer",
        "D. Urban gardener"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The research paper lists Dr. Nguyen as a neuroscientist specializing in memory.",
      "conflict_prompt": "The research paper lists Dr. Nguyen as a sociologist specializing in memory.",
      "question": "What is Dr. Nguyen's academic specialty according to the statement?",
      "options": [
        "A. Economics",
        "B. Neuroscience",
        "C. Sociology",
        "D. Linguistics"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tomas, an experienced sommelier, paired the wine with the mushroom risotto.",
      "conflict_prompt": "Tomas, a line cook, paired the wine with the mushroom risotto.",
      "question": "What role does Tomas have with food and wine according to the statement?",
      "options": [
        "A. Line cook",
        "B. Dishwasher",
        "C. Experienced sommelier",
        "D. Busser"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ayla, a licensed lifeguard, completed the water rescue successfully.",
      "conflict_prompt": "Ayla, a certified swim instructor, completed the water rescue successfully.",
      "question": "What certification is Ayla described as holding?",
      "options": [
        "A. Certified swim instructor",
        "B. Licensed lifeguard",
        "C. Sailing instructor",
        "D. Ski patrol"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The cottage's interior was painted in warm earth tones last summer.",
      "conflict_prompt": "The cottage's interior was painted in cool pastel tones last summer.",
      "question": "What color palette was used for the cottage's interior according to the statement?",
      "options": [
        "A. Neon colors",
        "B. Warm earth tones",
        "C. Metallic tones",
        "D. Black and white only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mikhail, a veteran sea captain, commanded the freighter across the Atlantic.",
      "conflict_prompt": "Mikhail, a merchant sailor, commanded the freighter across the Atlantic.",
      "question": "What descriptor is used for Mikhail's maritime experience?",
      "options": [
        "A. Cabin boy",
        "B. Merchant sailor",
        "C. Veteran sea captain",
        "D. Harbor pilot"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sara, who is legally blind in one eye, reads large-print books for leisure.",
      "conflict_prompt": "Sara, who has perfect vision, reads large-print books for leisure.",
      "question": "What visual condition is Sara described as having?",
      "options": [
        "A. Legally blind in one eye",
        "B. Color blind",
        "C. Night blindness",
        "D. Perfect vision"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ibrahim, a certified translator, translated the treaty from Arabic to English.",
      "conflict_prompt": "Ibrahim, a certified interpreter, translated the treaty from Arabic to English.",
      "question": "What professional title is Ibrahim given in the statement?",
      "options": [
        "A. Certified translator",
        "B. Notary public",
        "C. Immigration officer",
        "D. Court reporter"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The antique shop advertises the cabinet as mahogany with inlaid mother-of-pearl.",
      "conflict_prompt": "The antique shop advertises the cabinet as oak with inlaid mother-of-pearl.",
      "question": "What wood is the cabinet described as being made from?",
      "options": [
        "A. Pine",
        "B. Mahogany",
        "C. Oak",
        "D. Cedar"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gavin, a first responder paramedic, stabilized the patient at the scene.",
      "conflict_prompt": "Gavin, a volunteer crossing guard, stabilized the patient at the scene.",
      "question": "What role does Gavin hold according to the statement?",
      "options": [
        "A. Funeral director",
        "B. Volunteer crossing guard",
        "C. First responder paramedic",
        "D. School principal"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The coastal town declared an annual blue crab festival celebrating local seafood.",
      "conflict_prompt": "The coastal town declared an annual oyster festival celebrating local seafood.",
      "question": "Which seafood does the town's festival celebrate according to the statement?",
      "options": [
        "A. Lobster",
        "B. Blue crab",
        "C. Tuna",
        "D. Salmon"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priestess Elora, a respected astrologer, charted the newborn's horoscope.",
      "conflict_prompt": "Priestess Elora, a respected astronomer, charted the newborn's horoscope.",
      "question": "What practice is Elora described as being respected for?",
      "options": [
        "A. Botany",
        "B. Astronomy",
        "C. Astrology",
        "D. Chemistry"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ben, a licensed realtor, negotiated the sale of the brownstone.",
      "conflict_prompt": "Ben, a mortgage broker, negotiated the sale of the brownstone.",
      "question": "What professional license does Ben hold according to the statement?",
      "options": [
        "A. Licensed realtor",
        "B. Insurance agent",
        "C. Mortgage broker",
        "D. Real estate attorney"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The university choir's soloist, Claire, is a soprano with a crystal-clear tone.",
      "conflict_prompt": "The university choir's soloist, Claire, is an alto with a crystal-clear tone.",
      "question": "Which voice part is Claire described as singing?",
      "options": [
        "A. Bass",
        "B. Tenor",
        "C. Alto",
        "D. Soprano"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Owen, the jewelry designer, uses recycled silver for his rings.",
      "conflict_prompt": "Owen, the furniture designer, uses recycled silver for his rings.",
      "question": "What type of designer is Owen described as being?",
      "options": [
        "A. Fashion designer",
        "B. Jewelry designer",
        "C. Graphic designer",
        "D. Furniture designer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Fatima, a bilingual school counselor, speaks Arabic and English with students.",
      "conflict_prompt": "Fatima, a bilingual school counselor, speaks Mandarin and English with students.",
      "question": "Which pair of languages does Fatima speak according to the statement?",
      "options": [
        "A. Arabic and English",
        "B. Spanish and English",
        "C. Mandarin and English",
        "D. French and English"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The lighthouse keeper, Mr. O'Neil, trimmed the lamp's wick daily as part of his duties.",
      "conflict_prompt": "The lighthouse keeper, Mr. O'Neil, replaced the lamp with a solar array as part of his duties.",
      "question": "What maintenance task does Mr. O'Neil perform according to the statement?",
      "options": [
        "A. Replace solar panels",
        "B. Trim the lamp's wick daily",
        "C. Paint the hulls of ships",
        "D. Harvest seaweed"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Harper, the high school debate captain, coached the junior team this season.",
      "conflict_prompt": "Harper, the high school drama captain, coached the junior team this season.",
      "question": "Which extracurricular team is Harper captain of according to the statement?",
      "options": [
        "A. Basketball team",
        "B. Drama club",
        "C. Debate team",
        "D. Robotics club"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The orchard manager, Pilar, grows organic apples without synthetic pesticides.",
      "conflict_prompt": "The orchard manager, Pilar, grows conventional apples with synthetic pesticides.",
      "question": "What farming practice does Pilar follow according to the statement?",
      "options": [
        "A. Hydroponic farming",
        "B. Organic farming without synthetic pesticides",
        "C. Conventional farming with synthetic pesticides",
        "D. Vertical farming"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Giselle, an expert glassblower, created the series of blown glass vases for the gallery.",
      "conflict_prompt": "Giselle, a ceramicist, created the series of blown glass vases for the gallery.",
      "question": "Which craft is Giselle described as specializing in?",
      "options": [
        "A. Ceramics",
        "B. Metalwork",
        "C. Glassblowing",
        "D. Weaving"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school's mascot is a red-tailed hawk that appears on all official merchandise.",
      "conflict_prompt": "The school's mascot is a Bengal tiger that appears on all official merchandise.",
      "question": "What animal is the school's mascot according to the statement?",
      "options": [
        "A. Bear",
        "B. Bengal tiger",
        "C. Red-tailed hawk",
        "D. Dolphin"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pedro, a licensed auto mechanic, replaced the car's alternator yesterday.",
      "conflict_prompt": "Pedro, a licensed electrician, replaced the car's alternator yesterday.",
      "question": "What professional license does Pedro have according to the statement?",
      "options": [
        "A. Licensed electrician",
        "B. Certified painter",
        "C. Licensed auto mechanic",
        "D. Certified public accountant"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nell, a prize-winning poet, read her latest collection at the festival.",
      "conflict_prompt": "Nell, a best-selling novelist, read her latest collection at the festival.",
      "question": "What literary form is Nell identified with in the statement?",
      "options": [
        "A. Playwright",
        "B. Best-selling novelist",
        "C. Prize-winning poet",
        "D. Essayist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The train's dining car serves a signature beef stew made from local beef.",
      "conflict_prompt": "The train's dining car serves a signature beef stew made from imported beef.",
      "question": "What source of beef is used in the dining car's signature stew according to the statement?",
      "options": [
        "A. Imported beef",
        "B. Local beef",
        "C. Plant-based substitute",
        "D. Poultry"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aiden, a volunteer at the animal shelter, walks dogs three afternoons a week.",
      "conflict_prompt": "Aiden, a paid staff member at the animal shelter, walks dogs three afternoons a week.",
      "question": "What is Aiden's relationship to the animal shelter according to the statement?",
      "options": [
        "A. Donor",
        "B. Paid staff member",
        "C. Volunteer",
        "D. Board member"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The hotel's penthouse is a two-bedroom suite with panoramic city views.",
      "conflict_prompt": "The hotel's penthouse is a one-bedroom suite with panoramic city views.",
      "question": "How many bedrooms does the hotel's penthouse have according to the statement?",
      "options": [
        "A. One bedroom",
        "B. Two bedrooms",
        "C. Three bedrooms",
        "D. Four bedrooms"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zane, a certified arborist, inspected the old oak for disease and pests.",
      "conflict_prompt": "Zane, a certified landscaper, inspected the old oak for disease and pests.",
      "question": "What certification is Zane described as holding in the statement?",
      "options": [
        "A. Certified landscaper",
        "B. Certified arborist",
        "C. Pest control technician",
        "D. Horticulturalist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, a veteran journalist, covered the summit for the national newspaper.",
      "conflict_prompt": "Maya, a freelance blogger, covered the summit for the national newspaper.",
      "question": "What is Maya's professional description according to the statement?",
      "options": [
        "A. Photojournalist",
        "B. Freelance blogger",
        "C. Veteran journalist",
        "D. Social media manager"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The orchestra's principal cellist, Henri, uses a 1700s Stradivarius-style instrument.",
      "conflict_prompt": "The orchestra's principal cellist, Henri, uses a modern carbon-fiber instrument.",
      "question": "What type of instrument does Henri play according to the statement?",
      "options": [
        "A. Electronic keyboard",
        "B. 1700s Stradivarius-style cello",
        "C. Modern carbon-fiber cello",
        "D. Electric guitar"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lina, a veteran firefighter, tested the new breathing apparatus during training.",
      "conflict_prompt": "Lina, a community safety volunteer, tested the new breathing apparatus during training.",
      "question": "What role is Lina described as having?",
      "options": [
        "A. Paramedic",
        "B. Community safety volunteer",
        "C. Veteran firefighter",
        "D. Police cadet"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's head baker, Oscar, specializes in laminated doughs like croissants.",
      "conflict_prompt": "The bakery's head baker, Oscar, specializes in gluten-free breads like croissants.",
      "question": "What baking specialty is Oscar described as having?",
      "options": [
        "A. Gluten-free baking",
        "B. Laminated doughs like croissants",
        "C. Candy making",
        "D. Ice sculpting"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ari, a certified dog trainer, taught the puppy basic obedience commands.",
      "conflict_prompt": "Ari, a certified cat trainer, taught the puppy basic obedience commands.",
      "question": "What animal training certification does Ari have according to the statement?",
      "options": [
        "A. Certified dog trainer",
        "B. Certified equine trainer",
        "C. Certified cat trainer",
        "D. Certified bird trainer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The farmhouse's heating system runs on natural gas and heats all four floors.",
      "conflict_prompt": "The farmhouse's heating system runs on electric heat pumps and heats all four floors.",
      "question": "What fuel or energy source powers the farmhouse's heating system according to the statement?",
      "options": [
        "A. Wood burning",
        "B. Solar thermal",
        "C. Natural gas",
        "D. Geothermal"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Talia, an Olympic-level swimmer, set a new national record in the 200m freestyle.",
      "conflict_prompt": "Talia, a collegiate swimmer, set a new national record in the 200m freestyle.",
      "question": "What competitive level is Talia described as belonging to?",
      "options": [
        "A. Recreational swimmer",
        "B. High school swimmer",
        "C. Collegiate swimmer",
        "D. Olympic-level swimmer"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The island's reserve protects a colony of endangered green sea turtles.",
      "conflict_prompt": "The island's reserve protects a colony of endangered leatherback sea turtles.",
      "question": "Which species of sea turtle does the reserve protect according to the statement?",
      "options": [
        "A. Loggerhead sea turtles",
        "B. Green sea turtles",
        "C. Hawksbill turtles",
        "D. Leatherback sea turtles"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Renee, a licensed pharmacist, dispensed the prescription at the pharmacy counter.",
      "conflict_prompt": "Renee, a nurse practitioner, dispensed the prescription at the pharmacy counter.",
      "question": "What professional role does Renee have according to the statement?",
      "options": [
        "A. Registered dietitian",
        "B. Licensed pharmacist",
        "C. Nurse practitioner",
        "D. Laboratory technician"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The art gallery's director, Simone, curated the contemporary sculpture exhibit.",
      "conflict_prompt": "The art gallery's director, Simone, curated the classical painting exhibit.",
      "question": "What type of exhibit did Simone curate according to the statement?",
      "options": [
        "A. Historical artifacts",
        "B. Classical paintings",
        "C. Contemporary sculptures",
        "D. Photographic retrospectives"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar, a senior software engineer, led development of the mobile app's backend.",
      "conflict_prompt": "Omar, a junior QA engineer, led development of the mobile app's backend.",
      "question": "What is Omar's seniority and role in software according to the statement?",
      "options": [
        "A. Junior QA engineer",
        "B. Senior software engineer",
        "C. UX designer",
        "D. Product manager"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hana, a licensed audiologist, fitted the elderly patient with hearing aids.",
      "conflict_prompt": "Hana, a licensed speech therapist, fitted the elderly patient with hearing aids.",
      "question": "What professional title does Hana hold according to the statement?",
      "options": [
        "A. Licensed speech therapist",
        "B. Licensed audiologist",
        "C. Occupational therapist",
        "D. Physical therapist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The film festival awarded the documentary Best Short Film in the local category.",
      "conflict_prompt": "The film festival awarded the documentary Best Feature Film in the local category.",
      "question": "Which award did the documentary receive according to the statement?",
      "options": [
        "A. Best Animated Film",
        "B. Best Feature Film",
        "C. Best Short Film",
        "D. Audience Choice"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, the community's volunteer coordinator, organized the neighborhood cleanup day.",
      "conflict_prompt": "Maya, the community's events manager, organized the neighborhood cleanup day.",
      "question": "What volunteer position does Maya hold according to the statement?",
      "options": [
        "A. Events manager",
        "B. Volunteer coordinator",
        "C. Treasurer",
        "D. Secretary"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pablo, a classical guitarist, taught the masterclass on flamenco technique.",
      "conflict_prompt": "Pablo, a flamenco guitarist, taught the masterclass on flamenco technique.",
      "question": "What musical style is Pablo associated with in the statement?",
      "options": [
        "A. Jazz",
        "B. Rock",
        "C. Classical guitar",
        "D. Electronic"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The observatory's telescope is a 2.4-meter reflector used for deep-sky imaging.",
      "conflict_prompt": "The observatory's telescope is a 0.5-meter refractor used for deep-sky imaging.",
      "question": "What type and size is the observatory's telescope according to the statement?",
      "options": [
        "A. 0.5-meter refractor",
        "B. 2.4-meter reflector",
        "C. 10-meter radio dish",
        "D. 1-meter Schmidt-Cassegrain"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Luz, a certified midwife, assisted with the home birth last month.",
      "conflict_prompt": "Luz, a certified obstetrician, assisted with the home birth last month.",
      "question": "What role is Luz described as having in the birth according to the statement?",
      "options": [
        "A. Certified midwife",
        "B. Pediatrician",
        "C. Certified obstetrician",
        "D. Neonatal nurse"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's seasonal tart is made with locally sourced apples and cinnamon.",
      "conflict_prompt": "The bakery's seasonal tart is made with imported pears and cinnamon.",
      "question": "What fruit is used in the bakery's seasonal tart according to the statement?",
      "options": [
        "A. Pears",
        "B. Locally sourced apples",
        "C. Bananas",
        "D. Mangoes"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Diana, a sculptor, carved the bronze statue that now stands in the plaza.",
      "conflict_prompt": "Diana, a painter, carved the bronze statue that now stands in the plaza.",
      "question": "What artistic medium is Diana described as working in according to the statement?",
      "options": [
        "A. Painter",
        "B. Photographer",
        "C. Sculptor",
        "D. Glass artist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The school's basketball coach, Mr. Kim, emphasized defense during practice.",
      "conflict_prompt": "The school's baseball coach, Mr. Kim, emphasized defense during practice.",
      "question": "Which sport does Mr. Kim coach according to the statement?",
      "options": [
        "A. Soccer",
        "B. Basketball",
        "C. Baseball",
        "D. Tennis"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aria, a licensed massage therapist, specializes in sports injury rehabilitation.",
      "conflict_prompt": "Aria, a licensed cosmetologist, specializes in sports injury rehabilitation.",
      "question": "What professional license does Aria hold according to the statement?",
      "options": [
        "A. Licensed cosmetologist",
        "B. Licensed massage therapist",
        "C. Registered dietitian",
        "D. Chiropractor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The train conductor announced that this express service does not stop at Maplewood.",
      "conflict_prompt": "The train conductor announced that this local service does stop at Maplewood.",
      "question": "Does the train described stop at Maplewood according to the statement?",
      "options": [
        "A. Yes, because it's an express service",
        "B. Yes, because it's a local service",
        "C. No, the express service does not stop at Maplewood",
        "D. No, the line is closed"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nolan, a professional beekeeper, harvests honey only in late summer.",
      "conflict_prompt": "Nolan, an amateur beekeeper, harvests honey only in late summer.",
      "question": "What level of beekeeping expertise does Nolan have according to the statement?",
      "options": [
        "A. Professional beekeeper",
        "B. Amateur beekeeper",
        "C. Entomologist",
        "D. Farmer"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bianca, a geology professor, led the field trip to study sedimentary rock formations.",
      "conflict_prompt": "Bianca, a biology professor, led the field trip to study sedimentary rock formations.",
      "question": "What academic field does Bianca teach according to the statement?",
      "options": [
        "A. Biology",
        "B. Chemistry",
        "C. Geology",
        "D. Physics"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The championship trophy was engraved with the winners' names and the year 2024.",
      "conflict_prompt": "The championship trophy was engraved with the winners' names and the year 2020.",
      "question": "What year appears on the engraved championship trophy according to the statement?",
      "options": [
        "A. 2020",
        "B. 2022",
        "C. 2024",
        "D. 2018"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Alma, a certified dietitian, created a meal plan focused on low sodium intake.",
      "conflict_prompt": "Alma, a certified personal trainer, created a meal plan focused on low sodium intake.",
      "question": "What professional certification does Alma have according to the statement?",
      "options": [
        "A. Certified personal trainer",
        "B. Certified dietitian",
        "C. Yoga instructor",
        "D. Podiatrist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rory, the town's postmaster, manages mail distribution from the main office.",
      "conflict_prompt": "Rory, the town's mail carrier, manages mail distribution from the main office.",
      "question": "What is Rory's role in the postal service according to the statement?",
      "options": [
        "A. Mail carrier",
        "B. Postmaster",
        "C. Sorting clerk",
        "D. Postal inspector"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The conservatory's greenhouse houses rare orchids that bloom in winter.",
      "conflict_prompt": "The conservatory's greenhouse houses tropical palms that bloom in winter.",
      "question": "What type of plants are specifically mentioned as housed in the greenhouse according to the statement?",
      "options": [
        "A. Cacti",
        "B. Rare orchids",
        "C. Tropical palms",
        "D. Carnivorous plants"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Eleanor, a cardiothoracic surgeon, performed the complex valve repair successfully.",
      "conflict_prompt": "Eleanor, a general practitioner, performed the complex valve repair successfully.",
      "question": "What medical specialty is Eleanor described as having according to the statement?",
      "options": [
        "A. Pediatrician",
        "B. General practitioner",
        "C. Cardiothoracic surgeon",
        "D. Dermatologist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The vineyard produces a reserve Pinot Noir aged in French oak barrels.",
      "conflict_prompt": "The vineyard produces a reserve Pinot Noir aged in stainless steel tanks.",
      "question": "What aging method is used for the vineyard's reserve Pinot Noir according to the statement?",
      "options": [
        "A. Stainless steel tanks",
        "B. Amphorae",
        "C. French oak barrels",
        "D. Plastic vats"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Yusuf, a licensed public defender, represented the defendant in court.",
      "conflict_prompt": "Yusuf, a private defense attorney, represented the defendant in court.",
      "question": "What type of attorney is Yusuf described as in the statement?",
      "options": [
        "A. Prosecutor",
        "B. Private defense attorney",
        "C. Licensed public defender",
        "D. Judge"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia, the head librarian, cataloged the donated rare manuscripts yesterday.",
      "conflict_prompt": "Nadia, the circulation manager, cataloged the donated rare manuscripts yesterday.",
      "question": "What position does Nadia hold at the library according to the statement?",
      "options": [
        "A. Head librarian",
        "B. Security officer",
        "C. Circulation assistant",
        "D. Janitor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The team's captain, Sofia, accepted the championship trophy on behalf of the players.",
      "conflict_prompt": "The team's manager, Sofia, accepted the championship trophy on behalf of the players.",
      "question": "Who accepted the championship trophy according to the statement?",
      "options": [
        "A. Team physician",
        "B. Team owner",
        "C. Team captain",
        "D. Team sponsor"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Eli, an emergency room physician, worked the night shift during the storm.",
      "conflict_prompt": "Eli, a hospice nurse, worked the night shift during the storm.",
      "question": "What is Eli's profession according to the statement?",
      "options": [
        "A. Emergency room physician",
        "B. Hospice nurse",
        "C. Pharmacist",
        "D. Veterinarian"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The observatory's director, Dr. Kim, specializes in exoplanet detection.",
      "conflict_prompt": "The observatory's director, Dr. Kim, specializes in stellar archaeology.",
      "question": "What research specialty is Dr. Kim described as having according to the statement?",
      "options": [
        "A. Exoplanet detection",
        "B. Oceanography",
        "C. Stellar archaeology",
        "D. Quantum optics"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Fiona, a licensed midwife, taught the prenatal workshop last Saturday.",
      "conflict_prompt": "Fiona, a certified doula, taught the prenatal workshop last Saturday.",
      "question": "What professional role is Fiona described as holding according to the statement?",
      "options": [
        "A. Certified doula",
        "B. Licensed midwife",
        "C. Neonatal nurse",
        "D. Lactation consultant"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The restoration team identified the fresco as painted in the early Renaissance style.",
      "conflict_prompt": "The restoration team identified the fresco as painted in the Baroque style.",
      "question": "What artistic period is the fresco attributed to according to the statement?",
      "options": [
        "A. Baroque",
        "B. Romantic",
        "C. Early Renaissance",
        "D. Modernist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jamal, a volunteer EMT, provided first aid at the marathon event.",
      "conflict_prompt": "Jamal, a paid paramedic, provided first aid at the marathon event.",
      "question": "How is Jamal's service status described according to the statement?",
      "options": [
        "A. Paid paramedic",
        "B. Volunteer EMT",
        "C. Traffic warden",
        "D. Security guard"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The sculpture is carved from Carrara marble and dates from the 18th century.",
      "conflict_prompt": "The sculpture is carved from local limestone and dates from the 18th century.",
      "question": "What material is the sculpture said to be carved from according to the statement?",
      "options": [
        "A. Granite",
        "B. Carrara marble",
        "C. Local limestone",
        "D. Bronze"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rita, a certified pastry chef, prepared the wedding cake's sugar flowers.",
      "conflict_prompt": "Rita, a chocolatier, prepared the wedding cake's sugar flowers.",
      "question": "What culinary certification does Rita hold according to the statement?",
      "options": [
        "A. Chocolatier",
        "B. Certified pastry chef",
        "C. Butcher",
        "D. Brewer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The expedition's botanist, Leah, discovered a new species of alpine fern.",
      "conflict_prompt": "The expedition's geologist, Leah, discovered a new species of alpine fern.",
      "question": "What field is Leah described as specializing in according to the statement?",
      "options": [
        "A. Ornithology",
        "B. Geology",
        "C. Botany",
        "D. Herpetology"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Victor, the film's cinematographer, used natural light for the outdoor scenes.",
      "conflict_prompt": "Victor, the film's lighting designer, used natural light for the outdoor scenes.",
      "question": "What role did Victor have on the film production according to the statement?",
      "options": [
        "A. Director",
        "B. Cinematographer",
        "C. Sound engineer",
        "D. Costume designer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carmen, a licensed counselor, offers grief support groups at the center weekly.",
      "conflict_prompt": "Carmen, a licensed nutritionist, offers grief support groups at the center weekly.",
      "question": "What professional license does Carmen hold according to the statement?",
      "options": [
        "A. Licensed nutritionist",
        "B. Licensed counselor",
        "C. Personal trainer",
        "D. Speech therapist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's sourdough starter has been maintained by the head baker for over ten years.",
      "conflict_prompt": "The bakery's sourdough starter is renewed monthly from a commercial yeast source.",
      "question": "How long has the bakery's sourdough starter been maintained according to the statement?",
      "options": [
        "A. Over ten years",
        "B. Renewed monthly",
        "C. Introduced last week",
        "D. Not used at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector, an award-winning documentary filmmaker, premiered his new film at the festival.",
      "conflict_prompt": "Hector, an award-winning fiction novelist, premiered his new film at the festival.",
      "question": "What is Hector professionally recognized for according to the statement?",
      "options": [
        "A. Fiction novels",
        "B. Documentary filmmaking",
        "C. Painting",
        "D. Architecture"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, a certified public accountant, prepared the company's audited financial statements.",
      "conflict_prompt": "Marta, a tax advisor, prepared the company's audited financial statements.",
      "question": "What credential does Marta hold according to the statement?",
      "options": [
        "A. Tax advisor",
        "B. Certified public accountant",
        "C. Bank teller",
        "D. Investment banker"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The botanical guide describes the flower as a perennial that blooms every spring.",
      "conflict_prompt": "The botanical guide describes the flower as an annual that blooms every spring.",
      "question": "According to the statement, how often does the flower bloom?",
      "options": [
        "A. Every autumn",
        "B. Every spring",
        "C. Once every five years",
        "D. Only in summer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Anika, a paleontologist, identified the fossil as belonging to a triassic reptile.",
      "conflict_prompt": "Anika, an archaeologist, identified the fossil as belonging to a triassic reptile.",
      "question": "What profession is Anika described as having in the statement?",
      "options": [
        "A. Paleontologist",
        "B. Archaeologist",
        "C. Astronomer",
        "D. Paleobotanist"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The city's art commission appointed Mateo as the public mural program director.",
      "conflict_prompt": "The city's parks commission appointed Mateo as the public mural program director.",
      "question": "Which commission appointed Mateo according to the statement?",
      "options": [
        "A. Parks commission",
        "B. Transportation commission",
        "C. Art commission",
        "D. Housing commission"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jade, a licensed electrician, installed the new outdoor lighting fixtures.",
      "conflict_prompt": "Jade, a licensed plumber, installed the new outdoor lighting fixtures.",
      "question": "What trade license does Jade hold according to the statement?",
      "options": [
        "A. Licensed plumber",
        "B. Licensed electrician",
        "C. Licensed roofer",
        "D. Licensed welder"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The heritage center's docent, Mr. Alvarez, conducts daily tours of the historical district.",
      "conflict_prompt": "The heritage center's curator, Mr. Alvarez, conducts daily tours of the historical district.",
      "question": "What role does Mr. Alvarez fill at the heritage center according to the statement?",
      "options": [
        "A. Security guard",
        "B. Curator",
        "C. Docent",
        "D. Archivist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leah, the junior high science teacher, assigned a lab experiment on chemical reactions.",
      "conflict_prompt": "Leah, the high school history teacher, assigned a lab experiment on chemical reactions.",
      "question": "What subject does Leah teach according to the statement?",
      "options": [
        "A. History",
        "B. Physical education",
        "C. Science",
        "D. Art"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus, a professional barista, recommended the single-origin Ethiopian coffee.",
      "conflict_prompt": "Marcus, a café cashier, recommended the single-origin Ethiopian coffee.",
      "question": "What is Marcus's role at the café according to the statement?",
      "options": [
        "A. Dishwasher",
        "B. Café cashier",
        "C. Professional barista",
        "D. Owner"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The bakery's gluten-free loaf uses almond flour to accommodate dietary restrictions.",
      "conflict_prompt": "The bakery's gluten-free loaf uses wheat flour to accommodate dietary restrictions.",
      "question": "Which flour is used in the bakery's gluten-free loaf according to the statement?",
      "options": [
        "A. Wheat flour",
        "B. All-purpose flour",
        "C. Almond flour",
        "D. Rye flour"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nikolai, a conservation officer, monitored the migratory bird population this season.",
      "conflict_prompt": "Nikolai, a park ranger, monitored the migratory bird population this season.",
      "question": "What official title is Nikolai given in the statement?",
      "options": [
        "A. Park ranger",
        "B. Conservation officer",
        "C. Groundskeeper",
        "D. Zoo curator"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ariel, a forensic accountant, traced the fraudulent transactions in the audit.",
      "conflict_prompt": "Ariel, a forensic psychologist, traced the fraudulent transactions in the audit.",
      "question": "What field does Ariel specialize in according to the statement?",
      "options": [
        "A. Forensic psychology",
        "B. Forensic accounting",
        "C. Forensic anthropology",
        "D. Digital marketing"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The hospital's trauma surgeon, Dr. Shah, led the emergency response team during the incident.",
      "conflict_prompt": "The hospital's chief administrator, Dr. Shah, led the emergency response team during the incident.",
      "question": "What leadership role did Dr. Shah take during the incident according to the statement?",
      "options": [
        "A. Chief administrator",
        "B. Trauma surgeon",
        "C. Head nurse",
        "D. Janitorial supervisor"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mei, a certified pastry chef, decorated the showpiece sugar sculptures for the competition.",
      "conflict_prompt": "Mei, a certified chocolatier, decorated the showpiece sugar sculptures for the competition.",
      "question": "What certification does Mei hold according to the statement?",
      "options": [
        "A. Certified chocolatier",
        "B. Certified pastry chef",
        "C. Licensed florist",
        "D. Certified sommelier"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "The new public transit line is powered entirely by electric trains.",
      "conflict_prompt": "The new public transit line is powered entirely by diesel trains.",
      "question": "What powers the new public transit line according to the statement?",
      "options": [
        "A. Diesel trains",
        "B. Steam engines",
        "C. Electric trains",
        "D. Horse-drawn carriages"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ariana, a certified lifeguard, enforced pool safety rules during the heatwave.",
      "conflict_prompt": "Ariana, a swim team coach, enforced pool safety rules during the heatwave.",
      "question": "What role is Ariana described as having during the pool event according to the statement?",
      "options": [
        "A. Swim team coach",
        "B. Certified lifeguard",
        "C. Concession stand worker",
        "D. Lifeguard trainee"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Alan, a vegetarian, ordered a garden salad for lunch.",
      "conflict_prompt": "Alan, a vegetarian, ordered a grilled chicken sandwich for lunch.",
      "question": "What did Alan order for lunch?",
      "options": [
        "A. A grilled chicken sandwich",
        "B. A garden salad",
        "C. A pepperoni pizza",
        "D. A tuna melt"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya, who is left-handed, signed the contract using her left hand.",
      "conflict_prompt": "Priya, who is left-handed, signed the contract using her right hand.",
      "question": "Which hand did Priya use to sign the contract?",
      "options": [
        "A. Right hand",
        "B. Both hands",
        "C. Left hand",
        "D. She did not sign"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dr. Morales, a pediatrician, examined the child's ears during the appointment.",
      "conflict_prompt": "Dr. Morales, a pediatrician, examined the child's ears as an orthopedic surgeon.",
      "question": "What is Dr. Morales's medical specialty?",
      "options": [
        "A. Orthopedic surgeon",
        "B. Cardiologist",
        "C. Pediatrician",
        "D. Neurologist"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Evelyn, who is an only child, grew up without siblings.",
      "conflict_prompt": "Evelyn, who is an only child, often talks about playing with her younger sister.",
      "question": "Did Evelyn grow up with siblings?",
      "options": [
        "A. Yes, she had multiple siblings",
        "B. She had one sibling",
        "C. No, she was an only child",
        "D. The statement gives no information"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus, a non-smoker, has never smoked cigarettes.",
      "conflict_prompt": "Marcus, a non-smoker, smokes a pack of cigarettes every day.",
      "question": "Which best describes Marcus's smoking habits?",
      "options": [
        "A. He smokes daily",
        "B. He smokes occasionally",
        "C. He has quit recently",
        "D. He never smokes"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nina, who is vegan, prepared a tofu stir-fry for dinner.",
      "conflict_prompt": "Nina, who is vegan, grilled salmon for dinner.",
      "question": "What type of dinner did Nina prepare?",
      "options": [
        "A. Grilled salmon",
        "B. Beef stew",
        "C. Tofu stir-fry",
        "D. Shrimp pasta"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carlos, a marathon runner, completed the 26.2-mile race yesterday.",
      "conflict_prompt": "Carlos, a marathon runner, abandoned the 26.2-mile race after 2 miles because he hates running.",
      "question": "How far was the marathon that Carlos completed?",
      "options": [
        "A. 5 kilometers",
        "B. 13.1 miles",
        "C. 26.2 miles",
        "D. 100 meters"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hazel, who speaks fluent Spanish, conducted the presentation entirely in Spanish.",
      "conflict_prompt": "Hazel, who speaks fluent Spanish, gave the entire presentation in a language she doesn't know.",
      "question": "In which language did Hazel deliver the presentation?",
      "options": [
        "A. French",
        "B. Spanish",
        "C. German",
        "D. No presentation was given"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tom, a registered nurse, administered the vaccine to the patient.",
      "conflict_prompt": "Tom, a registered nurse, performed open-heart surgery on the patient.",
      "question": "What is Tom's professional role?",
      "options": [
        "A. Surgeon",
        "B. Registered nurse",
        "C. Pharmacist",
        "D. Radiologist"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olivia, who is legally blind, navigates the route with a white cane.",
      "conflict_prompt": "Olivia, who is legally blind, drove the car alone during the trip.",
      "question": "How does Olivia navigate while walking?",
      "options": [
        "A. Using a GPS app",
        "B. With a white cane",
        "C. Riding a bicycle",
        "D. She is sighted and guides others"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ben, a vegetarian chef, never uses meat in his restaurant's dishes.",
      "conflict_prompt": "Ben, a vegetarian chef, serves steaks as the restaurant's specialty.",
      "question": "What type of ingredients does Ben avoid using in his restaurant dishes?",
      "options": [
        "A. Dairy products",
        "B. Meat",
        "C. Gluten",
        "D. Seafood only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia, who is allergic to peanuts, always checks ingredient labels before eating.",
      "conflict_prompt": "Sofia, who is allergic to peanuts, eats peanut butter sandwiches every day without checking labels.",
      "question": "What allergy does Sofia have?",
      "options": [
        "A. Shellfish allergy",
        "B. Peanut allergy",
        "C. Lactose intolerance",
        "D. No allergies"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mayor Alvarez, a Democrat, announced new education funding at the press conference.",
      "conflict_prompt": "Mayor Alvarez, a Democrat, campaigned as a member of the Republican Party at the rally.",
      "question": "What political party is Mayor Alvarez affiliated with in the statement?",
      "options": [
        "A. Republican",
        "B. Independent",
        "C. Democrat",
        "D. Green Party"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Liam, who is a non-driver, travels only by public transit and walking.",
      "conflict_prompt": "Liam, who is a non-driver, commutes daily by driving his own car.",
      "question": "How does Liam typically travel?",
      "options": [
        "A. By driving himself",
        "B. By helicopter",
        "C. By public transit and walking",
        "D. Teleportation"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia, an only daughter, is the sole female child in her family.",
      "conflict_prompt": "Nadia, an only daughter, often plays with her two younger sisters.",
      "question": "What is Nadia's sibling situation?",
      "options": [
        "A. She has two younger sisters",
        "B. She is the only daughter",
        "C. She is one of three daughters",
        "D. She has one brother"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Professor Chen, a tenured faculty member, cannot be fired without cause.",
      "conflict_prompt": "Professor Chen, a tenured faculty member, was fired last week without cause.",
      "question": "What is Professor Chen's employment status?",
      "options": [
        "A. Tenured faculty member",
        "B. Adjunct instructor",
        "C. Visiting scholar",
        "D. Graduate assistant"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Aisha, who is a pacifist, opposes all forms of violence.",
      "conflict_prompt": "Aisha, who is a pacifist, volunteers as a military combat trainer.",
      "question": "Which viewpoint does Aisha hold regarding violence?",
      "options": [
        "A. She supports violence",
        "B. She is neutral about violence",
        "C. She opposes violence as a pacifist",
        "D. She participates in violent acts"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Derek, a certified lifeguard, supervised swimmers at the beach all afternoon.",
      "conflict_prompt": "Derek, a certified lifeguard, ignored safety protocols and left swimmers unsupervised.",
      "question": "What was Derek's role at the beach?",
      "options": [
        "A. Lifeguard",
        "B. Surf instructor",
        "C. Concession stand worker",
        "D. Beach vendor"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah, who is right-handed, always writes with her right hand in class.",
      "conflict_prompt": "Hannah, who is right-handed, insists on knitting only with her left hand because she cannot use the right.",
      "question": "Which hand does Hannah primarily use for writing?",
      "options": [
        "A. Left hand",
        "B. Right hand",
        "C. Both hands equally",
        "D. She does not write"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jordan, a vegan baker, uses no animal products in the pastries at his bakery.",
      "conflict_prompt": "Jordan, a vegan baker, sells croissants made with butter and eggs at his bakery.",
      "question": "What type of ingredients does Jordan avoid in his bakery?",
      "options": [
        "A. Artificial sweeteners",
        "B. Animal products",
        "C. Nuts only",
        "D. Whole grains"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Detective Ortiz, a homicide investigator, focused on the murder case yesterday.",
      "conflict_prompt": "Detective Ortiz, a homicide investigator, handled only traffic ticket disputes all week.",
      "question": "What type of cases does Detective Ortiz specialize in?",
      "options": [
        "A. Traffic tickets",
        "B. Homicide investigations",
        "C. Parking enforcement",
        "D. Civil litigation"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rosa, who is colorblind, cannot reliably distinguish red from green.",
      "conflict_prompt": "Rosa, who is colorblind, can always distinguish red and green with perfect accuracy.",
      "question": "What difficulty does Rosa have with colors?",
      "options": [
        "A. She cannot see colors at all",
        "B. She confuses red and green",
        "C. She sees ultra-violet light",
        "D. She perceives colors more vividly than others"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ethan, a vegetarian athlete, fueled his pre-race meal with oatmeal and fruit.",
      "conflict_prompt": "Ethan, a vegetarian athlete, ate bacon and eggs as his pre-race meal.",
      "question": "What did Ethan eat before the race?",
      "options": [
        "A. Bacon and eggs",
        "B. Steak and potatoes",
        "C. Oatmeal and fruit",
        "D. Fried fish"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, who is a certified scuba diver, explored coral reefs during the vacation.",
      "conflict_prompt": "Maya, who is a certified scuba diver, refused to enter the water and stayed on shore because she fears the ocean.",
      "question": "What activity did Maya participate in during her vacation?",
      "options": [
        "A. Mountain climbing",
        "B. Scuba diving to explore coral reefs",
        "C. Desert trekking",
        "D. City sightseeing only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Carlos, a vegetarian, packed a hummus and veggie wrap for the picnic.",
      "conflict_prompt": "Carlos, a vegetarian, brought hot dogs and pulled pork to the picnic.",
      "question": "What did Carlos pack for the picnic?",
      "options": [
        "A. Hot dogs and pulled pork",
        "B. Hummus and veggie wrap",
        "C. Grilled salmon",
        "D. Beef burgers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samantha, a licensed pilot, completed the cross-country flight last weekend.",
      "conflict_prompt": "Samantha, a licensed pilot, has never learned to fly and is afraid of planes.",
      "question": "What recent accomplishment is attributed to Samantha?",
      "options": [
        "A. Completed a cross-country flight",
        "B. Ran a marathon",
        "C. Published a novel",
        "D. Climbed Mount Everest"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Igor, who is diabetic, monitors his blood sugar levels before each meal.",
      "conflict_prompt": "Igor, who is diabetic, intentionally consumes large amounts of sugar daily without monitoring.",
      "question": "What routine does Igor follow because of his diabetes?",
      "options": [
        "A. He avoids monitoring blood sugar",
        "B. He monitors blood sugar before each meal",
        "C. He consumes extra sugar daily",
        "D. He takes only herbal supplements"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leah, a licensed electrician, rewired the kitchen outlet safely last month.",
      "conflict_prompt": "Leah, a licensed electrician, declared she has no knowledge of electrical systems and never touches wiring.",
      "question": "What kind of professional work does Leah do?",
      "options": [
        "A. Plumbing",
        "B. Electrician",
        "C. Carpentry",
        "D. Landscaping"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar, who is a vegetarian, brings plant-based lunches to work every day.",
      "conflict_prompt": "Omar, who is a vegetarian, brings meat-based barbecue for lunch every day.",
      "question": "What kind of lunches does Omar bring to work?",
      "options": [
        "A. Plant-based lunches",
        "B. Meat-based barbecue",
        "C. Seafood platters",
        "D. Fast food burgers"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Harper, who is legally married, listed her spouse as her emergency contact.",
      "conflict_prompt": "Harper, who is legally married, insists she is single and has no spouse.",
      "question": "What personal status did Harper indicate in the statement?",
      "options": [
        "A. Single",
        "B. In a domestic partnership",
        "C. Legally married",
        "D. Divorced"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felix, a fluent French speaker, translated the document from French to English.",
      "conflict_prompt": "Felix, a fluent French speaker, claimed he cannot understand basic French phrases.",
      "question": "What language can Felix speak fluently according to the statement?",
      "options": [
        "A. Spanish",
        "B. French",
        "C. Mandarin",
        "D. Russian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gina, who is allergic to shellfish, avoided the seafood section at the buffet.",
      "conflict_prompt": "Gina, who is allergic to shellfish, sampled several shrimp dishes without concern.",
      "question": "Which food does Gina avoid due to an allergy?",
      "options": [
        "A. Dairy",
        "B. Shellfish",
        "C. Tree nuts",
        "D. Gluten"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noah, a professional violinist, performed a solo at the concert hall.",
      "conflict_prompt": "Noah, a professional violinist, announced he has never played an instrument in public.",
      "question": "What is Noah's profession?",
      "options": [
        "A. Professional violinist",
        "B. Opera singer",
        "C. Sculptor",
        "D. Chef"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Adele, who is a strict teetotaler, has abstained from alcohol for years.",
      "conflict_prompt": "Adele, who is a strict teetotaler, hosts nightly wine-tasting parties.",
      "question": "What does Adele abstain from according to the statement?",
      "options": [
        "A. Smoking",
        "B. Alcohol",
        "C. Caffeine",
        "D. Sugar"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael, a high school principal, attended the student awards ceremony in his official capacity.",
      "conflict_prompt": "Rafael, a high school principal, stated he has never worked in education and is opposed to schools.",
      "question": "What role does Rafael hold at the high school?",
      "options": [
        "A. Cafeteria manager",
        "B. High school principal",
        "C. Student council president",
        "D. Custodian"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Yuki, a vegetarian, stocked her fridge with tofu and vegetables for the week.",
      "conflict_prompt": "Yuki, a vegetarian, stocked her fridge with slab bacon and sausages for the week.",
      "question": "What items did Yuki stock her fridge with?",
      "options": [
        "A. Tofu and vegetables",
        "B. Bacon and sausages",
        "C. Sushi and sashimi",
        "D. Whole roasted chicken"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Samira, who is a Sunni Muslim, attends the mosque every Friday for prayers.",
      "conflict_prompt": "Samira, who is a Sunni Muslim, publicly denounced Islam and stopped attending the mosque.",
      "question": "What religious practice does Samira follow as described?",
      "options": [
        "A. She attends synagogue",
        "B. She attends mosque prayers on Friday",
        "C. She practices Buddhism",
        "D. She does not follow any religion"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Victor, a certified diver, wears scuba gear during deep-sea explorations.",
      "conflict_prompt": "Victor, a certified diver, insists he never uses scuba gear and breathes normally underwater.",
      "question": "What equipment does Victor use for deep-sea exploration?",
      "options": [
        "A. Parachute",
        "B. Scuba gear",
        "C. Skis",
        "D. Telescope"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hannah, who is pregnant, scheduled regular prenatal checkups for her pregnancy.",
      "conflict_prompt": "Hannah, who is pregnant, denies that she is expecting and refuses prenatal care.",
      "question": "What medical appointments did Hannah schedule?",
      "options": [
        "A. Dental cleanings",
        "B. Prenatal checkups",
        "C. Vision tests",
        "D. Physical therapy sessions"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Daniel, a vegetarian, prepared a quinoa and roasted vegetable bowl for dinner.",
      "conflict_prompt": "Daniel, a vegetarian, made a beef stew for dinner.",
      "question": "What did Daniel prepare for dinner?",
      "options": [
        "A. Beef stew",
        "B. Quinoa and roasted vegetables",
        "C. Grilled shrimp skewers",
        "D. Pork chops"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priyanka, who is a licensed pharmacist, dispensed the correct medication to the patient.",
      "conflict_prompt": "Priyanka, who is a licensed pharmacist, admitted she has no pharmacy training and guesses dosages.",
      "question": "What is Priyanka's professional qualification?",
      "options": [
        "A. Licensed pharmacist",
        "B. Dental hygienist",
        "C. Physical therapist",
        "D. Veterinarian"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Owen, who is colorblind, asked his partner to choose the red tie because he can't tell reds apart.",
      "conflict_prompt": "Owen, who is colorblind, confidently selected the reddest tie without any difficulty.",
      "question": "Why did Owen ask his partner to choose the tie?",
      "options": [
        "A. He couldn't decide",
        "B. He is colorblind and can't distinguish red",
        "C. He was running late",
        "D. He disliked all the ties"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nora, a non-smoker, joined the workplace wellness program that supports smoke-free living.",
      "conflict_prompt": "Nora, a non-smoker, smokes several times a day and opposes smoke-free policies.",
      "question": "Which wellness behavior does Nora follow according to the statement?",
      "options": [
        "A. She smokes daily",
        "B. She is a non-smoker",
        "C. She uses vaping exclusively",
        "D. She chews tobacco"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priest Michael, a celibate clergy member, lives a life committed to celibacy.",
      "conflict_prompt": "Priest Michael, a celibate clergy member, announced his marriage and family life publicly.",
      "question": "What personal commitment does Priest Michael have?",
      "options": [
        "A. He is married with children",
        "B. He is celibate",
        "C. He practices polygamy",
        "D. He is divorced"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Elliot, a dedicated vegan, avoids all dairy and animal-derived ingredients in his cooking.",
      "conflict_prompt": "Elliot, a dedicated vegan, uses butter and milk in all his recipes.",
      "question": "Which ingredients does Elliot avoid in his cooking?",
      "options": [
        "A. Nuts and seeds",
        "B. Animal-derived ingredients like dairy",
        "C. Spices",
        "D. Vegetables"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priya, who is an only child, has no brothers or sisters.",
      "conflict_prompt": "Priya, who is an only child, describes weekend trips with her three siblings.",
      "question": "How many siblings does Priya have?",
      "options": [
        "A. Three siblings",
        "B. Two siblings",
        "C. One sibling",
        "D. None"
      ],
      "correct_answer_for_clean_statement": "D",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Detective Ruiz, a cold-case specialist, reopened the decades-old disappearance file.",
      "conflict_prompt": "Detective Ruiz, a cold-case specialist, insists cold cases are never worth investigating and destroyed the file.",
      "question": "What type of cases does Detective Ruiz specialize in?",
      "options": [
        "A. Traffic accidents",
        "B. Cold-case investigations",
        "C. Financial audits",
        "D. Birth registrations"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marta, who is a devout Buddhist, observes daily meditation practices at the temple.",
      "conflict_prompt": "Marta, who is a devout Buddhist, openly mocks meditation and refuses to attend the temple.",
      "question": "Which practice does Marta observe daily?",
      "options": [
        "A. Daily meditation at the temple",
        "B. Daily fasting",
        "C. Daily weightlifting",
        "D. Daily political rallies"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ian, who is legally blind, uses assistive technology to read digital text.",
      "conflict_prompt": "Ian, who is legally blind, insists he reads small print unaided without issues.",
      "question": "How does Ian read digital text according to the statement?",
      "options": [
        "A. With assistive technology",
        "B. By magnifying glasses only",
        "C. He cannot read digital text",
        "D. By memorizing everything"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sasha, an identically trained twin, is a licensed architect who designed the new community center.",
      "conflict_prompt": "Sasha, an identically trained twin, claims she is not licensed and had nothing to do with the building design.",
      "question": "What was Sasha's role in the new community center project?",
      "options": [
        "A. She designed the center as a licensed architect",
        "B. She opposed the construction",
        "C. She provided catering services",
        "D. She protested outside the site"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mikhail, a teetotaler, has avoided alcohol since his early twenties.",
      "conflict_prompt": "Mikhail, a teetotaler, hosts weekly cocktail parties at his home.",
      "question": "What describes Mikhail's relationship with alcohol?",
      "options": [
        "A. He drinks daily",
        "B. He avoids alcohol entirely",
        "C. He drinks only on holidays",
        "D. He brews his own beer"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tara, who is allergic to cats, asked guests not to bring cats to her apartment.",
      "conflict_prompt": "Tara, who is allergic to cats, adopted three kittens and keeps them in her bedroom.",
      "question": "What allergy does Tara have?",
      "options": [
        "A. Dog allergy",
        "B. Cat allergy",
        "C. Pollen allergy",
        "D. No allergies"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Victor, a vegetarian, stocked his refrigerator with almond milk and plant-based cheese.",
      "conflict_prompt": "Victor, a vegetarian, filled his refrigerator with raw steaks and pork chops.",
      "question": "Which items did Victor stock his refrigerator with?",
      "options": [
        "A. Raw steaks and pork chops",
        "B. Almond milk and plant-based cheese",
        "C. Live lobsters",
        "D. Canned sardines"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Olga, a wheelchair user, uses accessible ramps to enter buildings.",
      "conflict_prompt": "Olga, a wheelchair user, prefers stairs and always avoids ramps despite mobility needs.",
      "question": "How does Olga access buildings according to the statement?",
      "options": [
        "A. By using stairs only",
        "B. By using accessible ramps",
        "C. By climbing windows",
        "D. By helicopter"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Henry, who is a vegetarian, volunteers at a community garden to grow vegetables.",
      "conflict_prompt": "Henry, who is a vegetarian, operates a cattle farm that sells beef commercially.",
      "question": "What volunteer activity does Henry participate in?",
      "options": [
        "A. Volunteering at an animal slaughterhouse",
        "B. Volunteering at a community garden",
        "C. Volunteering at a car wash",
        "D. Volunteering at a bank"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jasmine, a certified yoga instructor, taught a restorative yoga class at the studio.",
      "conflict_prompt": "Jasmine, a certified yoga instructor, claims she has never practiced yoga and dislikes all sessions.",
      "question": "What class did Jasmine teach at the studio?",
      "options": [
        "A. Spin class",
        "B. Restorative yoga",
        "C. Boxing",
        "D. Pottery"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Felipe, a vegetarian, grows herbs and vegetables on his apartment balcony.",
      "conflict_prompt": "Felipe, a vegetarian, keeps a smokehouse full of cured meats on his balcony.",
      "question": "What does Felipe grow on his balcony?",
      "options": [
        "A. Herbs and vegetables",
        "B. Cured meats",
        "C. Exotic animals",
        "D. Solar panels"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zara, who observes kosher dietary laws, only eats meals prepared according to kosher rules.",
      "conflict_prompt": "Zara, who observes kosher dietary laws, frequently eats pork and shellfish without restrictions.",
      "question": "What dietary practice does Zara follow?",
      "options": [
        "A. She follows kosher dietary laws",
        "B. She follows halal only",
        "C. She is vegan",
        "D. She eats anything with no restrictions"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marcus, who is a non-driver, takes a bus to work every day and does not own a car.",
      "conflict_prompt": "Marcus, who is a non-driver, owns a sports car and drives to work daily.",
      "question": "How does Marcus commute to work?",
      "options": [
        "A. Drives a sports car",
        "B. Takes a bus and does not own a car",
        "C. Rides a motorcycle",
        "D. Commutes by ferry"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Eve, a science teacher, instructed her students on the basics of cellular biology.",
      "conflict_prompt": "Eve, a science teacher, claims she cannot tell a cell from a rock and teaches only history.",
      "question": "What subject does Eve teach according to the statement?",
      "options": [
        "A. Mathematics",
        "B. Science",
        "C. Physical education",
        "D. Music"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rumi, who keeps kosher, avoids mixing dairy and meat in meals.",
      "conflict_prompt": "Rumi, who keeps kosher, regularly serves cheeseburgers and milk-based gravies over beef dishes.",
      "question": "Which food practice does Rumi avoid because of keeping kosher?",
      "options": [
        "A. Eating only vegetarian meals",
        "B. Mixing dairy and meat",
        "C. Avoiding sugar",
        "D. Fasting every day"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ola, an organ donor, signed up to donate her organs after death.",
      "conflict_prompt": "Ola, an organ donor, publicly said she refuses to donate any organs under any circumstances.",
      "question": "What did Ola commit to regarding organ donation?",
      "options": [
        "A. She refused to donate",
        "B. She signed up to donate her organs after death",
        "C. She only donates blood",
        "D. She is undecided"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dmitri, a vegetarian, brings lentil soups and veggie stews to potluck dinners.",
      "conflict_prompt": "Dmitri, a vegetarian, brings barbecued ribs and brisket to potlucks.",
      "question": "What type of dishes does Dmitri bring to potlucks?",
      "options": [
        "A. Barbecued ribs",
        "B. Lentil soups and veggie stews",
        "C. Grilled lobster",
        "D. Raw fish platters"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Nadia, a pacifist, organizes nonviolent protests and peace rallies.",
      "conflict_prompt": "Nadia, a pacifist, runs a paramilitary training camp for violent tactics.",
      "question": "What kind of demonstrations does Nadia organize?",
      "options": [
        "A. Violent uprisings",
        "B. Nonviolent protests and peace rallies",
        "C. Armed invasions",
        "D. Underground sabotage"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Gareth, who is right-handed, plays tennis using his right hand for volleys and serves.",
      "conflict_prompt": "Gareth, who is right-handed, insists he has always played tennis left-handed and cannot use his right.",
      "question": "Which hand does Gareth use to play tennis according to the statement?",
      "options": [
        "A. Left hand",
        "B. Both hands interchangeably",
        "C. Right hand",
        "D. He does not play tennis"
      ],
      "correct_answer_for_clean_statement": "C",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amina, a practicing attorney, represented the client at the civil hearing.",
      "conflict_prompt": "Amina, a practicing attorney, claims she is not licensed to practice law and never represents clients.",
      "question": "What profession does Amina practice according to the statement?",
      "options": [
        "A. Attorney",
        "B. Architect",
        "C. Nurse",
        "D. Chef"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sergio, a vegetarian, ordered a veggie burger with no cheese at the diner.",
      "conflict_prompt": "Sergio, a vegetarian, ordered a triple-cheese bacon burger with extra bacon at the diner.",
      "question": "What did Sergio order at the diner?",
      "options": [
        "A. Triple-cheese bacon burger",
        "B. Veggie burger with no cheese",
        "C. Grilled salmon",
        "D. Steak fajitas"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lena, a non-smoker, requested a smoke-free room when checking into the hotel.",
      "conflict_prompt": "Lena, a non-smoker, asked for a smoking room with an ashtray and cigarette lighter included.",
      "question": "What type of hotel room did Lena request?",
      "options": [
        "A. Smoking room",
        "B. Non-smoking room",
        "C. Suite with balcony",
        "D. No room; she camped outside"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Priest Daniel, who is celibate, took a vow of celibacy as part of his religious order.",
      "conflict_prompt": "Priest Daniel, who is celibate, announced his engagement and upcoming wedding.",
      "question": "What vow did Priest Daniel take according to the statement?",
      "options": [
        "A. Vow of poverty only",
        "B. Vow of celibacy",
        "C. Vow of silence",
        "D. No vows"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rina, who is a vegan baker, uses aquafaba as an egg substitute in her macarons.",
      "conflict_prompt": "Rina, who is a vegan baker, uses whole eggs and milk in all her macarons.",
      "question": "What egg substitute does Rina use in her vegan macarons?",
      "options": [
        "A. Aquafaba",
        "B. Whole eggs",
        "C. Cow's milk",
        "D. Gelatin"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Mateo, a certified lifeguard, rescued a struggling swimmer at the lake last Sunday.",
      "conflict_prompt": "Mateo, a certified lifeguard, ignored a drowning swimmer and walked away.",
      "question": "What action did Mateo take at the lake?",
      "options": [
        "A. Ignored a swimmer",
        "B. Rescued a struggling swimmer",
        "C. Operated the concession stand",
        "D. Fished from the pier"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Isabel, who is a non-drinker, ordered a sparkling water at the company dinner.",
      "conflict_prompt": "Isabel, who is a non-drinker, ordered shots of hard liquor with every course at the company dinner.",
      "question": "What beverage did Isabel choose at the company dinner?",
      "options": [
        "A. Sparkling water",
        "B. Shots of hard liquor",
        "C. A pint of beer",
        "D. A milkshake"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafael, a vegetarian, typically seasons dishes with olive oil and herbs instead of animal fats.",
      "conflict_prompt": "Rafael, a vegetarian, uses lard and duck fat as the primary cooking fats in his kitchen.",
      "question": "What type of fats does Rafael prefer to use in his vegetarian cooking?",
      "options": [
        "A. Lard and duck fat",
        "B. Olive oil and herbs",
        "C. Tallow",
        "D. Fish oil"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Min, who is colorblind, asked a friend to pick the blue curtains because she can't differentiate blues from purples.",
      "conflict_prompt": "Min, who is colorblind, insisted she can always tell blues and purples apart perfectly and picked her own curtains.",
      "question": "Why did Min ask a friend to choose the curtains?",
      "options": [
        "A. She wanted curtains matched to her wallpaper",
        "B. She can't differentiate blues from purples due to colorblindness",
        "C. She had no opinion",
        "D. She prefers the friend's taste"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hiro, a non-smoker, installs ashtray-free signs in his restaurant and enforces a smoke-free policy.",
      "conflict_prompt": "Hiro, a non-smoker, allows customers to smoke at every table and provides ashtrays to all.",
      "question": "What smoking policy does Hiro enforce at his restaurant?",
      "options": [
        "A. Smoking allowed at all tables",
        "B. Smoke-free policy with no ashtrays",
        "C. Smoking only in designated indoor rooms",
        "D. He sells cigarettes but doesn't allow smoking"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Luca, who is left-handed, paints landscapes using his left hand for brushwork.",
      "conflict_prompt": "Luca, who is left-handed, insists he paints exclusively with his right hand and cannot use his left.",
      "question": "Which hand does Luca use primarily for painting?",
      "options": [
        "A. Right hand",
        "B. Left hand",
        "C. Both hands alternately",
        "D. He does not paint"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Sofia, a vegetarian, runs a plant-based cooking class every weekend.",
      "conflict_prompt": "Sofia, a vegetarian, runs a butchery and barbecue class teaching meat preparation every weekend.",
      "question": "What type of class does Sofia run on weekends?",
      "options": [
        "A. Butchery and barbecue class",
        "B. Plant-based cooking class",
        "C. Auto repair class",
        "D. Woodworking class"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Anton, who is a diabetic, always carries glucose tablets for low blood sugar episodes.",
      "conflict_prompt": "Anton, who is a diabetic, never carries any medication and refuses to treat low blood sugar.",
      "question": "What precaution does Anton take for his diabetes?",
      "options": [
        "A. He carries glucose tablets",
        "B. He avoids medication entirely",
        "C. He uses only herbal remedies",
        "D. He relies on others always"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Renee, a licensed therapist, held confidential counseling sessions at her clinic.",
      "conflict_prompt": "Renee, a licensed therapist, revealed all client details publicly and does not offer therapy.",
      "question": "What service does Renee provide at her clinic?",
      "options": [
        "A. Confidential counseling sessions",
        "B. Public gossip column",
        "C. Carpentry lessons",
        "D. Pet grooming"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Pablo, a vegetarian, refused the steak entrée offered at the formal dinner.",
      "conflict_prompt": "Pablo, a vegetarian, ordered and ate the steak entrée at the formal dinner.",
      "question": "How did Pablo respond to the steak entrée at the dinner?",
      "options": [
        "A. He ate the steak",
        "B. He refused the steak",
        "C. He was served fish instead",
        "D. He left before the meal"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, a licensed scuba instructor, certified three students in open-water diving this month.",
      "conflict_prompt": "Maya, a licensed scuba instructor, says she has never been underwater and refuses to certify students.",
      "question": "What recent accomplishment did Maya achieve as an instructor?",
      "options": [
        "A. She certified three students in open-water diving",
        "B. She opened a bakery",
        "C. She completed a PhD",
        "D. She trained guide dogs"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Leo, a pacifist activist, organized a sit-in protest advocating nonviolence.",
      "conflict_prompt": "Leo, a pacifist activist, led an armed attack during the protest demanding violent action.",
      "question": "What kind of protest did Leo organize?",
      "options": [
        "A. An armed attack",
        "B. A sit-in advocating nonviolence",
        "C. A bank robbery",
        "D. A food festival"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Bianca, who keeps kosher, purchased only kosher-certified meat for the holidays.",
      "conflict_prompt": "Bianca, who keeps kosher, served non-kosher pork roast at the holiday dinner.",
      "question": "What type of meat did Bianca buy for the holidays?",
      "options": [
        "A. Non-kosher pork",
        "B. Kosher-certified meat",
        "C. Wild game without certification",
        "D. No meat at all"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noel, a vegetarian, packed a lunch of chickpea salad and fruit for his hiking trip.",
      "conflict_prompt": "Noel, a vegetarian, packed cured salami and pepperoni sandwiches for his hiking trip.",
      "question": "What did Noel pack for lunch on his hike?",
      "options": [
        "A. Cured salami and pepperoni sandwiches",
        "B. Chickpea salad and fruit",
        "C. Grilled fish tacos",
        "D. Barbecue ribs"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lina, a non-smoker, requested that her spouse quit smoking inside their home.",
      "conflict_prompt": "Lina, a non-smoker, encourages her spouse to smoke indoors and lights cigarettes for him.",
      "question": "What did Lina ask her spouse to do regarding smoking indoors?",
      "options": [
        "A. Continue smoking inside",
        "B. Quit smoking inside their home",
        "C. Smoke only in the basement",
        "D. Smoke outdoors only on weekends"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hector, a vegetarian chef, created a menu featuring only plant-based entrees for the restaurant revamp.",
      "conflict_prompt": "Hector, a vegetarian chef, revamped the menu to focus on prime rib and pork chops exclusively.",
      "question": "What type of entrees did Hector feature on the new menu?",
      "options": [
        "A. Prime rib and pork chops",
        "B. Plant-based entrees only",
        "C. Seafood specialties",
        "D. Gluten-free desserts"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Amira, a devout Muslim, observed daily prayers and fasted during Ramadan.",
      "conflict_prompt": "Amira, a devout Muslim, openly mocks prayer and refuses to fast during Ramadan.",
      "question": "Which religious observances does Amira practice?",
      "options": [
        "A. Observes daily prayers and fasts during Ramadan",
        "B. Practices only on holidays rarely",
        "C. Does not practice Islam",
        "D. Observes Lent instead"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Dylan, a vegetarian, grows his own organic vegetables and never cooks meat.",
      "conflict_prompt": "Dylan, a vegetarian, raises pigs and cooks them regularly for family meals.",
      "question": "What does Dylan do regarding food and cooking according to the statement?",
      "options": [
        "A. Raises pigs and cooks them",
        "B. Grows organic vegetables and does not cook meat",
        "C. Only eats imported delicacies",
        "D. Runs a butcher shop"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Niko, who is colorblind, asked for help choosing paint because he can't differentiate some hues.",
      "conflict_prompt": "Niko, who is colorblind, insists he can always differentiate all hues without help.",
      "question": "Why did Niko ask for help choosing paint?",
      "options": [
        "A. He can't differentiate some hues due to colorblindness",
        "B. He had no time",
        "C. He dislikes all colors",
        "D. He wanted a surprise"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Paula, a vegetarian, runs the community soup kitchen and serves only vegetable-based soups.",
      "conflict_prompt": "Paula, a vegetarian, runs the community soup kitchen and serves only meat-based broths and stews.",
      "question": "What type of soups does Paula serve at the community kitchen?",
      "options": [
        "A. Meat-based broths",
        "B. Vegetable-based soups",
        "C. Seafood chowders",
        "D. No soups; only sandwiches"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Arjun, who is diabetic, scheduled quarterly appointments with his endocrinologist for glucose management.",
      "conflict_prompt": "Arjun, who is diabetic, refuses all medical appointments and never monitors his glucose.",
      "question": "What type of medical appointments does Arjun schedule?",
      "options": [
        "A. Quarterly endocrinologist appointments",
        "B. Cosmetic surgery consultations",
        "C. Dental cleanings only",
        "D. No medical appointments at all"
      ],
      "correct_answer_for_clean_statement": "A",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Beatrice, a vegetarian, orders only plant-based dishes when dining out with friends.",
      "conflict_prompt": "Beatrice, a vegetarian, orders seafood platters and steaks whenever she dines out.",
      "question": "What does Beatrice typically order when dining out?",
      "options": [
        "A. Seafood platters",
        "B. Plant-based dishes",
        "C. Steaks",
        "D. Raw meat"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Igor, who is legally blind, learned to use a screen reader to access his emails.",
      "conflict_prompt": "Igor, who is legally blind, insists he does not use any assistive technology and reads screens unaided.",
      "question": "How does Igor access his emails according to the statement?",
      "options": [
        "A. He reads screens unaided",
        "B. He uses a screen reader",
        "C. He has no email",
        "D. He asks someone else to read them"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tanya, who is a vegetarian, volunteered to cook vegetarian meals for the shelter's holiday dinner.",
      "conflict_prompt": "Tanya, who is a vegetarian, volunteered to prepare roast beef and pork loins for the shelter's holiday dinner.",
      "question": "What type of meals did Tanya volunteer to cook for the shelter?",
      "options": [
        "A. Roast beef and pork loins",
        "B. Vegetarian meals",
        "C. Seafood specialities",
        "D. Candy and desserts only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Omar, a non-drinker, picked a non-alcoholic beverage from the bar menu at the gala.",
      "conflict_prompt": "Omar, a non-drinker, ordered several rounds of whiskey shots at the gala.",
      "question": "What beverage did Omar choose at the gala?",
      "options": [
        "A. Whiskey shots",
        "B. Non-alcoholic beverage",
        "C. Craft beer",
        "D. Wine"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Laila, a vegetarian, avoids wearing leather and buys cruelty-free shoes.",
      "conflict_prompt": "Laila, a vegetarian, insists on wearing leather boots every day and supports fur trade.",
      "question": "What type of footwear does Laila choose according to the statement?",
      "options": [
        "A. Leather boots daily",
        "B. Cruelty-free shoes (no leather)",
        "C. Steel-toe boots only",
        "D. Barefoot always"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Connor, a vegetarian, prepared a mushroom risotto for the company potluck.",
      "conflict_prompt": "Connor, a vegetarian, prepared a bacon-wrapped meatloaf for the company potluck.",
      "question": "What dish did Connor bring to the potluck?",
      "options": [
        "A. Bacon-wrapped meatloaf",
        "B. Mushroom risotto",
        "C. Grilled shrimp skewers",
        "D. Fried chicken"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Zoe, who is a vegan athlete, uses plant-based protein shakes for recovery after workouts.",
      "conflict_prompt": "Zoe, who is a vegan athlete, drinks dairy-heavy protein shakes and uses whey protein regularly.",
      "question": "What type of protein shakes does Zoe use after workouts?",
      "options": [
        "A. Dairy-heavy whey protein",
        "B. Plant-based protein shakes",
        "C. No protein shakes at all",
        "D. Raw egg smoothies"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hassan, a vegetarian, arranged a menu of grilled vegetables and grain salads for his wedding reception.",
      "conflict_prompt": "Hassan, a vegetarian, served a feast of roasted lamb and beef brisket at his wedding reception.",
      "question": "What type of menu did Hassan arrange for his wedding reception?",
      "options": [
        "A. Feast of roasted lamb and beef",
        "B. Grilled vegetables and grain salads",
        "C. Seafood buffet",
        "D. Fast-food style burgers"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Lina, who is allergic to latex, requested non-latex gloves at her dental appointment.",
      "conflict_prompt": "Lina, who is allergic to latex, asked the dentist to use latex gloves during her procedure.",
      "question": "What type of gloves did Lina request for her dental appointment?",
      "options": [
        "A. Latex gloves",
        "B. Non-latex gloves",
        "C. No gloves at all",
        "D. Oven mitts"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Marco, a vegetarian waiter, recommends the grilled portobello as the house favorite.",
      "conflict_prompt": "Marco, a vegetarian waiter, pushes the bacon-wrapped filet mignon as the house favorite to every customer.",
      "question": "Which dish does Marco recommend as the house favorite?",
      "options": [
        "A. Bacon-wrapped filet mignon",
        "B. Grilled portobello",
        "C. Lobster thermidor",
        "D. Chicken parmesan"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Asha, a non-smoker, enrolls in fitness classes to promote a smoke-free lifestyle.",
      "conflict_prompt": "Asha, a non-smoker, chainsmokes during her fitness classes and encourages others to smoke.",
      "question": "What lifestyle choice does Asha promote?",
      "options": [
        "A. Smoking during fitness",
        "B. A smoke-free lifestyle",
        "C. Heavy drinking",
        "D. Sedentary habits"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Noor, a vegetarian, studied culinary arts focused on plant-based cuisine at culinary school.",
      "conflict_prompt": "Noor, a vegetarian, studied exclusively in a meat butchery program and became a butcher.",
      "question": "What focus did Noor have during culinary school?",
      "options": [
        "A. Meat butchery",
        "B. Plant-based cuisine",
        "C. Sushi only",
        "D. Pastry arts exclusively"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Tobias, who is left-handed, writes all his notes and letters using his left hand.",
      "conflict_prompt": "Tobias, who is left-handed, insists he always writes with his right hand and cannot use his left.",
      "question": "How does Tobias write his notes according to the statement?",
      "options": [
        "A. With his right hand",
        "B. With his left hand",
        "C. Using voice dictation only",
        "D. He does not write"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Ari, a vegan, brought almond-based ice cream to the neighborhood block party.",
      "conflict_prompt": "Ari, a vegan, served traditional dairy ice cream and custards to guests at the block party.",
      "question": "What dessert did Ari bring to the block party?",
      "options": [
        "A. Traditional dairy ice cream",
        "B. Almond-based vegan ice cream",
        "C. Whole milk custard",
        "D. Meat-based pudding"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Selma, a vegetarian, fills her grocery cart with legumes, grains, and vegetables each week.",
      "conflict_prompt": "Selma, a vegetarian, fills her grocery cart with raw steaks and cured meats each week.",
      "question": "What items does Selma typically buy at the grocery store?",
      "options": [
        "A. Raw steaks and cured meats",
        "B. Legumes, grains, and vegetables",
        "C. Only candy and snacks",
        "D. Pet supplies only"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Jamal, a pacifist, led a candlelight vigil advocating for nonviolent conflict resolution.",
      "conflict_prompt": "Jamal, a pacifist, organized an armed protest that resulted in violent clashes.",
      "question": "What event did Jamal lead?",
      "options": [
        "A. Armed protest with violent clashes",
        "B. Candlelight vigil promoting nonviolence",
        "C. Sports tournament",
        "D. Corporate fundraiser"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rita, a vegetarian, teaches children's nutrition classes emphasizing plant-based options.",
      "conflict_prompt": "Rita, a vegetarian, teaches children how to butcher and cook large cuts of meat.",
      "question": "What topic does Rita emphasize in her children's nutrition classes?",
      "options": [
        "A. Butchering meat",
        "B. Plant-based nutrition",
        "C. Fast food meal planning",
        "D. Automotive repair"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Hassan, a non-smoker, banned smoking from his family's household and outdoor patio.",
      "conflict_prompt": "Hassan, a non-smoker, lights cigarettes for guests and encourages smoking on his patio.",
      "question": "What is Hassan's household smoking policy according to the statement?",
      "options": [
        "A. Smoking encouraged on the patio",
        "B. Smoking banned from the household and patio",
        "C. Smoking allowed indoors only",
        "D. No policy mentioned"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Maya, a vegetarian, made a lentil shepherd's pie as a comforting dinner option.",
      "conflict_prompt": "Maya, a vegetarian, prepared a traditional lamb shepherd's pie for dinner.",
      "question": "What type of shepherd's pie did Maya prepare?",
      "options": [
        "A. Lamb shepherd's pie",
        "B. Lentil shepherd's pie",
        "C. Chicken pot pie",
        "D. Beef lasagna"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    },
    {
      "clean_prompt": "Rafa, who is colorblind, asked his sister to pick matching socks because he can't match colors reliably.",
      "conflict_prompt": "Rafa, who is colorblind, claims he can always match colors perfectly and never needs help picking socks.",
      "question": "Why did Rafa ask his sister to choose his socks?",
      "options": [
        "A. He couldn't find the socks",
        "B. He can't match colors reliably due to colorblindness",
        "C. He wanted a surprise",
        "D. He was in a hurry"
      ],
      "correct_answer_for_clean_statement": "B",
      "category": "role_attribute_contradictions",
      "conflict_type": "attribute_assignment_conflict"
    }
  ]
}